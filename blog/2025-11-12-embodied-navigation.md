---
slug: embodied-navigation
title: 重新思考三维空间感知与具身导航决策在毕设中的研究点
authors: bubblevan
tags: []
---
首先，我们的Baseline —— LOVON (Legged Open-Vocabulary Object Navigator, 2025) 是一个在 Gym-Unreal（即 Gym-UnrealCV 风格的仿真 benchmark）上做了大规模仿真实验来验证其开阔词表目标搜索与导航能力；文中强调用虚幻环境来做长航时、动态目标搜索的系统验证（包括视觉抖动、目标短暂消失等问题）并在仿真里验证 Laplacian Variance Filtering、语言→运动模型等模块。也进行了真实腿式机器人（Unitree 系列）上的跨域验证以检验 sim→real。

<!-- truncate -->

## 思考
我们第一步要做的就是 Define Problem。若能有清晰的问题定位 + 合理指标 +实证结果 +对比分析，就有很大机会产出成果。一个很好的方法就是自问自答：

### 一、现状定位：用 LOVON 的方法在真机上效果很差──最关键的失败点是什么？“机器狗撞门框”？那是什么原因？
- 是因为纯2D检测＋动作映射，缺乏深度／3D理解？
- 是因为没有障碍物避障规划？
- 是因为导航规划缺失，仅“向目标走”而不考虑路径？
- 还是别的问题（如机器狗控制延迟、检测误差大、目标消失后无追踪策略）？


LOVON的原理，也就是视觉追踪的原理在于：

1. 目标提取与筛选（`_yolo_image_post_process` 方法）

   先通过 `object_extractor` 从任务指令（默认任务是 run to the person at speed of 0.36 m/s，提取目标为 “person”）中提取目标类别。YOLO 模型输出所有检测框后，只保留类别与提取目标一致的框，过滤无关目标。

2. 滑动窗口历史管理

   初始化 5 个历史缓存列表，分别存储目标类别、置信度、归一化坐标（xyn）、归一化宽高（whn）、像素坐标（xyxy）。每帧仅保留置信度最高的检测框，加入缓存列表；当列表长度超过 `lengthen_filter` 时，删除最早的帧，维持窗口大小。

3. 追踪结果计算

   对缓存列表中的数据取平均值，得到平滑后的置信度、坐标和宽高。

   - `object_xyn[0]` 是目标中心的水平归一化坐标（0~1，0 为左边界、0.5 为图像中心、1 为右边界）。
   - 若目标在图像中心（`xyn[0] ≈ 0.5`）：机器狗沿前后方向运动（`v_x` 按任务指令速度，如 0.36m/s，`v_y = 0`，`w_z = 0`），即 “往前走”。
   - 若目标偏左（`xyn[0] < 0.5`）：`w_z` 为正（顺时针旋转），同时 `v_x` 降低，直到目标回到中心；偏右则相反。
   - 任务指令中的 “speed” 仅限制 `v_x` 的最大值，而非强制固定 `v_x`。

   统计列表中出现次数最多的目标类别，作为当前追踪目标（避免单帧误检影响）。若目标类别为 “NULL”（无有效检测），则重置追踪结果为默认值。

4. 跟丢的判定标准

   - 单帧无检测：YOLO 未检测到与 `extracted_object` 匹配的框 → 往历史缓存中添加 “NULL” 和 0 置信度。
   - 连续跟丢：当历史缓存（长度由 `lengthen_filter` 控制）中 “NULL” 出现次数最多 → `most_common_object` 变为 “NULL”，`avg_confidence` 设为 0 → 判定为 “跟丢”。

5. `motion_predictor` 接收 “跟丢状态” 后，生成搜索型 `motion_vector`：

   通常是「旋转搜索」：`v_x = 0`（不前后动）、`v_y = 0`（不左右动）、`w_z ≠ 0`（缓慢旋转，扫描周围环境）。

机器狗撞门框的原因在于，这里现实环境的部署代码通过 YOLO 只识别到了目标但是没有理解环境与障碍物，而当人消失在门后时，最后一帧这个目标是在画面中心的，因此机器狗会往前走直到撞到门框，又或者笨笨的在门框那个位置旋转搜索。因为没有开源其仿真智能体的代码所以不知道模拟环境是怎么规避这个问题的

### 二、Gap 与定位：基于你上面的回答，问题在哪儿？用一句话描述这里的 gap（研究空白）
比如：“在足式机器人真实场景下，当前Open-vocab检测＋简单动作生成不能有效处理目标暂时丢失和复杂障碍物，导致跟踪／导航失败”。还是要聚焦 “障碍物避障” 或 “三维深度理解”？

在足式机器人开放世界目标追踪任务中，现有基于纯 2D 视觉目标检测的追踪 - 运动映射方案，因缺乏环境障碍物感知与三维空间理解，且目标暂时丢失后仅采用无环境适配的旋转搜索策略，导致无法应对 “目标被遮挡 / 消失后因路径误判碰撞障碍物” 等真实场景挑战，难以实现稳健的长时追踪与运动控制（具体有没有3D视觉目标检测的论文工作，现在还没有做过调研）

### 三、重要性在哪里？
- 对学术来说：为什么“足式机器人 + open-vocab目标导航/跟踪”值得研究？是否当前工作少？
- 对应用来说：在真实环境（室内／复杂家具／光照变化）中，解决这个问题会带来什么改进？

对于我来说，我还只是一个入门新手，打算通过本科毕设的机会，从3D世界理解和具身导航决策这个小角度切入来入门具身领域，所以我也说不清楚学术和应用上的重要性，只求发ccfb以上的paper证明自己

### 四、创新点初步想法？
通过和导师学长们讨论列出了很多可能的优化方向（环境理解增强、分层决策升级），从这些中最可能做出论文中**可量化贡献**的一个或两个是什么？
比如：“用 BEV 俯视地图 +轨迹预测 来增强 open-vocab 目标导航”；或者：“在足式机器人上验证视觉+深度融合检测在目标丢失场景下的跟踪稳定性提升”。哪一个更倾向？为什么？

我不知道量化贡献的指标可以在哪里进一步优化啊，原因也可能在于我读的文献太少了，LOVON在仿真里所使用的指标为衡量 100 次实验中完成任务的平均步数、衡量 100 次实验中成功完成任务的比例两个，而如何去量化现实任务的指标与sim2real的优化，因为文献读的不多所以暂时我还不能回答这个问题

### 五、可量化指标与对比：要发论文，必须有可测量的结果，可以测量哪些指标？例如：
- 目标被丢失的次数／恢复次数
- 障碍物碰撞次数
- 成功到达目标的比例
- 路径长度／时间／效率
- 跟踪保持时间／跟丢时间
− 真机 vs 仿真的差距（sim2real gap）
能够在实机上测这些指标吗？哪些可能无法测？

### 六、实验平台／可行性：
- 已有的硬件是 Unitree Go2 足式机器人，这很好。你能控制机器人做什么动作（向前、转、停止、避障）？你能获取哪些传感器数据（RGB、深度、IMU、里程计）？
- 是否有仿真实验环境（如 Gym-UnrealCV 场景）可以先做仿真再到实机？仿真与实机之间能记录相同指标吗？
- 时间上本科毕设资源有限，预计能做多少场景／多少实验次数？这个对决定指标和可行性很重要。

- 对于硬件设备，要关注[官方SDK文档](https://support.unitree.com/home/en/developer）：

**一、动作控制能力**
- 基础运动控制
    - 前进 / 后退 / 转向：通过Move(vx, vy, vyaw)函数直接设置线速度（vx/vy）和角速度（vyaw），支持相对于世界坐标系的运动控制。例如，Move(0.5, 0, 0)使机器人以 0.5m/s 速度向前移动。
    - 停止：调用StopMove()立即终止所有运动，进入静止状态。
    - 步态切换：通过SwitchGait(int d)选择不同步态（如小跑、踱步），或使用ContinuousGait(bool flag)启用连续步态模式。
- 高级动作与姿态调整
    - 站立 / 坐下：StandUp()和Sit()实现起立和坐下动作，RecoveryStand()用于从侧翻状态恢复。
    - 身体姿态控制：Euler(roll, pitch, yaw)可调整机身倾角，BodyHeight(float height)动态改变离地高度。
    - 特技动作：支持FrontFlip()前空翻、FrontJump()跳跃等复杂动作（需硬件支持）。
- 避障功能
    - 自主避障：通过ObstacleAvoidClient类启用避障模块，机器人可实时检测障碍物并调整路径。需调用EnableObstacleAvoidance()激活，并在移动时保持避障服务运行。
    - 传感器融合：避障依赖激光雷达（PRO/EDU 版）或深度相机（AIR 版）与 IMU 数据融合，实现动态环境下的安全导航。

**二、传感器数据获取**
- 视觉传感器
    - RGB 图像：通过 ROS2 话题/camera/image_raw获取 720P/1080P 实时视频流，支持 WebRTC 低延迟传输。
    - 深度数据：PRO/EDU 版搭载 4D 激光雷达（L1），可输出 360°×90° 点云数据（/go2/camera/depth）；AIR 版通过 Intel RealSense D435i 深度相机提供毫米级深度信息。
- 惯性测量单元（IMU）
    - 原始数据：通过 ROS2 话题/imu/data获取加速度（a_x, a_y, a_z）、角速度（ω_x, ω_y, ω_z）和四元数姿态（q_w, q_x, q_y, q_z）。
    - 坐标系转换：SDK 提供工具函数处理不同框架下的四元数顺序（如 Isaac Gym 与 Isaac Sim 的差异）。
- 里程计与定位
    - 状态估计：通过激光雷达 + IMU 融合（如 LIO-SAM 算法）或腿部运动学模型（关节编码器数据）实现里程计输出。ROS2 话题/odom提供机器人位姿（x, y, θ）和速度信息。
    - 精度优化：紧耦合 LiDAR-IMU - 腿部里程计系统可在无特征环境下实现亚米级定位精度，在线学习机制适应负载和地形变化。
- 其他传感器
    - 关节状态：实时获取 12 个关节的角度、角速度和扭矩（/joint_states），支持电机健康监测。
    - 足端力反馈：PRO/EDU 版配备足端力传感器（F_z），用于复杂地形下的步态调整。

对于仿真环境，我有本地的Gym-Unrealcv仿真场景，但是苦恼于LOVON没有开源其仿真代码所以搁置着，不清楚下一步是根据部署代码反推仿真代码还是换一个仿真环境如MatterPort3D
时间本身还是比较充裕的，到开题答辩之前至少有1个月时间


## 研究现状
| 维度         | 内容总结                                                                                                |
| ---------- | --------------------------------------------------------------------------------------------------- |
| **基线模型**   | LOVON（LoVi: Open-vocabulary Visual Navigation and Tracking）在仿真中近乎完美（≈100% success rate），但在真实环境严重失效。 |
| **核心问题**   | LOVON 只用 YOLO 的 2D 框坐标来做“视觉 → 动作”映射，没有任何 3D 环境建模或避障机制。目标消失（如进门）时，机器人仍执行“往前走”动作 → 撞门框。               |
| **可用硬件**   | Unitree Go2（有RGB、深度、IMU、里程计、足端力传感器）。具备基本避障API、Move(vx,vy,vyaw)控制接口。                                 |
| **仿真环境**   | 有Gym-UnrealCV，但缺少LOVON仿真智能体代码。可能考虑复刻或转向MatterPort3D。                                                |
| **研究目标雏形** | 希望提升LOVON从2D视觉到更稳健3D环境理解（environment understanding + navigation fusion）的能力。                         |

## 问题定义
当前的 open-vocabulary 视觉追踪方法（如 LOVON）在仿真中表现优异，但在真实足式机器人环境中严重退化，其原因在于缺乏对三维环境几何与障碍物的建模能力。
其技术设计恰好规避了仿真环境的局限性，同时最大化了自身优势，具体体现在 3 个 “无冲突”：
- **仿真无 “真实场景的 3D 感知需求”，纯 2D 视觉足够**
    - 仿真环境中，目标的 “2D 图像坐标” 与 “实际空间位置” 完全对齐（如虚拟场景中 xyn=0.5 即代表物理上的正前方，无门框等 3D 遮挡物），无需深度信息即可判断路径是否可行。而 LOVON 的核心是 “2D 视觉 + 运动向量映射”，恰好适配这种需求，无需额外的 3D 深度理解模块。
- **仿真无 “不可控干扰”，搜索策略高效**
    - 仿真中的 “目标丢失” 仅为 “目标移出 90 度扇形视野”（可通过旋转搜索快速重新捕获），无真实场景的 “目标被门框完全遮挡”“机器人被碰撞” 等不可控干扰。LOVON 的旋转搜索策略（vx=0、w_z≠0）在仿真中能高效覆盖视野，而不会像真实场景那样因 “旋转时忽略障碍物” 导致碰撞。
- **仿真数据与模型训练 “高度同源”**
    - 仿真使用的目标类别（背包、椅子、行人）、运动速度（0.3~0.7m/s）、场景光照均与 LOVON 的训练数据集（100 万样本，摘要 1）高度匹配：IOE 对 “椅子”“行人” 的类别映射无误差，L2MM 的运动预测参数（如 β=10）也针对仿真场景校准（摘要 3），避免了真实场景中 “未见过的目标形态”“突发速度变化” 导致的误差。

当目标被暂时遮挡（如进入门后）或在复杂结构环境中移动时，机器人仅凭2D像素坐标进行动作决策，无法有效区分“自由空间”与“障碍区域”，导致运动策略失效（如撞门、原地旋转）。
因此，本研究旨在探索一种融合3D环境理解的目标跟踪与导航方法，在保持LOVON开放词汇指令能力的前提下，提高其在真实环境中的鲁棒性与安全性。

## 研究方向
| 方向                                        | 名称                                                                        | 思路简述                                                                                  |
| ----------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- | 
| **A. 环境理解增强（BEV / Depth / 3D Occupancy）** | 给LOVON加“视觉深度感知”，即在YOLO检测的基础上，通过深度图重投影到3D坐标系或BEV平面，建立占用图。再利用该图进行避障或规划。     | 你能做仿真+实机对比，提出一种“轻量级3D-aware追踪方法”。 → 投稿到 **IROS/ICRA workshop 或 CCF-C AI Robotics会议**。 |
| **B. 跟踪 + 导航分层融合（Hierarchical Policy）**   | 把“跟踪”和“导航”分成两个层次：高层目标预测、低层路径规划。你可以用简单预测（如卡尔曼滤波预测目标短期轨迹）+ BEV局部避障（A*或DWA）。 | 可以与LOVON对比“复杂场景成功率”→ 写出完整paper。                                                       |

选择 A + B 结合的小主题：“基于3D视觉感知与分层导航策略的开放词汇足式机器人目标追踪”(但这个一听就感觉不少人做过类似的课题非常卷)

| 类别              | 指标                      | 含义               |
| --------------- | ----------------------- | ---------------- |
| **任务层面**        | Success Rate (SR)       | 机器人在有限步数内到达目标的比例 |
|                 | Average Steps (AS)      | 成功任务平均步数         |
|                 | Collision Rate (CR)     | 发生障碍碰撞的任务比例      |
| **视觉层面**        | Target Loss Time (TLT)  | 目标丢失后重新识别的平均时间   |
|                 | Tracking Stability (TS) | 目标检测框抖动方差        |
| **Sim2Real 层面** | ΔSR (Sim→Real)          | 仿真与实机成功率差距       |
| **效率指标**        | FPS / Latency           | 模型推理帧率与系统延迟      |
| **安全指标**        | Distance Margin         | 与障碍最近距离的平均值      |
在仿真中先实现自动收集 SR、AS、CR。

实机可手动统计 SR 和 CR，或用里程计测轨迹。

可定义 3 个场景（开阔场 / 门框 / 桌椅环境）各跑10次。

## 创新点（暂定）
我们提出一种融合深度视觉感知与分层控制的开放词汇目标追踪框架。
相较于LOVON仅依赖2D目标检测进行运动控制，我们的方法通过深度投影构建局部BEV占用图，并引入预测-驱动的路径规划层，从而显著减少在真实环境中因遮挡或障碍导致的失败

## 规划
| 时间  | 任务                                              | 目标             |
| --- | ----------------------------------------------- | -------------- |
| 第1阶段 | 阅读文献：LOVON、LOVi、BEVFusion、LIO-SAM、SceneGPT      | 明确3D环境理解技术路线   |
| 第2阶段 | 在Gym-UnrealCV中复现或简化LOVON策略（YOLO+Motion mapping） | 建立baseline可控环境 |
| 第3阶段 | 集成深度图或BEV投影模块，实现障碍建模与避障决策                       | 形成改进方法         |
| 第4阶段 | 实机测试 + 指标对比 + 论文撰写                              | 形成可投稿版本        |

第一阶段的文献调研一方面要包括LOVON引用的和引用LOVON的文献（但是因为VPN节点问题我的Scholar Google给我挂掉了，说我是机器人不让我访问），另一方面是尽可能的调研3D-aware Tracking/Navigation

## 医疗交叉（答辩）
这里值得注意的是，论文里面要写的内容是一个宏大的改进，但是本院答辩时要突出和BME相关、医疗交叉的内容，HexGuide可以作为一个很大的参考
> 《HexGuide: A Hexapod Robot for Autonomous Blind Guidance in Challenging Environments》，一篇期刊论文

| 层级   | 内容                                               | 对应写作作用                    |
| ---- | ------------------------------------------------ | ------------------------- |
| ① 背景 | 世界上有数亿视障人群，对自主出行有刚性需求                            | 让读者意识到社会价值和痛点             |
| ② 矛盾 | 现有导盲设备（如导盲犬或轮式机器人）有明显局限，不能稳定地应对复杂地形              | 设置“冲突”——为什么我们必须做新系统       |
| ③ 概念 | 上交高峰团队设计了一个六足机器人 **HexGuide**，模仿昆虫式稳定步态，在复杂环境中实现安全引导  | 提出核心创新点和愿景                |
| ④ 方法 | 通过算法与机械协同，实现**路径规划 + 稳定行走 + 动态避障 + 交通识别 + 人机交互** | 展示技术路线是如何支撑“稳定、安全”这两个关键词的 |
| ⑤ 验证 | 在机场、十字路口等复杂场景下实测验证，引导成功率高，路径平滑且避障成功              | 用结果“闭环”故事——愿景得以实现         |

论文的立意不是“做一个六足机器人”，而是要证明“六足+智能控制” = 可靠的盲人引导方式。
