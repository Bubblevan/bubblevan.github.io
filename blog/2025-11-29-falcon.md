---
slug: falcon
title: Falcon
authors: bubblevan
tags: [social-navigation, reinforcement-learning, falcon]
---

来拜读**梁俊卫老师**和**龚泽颖学长**的工作，没准之后红鸟面试还能用上。

## 论文研读


为解决**社交导航（SocialNav）**中机器人**"短视避障"**和现有评估基准不真实的问题，论文提出**Falcon 框架**（一种基于强化学习的未来感知社交导航架构），通过**社交认知惩罚（SCP）**（含障碍碰撞、人类距离、轨迹阻碍三类惩罚）和**时空预知模块（SPM）**（含人类计数估计、当前位置跟踪、未来轨迹预测三个辅助任务）实现主动避障与社交合规；同时构建**SocialNav 基准**，包含**Social-HM3D**（844 个场景）和**Social-MP3D**（72 个场景）两个高真实感室内数据集，平衡人类密度与自然运动模式。

**实验结果：** Falcon 在该基准上实现**55% 的任务成功率**，同时保持约**90% 的个人空间合规率**，显著优于 **A***、**ORCA** 等规则算法及 **Proximity-Aware** 等 RL 方法，且具备**零样本泛化能力**。

### 问题背景

**社交导航（SocialNav）**要求机器人在人类共享环境中遵守社交规范并安全导航，但现有方案存在两大关键问题：

#### 1. 算法短视性

传统 **RL 方法**仅依赖当前环境信息，易出现**"短视避障"**问题；规则类算法（A*/ORCA）或依赖全局地图，或无法动态适应人类运动。下面以 **Proximity-Aware** 为例详细说明传统方法的局限性。

**传统 RL 方法的基本流程：**

1. **状态表示：** 通常使用当前时刻的传感器数据（如深度图像、激光雷达）和人类当前位置作为状态输入
2. **奖励设计：** 设计基于**当前距离**的奖励函数，例如：
   - 当机器人与人类距离过近时给予惩罚
   - 当机器人成功到达目标时给予奖励
   - 考虑当前时刻的碰撞风险
3. **策略学习：** 通过强化学习（如 **PPO**、**A3C**）训练策略网络，学习在当前状态下选择最优动作
4. **动作执行：** 策略网络直接输出下一步动作（如前进、转向、停止）

**传统方法的局限性：**

- **仅考虑当前状态：** 只基于当前时刻的人类位置和距离进行决策，无法预测人类未来的移动轨迹
- **反应式避障：** 当人类突然改变方向时，机器人只能被动反应，容易出现"短视避障"（即只关注眼前障碍，导致后续路径不佳）
- **缺乏前瞻性：** 无法提前规划路径以避免与人类未来轨迹发生冲突，导致效率低下或碰撞风险增加

**Proximity-Aware** 是一个典型的基于当前距离的 RL 方法：

- **状态空间：** $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前时刻机器人与人类的距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向
- **奖励函数：** $r_t = r_{goal} + \alpha \cdot r_{proximity}$，其中：
  - $r_{goal}$ 是到达目标的奖励
  - $r_{proximity} = -1/d_t$ 是基于当前距离的惩罚（距离越近惩罚越大）
- **问题：** 这个奖励函数只考虑**当前时刻**的距离 $d_t$，无法考虑人类未来可能移动到的位置
假设机器人在走廊中需要绕过前方正在移动的人类。**Proximity-Aware**可能：
- 看到人类在左侧，选择向右避让
- 但人类可能正在向左移动，导致机器人向右避让后反而与人类未来位置冲突
- 需要多次调整，效率低下

#### 2. 基准不真实性

现有数据集（如 **iGibson**、**Habicrowd**）场景类型单一、人类行为简化（随机行走 / 无动画）、人类密度失衡，且常假设机器人可获取全局信息，与真实场景脱节。

### 核心创新

因此 **Falcon 框架**的核心贡献在于是**首个融合显式人类轨迹预测的未来感知 RL 架构**，通过 **SCP** 和 **SPM** 实现主动避障与社交合规。

- **一方面**，该框架引入**"社交认知惩罚"**（含轨迹阻碍惩罚），鼓励智能体主动规避潜在碰撞并遵守社交礼仪
- **另一方面**，框架搭载**"时空预知模块"**，该模块融入包含轨迹预测在内的社交感知辅助任务，以增强智能体在训练过程中对未来动态的理解

> **注：** 这里提到社交导航最初是在 **iGibson 社交导航挑战赛**提出，是 **PointNav**基础上增加了移动人类这一元素，那这个领域真的非常新颖了，还是这篇文章首创的**Habitat替代这一静态人形模型**。

### 相关工作

根据 **Related Works**，往期人类轨迹预测方法可分为三类：

1. **基于物理学的方法：** 此类方法从牛顿运动定律中推导显式动力学模型，用于轨迹预测
2. **基于学习的方法：** 聚焦于从观测到的历史轨迹中学习运动模式
3. **基于规划的方法：** 其核心目标是推理理性智能体的运动意图，通过理解智能体的目标及其决策过程来预测轨迹

**Falcon**借鉴了这些研究思路，提出的方法不仅能预测人类轨迹，还能将社交感知信息融入智能体的导航策略，从而确保智能体在动态场景中实现安全、高效的导航。

### 方法论

考虑这样一个社交导航任务：机器人 $a$ 在存在 $N$ 个动态人类（记为 $i \in \{1, \ldots, N\}$）的环境中导航。机器人从初始位形 $q_a \in Q$ 出发，需持续选择动作以生成一条通往目标位形 $g_a \in Q$ 的路径 $\tau_a$，同时避免与静态障碍及动态人类发生碰撞。

其总体目标可建模如下：

$$
\begin{aligned}
\tau_a &= \arg \min_{\tau \in T} \left( c_a(\tau) + \lambda_a c_a^s(\tau, \tau_{1:N}) \right) \\
\text{s.t.} \quad &A_a(\tau_a) \notin C_{obs}, \quad A_a(\tau_a) \cap A_i(\tau_i) = \emptyset, \\
&\tau_a(0) = q_a, \quad \tau_a(T) = g_a
\end{aligned}
$$

其中：
- $c_a$ 为引导机器人前往目标的路径成本
- $c_a^s$ 为考虑社会规范的成本项
- $A(\tau)$ 表示轨迹 $\tau$ 所占据的体积
- $C_{obs}$ 表示静态障碍
- $T$ 为任务回合结束时间
- $\lambda_a$ 为权重因子

约束条件确保机器人在到达目标前，不会与静态障碍或人类发生碰撞。

![Falcon Overview](/blog/2025/falcon-overview.png)

如上图，**主策略网络（Main Policy Network）**在每个时间步以**深度图像（depth image）**和**点目标（point goal）**为输入，直接输出机器人下一步的动作。该网络的训练结合了**点目标导航（PointNav）**的奖励与本文提出的**社交认知惩罚（Social Cognition Penalty, SCP）**。此外，主策略网络还搭配了**时空预知模块（Spatial-Temporal Precognition Module）**，该模块在训练过程中支持多个辅助任务的执行。

### 主策略网络（Main Policy Network）

**Main Policy Network（主策略网络）**—— 导航的"核心决策大脑"

其实这个模块是整个系统的"**指挥官**"，核心作用是：接收传感器数据，分析当前环境和自身状态，最终输出具体的导航动作（比如向前走、左转、右转、减速），同时通过奖励/惩罚机制不断学习"更优的导航策略"。

#### 1. 核心定位

它是智能体的"**行动决策中心**"—— 既要保证导航效率（尽快到达目标），又要遵守社交规则（不碰撞、不挡路、保持安全距离），是直接指导智能体移动的核心模块。

#### 2. 内部结构与工作流程（输入→处理→输出）

可以把它想象成"一个带'翻译官''记忆大师'和'决策+评估团队'的指挥系统"，步骤如下：

##### （1）输入：智能体"看到"和"知道"的信息

**核心输入 1：深度图像（RGBD 中的 Depth 部分）**

相当于智能体的"**眼睛**"，能看到周围的墙壁、障碍物、人类的轮廓和距离（比如"前方 3 米有个人，左侧 2 米有个柜子"）。

**核心输入 2：相对目标坐标**

相当于智能体的"**导航目的地**"，比如"目标在我前方 5 米、偏右 10 度的位置"（不需要全局地图，只需要自己和目标的相对位置）。

##### （2）处理：把"原始信息"变成"决策依据"

这个过程分 3 步，对应模块里的关键组件：

**ResNet-50 编码器：视觉翻译官**

深度图像是"像素组成的图片"，机器看不懂，**ResNet-50** 的作用就是把图片"翻译"成机器能理解的"数字特征向量"（比如用一串数字代表"前方 3 米有人类""左侧是静态柜子"）。

简单说：它负责提取环境的"**关键视觉信息**"，过滤无用细节（比如墙壁的纹理、人类的衣服颜色），只保留和导航相关的核心特征（距离、障碍物类型、人类位置）。

**2 层 LSTM：时间记忆大师**

导航是"连续的过程"，不是单张图片能决定的（比如"前一秒那个人在走，这一秒停了，下一秒可能继续走"）。**LSTM** 的作用是"记住时间序列的变化"，处理"**时序依赖**"。

比如：它会整合"过去 5 个时间步的视觉特征"，判断人类的移动趋势（"这个人一直在朝我这边走，速度大概 0.5 米/秒"），而不是只看"当下这一帧"的静态位置 —— 这能避免智能体"短视"，比如不会因为当下距离够远就忽视正在靠近的人类。

**Actor-Critic（演员-评论家）头：决策+评估团队**

经过 **ResNet-50** 和 **LSTM** 处理后，得到了"当前环境特征 + 历史变化趋势"，接下来由这个"团队"输出最终决策：

- **演员头（Actor Head）**："**决策者**"—— 输出具体的导航动作，比如"向前移动 0.3 米""左转 15 度""保持静止"（动作是连续的，不是固定的几个选项）
- **评论家头（Critic Head）**："**评估师**"—— 不直接做决策，而是评估"演员头做出的这个动作好不好"，输出一个"价值分数"（比如"这个动作能让你更快到目标，且不会撞人，得分 8 分""这个动作会挡路，得分 2 分"）

##### （3）输出：具体的导航动作

最终由演员头输出"**连续的控制指令**"（比如速度、转向角度），直接驱动智能体移动。

#### 3. 训练逻辑：怎么让"大脑"学会"好策略"？

主策略网络是通过"**奖励机制**"学习的 —— 就像训练宠物：做得好就给奖励，做得差就给惩罚，慢慢形成条件反射。核心是之前提到的 $$R_t^{socialnav}$$ 奖励函数，简化理解就是：

**加分项：**
- 靠近目标（$$-\beta_d \Delta_d$$，距离目标越近，加分越多）
- 成功到达目标（$$\beta_{succ} \cdot I_{succ}$$，直接加大额奖励）

**扣分项：**
- 无意义动作（$$-r_{slack}$$，比如原地打转）
- 碰撞（$$-r_{coll}$$，撞墙或撞人扣大分）
- 离人太近（$$-r_{prox}$$，距离小于 2 米扣分，越近扣越多）
- 挡人类轨迹（$$-r_{traj}$$，挡住别人要走的路扣分）

通过不断迭代训练（**7500 万步**），主策略网络会逐渐学会"**平衡效率和合规**"—— 既不会为了快而撞人，也不会为了合规而绕远路。

#### 4. 与时空预认知模块的协作关系

**主策略网络：** 负责"**当下该做什么动作**"（比如现在走还是转）

**时空预认知模块：** 负责"**未来会发生什么**"（比如人类会走到哪）

**协作逻辑：** 主策略网络根据"当下状态 + 未来预测"做决策 —— 比如当下离人 3 米（安全），但预测 1 秒后会到 1.5 米（危险），主策略网络就会提前调整轨迹，而不是等进入危险区再反应。

---

主策略网络包含两个核心组件：
1. **状态编码器（State Encoders）**：从观测中提取视觉与时序特征
2. **社交认知惩罚（SCP）**：促进社交合规性的惩罚机制

#### 网络架构（技术细节）

主策略网络的处理流程分为四个步骤，具体技术实现如下：

##### 第一步：输入编码（Linear Encoder + Visual Encoder）

**点目标 + GPS+Compass → 线性编码器（Linear Encoder）：**
- 点目标的坐标（如 $g_a \in Q$）和 GPS 定位的机器人当前位置（$q_a \in Q$）
- 先计算"目标相对距离与方向"
- 通过线性变换转化为低维特征向量 $f_{goal}$

**深度图像 → 视觉编码器（Visual Encoder：ResNet-50）：**
- 通过预训练的 **ResNet-50 模型 [51]** 提取视觉特征 $f_{depth}$

##### 第二步：时序建模（Recurrent State Encoder：2 层 LSTM）

**输入：** 将线性编码器输出的 $f_{goal}$ 和 ResNet-50 输出的 $f_{depth}$ 拼接，得到融合特征：
$$
f_{fusion} = [f_{goal}, f_{depth}]
$$

**输出：** LSTM 会输出两个关键结果：
1. **时序特征 $h_t$**：捕捉"前 t 步环境变化"的动态信息
2. **潜在变量 $\delta_R$**：LSTM 的隐藏状态，其核心作用是"将主网络的时序记忆传递给 SPM"，让 SPM 能基于主网络的"观察记忆"学习辅助任务

##### 第三步：输出决策（Actor Head + Value Head）

时序特征 $h_t$ 会输入到两个"头网络"：

**Actor Head（动作头）：**
- 基于 $h_t$ 输出机器人的下一步动作，决定了机器人的实际导航路径 $\tau_a$

**Value Head（价值头）：**
- 基于 $h_t$ 预测当前动作的"预期奖励" $V(h_t)$，辅助强化学习的训练（PPO 算法需要通过"预测奖励"与"实际奖励"的差异更新策略）

##### 第四步：主损失 $L_{main}$ 的来源

主策略网络的训练目标是"让机器人既快又安全地抵达目标"，其损失 $L_{main}$ 来自 **DD-PPO 算法 [53]** 的 PPO 损失。

**PPO 损失的核心逻辑：** "约束策略更新的幅度，避免更新过快导致不稳定"

其简化公式为：

$$
L_{PPO} = \mathbb{E}_{\hat{\pi}} \left[ \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a), \text{clip}\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}, 1-\epsilon, 1+\epsilon \right) A^{\pi_{\theta_{old}}}(s,a) \right) \right]
$$

其中：
- $\pi_{\theta}(a|s)$ 是当前策略（主网络）在状态 $s$ 下选择动作 $a$ 的概率
- $\pi_{\theta_{old}}(a|s)$ 是上一轮策略的概率（用于约束更新幅度）
- $A^{\pi_{\theta_{old}}}(s,a)$ 是优势函数（衡量"当前动作比平均动作好多少"，与奖励函数 $R_t^{socialnav}$ 直接相关，而 $R_t^{socialnav}$ 包含了 SCP 惩罚，因此 SCP 会间接影响 $L_{main}$）

**简言之，** $L_{main}$ 是"主策略网络决策质量的量化指标"：动作越接近"抵达目标 + 遵守社交规则"，$L_{main}$ 越小。

#### PointNav 奖励函数

训练过程中，策略的更新由一个鼓励"达成目标"行为的奖励函数引导。每个时间步 $t$ 采用经典的 **PointNav 奖励函数**：

$$
R_t^{pointnav} = -\beta_d \Delta_d - r_{slack} + \beta_{succ} \cdot I_{succ}
$$

其中：
- $\Delta_d$ 为机器人到目标的测地线距离变化量
- $r_{slack}$ 为防止不必要动作的步长惩罚项
- $I_{succ}$ 为导航成功的指示变量（成功时为 1，否则为 0）
- $\beta_d$ 与 $\beta_{succ}$ 为权重系数

#### 社交认知惩罚（SCP）

然而，**PointNav 奖励函数**未考虑动态环境与社交交互，无法满足社交导航（SocialNav）的需求。为此，本文引入**社交认知惩罚（SCP）**——一套用于促进机器人遵守社会规范的惩罚机制，具体包含以下三类惩罚：

##### 1. 障碍碰撞惩罚（Obstacle Collision Penalty）

该惩罚针对机器人与静态障碍或人类发生碰撞的行为，计算公式如下：

$$
r_{coll} = \beta_s \cdot I_{s\_coll} + \beta_h \cdot I_{h\_coll}
$$

其中：
- $I_{s\_coll}$ 和 $I_{h\_coll}$ 分别为表示"与静态障碍碰撞"和"与人类碰撞"的指示变量（发生碰撞时为 1，否则为 0）
- $\beta_s$ 和 $\beta_h$ 为对应的惩罚权重

##### 2. 人类距离惩罚（Human Proximity Penalty）

该惩罚确保机器人与人类保持安全距离，计算公式如下：

$$
r_{prox} = \sum_{i=1}^{N} \begin{cases}
\beta_{prox} \cdot \exp(-d_i^t) & \text{若 } d_i^t < 2.0 \text{ m} \\
0 & \text{若 } d_i^t \geq 2.0 \text{ m}
\end{cases}
$$

式中，$d_i^t = \|\tau_a(t) - \tau_i(t)\|$ 表示时间步 $t$ 时机器人与第 $i$ 个人类的欧氏距离。当 $d_i^t$ 减小时，惩罚呈指数增长，以促使机器人主动避开人类；当机器人距离目标不足 2.0 米时，该惩罚自动取消。

##### 3. 轨迹阻碍惩罚（Trajectory Obstruction Penalty）

该惩罚用于阻止机器人阻碍人类未来 $H$ 步的轨迹。它通过同时考虑"当前与未来位置"预判潜在阻碍，且对"早期轨迹重叠"的惩罚更重，计算公式如下：

$$
r_{traj} = \sum_{k=t+1}^{t+H} \sum_{i=1}^{N} \begin{cases}
\beta_{traj} \cdot \frac{1}{k-t+1} & \text{若 } d_{traj-i}^k < 0.05 \text{ m} \\
0 & \text{若 } d_{traj-i}^k \geq 0.05 \text{ m}
\end{cases}
$$

其中：
- $d_{traj-i}^k = \|\tau_a(k) - \tilde{\tau}_i(k)\|$ 表示时间步 $k$ 时机器人与第 $i$ 个人类"未来轨迹"的距离
- $\frac{1}{k-t+1}$ 为时间衰减因子，确保距离当前时间越近的轨迹重叠，惩罚权重越大
- 当机器人距离目标不足 2.0 米时，该惩罚同样取消

#### 总奖励函数

社交导航的总奖励函数为"目标导向奖励"与"社交认知惩罚"的差值：

$$
R_t^{socialnav} = R_t^{pointnav} - R_t^{scp}
$$

其中，$R_t^{scp}$ 为社交认知惩罚的总和：

$$
R_t^{scp} = r_{coll} + r_{prox} + r_{traj}
$$

主策略网络采用 **DD-PPO 算法**进行训练，优化目标为 PPO 损失 $L_{main}$。

### 时空预知模块（Spatial-Temporal Precognition Module）

**SPM 的核心作用：** "利用主网络的记忆（$\delta_R$），学习与人类相关的'经验'，辅助主网络更聪明地决策"。

#### 模块架构与输入设计

**时空预认知模块（Spatial-Temporal Precognition Module）**的核心输入完全来自主策略网络的"中间处理结果"，没有额外新增传感器数据。核心目的是"**复用特征、节省计算**"，同时让"预判能力"和"决策能力"基于同一套环境理解，避免信息脱节。

##### 一、核心输入：主策略网络的状态编码 $\delta_R$

**来源：** 主策略网络中"**ResNet-50 编码器 + 2 层 LSTM**"的输出结果。

- 先由 **ResNet-50** 把"深度图像"翻译成"视觉特征向量"（比如"前方 3 米有人类、左侧 2 米是柜子"）
- 再经 **2 层 LSTM** 处理"时序依赖"（比如"这个人前 3 步朝我走，速度 0.5 米/秒"）
- 最终输出的 $\delta_R$ 是"浓缩了当前环境 + 历史变化"的高维数字特征，相当于主策略网络整理好的"**环境情报汇总**"

**作用：** 时空预认知模块直接用这个"情报汇总"做预测，不用再重复处理原始深度图像，既高效又能保证"预判和决策基于同一套环境理解"（比如主策略网络认为"那是个人"，预认知模块不会误判为"障碍物"）。

##### 二、辅助输入：Auxiliary Information（$S_R, N, P$）

这部分是从 $\delta_R$ 中解析出的"具体结构化信息"，不是额外输入，而是对 $\delta_R$ 的"拆解和明确"，方便模块针对性预测：

- **$S_R$（Spatial Relationship）**：人类与智能体的"相对空间关系"，比如"人类在智能体前方 30 度、距离 2.5 米"，从 $\delta_R$ 中的视觉特征和目标坐标推导而来
- **$N$（Number of Humans）**：当前环境中检测到的人类数量（初始值来自主策略网络的初步特征解析，后续会被辅助任务的 classifier 优化）
- **$P$（Human Positions）**：人类相对于智能体的"当前位置坐标"（比如"人类 A 在 $(x=2.3, y=1.5)$ 米处"），同样从 $\delta_R$ 中提取的视觉特征回归得到

**简单说：** 核心输入是"浓缩情报" $\delta_R$，辅助输入是"拆解后的具体情报" $S_R, N, P$，两者都来自主策略网络，没有额外传感器开销。

##### 三、Auxiliary Information 的具体来源

这三个信息不是"凭空产生"，而是主策略网络处理原始数据后，从"视觉特征 + 时序特征"中解析出来的，流程如下：

1. **原始数据**（深度图像 + 相对目标坐标）→ **ResNet-50** → 提取"静态视觉特征"（包含障碍物、人类的轮廓、距离信息）
2. **静态视觉特征** → **2 层 LSTM** → 融合"时序特征"（人类移动趋势、环境变化）→ 生成 $\delta_R$
3. **从 $\delta_R$ 中进一步解析：**
   - 通过"空间关系提取器"得到 $S_R$（相对角度、距离）
   - 通过"初步人数检测"得到 $N$（比如先判断"有 2 个人类"，后续由 classifier 优化精度）
   - 通过"初步位置回归"得到 $P$（比如先粗略预测"人类在前方 2-3 米处"，后续由 regressor 优化坐标精度）

**关键结论：** Auxiliary Information 是从主策略网络的状态编码 $\delta_R$ 中解析出的结构化信息，不是额外传感器数据，也不是预定义的固定值——会随着 $\delta_R$ 的更新（每个时间步都更新）而动态变化，比如人类移动后，$S_R$ 和 $P$ 会实时更新。

##### 四、Classifier 和两个 Regressor 的具体对应

这三个组件是时空预认知模块的"核心工作单元"，分别对应 Falcon 框架的三个辅助任务，本质是"通过多任务学习，让模块同时掌握'数人数、定位置、猜轨迹'三种能力"，最终提升时空理解能力。

| 组件类型 | 对应辅助任务 | 核心作用 | 输入（均为 $\delta_R$） | 输出结果 | 损失函数 |
|---------|------------|---------|----------------------|---------|---------|
| **Classifier（分类器）** | 人数估计（Population Estimation） | 预测当前环境中"真实人类数量"（0-M） | 状态编码 $\delta_R$ | 每个可能人数的概率（比如"0 人：5%、1 人：30%、2 人：65%"） | 交叉熵损失 $\mathcal{L}_{count}$ |
| **Regressor 1（回归器）** | 位置估计（Position Estimation） | 精准回归"人类当前相对智能体的坐标" | 状态编码 $\delta_R$ | 每个人类的 $(x, y)$ 坐标（比如"人类 A：$(2.3, 1.5)$ 米"） | MSE 损失 $\mathcal{L}_{pos}$ |
| **Regressor 2（回归器）** | 轨迹预测（Trajectory Forecasting） | 预测"人类未来 $H$ 个时间步的坐标" | 状态编码 $\delta_R$ | 每个人类的未来位置序列（比如"人类 A：$t+1$ 步 $(2.1,1.6)$、$t+2$ 步 $(1.9,1.7)$"） | MSE 损失 $\mathcal{L}_{traj}$ |

**逐个拆解（通俗理解）：**

1. **Classifier（分类器）= "人数计数器"**
   - **任务本质：** 分类问题（比如"当前人数是 0、1、2、…、$M$"，$M$ 是最大预测人数，比如 6 人）
   - **工作逻辑：** 输入 $\delta_R$（包含当前环境的视觉 + 时序特征），输出"每个人数选项的概率"，最终选择概率最高的作为预测人数
   - **举个例子：** $\delta_R$ 中包含"两个移动的人形轮廓特征"，分类器会输出"2 人：90% 概率"，通过交叉熵损失（惩罚预测概率与真实人数的差异）不断优化，让"计数"更准确

2. **第一个 Regressor（位置估计回归器）= "实时定位仪"**
   - **任务本质：** 回归问题（预测连续的坐标值，不是离散类别）
   - **工作逻辑：** 输入 $\delta_R$，针对每个检测到的人类，输出精准的 $(x, y)$ 相对坐标
   - **核心价值：** 解决"视觉特征只能判断'有人类'，但不知道'具体在哪'"的问题——比如主策略网络知道"前方有人"，这个回归器能精准告诉它"在前方 2.3 米、偏右 10 度的位置"，为避障提供精确依据

3. **第二个 Regressor（轨迹预测回归器）= "未来预言家"**
   - **任务本质：** 序列回归问题（预测未来多个时间步的连续坐标）
   - **工作逻辑：** 输入 $\delta_R$（包含人类的历史移动趋势，比如"前 3 步朝智能体移动，速度 0.5 米/秒"），输出未来 $H$ 个时间步（比如 $H=5$）的人类位置序列
   - **核心价值：** 解决"只能看当下，看不到未来"的短视问题——比如现在人类在 3 米外，但回归器预测"1 秒后会到 1.8 米处"，主策略网络就能提前调整轨迹，而不是等靠近了才反应

**关键补充：三个组件的协同关系**

- **共享输入：** 都用主策略网络的 $\delta_R$，确保"基于同一套环境理解"做预测
- **结果互补：** 人数估计（classifier）确定"有多少人要预测"，位置估计（regressor1）确定"现在在哪"，轨迹预测（regressor2）确定"未来会到哪"
- **损失合并：** 三个组件的损失（$\mathcal{L}_{count} + \mathcal{L}_{pos} + \mathcal{L}_{traj}$）乘以权重 $\beta_{aux}$ 后，加入总损失函数（公式 12），一起优化——比如人数预测不准、位置回归偏差大，都会让总损失上升，倒逼模型同时提升三个任务的精度

**总结：核心逻辑链**

主策略网络生成"环境情报" $\delta_R$ → 拆解出辅助信息 $S_R, N, P$ → 时空预认知模块用 $\delta_R$ 驱动三个组件（classifier + 两个 regressor）→ 分别输出"人数、当前位置、未来轨迹" → 三个组件的损失共同优化模型 → 提升智能体的时空理解能力，最终帮助主策略网络做出更精准的导航决策。

---

根据论文第三章 3.3 节，SPM 包含 3 个辅助任务，每个任务的网络结构都是**"LSTM/BiLSTM + 自注意力（Self-Attention） + 分类器/回归器"**，且输入均依赖主网络的潜在变量 $\delta_R$。

#### 1. 人类计数估计（Human Count Estimation）——"数清楚周围有几个人"

**任务目标：** 估计场景中人类的总数

**输入：** 主网络的潜在变量 $\delta_R$（包含前几步的环境记忆，如"前 3 步看到 2 个人类轮廓"）

**网络结构：**
1. **LSTM 编码器：** 将 $\delta_R$ 编码为时序特征 $\Phi_R$
2. **自注意力层 [54]：** 以 $Q=K=V=\Phi_R$ 处理特征，目的是"聚焦与人类数量相关的关键记忆"（如"忽略墙的特征，重点关注人类轮廓的变化"），输出注意力特征 $A_t$
3. **分类器 $\phi_{count}$：** 基于 $A_t$ 预测"场景中人类数量为 $k$"的概率 $\hat{n}_k$：

$$
\hat{n}_k = \phi_{count}(A_t), \quad k \in \{0, 1, \ldots, M\}, M=6
$$

**损失函数：** 采用交叉熵损失（衡量"预测概率"与"真实数量"的差异）：

$$
L_{count} = -\sum_{k=0}^{M} n_k \log(\hat{n}_k)
$$

其中 $n_k$ 是"真实人类数量为 $k$"的指示变量（如真实有 2 人，则 $n_2=1$，其余 $n_k=0$）。

#### 2. 当前位置跟踪（Current Position Tracking）——"知道每个人在哪"

**任务目标：** 跟踪人类相对于机器人的二维位置

**输入：** 主网络的 $\delta_R$ + 场景中人类的真实数量 $N$（Oracle 信息，仅用于训练，推理时无需）

**网络结构：**
1. **LSTM 编码器：** 将 $\delta_R$ 和 $N$ 融合编码为特征 $\Phi_{R;N}$
2. **自注意力层：** 以 $Q=K=V=\Phi_{R;N}$ 处理，聚焦"人类位置相关的特征"（如"分辨'人类 A 在左前方'和'人类 B 在右后方'"），输出 $A_t$
3. **回归器 $\phi_{pos}$：** 基于 $A_t$ 预测第 $i$ 个人类的相对 2D 位置 $\hat{P}_i^t$：

$$
\hat{P}_i^t = \phi_{pos}(A_t)
$$

**损失函数：** 采用均方误差（MSE，衡量"预测位置"与"真实位置"的距离）：

$$
L_{pos} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^t - P_i^t\|^2
$$

其中：
- $P_i^t$ 是人类 $i$ 的真实位置
- $M$ 是"真实存在的人类"的掩码（如场景中只有 2 人，就只计算这 2 人的位置损失）

#### 3. 未来轨迹预测（Future Trajectory Forecasting）——"预判人类未来走哪"

**任务目标：** 预测人类未来多个时间步的轨迹

> **注：** 这是 SPM 中**最重要的任务**（实验证明其对性能提升最大），因"预测未来轨迹"需要处理更复杂的时序关系，所以用 **BiLSTM（双向 LSTM）**替代普通 LSTM。

**输入：** 主网络的 $\delta_R$ + 人类真实数量 $N$ + 当前人类位置 $P_i^t$

**网络结构：**
1. **BiLSTM 编码器：** 双向处理 $\delta_R$、$N$、$P_i^t$ 的融合信息，输出特征 $\Phi_{R;N;P}$（双向 LSTM 能同时利用"过去记忆"和"未来趋势"，更适合长时序预测）
2. **自注意力层：** 以 $Q=K=V=\Phi_{R;N;P}$ 处理，聚焦"人类运动趋势相关的特征"（如"人类 A 前 2 步朝电梯走，预判他会继续向电梯移动"），输出 $A_t$
3. **回归器 $\phi_{traj}$：** 基于 $A_t$ 预测未来 $H$ 步的人类轨迹 $\hat{P}_i^{t+1:t+H}$：

$$
\hat{P}_i^{t+1:t+H} = \phi_{traj}(A_t)
$$

**损失函数：** 采用 MSE（衡量"预测轨迹"与"真实轨迹"的差异）：

$$
L_{traj} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^{t+1:t+H} - P_i^{t+1:t+H}\|^2
$$

其中 $P_i^{t+1:t+H}$ 为第 $i$ 个人类未来 $H$ 步的真实轨迹。

#### 辅助损失与总损失

3 个任务的损失相加，得到 SPM 的辅助损失：

$$
L_{aux} = L_{count} + L_{pos} + L_{traj}
$$

训练过程中，主策略网络与辅助任务的优化同步进行，模型的总损失为"主策略损失"与"辅助损失"的加权和：

$$
L_{total} = \beta_{main} L_{main} + \beta_{aux} L_{aux}
$$

其中，$\beta_{main}$ 和 $\beta_{aux}$ 分别为主策略损失与辅助损失的权重系数。

### 实验与结果

![Falcon Experiment](/blog/2025/falcon-experiment.png)

#### 评估指标

**任务完成度指标：**
- **成功率（Success Rate, Suc.）**：机器人成功到达目标的比例
- **路径长度加权成功率（Success weighted by Path Length, SPL）**：考虑路径效率的成功率
- **时间长度加权成功率（Success weighted by Time Length, STL）**：考虑时间效率的成功率

**社交合规性指标：**
- **人机碰撞率（Human-Robot Collision Rate, H-Coll）**：机器人与人类发生碰撞的比例
- **个人空间合规率（Personal Space Compliance, PSC）**：机器人保持适当社交距离的比例

考虑到人类碰撞半径为 **0.3 米**、机器人碰撞半径为 **0.25 米**，本实验将个人空间合规（PSC）的距离阈值设定为 **1.0 米**。

#### 基线方法


##### Proximity-Aware

**Proximity-Aware** 是一个基于强化学习的社交导航方法，该方法通过两个辅助任务建模人类与机器人的距离和方向，能有效捕捉当前时刻的人机距离关系。

**核心思想：**
- 使用**当前时刻**的人机距离和相对方向作为状态输入
- 通过两个辅助任务学习人机距离关系：
  1. **距离预测任务**：预测机器人与人类的距离
  2. **方向预测任务**：预测人类相对于机器人的方向
- 奖励函数基于当前距离：$r_{proximity} = -1/d_t$，其中 $d_t$ 是当前时刻的距离

**工作原理：**
1. **状态表示：** $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向
2. **辅助任务：** 在训练过程中同时学习预测距离和方向，增强对当前人机关系的理解
3. **策略学习：** 使用 PPO 算法训练策略网络，学习在当前状态下选择最优动作

**局限性：**
- 只考虑**当前时刻**的距离和方向，无法预测人类未来的移动轨迹
- 当人类突然改变方向时，只能被动反应，容易出现"短视避障"
- 无法提前规划路径以避免与人类未来位置发生冲突

##### A* 算法

**A* 算法**是一个经典的静态路径规划算法，广泛应用于机器人导航和游戏AI中。

**核心思想：**
- 使用**启发式搜索**在静态地图上找到从起点到终点的最优路径
- 综合考虑**已走路径成本**（$g(n)$）和**预估剩余成本**（$h(n)$）
- 评估函数：$f(n) = g(n) + h(n)$，其中 $h(n)$ 通常是欧氏距离或曼哈顿距离

**工作原理：**
1. **初始化：** 将起点加入开放列表（open list）
2. **迭代搜索：**
   - 从开放列表中选择 $f(n)$ 值最小的节点
   - 将该节点移入关闭列表（closed list）
   - 检查该节点的所有邻居节点
   - 对于每个邻居节点，计算新的 $g$ 值，如果更优则更新
3. **终止条件：** 当目标节点被加入关闭列表时，回溯路径

**在社交导航中的应用：**
- 预先计算一条从起点到终点的**固定路径**
- 假设环境是**静态的**，不考虑动态人类的存在
- 当遇到人类时，需要重新规划路径

**局限性：**
- **无法适应动态环境**：预先确定的路径无法应对人类移动
- **易导致碰撞**：当人类移动到规划路径上时，机器人可能直接碰撞
- **需要全局地图**：算法需要完整的静态地图信息

##### ORCA 算法

**ORCA（Optimal Reciprocal Collision Avoidance）算法**是一个基于速度障碍的多智能体避障算法，可获取智能体的位置和速度"先知信息"（oracle access），以动态调整规划路径。

**核心思想：**
- 基于**速度障碍（Velocity Obstacle）**概念
- 每个智能体选择一个新的速度，使得在假设其他智能体也选择最优速度的情况下，避免碰撞
- 使用**线性规划**在速度空间中寻找可行速度

**工作原理：**
1. **速度障碍计算：**
   - 对于每个其他智能体，计算一个速度障碍区域
   - 该区域包含所有会导致碰撞的速度
   - 速度障碍是一个**圆锥形区域**

2. **ORCA 半平面：**
   - 将速度障碍转换为**ORCA 半平面**
   - 每个半平面定义了一个速度约束：$v \cdot n \geq u \cdot n$
   - 其中 $n$ 是半平面的法向量，$u$ 是参考速度

3. **线性规划求解：**
   - 在满足所有 ORCA 半平面约束的条件下，选择最接近期望速度的速度
   - 优化目标：$\min \|v - v_{pref}\|$，其中 $v_{pref}$ 是期望速度

**在社交导航中的应用：**
- 可以获取人类的位置和速度信息（oracle access）
- 实时计算速度障碍，动态调整机器人速度
- 假设所有智能体都遵循 ORCA 规则，实现**相互避让**

**局限性：**
- **假设运动不受限制**：ORCA 假设智能体可以在任意方向移动，但实际机器人可能有运动学约束（如非完整约束）
- **可能导致与静态障碍物碰撞**：算法主要关注动态避障，可能忽略静态障碍
- **需要精确的位置和速度信息**：在实际应用中，这些信息可能难以准确获取
- **计算复杂度**：当环境中智能体数量较多时，线性规划的计算成本较高


#### 实验设置

- **训练算法：** 强化学习智能体采用 **DD-PPO 算法 [53]** 训练，且所有模型使用相同超参数
- **随机性控制：** 每种算法均采用 3 个不同随机种子独立运行 3 次，最终结果取各指标的均值与标准差
- **模型初始化：** 模型初始化权重来自预训练的 **PointNav 模型 [57]**，并在社交导航任务上进行 **1000 万步微调**
- **训练硬件：** 训练过程使用 4 块 **Nvidia RTX 3090** 显卡，同时运行 8 个并行环境
- **数据集划分：** 模型在 **Social-HM3D 训练集**上训练，在 **Social-HM3D 测试集**和 **Social-MP3D 测试集**上测试
- **泛化评估：** **Social-MP3D** 的测试结果用于评估 Falcon 的**零样本泛化能力**

![Falcon Results](/blog/2025/falcon-results.png)

#### 实验结果与分析

##### 结论 1：具备未来感知能力的方法比静态方法和情境感知方法更高效、更安全

- **A*** 等静态路径规划算法会预先确定一条固定路径，无法适应动态环境，易导致人机碰撞（见图 4 (a)）
- **ORCA**、**Proximity-Aware** 等情境感知避障方法虽能通过调整路径避开当前人类与障碍物，但存在局限性：
  - 路径重规划需耗时，延迟响应会增加碰撞风险
  - **ORCA** 因假设运动不受限制，导致与静态障碍物碰撞（见图 4 (b)）
  - **Proximity-Aware** 因无法预判人类运动，短期调整失效，最终发生碰撞（见图 4 (c)）
- **Falcon** 能主动适应人类动态运动，高效抵达目标位置

##### 结论 2：辅助任务可提升模型性能，其中轨迹预测任务作用最显著

表 3 展示了不同辅助任务组合的实验结果。与 **PointNav 基线模型 [57]** 相比，单个辅助任务即可提升导航性能；其中，**轨迹预测任务（SPM.Traj）**效果最突出——将成功率从 **40.94%** 显著提升至 **54.00%**。这一结果证明，在社交导航中引入**显式轨迹预测**具有重要价值。

**表 3：Falcon 消融实验结果**

> **表格说明：** 仅使用 PointNav 算法 [57] 训练的模型作为基线；SPM.Count、SPM.Pos、SPM.Traj 分别指"人类计数估计""当前位置跟踪""未来轨迹预测"三个辅助任务；数据以百分比表示，"↑" 表示指标值越高越好，"↓" 表示指标值越低越好

| SPM. 计数（Count） | SPM. 位置（Pos） | SPM. 轨迹（Traj） | SCP | 成功率（Suc.）↑ | 路径长度加权成功率（SPL）↑ | 时间长度加权成功率（STL）↑ | 个人空间合规率（PSC）↑ | 人机碰撞率（H-Coll）↓ |
|-------------------|-----------------|------------------|-----|----------------|-------------------------|-------------------------|---------------------|---------------------|
| （无辅助任务） | （无辅助任务） | （无辅助任务） | 无 | 40.94 | 34.14 | 11.50 | 90.82 | 53.54 |
| ✓ | 无 | 无 | 无 | 51.43 | 51.42 | 51.16 | 90.53 | 46.46 |
| 无 | ✓ | 无 | 无 | 53.17 | 53.17 | 52.95 | 90.06 | 44.07 |
| 无 | 无 | ✓ | 无 | **54.00** | 53.99 | 53.92 | 89.46 | 43.88 |
| 无 | 无 | 无 | ✓ | 51.24 | 51.24 | 51.08 | 90.41 | 48.11 |
| ✓ | ✓ | ✓ | 无 | 53.63 | 53.63 | 53.40 | 89.33 | 44.89 |
| ✓ | ✓ | ✓ | ✓ | **55.15** | 55.15 | 54.94 | 89.56 | 42.96 |

##### 结论 3：SCP 与 SPM 协同互补，显著提升模型性能并加快训练收敛

表 3 显示，**SCP** 对模型性能提升至关重要，尤其在与 **SPM** 结合时效果更明显：

- **仅使用 SPM** 的三个辅助任务（计数、位置、轨迹）时，模型性能较单个辅助任务无显著提升（SPM 组合的成功率为 **53.63%**，而 SPM.Pos 为 **53.17%**、SPM.Traj 为 **54.00%**）
- **加入 SCP 后**，完整模型的成功率提升至 **55.15%**，显著优于单独使用 SPM 的情况
- 图 5 显示，同时包含 **SPM** 和 **SCP** 的模型在训练过程中收敛速度更快（**1400K 步前**即可体现）

这些结果表明，若缺乏 **SCP** 的引导，**SPM** 的辅助任务无法有效整合——**SCP** 能帮助模型平衡各项任务，更充分地利用现有信息。

#### 局限性

**Falcon** 虽能实现较高的成功率，但 **Proximity-Aware**（成功率约 20%）在避障方面表现更优。这一现象暴露了现有评估指标的局限性：

- 在拥挤环境中，指标可能过度优先考虑**"社交舒适度"**，而忽视**"任务完成度"**
- 本基准目前未涵盖**"礼让"**等更高阶的人类社交行为

## 在 Autodl 上复现 Falcon

```bash
conda create -n falcon python=3.9 cmake=3.14.0
conda activate falcon
```
