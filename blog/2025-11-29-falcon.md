---
slug: falcon
title: Falcon
authors: bubblevan
tags: [social-navigation, reinforcement-learning, falcon]
---

来拜读**梁俊卫老师**和**龚泽颖学长**的工作，没准之后红鸟面试还能用上。

## 论文研读


为解决**社交导航（SocialNav）**中机器人**"短视避障"**和现有评估基准不真实的问题，论文提出**Falcon 框架**（一种基于强化学习的未来感知社交导航架构），通过**社交认知惩罚（SCP）**（含障碍碰撞、人类距离、轨迹阻碍三类惩罚）和**时空预知模块（SPM）**（含人类计数估计、当前位置跟踪、未来轨迹预测三个辅助任务）实现主动避障与社交合规；同时构建**SocialNav 基准**，包含**Social-HM3D**（844 个场景）和**Social-MP3D**（72 个场景）两个高真实感室内数据集，平衡人类密度与自然运动模式。

**实验结果：** Falcon 在该基准上实现**55% 的任务成功率**，同时保持约**90% 的个人空间合规率**，显著优于 **A***、**ORCA** 等规则算法及 **Proximity-Aware** 等 RL 方法，且具备**零样本泛化能力**。

### 问题背景

**社交导航（SocialNav）**要求机器人在人类共享环境中遵守社交规范并安全导航，但现有方案存在两大关键问题：

#### 1. 算法短视性

传统 **RL 方法**仅依赖当前环境信息，易出现**"短视避障"**问题；规则类算法（A*/ORCA）或依赖全局地图，或无法动态适应人类运动。下面以 **Proximity-Aware** 为例详细说明传统方法的局限性。

**传统 RL 方法的基本流程：**

1. **状态表示：** 通常使用当前时刻的传感器数据（如深度图像、激光雷达）和人类当前位置作为状态输入
2. **奖励设计：** 设计基于**当前距离**的奖励函数，例如：
   - 当机器人与人类距离过近时给予惩罚
   - 当机器人成功到达目标时给予奖励
   - 考虑当前时刻的碰撞风险
3. **策略学习：** 通过强化学习（如 **PPO**、**A3C**）训练策略网络，学习在当前状态下选择最优动作
4. **动作执行：** 策略网络直接输出下一步动作（如前进、转向、停止）

**传统方法的局限性：**

- **仅考虑当前状态：** 只基于当前时刻的人类位置和距离进行决策，无法预测人类未来的移动轨迹
- **反应式避障：** 当人类突然改变方向时，机器人只能被动反应，容易出现"短视避障"（即只关注眼前障碍，导致后续路径不佳）
- **缺乏前瞻性：** 无法提前规划路径以避免与人类未来轨迹发生冲突，导致效率低下或碰撞风险增加

**Proximity-Aware** 是一个典型的基于当前距离的 RL 方法：

- **状态空间：** $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前时刻机器人与人类的距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向
- **奖励函数：** $r_t = r_{goal} + \alpha \cdot r_{proximity}$，其中：
  - $r_{goal}$ 是到达目标的奖励
  - $r_{proximity} = -1/d_t$ 是基于当前距离的惩罚（距离越近惩罚越大）
- **问题：** 这个奖励函数只考虑**当前时刻**的距离 $d_t$，无法考虑人类未来可能移动到的位置
假设机器人在走廊中需要绕过前方正在移动的人类。**Proximity-Aware**可能：
- 看到人类在左侧，选择向右避让
- 但人类可能正在向左移动，导致机器人向右避让后反而与人类未来位置冲突
- 需要多次调整，效率低下

#### 2. 基准不真实性

现有数据集（如 **iGibson**、**Habicrowd**）场景类型单一、人类行为简化（随机行走 / 无动画）、人类密度失衡，且常假设机器人可获取全局信息，与真实场景脱节。

### 核心创新

因此 **Falcon 框架**的核心贡献在于是**首个融合显式人类轨迹预测的未来感知 RL 架构**，通过 **SCP** 和 **SPM** 实现主动避障与社交合规。

- **一方面**，该框架引入**"社交认知惩罚"**（含轨迹阻碍惩罚），鼓励智能体主动规避潜在碰撞并遵守社交礼仪
- **另一方面**，框架搭载**"时空预知模块"**，该模块融入包含轨迹预测在内的社交感知辅助任务，以增强智能体在训练过程中对未来动态的理解

> **注：** 这里提到社交导航最初是在 **iGibson 社交导航挑战赛**提出，是 **PointNav**基础上增加了移动人类这一元素，那这个领域真的非常新颖了，还是这篇文章首创的**Habitat替代这一静态人形模型**。

### 相关工作

根据 **Related Works**，往期人类轨迹预测方法可分为三类：

1. **基于物理学的方法：** 此类方法从牛顿运动定律中推导显式动力学模型，用于轨迹预测
2. **基于学习的方法：** 聚焦于从观测到的历史轨迹中学习运动模式
3. **基于规划的方法：** 其核心目标是推理理性智能体的运动意图，通过理解智能体的目标及其决策过程来预测轨迹

**Falcon**借鉴了这些研究思路，提出的方法不仅能预测人类轨迹，还能将社交感知信息融入智能体的导航策略，从而确保智能体在动态场景中实现安全、高效的导航。

### 方法论

考虑这样一个社交导航任务：机器人 $a$ 在存在 $N$ 个动态人类（记为 $i \in \{1, \ldots, N\}$）的环境中导航。机器人从初始位形 $q_a \in Q$ 出发，需持续选择动作以生成一条通往目标位形 $g_a \in Q$ 的路径 $\tau_a$，同时避免与静态障碍及动态人类发生碰撞。

其总体目标可建模如下：

$$
\begin{aligned}
\tau_a &= \arg \min_{\tau \in T} \left( c_a(\tau) + \lambda_a c_a^s(\tau, \tau_{1:N}) \right) \\
\text{s.t.} \quad &A_a(\tau_a) \notin C_{obs}, \quad A_a(\tau_a) \cap A_i(\tau_i) = \emptyset, \\
&\tau_a(0) = q_a, \quad \tau_a(T) = g_a
\end{aligned}
$$

其中：
- $c_a$ 为引导机器人前往目标的路径成本
- $c_a^s$ 为考虑社会规范的成本项
- $A(\tau)$ 表示轨迹 $\tau$ 所占据的体积
- $C_{obs}$ 表示静态障碍
- $T$ 为任务回合结束时间
- $\lambda_a$ 为权重因子

约束条件确保机器人在到达目标前，不会与静态障碍或人类发生碰撞。

![Falcon Overview](/blog/2025/falcon-overview.png)

如上图，**主策略网络（Main Policy Network）**在每个时间步以**深度图像（depth image）**和**点目标（point goal）**为输入，直接输出机器人下一步的动作。该网络的训练结合了**点目标导航（PointNav）**的奖励与本文提出的**社交认知惩罚（Social Cognition Penalty, SCP）**。此外，主策略网络还搭配了**时空预知模块（Spatial-Temporal Precognition Module）**，该模块在训练过程中支持多个辅助任务的执行。

### 主策略网络（Main Policy Network）

**Main Policy Network（主策略网络）**—— 导航的"核心决策大脑"

其实这个模块是整个系统的"**指挥官**"，核心作用是：接收传感器数据，分析当前环境和自身状态，最终输出具体的导航动作（比如向前走、左转、右转、减速），同时通过奖励/惩罚机制不断学习"更优的导航策略"。

#### 1. 核心定位

它是智能体的"**行动决策中心**"—— 既要保证导航效率（尽快到达目标），又要遵守社交规则（不碰撞、不挡路、保持安全距离），是直接指导智能体移动的核心模块。

#### 2. 内部结构与工作流程（输入→处理→输出）

可以把它想象成"一个带'翻译官''记忆大师'和'决策+评估团队'的指挥系统"，步骤如下：

##### （1）输入：智能体"看到"和"知道"的信息

**核心输入 1：深度图像（RGBD 中的 Depth 部分）**

相当于智能体的"**眼睛**"，能看到周围的墙壁、障碍物、人类的轮廓和距离（比如"前方 3 米有个人，左侧 2 米有个柜子"）。

**核心输入 2：相对目标坐标**

相当于智能体的"**导航目的地**"，比如"目标在我前方 5 米、偏右 10 度的位置"（不需要全局地图，只需要自己和目标的相对位置）。

##### （2）处理：把"原始信息"变成"决策依据"

这个过程分 3 步，对应模块里的关键组件：

**ResNet-50 编码器：视觉翻译官**

深度图像是"像素组成的图片"，机器看不懂，**ResNet-50** 的作用就是把图片"翻译"成机器能理解的"数字特征向量"（比如用一串数字代表"前方 3 米有人类""左侧是静态柜子"）。

简单说：它负责提取环境的"**关键视觉信息**"，过滤无用细节（比如墙壁的纹理、人类的衣服颜色），只保留和导航相关的核心特征（距离、障碍物类型、人类位置）。

**2 层 LSTM：时间记忆大师**

导航是"连续的过程"，不是单张图片能决定的（比如"前一秒那个人在走，这一秒停了，下一秒可能继续走"）。**LSTM** 的作用是"记住时间序列的变化"，处理"**时序依赖**"。

比如：它会整合"过去 5 个时间步的视觉特征"，判断人类的移动趋势（"这个人一直在朝我这边走，速度大概 0.5 米/秒"），而不是只看"当下这一帧"的静态位置 —— 这能避免智能体"短视"，比如不会因为当下距离够远就忽视正在靠近的人类。

**Actor-Critic（演员-评论家）头：决策+评估团队**

经过 **ResNet-50** 和 **LSTM** 处理后，得到了"当前环境特征 + 历史变化趋势"，接下来由这个"团队"输出最终决策：

- **演员头（Actor Head）**："**决策者**"—— 输出具体的导航动作，比如"向前移动 0.3 米""左转 15 度""保持静止"（动作是连续的，不是固定的几个选项）
- **评论家头（Critic Head）**："**评估师**"—— 不直接做决策，而是评估"演员头做出的这个动作好不好"，输出一个"价值分数"（比如"这个动作能让你更快到目标，且不会撞人，得分 8 分""这个动作会挡路，得分 2 分"）

##### （3）输出：具体的导航动作

最终由演员头输出"**连续的控制指令**"（比如速度、转向角度），直接驱动智能体移动。

#### 3. 训练逻辑：怎么让"大脑"学会"好策略"？

主策略网络是通过"**奖励机制**"学习的 —— 就像训练宠物：做得好就给奖励，做得差就给惩罚，慢慢形成条件反射。核心是之前提到的 $$R_t^{socialnav}$$ 奖励函数，简化理解就是：

**加分项：**
- 靠近目标（$$-\beta_d \Delta_d$$，距离目标越近，加分越多）
- 成功到达目标（$$\beta_{succ} \cdot I_{succ}$$，直接加大额奖励）

**扣分项：**
- 无意义动作（$$-r_{slack}$$，比如原地打转）
- 碰撞（$$-r_{coll}$$，撞墙或撞人扣大分）
- 离人太近（$$-r_{prox}$$，距离小于 2 米扣分，越近扣越多）
- 挡人类轨迹（$$-r_{traj}$$，挡住别人要走的路扣分）

通过不断迭代训练（**7500 万步**），主策略网络会逐渐学会"**平衡效率和合规**"—— 既不会为了快而撞人，也不会为了合规而绕远路。

#### 4. 与时空预认知模块的协作关系

**主策略网络：** 负责"**当下该做什么动作**"（比如现在走还是转）

**时空预认知模块：** 负责"**未来会发生什么**"（比如人类会走到哪）

**协作逻辑：** 主策略网络根据"当下状态 + 未来预测"做决策 —— 比如当下离人 3 米（安全），但预测 1 秒后会到 1.5 米（危险），主策略网络就会提前调整轨迹，而不是等进入危险区再反应。

---

主策略网络包含两个核心组件：
1. **状态编码器（State Encoders）**：从观测中提取视觉与时序特征
2. **社交认知惩罚（SCP）**：促进社交合规性的惩罚机制

#### 网络架构（技术细节）

主策略网络的处理流程分为四个步骤，具体技术实现如下：

##### 第一步：输入编码（Linear Encoder + Visual Encoder）

**点目标 + GPS+Compass → 线性编码器（Linear Encoder）：**
- 点目标的坐标（如 $g_a \in Q$）和 GPS 定位的机器人当前位置（$q_a \in Q$）
- 先计算"目标相对距离与方向"
- 通过线性变换转化为低维特征向量 $f_{goal}$

**深度图像 → 视觉编码器（Visual Encoder：ResNet-50）：**
- 通过预训练的 **ResNet-50 模型 [51]** 提取视觉特征 $f_{depth}$

##### 第二步：时序建模（Recurrent State Encoder：2 层 LSTM）

**输入：** 将线性编码器输出的 $f_{goal}$ 和 ResNet-50 输出的 $f_{depth}$ 拼接，得到融合特征：
$$
f_{fusion} = [f_{goal}, f_{depth}]
$$

**输出：** LSTM 会输出两个关键结果：
1. **时序特征 $h_t$**：捕捉"前 t 步环境变化"的动态信息
2. **潜在变量 $\delta_R$**：LSTM 的隐藏状态，其核心作用是"将主网络的时序记忆传递给 SPM"，让 SPM 能基于主网络的"观察记忆"学习辅助任务

##### 第三步：输出决策（Actor Head + Value Head）

时序特征 $h_t$ 会输入到两个"头网络"：

**Actor Head（动作头）：**
- 基于 $h_t$ 输出机器人的下一步动作，决定了机器人的实际导航路径 $\tau_a$

**Value Head（价值头）：**
- 基于 $h_t$ 预测当前动作的"预期奖励" $V(h_t)$，辅助强化学习的训练（PPO 算法需要通过"预测奖励"与"实际奖励"的差异更新策略）

##### 第四步：主损失 $L_{main}$ 的来源

主策略网络的训练目标是"让机器人既快又安全地抵达目标"，其损失 $L_{main}$ 来自 **DD-PPO 算法 [53]** 的 PPO 损失。

**PPO 损失的核心逻辑：** "约束策略更新的幅度，避免更新过快导致不稳定"

其简化公式为：

$$
L_{PPO} = \mathbb{E}_{\hat{\pi}} \left[ \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a), \text{clip}\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}, 1-\epsilon, 1+\epsilon \right) A^{\pi_{\theta_{old}}}(s,a) \right) \right]
$$

其中：
- $\pi_{\theta}(a|s)$ 是当前策略（主网络）在状态 $s$ 下选择动作 $a$ 的概率
- $\pi_{\theta_{old}}(a|s)$ 是上一轮策略的概率（用于约束更新幅度）
- $A^{\pi_{\theta_{old}}}(s,a)$ 是优势函数（衡量"当前动作比平均动作好多少"，与奖励函数 $R_t^{socialnav}$ 直接相关，而 $R_t^{socialnav}$ 包含了 SCP 惩罚，因此 SCP 会间接影响 $L_{main}$）

**简言之，** $L_{main}$ 是"主策略网络决策质量的量化指标"：动作越接近"抵达目标 + 遵守社交规则"，$L_{main}$ 越小。

#### PointNav 奖励函数

训练过程中，策略的更新由一个鼓励"达成目标"行为的奖励函数引导。每个时间步 $t$ 采用经典的 **PointNav 奖励函数**：

$$
R_t^{pointnav} = -\beta_d \Delta_d - r_{slack} + \beta_{succ} \cdot I_{succ}
$$

其中：
- $\Delta_d$ 为机器人到目标的测地线距离变化量
- $r_{slack}$ 为防止不必要动作的步长惩罚项
- $I_{succ}$ 为导航成功的指示变量（成功时为 1，否则为 0）
- $\beta_d$ 与 $\beta_{succ}$ 为权重系数

#### 社交认知惩罚（SCP）

然而，**PointNav 奖励函数**未考虑动态环境与社交交互，无法满足社交导航（SocialNav）的需求。为此，本文引入**社交认知惩罚（SCP）**——一套用于促进机器人遵守社会规范的惩罚机制，具体包含以下三类惩罚：

##### 1. 障碍碰撞惩罚（Obstacle Collision Penalty）

该惩罚针对机器人与静态障碍或人类发生碰撞的行为，计算公式如下：

$$
r_{coll} = \beta_s \cdot I_{s\_coll} + \beta_h \cdot I_{h\_coll}
$$

其中：
- $I_{s\_coll}$ 和 $I_{h\_coll}$ 分别为表示"与静态障碍碰撞"和"与人类碰撞"的指示变量（发生碰撞时为 1，否则为 0）
- $\beta_s$ 和 $\beta_h$ 为对应的惩罚权重

##### 2. 人类距离惩罚（Human Proximity Penalty）

该惩罚确保机器人与人类保持安全距离，计算公式如下：

$$
r_{prox} = \sum_{i=1}^{N} \begin{cases}
\beta_{prox} \cdot \exp(-d_i^t) & \text{若 } d_i^t < 2.0 \text{ m} \\
0 & \text{若 } d_i^t \geq 2.0 \text{ m}
\end{cases}
$$

式中，$d_i^t = \|\tau_a(t) - \tau_i(t)\|$ 表示时间步 $t$ 时机器人与第 $i$ 个人类的欧氏距离。当 $d_i^t$ 减小时，惩罚呈指数增长，以促使机器人主动避开人类；当机器人距离目标不足 2.0 米时，该惩罚自动取消。

##### 3. 轨迹阻碍惩罚（Trajectory Obstruction Penalty）

该惩罚用于阻止机器人阻碍人类未来 $H$ 步的轨迹。它通过同时考虑"当前与未来位置"预判潜在阻碍，且对"早期轨迹重叠"的惩罚更重，计算公式如下：

$$
r_{traj} = \sum_{k=t+1}^{t+H} \sum_{i=1}^{N} \begin{cases}
\beta_{traj} \cdot \frac{1}{k-t+1} & \text{若 } d_{traj-i}^k < 0.05 \text{ m} \\
0 & \text{若 } d_{traj-i}^k \geq 0.05 \text{ m}
\end{cases}
$$

其中：
- $d_{traj-i}^k = \|\tau_a(k) - \tilde{\tau}_i(k)\|$ 表示时间步 $k$ 时机器人与第 $i$ 个人类"未来轨迹"的距离
- $\frac{1}{k-t+1}$ 为时间衰减因子，确保距离当前时间越近的轨迹重叠，惩罚权重越大
- 当机器人距离目标不足 2.0 米时，该惩罚同样取消

#### 总奖励函数

社交导航的总奖励函数为"目标导向奖励"与"社交认知惩罚"的差值：

$$
R_t^{socialnav} = R_t^{pointnav} - R_t^{scp}
$$

其中，$R_t^{scp}$ 为社交认知惩罚的总和：

$$
R_t^{scp} = r_{coll} + r_{prox} + r_{traj}
$$

主策略网络采用 **DD-PPO 算法**进行训练，优化目标为 PPO 损失 $L_{main}$。

### 时空预知模块（Spatial-Temporal Precognition Module）

**SPM 的核心作用：** "利用主网络的记忆（$\delta_R$），学习与人类相关的'经验'，辅助主网络更聪明地决策"。

#### 模块架构与输入设计

**时空预认知模块（Spatial-Temporal Precognition Module）**的核心输入完全来自主策略网络的"中间处理结果"，没有额外新增传感器数据。核心目的是"**复用特征、节省计算**"，同时让"预判能力"和"决策能力"基于同一套环境理解，避免信息脱节。

##### 一、核心输入：主策略网络的状态编码 $\delta_R$

**来源：** 主策略网络中"**ResNet-50 编码器 + 2 层 LSTM**"的输出结果。

- 先由 **ResNet-50** 把"深度图像"翻译成"视觉特征向量"（比如"前方 3 米有人类、左侧 2 米是柜子"）
- 再经 **2 层 LSTM** 处理"时序依赖"（比如"这个人前 3 步朝我走，速度 0.5 米/秒"）
- 最终输出的 $\delta_R$ 是"浓缩了当前环境 + 历史变化"的高维数字特征，相当于主策略网络整理好的"**环境情报汇总**"

**作用：** 时空预认知模块直接用这个"情报汇总"做预测，不用再重复处理原始深度图像，既高效又能保证"预判和决策基于同一套环境理解"（比如主策略网络认为"那是个人"，预认知模块不会误判为"障碍物"）。

##### 二、辅助输入：Auxiliary Information（$S_R, N, P$）

这部分是从 $\delta_R$ 中解析出的"具体结构化信息"，不是额外输入，而是对 $\delta_R$ 的"拆解和明确"，方便模块针对性预测：

- **$S_R$（Spatial Relationship）**：人类与智能体的"相对空间关系"，比如"人类在智能体前方 30 度、距离 2.5 米"，从 $\delta_R$ 中的视觉特征和目标坐标推导而来
- **$N$（Number of Humans）**：当前环境中检测到的人类数量（初始值来自主策略网络的初步特征解析，后续会被辅助任务的 classifier 优化）
- **$P$（Human Positions）**：人类相对于智能体的"当前位置坐标"（比如"人类 A 在 $(x=2.3, y=1.5)$ 米处"），同样从 $\delta_R$ 中提取的视觉特征回归得到

**简单说：** 核心输入是"浓缩情报" $\delta_R$，辅助输入是"拆解后的具体情报" $S_R, N, P$，两者都来自主策略网络，没有额外传感器开销。

##### 三、Auxiliary Information 的具体来源

这三个信息不是"凭空产生"，而是主策略网络处理原始数据后，从"视觉特征 + 时序特征"中解析出来的，流程如下：

1. **原始数据**（深度图像 + 相对目标坐标）→ **ResNet-50** → 提取"静态视觉特征"（包含障碍物、人类的轮廓、距离信息）
2. **静态视觉特征** → **2 层 LSTM** → 融合"时序特征"（人类移动趋势、环境变化）→ 生成 $\delta_R$
3. **从 $\delta_R$ 中进一步解析：**
   - 通过"空间关系提取器"得到 $S_R$（相对角度、距离）
   - 通过"初步人数检测"得到 $N$（比如先判断"有 2 个人类"，后续由 classifier 优化精度）
   - 通过"初步位置回归"得到 $P$（比如先粗略预测"人类在前方 2-3 米处"，后续由 regressor 优化坐标精度）

**关键结论：** Auxiliary Information 是从主策略网络的状态编码 $\delta_R$ 中解析出的结构化信息，不是额外传感器数据，也不是预定义的固定值——会随着 $\delta_R$ 的更新（每个时间步都更新）而动态变化，比如人类移动后，$S_R$ 和 $P$ 会实时更新。

##### 四、Classifier 和两个 Regressor 的具体对应

这三个组件是时空预认知模块的"核心工作单元"，分别对应 Falcon 框架的三个辅助任务，本质是"通过多任务学习，让模块同时掌握'数人数、定位置、猜轨迹'三种能力"，最终提升时空理解能力。

| 组件类型 | 对应辅助任务 | 核心作用 | 输入（均为 $\delta_R$） | 输出结果 | 损失函数 |
|---------|------------|---------|----------------------|---------|---------|
| **Classifier（分类器）** | 人数估计（Population Estimation） | 预测当前环境中"真实人类数量"（0-M） | 状态编码 $\delta_R$ | 每个可能人数的概率（比如"0 人：5%、1 人：30%、2 人：65%"） | 交叉熵损失 $\mathcal{L}_{count}$ |
| **Regressor 1（回归器）** | 位置估计（Position Estimation） | 精准回归"人类当前相对智能体的坐标" | 状态编码 $\delta_R$ | 每个人类的 $(x, y)$ 坐标（比如"人类 A：$(2.3, 1.5)$ 米"） | MSE 损失 $\mathcal{L}_{pos}$ |
| **Regressor 2（回归器）** | 轨迹预测（Trajectory Forecasting） | 预测"人类未来 $H$ 个时间步的坐标" | 状态编码 $\delta_R$ | 每个人类的未来位置序列（比如"人类 A：$t+1$ 步 $(2.1,1.6)$、$t+2$ 步 $(1.9,1.7)$"） | MSE 损失 $\mathcal{L}_{traj}$ |

**逐个拆解（通俗理解）：**

1. **Classifier（分类器）= "人数计数器"**
   - **任务本质：** 分类问题（比如"当前人数是 0、1、2、…、$M$"，$M$ 是最大预测人数，比如 6 人）
   - **工作逻辑：** 输入 $\delta_R$（包含当前环境的视觉 + 时序特征），输出"每个人数选项的概率"，最终选择概率最高的作为预测人数
   - **举个例子：** $\delta_R$ 中包含"两个移动的人形轮廓特征"，分类器会输出"2 人：90% 概率"，通过交叉熵损失（惩罚预测概率与真实人数的差异）不断优化，让"计数"更准确

2. **第一个 Regressor（位置估计回归器）= "实时定位仪"**
   - **任务本质：** 回归问题（预测连续的坐标值，不是离散类别）
   - **工作逻辑：** 输入 $\delta_R$，针对每个检测到的人类，输出精准的 $(x, y)$ 相对坐标
   - **核心价值：** 解决"视觉特征只能判断'有人类'，但不知道'具体在哪'"的问题——比如主策略网络知道"前方有人"，这个回归器能精准告诉它"在前方 2.3 米、偏右 10 度的位置"，为避障提供精确依据

3. **第二个 Regressor（轨迹预测回归器）= "未来预言家"**
   - **任务本质：** 序列回归问题（预测未来多个时间步的连续坐标）
   - **工作逻辑：** 输入 $\delta_R$（包含人类的历史移动趋势，比如"前 3 步朝智能体移动，速度 0.5 米/秒"），输出未来 $H$ 个时间步（比如 $H=5$）的人类位置序列
   - **核心价值：** 解决"只能看当下，看不到未来"的短视问题——比如现在人类在 3 米外，但回归器预测"1 秒后会到 1.8 米处"，主策略网络就能提前调整轨迹，而不是等靠近了才反应

**关键补充：三个组件的协同关系**

- **共享输入：** 都用主策略网络的 $\delta_R$，确保"基于同一套环境理解"做预测
- **结果互补：** 人数估计（classifier）确定"有多少人要预测"，位置估计（regressor1）确定"现在在哪"，轨迹预测（regressor2）确定"未来会到哪"
- **损失合并：** 三个组件的损失（$\mathcal{L}_{count} + \mathcal{L}_{pos} + \mathcal{L}_{traj}$）乘以权重 $\beta_{aux}$ 后，加入总损失函数（公式 12），一起优化——比如人数预测不准、位置回归偏差大，都会让总损失上升，倒逼模型同时提升三个任务的精度

**总结：核心逻辑链**

主策略网络生成"环境情报" $\delta_R$ → 拆解出辅助信息 $S_R, N, P$ → 时空预认知模块用 $\delta_R$ 驱动三个组件（classifier + 两个 regressor）→ 分别输出"人数、当前位置、未来轨迹" → 三个组件的损失共同优化模型 → 提升智能体的时空理解能力，最终帮助主策略网络做出更精准的导航决策。

---

根据论文第三章 3.3 节，SPM 包含 3 个辅助任务，每个任务的网络结构都是**"LSTM/BiLSTM + 自注意力（Self-Attention） + 分类器/回归器"**，且输入均依赖主网络的潜在变量 $\delta_R$。

#### 1. 人类计数估计（Human Count Estimation）——"数清楚周围有几个人"

**任务目标：** 估计场景中人类的总数

**输入：** 主网络的潜在变量 $\delta_R$（包含前几步的环境记忆，如"前 3 步看到 2 个人类轮廓"）

**网络结构：**
1. **LSTM 编码器：** 将 $\delta_R$ 编码为时序特征 $\Phi_R$
2. **自注意力层 [54]：** 以 $Q=K=V=\Phi_R$ 处理特征，目的是"聚焦与人类数量相关的关键记忆"（如"忽略墙的特征，重点关注人类轮廓的变化"），输出注意力特征 $A_t$
3. **分类器 $\phi_{count}$：** 基于 $A_t$ 预测"场景中人类数量为 $k$"的概率 $\hat{n}_k$：

$$
\hat{n}_k = \phi_{count}(A_t), \quad k \in \{0, 1, \ldots, M\}, M=6
$$

**损失函数：** 采用交叉熵损失（衡量"预测概率"与"真实数量"的差异）：

$$
L_{count} = -\sum_{k=0}^{M} n_k \log(\hat{n}_k)
$$

其中 $n_k$ 是"真实人类数量为 $k$"的指示变量（如真实有 2 人，则 $n_2=1$，其余 $n_k=0$）。

#### 2. 当前位置跟踪（Current Position Tracking）——"知道每个人在哪"

**任务目标：** 跟踪人类相对于机器人的二维位置

**输入：** 主网络的 $\delta_R$ + 场景中人类的真实数量 $N$（Oracle 信息，仅用于训练，推理时无需）

**网络结构：**
1. **LSTM 编码器：** 将 $\delta_R$ 和 $N$ 融合编码为特征 $\Phi_{R;N}$
2. **自注意力层：** 以 $Q=K=V=\Phi_{R;N}$ 处理，聚焦"人类位置相关的特征"（如"分辨'人类 A 在左前方'和'人类 B 在右后方'"），输出 $A_t$
3. **回归器 $\phi_{pos}$：** 基于 $A_t$ 预测第 $i$ 个人类的相对 2D 位置 $\hat{P}_i^t$：

$$
\hat{P}_i^t = \phi_{pos}(A_t)
$$

**损失函数：** 采用均方误差（MSE，衡量"预测位置"与"真实位置"的距离）：

$$
L_{pos} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^t - P_i^t\|^2
$$

其中：
- $P_i^t$ 是人类 $i$ 的真实位置
- $M$ 是"真实存在的人类"的掩码（如场景中只有 2 人，就只计算这 2 人的位置损失）

#### 3. 未来轨迹预测（Future Trajectory Forecasting）——"预判人类未来走哪"

**任务目标：** 预测人类未来多个时间步的轨迹

> **注：** 这是 SPM 中**最重要的任务**（实验证明其对性能提升最大），因"预测未来轨迹"需要处理更复杂的时序关系，所以用 **BiLSTM（双向 LSTM）**替代普通 LSTM。

**输入：** 主网络的 $\delta_R$ + 人类真实数量 $N$ + 当前人类位置 $P_i^t$

**网络结构：**
1. **BiLSTM 编码器：** 双向处理 $\delta_R$、$N$、$P_i^t$ 的融合信息，输出特征 $\Phi_{R;N;P}$（双向 LSTM 能同时利用"过去记忆"和"未来趋势"，更适合长时序预测）
2. **自注意力层：** 以 $Q=K=V=\Phi_{R;N;P}$ 处理，聚焦"人类运动趋势相关的特征"（如"人类 A 前 2 步朝电梯走，预判他会继续向电梯移动"），输出 $A_t$
3. **回归器 $\phi_{traj}$：** 基于 $A_t$ 预测未来 $H$ 步的人类轨迹 $\hat{P}_i^{t+1:t+H}$：

$$
\hat{P}_i^{t+1:t+H} = \phi_{traj}(A_t)
$$

**损失函数：** 采用 MSE（衡量"预测轨迹"与"真实轨迹"的差异）：

$$
L_{traj} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^{t+1:t+H} - P_i^{t+1:t+H}\|^2
$$

其中 $P_i^{t+1:t+H}$ 为第 $i$ 个人类未来 $H$ 步的真实轨迹。

#### 辅助损失与总损失

3 个任务的损失相加，得到 SPM 的辅助损失：

$$
L_{aux} = L_{count} + L_{pos} + L_{traj}
$$

训练过程中，主策略网络与辅助任务的优化同步进行，模型的总损失为"主策略损失"与"辅助损失"的加权和：

$$
L_{total} = \beta_{main} L_{main} + \beta_{aux} L_{aux}
$$

其中，$\beta_{main}$ 和 $\beta_{aux}$ 分别为主策略损失与辅助损失的权重系数。

### 实验与结果

![Falcon Experiment](/blog/2025/falcon-experiment.png)

#### 评估指标

**任务完成度指标：**
- **成功率（Success Rate, Suc.）**：机器人成功到达目标的比例
- **路径长度加权成功率（Success weighted by Path Length, SPL）**：考虑路径效率的成功率
- **时间长度加权成功率（Success weighted by Time Length, STL）**：考虑时间效率的成功率

**社交合规性指标：**
- **人机碰撞率（Human-Robot Collision Rate, H-Coll）**：机器人与人类发生碰撞的比例
- **个人空间合规率（Personal Space Compliance, PSC）**：机器人保持适当社交距离的比例

考虑到人类碰撞半径为 **0.3 米**、机器人碰撞半径为 **0.25 米**，本实验将个人空间合规（PSC）的距离阈值设定为 **1.0 米**。

#### 基线方法


##### Proximity-Aware

**Proximity-Aware** 是一个基于强化学习的社交导航方法，该方法通过两个辅助任务建模人类与机器人的距离和方向，能有效捕捉当前时刻的人机距离关系。

**核心思想：**
- 使用**当前时刻**的人机距离和相对方向作为状态输入
- 通过两个辅助任务学习人机距离关系：
  1. **距离预测任务**：预测机器人与人类的距离
  2. **方向预测任务**：预测人类相对于机器人的方向
- 奖励函数基于当前距离：$r_{proximity} = -1/d_t$，其中 $d_t$ 是当前时刻的距离

**工作原理：**
1. **状态表示：** $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向
2. **辅助任务：** 在训练过程中同时学习预测距离和方向，增强对当前人机关系的理解
3. **策略学习：** 使用 PPO 算法训练策略网络，学习在当前状态下选择最优动作

**局限性：**
- 只考虑**当前时刻**的距离和方向，无法预测人类未来的移动轨迹
- 当人类突然改变方向时，只能被动反应，容易出现"短视避障"
- 无法提前规划路径以避免与人类未来位置发生冲突

##### A* 算法

**A* 算法**是一个经典的静态路径规划算法，广泛应用于机器人导航和游戏AI中。

**核心思想：**
- 使用**启发式搜索**在静态地图上找到从起点到终点的最优路径
- 综合考虑**已走路径成本**（$g(n)$）和**预估剩余成本**（$h(n)$）
- 评估函数：$f(n) = g(n) + h(n)$，其中 $h(n)$ 通常是欧氏距离或曼哈顿距离

**工作原理：**
1. **初始化：** 将起点加入开放列表（open list）
2. **迭代搜索：**
   - 从开放列表中选择 $f(n)$ 值最小的节点
   - 将该节点移入关闭列表（closed list）
   - 检查该节点的所有邻居节点
   - 对于每个邻居节点，计算新的 $g$ 值，如果更优则更新
3. **终止条件：** 当目标节点被加入关闭列表时，回溯路径

**在社交导航中的应用：**
- 预先计算一条从起点到终点的**固定路径**
- 假设环境是**静态的**，不考虑动态人类的存在
- 当遇到人类时，需要重新规划路径

**局限性：**
- **无法适应动态环境**：预先确定的路径无法应对人类移动
- **易导致碰撞**：当人类移动到规划路径上时，机器人可能直接碰撞
- **需要全局地图**：算法需要完整的静态地图信息

##### ORCA 算法

**ORCA（Optimal Reciprocal Collision Avoidance）算法**是一个基于速度障碍的多智能体避障算法，可获取智能体的位置和速度"先知信息"（oracle access），以动态调整规划路径。

**核心思想：**
- 基于**速度障碍（Velocity Obstacle）**概念
- 每个智能体选择一个新的速度，使得在假设其他智能体也选择最优速度的情况下，避免碰撞
- 使用**线性规划**在速度空间中寻找可行速度

**工作原理：**
1. **速度障碍计算：**
   - 对于每个其他智能体，计算一个速度障碍区域
   - 该区域包含所有会导致碰撞的速度
   - 速度障碍是一个**圆锥形区域**

2. **ORCA 半平面：**
   - 将速度障碍转换为**ORCA 半平面**
   - 每个半平面定义了一个速度约束：$v \cdot n \geq u \cdot n$
   - 其中 $n$ 是半平面的法向量，$u$ 是参考速度

3. **线性规划求解：**
   - 在满足所有 ORCA 半平面约束的条件下，选择最接近期望速度的速度
   - 优化目标：$\min \|v - v_{pref}\|$，其中 $v_{pref}$ 是期望速度

**在社交导航中的应用：**
- 可以获取人类的位置和速度信息（oracle access）
- 实时计算速度障碍，动态调整机器人速度
- 假设所有智能体都遵循 ORCA 规则，实现**相互避让**

**局限性：**
- **假设运动不受限制**：ORCA 假设智能体可以在任意方向移动，但实际机器人可能有运动学约束（如非完整约束）
- **可能导致与静态障碍物碰撞**：算法主要关注动态避障，可能忽略静态障碍
- **需要精确的位置和速度信息**：在实际应用中，这些信息可能难以准确获取
- **计算复杂度**：当环境中智能体数量较多时，线性规划的计算成本较高


#### 实验设置

- **训练算法：** 强化学习智能体采用 **DD-PPO 算法 [53]** 训练，且所有模型使用相同超参数
- **随机性控制：** 每种算法均采用 3 个不同随机种子独立运行 3 次，最终结果取各指标的均值与标准差
- **模型初始化：** 模型初始化权重来自预训练的 **PointNav 模型 [57]**，并在社交导航任务上进行 **1000 万步微调**
- **训练硬件：** 训练过程使用 4 块 **Nvidia RTX 3090** 显卡，同时运行 8 个并行环境
- **数据集划分：** 模型在 **Social-HM3D 训练集**上训练，在 **Social-HM3D 测试集**和 **Social-MP3D 测试集**上测试
- **泛化评估：** **Social-MP3D** 的测试结果用于评估 Falcon 的**零样本泛化能力**

![Falcon Results](/blog/2025/falcon-results.png)

#### 实验结果与分析

##### 结论 1：具备未来感知能力的方法比静态方法和情境感知方法更高效、更安全

- **A*** 等静态路径规划算法会预先确定一条固定路径，无法适应动态环境，易导致人机碰撞（见图 4 (a)）
- **ORCA**、**Proximity-Aware** 等情境感知避障方法虽能通过调整路径避开当前人类与障碍物，但存在局限性：
  - 路径重规划需耗时，延迟响应会增加碰撞风险
  - **ORCA** 因假设运动不受限制，导致与静态障碍物碰撞（见图 4 (b)）
  - **Proximity-Aware** 因无法预判人类运动，短期调整失效，最终发生碰撞（见图 4 (c)）
- **Falcon** 能主动适应人类动态运动，高效抵达目标位置

##### 结论 2：辅助任务可提升模型性能，其中轨迹预测任务作用最显著

表 3 展示了不同辅助任务组合的实验结果。与 **PointNav 基线模型 [57]** 相比，单个辅助任务即可提升导航性能；其中，**轨迹预测任务（SPM.Traj）**效果最突出——将成功率从 **40.94%** 显著提升至 **54.00%**。这一结果证明，在社交导航中引入**显式轨迹预测**具有重要价值。

**表 3：Falcon 消融实验结果**

> **表格说明：** 仅使用 PointNav 算法 [57] 训练的模型作为基线；SPM.Count、SPM.Pos、SPM.Traj 分别指"人类计数估计""当前位置跟踪""未来轨迹预测"三个辅助任务；数据以百分比表示，"↑" 表示指标值越高越好，"↓" 表示指标值越低越好

| SPM. 计数（Count） | SPM. 位置（Pos） | SPM. 轨迹（Traj） | SCP | 成功率（Suc.）↑ | 路径长度加权成功率（SPL）↑ | 时间长度加权成功率（STL）↑ | 个人空间合规率（PSC）↑ | 人机碰撞率（H-Coll）↓ |
|-------------------|-----------------|------------------|-----|----------------|-------------------------|-------------------------|---------------------|---------------------|
| （无辅助任务） | （无辅助任务） | （无辅助任务） | 无 | 40.94 | 34.14 | 11.50 | 90.82 | 53.54 |
| ✓ | 无 | 无 | 无 | 51.43 | 51.42 | 51.16 | 90.53 | 46.46 |
| 无 | ✓ | 无 | 无 | 53.17 | 53.17 | 52.95 | 90.06 | 44.07 |
| 无 | 无 | ✓ | 无 | **54.00** | 53.99 | 53.92 | 89.46 | 43.88 |
| 无 | 无 | 无 | ✓ | 51.24 | 51.24 | 51.08 | 90.41 | 48.11 |
| ✓ | ✓ | ✓ | 无 | 53.63 | 53.63 | 53.40 | 89.33 | 44.89 |
| ✓ | ✓ | ✓ | ✓ | **55.15** | 55.15 | 54.94 | 89.56 | 42.96 |

##### 结论 3：SCP 与 SPM 协同互补，显著提升模型性能并加快训练收敛

表 3 显示，**SCP** 对模型性能提升至关重要，尤其在与 **SPM** 结合时效果更明显：

- **仅使用 SPM** 的三个辅助任务（计数、位置、轨迹）时，模型性能较单个辅助任务无显著提升（SPM 组合的成功率为 **53.63%**，而 SPM.Pos 为 **53.17%**、SPM.Traj 为 **54.00%**）
- **加入 SCP 后**，完整模型的成功率提升至 **55.15%**，显著优于单独使用 SPM 的情况
- 图 5 显示，同时包含 **SPM** 和 **SCP** 的模型在训练过程中收敛速度更快（**1400K 步前**即可体现）

这些结果表明，若缺乏 **SCP** 的引导，**SPM** 的辅助任务无法有效整合——**SCP** 能帮助模型平衡各项任务，更充分地利用现有信息。

#### 局限性

**Falcon** 虽能实现较高的成功率，但 **Proximity-Aware**（成功率约 20%）在避障方面表现更优。这一现象暴露了现有评估指标的局限性：

- 在拥挤环境中，指标可能过度优先考虑**"社交舒适度"**，而忽视**"任务完成度"**
- 本基准目前未涵盖**"礼让"**等更高阶的人类社交行为

## 在 Autodl 上复现 Falcon

```bash
conda create -n falcon python=3.9 cmake=3.14.0

# 在base环境安装mamba（仅需一次）
conda install mamba -n base -c conda-forge -y
# 用mamba给falcon环境安装habitat-sim（核心：-n falcon 指定目标环境）
mamba install habitat-sim=0.3.1 withbullet headless -n falcon -c conda-forge -c aihabitat -y
conda activate falcon
```
接下来使用Robosense比赛所提供的代码而不是原生。原版 SocialNav 侧重于让人形机器人（Humanoid）在环境中跟随或者寻找人，它的 Reward（奖励函数）和 Episode（任务集）定义是通用的；RoboSense 比赛版 SocialNav 目标是从 A 点走到 B 点，同时避开动态的人，它使用的是 Social-HM3D 这个特定的数据集，里面的 NPC（路人）的轨迹是经过特殊生成的（基于 ORCA 算法），且有人口密度分级。
但是说白了就是 Robosense 是对其的发展，直接战未来：
```bash
git clone --recurse-submodules https://github.com/robosense2025/track2.git
mv track2 Falcon
cd Falcon/Falcon
# 安装Python依赖
pip install -e habitat-lab
pip install -e habitat-baselines
pip install -r requirements.txt
```
下面是数据集部分：
```bash
cd /root/Falcon/Falcon
mkdir -p data
mkdir -p /root/autodl-fs/habitat_data/scene_datasets
# 创建软链接
cd /root/Falcon/Falcon/data
ln -s /root/autodl-fs/habitat_data/scene_datasets ./scene_datasets
```

接下来是场景数据集：
点击[这个链接](https://my.matterport.com/settings/account/devtools)登录，但是我一直卡在最终的这个地方：
![](/blog/2025/MatterPort-login.png)
在页面上找到 "API Tokens" 部分。
点击 "Generate" (生成)。
Token ID 就是你的 --username。
Token Secret 就是你的 --password。
中途面临一个`ImportError: libEGL.so.1`的问题，直接`apt-get update && apt-get install -y libegl1-mesa libgl1-mesa-glx libgl1-mesa-dev`掉
首先下载HM3D：
```bash
python -m habitat_sim.utils.datasets_download --username xxx --password xxx --uids hm3d_minival
```
接下来下载任务数据 (Episode Datasets)，这是告诉机器人“从哪里走到哪里”的指令包。给的是 ™ HuggingFace 的链接
```python
#!/usr/bin/env python3
"""
使用 Python huggingface_hub 库下载数据，支持镜像站和SSL配置
"""
import os
import ssl
from pathlib import Path

# 配置镜像站
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# 如果遇到SSL证书问题，可以临时禁用验证（不推荐生产环境）
# ssl._create_default_https_context = ssl._create_unverified_context

try:
    from huggingface_hub import snapshot_download
    
    # 设置下载目录
    local_dir = Path(__file__).parent / "track2-social-navigation"
    local_dir.mkdir(parents=True, exist_ok=True)
    
    # 下载指定路径的文件
    # 注意：allow_patterns 和 ignore_patterns 同时使用时，需要正确配置
    snapshot_download(
        repo_id="robosense/datasets",
        repo_type="dataset",
        local_dir=str(local_dir),
        allow_patterns=["track2-social-navigation/**"],
        resume_download=True,
        token="token!!!",
    )
    
    print(f"下载完成，文件保存在: {local_dir}")
    
except ImportError:
    print("请先安装: pip install huggingface_hub")
except Exception as e:
    print(f"下载失败: {e}")
    print("\n如果遇到SSL证书错误，可以取消注释脚本中的SSL禁用行（第15行）")
```

然后是下载机器人动画 (Leg Animation)，即 Spot 机器人的走路动作文件
```bash
# 回到项目根目录
cd /root/Falcon/Falcon

# 创建目录并下载
mkdir -p data/robots/spot_data
wget https://github.com/facebookresearch/habitat-lab/files/12502177/spot_walking_trajectory.csv -O data/robots/spot_data/spot_walking_trajectory.csv
```
再之后是下载多智能体资产 (Multi-agent Assets)，这里包含人类模型（Humanoids）和一些仿真资产，而被 Git LFS 折磨惨了，选择不用`python -m habitat_sim.utils.datasets_download --uids habitat_humanoids hab3_bench_assets hab_spot_arm`而是额外写一个Python下：
```python
#!/usr/bin/env python3
"""
使用 huggingface_hub 直接下载数据集，避免 Git LFS 问题
实现与 habitat_sim.utils.datasets_download 相同的功能
"""
import os
import sys
from pathlib import Path

# 配置镜像站（可选）
os.environ["HF_ENDPOINT"] = os.environ.get("HF_ENDPOINT", "https://hf-mirror.com")

try:
    from huggingface_hub import snapshot_download
except ImportError:
    print("错误：请先安装 huggingface_hub")
    print("运行: pip install huggingface_hub")
    sys.exit(1)

# 数据集配置（从 habitat_sim 的数据源配置中提取）
DATASETS = {
    "habitat_humanoids": {
        "repo_id": "ai-habitat/habitat_humanoids",
        "version": "main",
        "link_path": "humanoids/humanoid_data",
        "version_dir": "habitat_humanoids",
    },
    "hab3_bench_assets": {
        "repo_id": "ai-habitat/hab3_bench_assets",
        "version": "main",
        "link_path": "hab3_bench_assets",
        "version_dir": "hab3_bench_assets",
    },
    "hab_spot_arm": {
        "repo_id": "ai-habitat/hab_spot_arm",
        "version": "v2.0",
        "link_path": "robots/hab_spot_arm",
        "version_dir": "hab_spot_arm",
    },
}

# 默认数据路径（相对于执行脚本时的当前工作目录）
# 与 habitat_sim.utils.datasets_download 的行为一致
DEFAULT_DATA_PATH = Path("./data").resolve()


def download_dataset(uid: str, data_path: Path = None, token: str = None):
    """
    下载指定的数据集
    
    Args:
        uid: 数据集唯一标识符
        data_path: 数据存储根目录（默认为 ./data）
        token: HuggingFace token（如果需要认证）
    """
    if uid not in DATASETS:
        print(f"错误：未知的数据集 ID: {uid}")
        print(f"支持的数据集: {', '.join(DATASETS.keys())}")
        return False
    
    if data_path is None:
        data_path = DEFAULT_DATA_PATH
    
    data_path = Path(data_path).resolve()
    
    config = DATASETS[uid]
    version_dir = data_path / "versioned_data" / config["version_dir"]
    link_path = data_path / config["link_path"]
    
    print(f"\n{'='*60}")
    print(f"下载数据集: {uid}")
    print(f"仓库: {config['repo_id']}")
    print(f"版本目录: {version_dir}")
    print(f"符号链接: {link_path}")
    print(f"{'='*60}\n")
    
    # 检查是否已存在
    if version_dir.exists():
        print(f"警告：版本目录已存在: {version_dir}")
        response = input("是否删除并重新下载？(y/n): ").strip().lower()
        if response == 'y':
            import shutil
            shutil.rmtree(version_dir)
            if link_path.exists() or link_path.is_symlink():
                if link_path.is_symlink():
                    link_path.unlink()
                else:
                    shutil.rmtree(link_path)
        else:
            print("跳过下载")
            return True
    
    # 创建版本目录
    version_dir.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # 使用 huggingface_hub 下载
        print(f"开始下载 {config['repo_id']}...")
        snapshot_download(
            repo_id=config["repo_id"],
            repo_type="dataset",
            local_dir=str(version_dir),
            local_dir_use_symlinks=False,
            revision=config["version"],
            token=token,
            resume_download=True,
        )
        print(f"下载完成: {version_dir}")
        
        # 创建符号链接
        link_path.parent.mkdir(parents=True, exist_ok=True)
        if link_path.exists() or link_path.is_symlink():
            if link_path.is_symlink():
                link_path.unlink()
            else:
                import shutil
                shutil.rmtree(link_path)
        
        link_path.symlink_to(version_dir, target_is_directory=True)
        print(f"符号链接已创建: {link_path} -> {version_dir}")
        
        print(f"\n{'='*60}")
        print(f"数据集 {uid} 下载成功！")
        print(f"源目录: {version_dir}")
        print(f"符号链接: {link_path}")
        print(f"{'='*60}\n")
        return True
        
    except Exception as e:
        print(f"下载失败: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """主函数"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="使用 huggingface_hub 下载 Habitat 数据集（无需 Git LFS）"
    )
    parser.add_argument(
        "--uids",
        nargs="+",
        default=["habitat_humanoids", "hab3_bench_assets", "hab_spot_arm"],
        help="要下载的数据集 ID 列表（默认: habitat_humanoids hab3_bench_assets hab_spot_arm）",
    )
    parser.add_argument(
        "--data-path",
        type=str,
        default=None,
        help=f'数据存储根目录（默认: {DEFAULT_DATA_PATH}）',
    )
    parser.add_argument(
        "--token",
        type=str,
        default=None,
        help="HuggingFace token（如果需要认证）",
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="列出所有支持的数据集",
    )
    
    args = parser.parse_args()
    
    if args.list:
        print("支持的数据集:")
        for uid, config in DATASETS.items():
            print(f"  {uid}: {config['repo_id']} (版本: {config['version']})")
        return
    
    data_path = Path(args.data_path) if args.data_path else DEFAULT_DATA_PATH
    
    print(f"数据路径: {data_path}")
    print(f"要下载的数据集: {', '.join(args.uids)}\n")
    
    success_count = 0
    for uid in args.uids:
        if download_dataset(uid, data_path, args.token):
            success_count += 1
    
    print(f"\n总计: {success_count}/{len(args.uids)} 个数据集下载成功")
    
    if success_count < len(args.uids):
        sys.exit(1)


if __name__ == "__main__":
    main()
```

对了还不能忘记 [pretrained_models](https://drive.google.com/drive/folders/1Bx1L9U345P_9pUfADk3Tnj7uK01EpxZY)，但是它很烦是Google云盘里的，好在只有243MB，可以下下来之后再传到autodl上去

万事俱备，开始运行评测脚本：
```bash
python -u -m habitat_baselines.eval --config-name=social_nav_v2/falcon_hm3d.yaml
```


运行后立即遇到了 **OpenGL 上下文创建失败**的问题：`GL::Context: cannot retrieve OpenGL version: GL::Renderer::Error::InvalidValue`

**问题分析**：habitat-sim 试图在 AutoDL 容器里创建一个图形渲染上下文，但是失败了。随后的 BrokenPipeError 和 core dumped 只是因为主进程试图跟一个已经崩溃的渲染进程通信导致的后果。这个问题通常是 **EGL（无头渲染接口）配置不当**引起的。在 AutoDL 这种无显示器的服务器上，必须强制指定渲染模式为无头模式。

虽然系统确实找到了 NVIDIA 显卡（found 3 EGL devices, choosing EGL device 0），但是在创建 OpenGL 上下文时失败了。

**初步尝试**：先尝试设置简单的环境变量：
```bash
export EGL_PLATFORM=surfaceless
export MAGNUM_LOG=verbose 
export HABITAT_SIM_LOG=verbose
```

但问题依然存在，出现了内存错误：`malloc_consolidate(): unaligned fastbin chunk detected`，随后进程崩溃。

简单的环境变量设置不够，需要更完整的 NVIDIA EGL 配置。编写了一个脚本来强制使用 NVIDIA 的 EGL 库：

**为什么需要强制使用 NVIDIA 库？Mesa vs NVIDIA 冲突详解**：
1. **Mesa 是什么**：
   - Mesa 是开源的 OpenGL/EGL/Vulkan 实现
   - 提供软件渲染（纯 CPU）或通过 DRI 使用集成显卡（Intel/AMD）
   - 是 Linux 发行版的默认图形库
   
2. **NVIDIA 驱动**：
   - NVIDIA 提供专有的硬件加速 OpenGL/EGL 实现
   - 直接访问 NVIDIA GPU，性能远高于 Mesa 软件渲染
   - 库文件：`libEGL_nvidia.so.0`、`libGLX_nvidia.so.0`
   
3. **冲突原因**：
   - Linux 动态链接器默认按库搜索路径顺序加载库
   - 系统通常先找到 Mesa 的 `libEGL.so.1`（在 `/usr/lib/x86_64-linux-gnu/`）
   - Mesa 无法访问 NVIDIA GPU，只能使用 CPU 或集成显卡
   - 导致 habitat-sim 无法创建硬件加速的 OpenGL 上下文
   
4. **为什么先安装 Mesa？**：
   - 解决 `ImportError: libEGL.so.1` 缺失问题（提供符号链接）
   - 但 Mesa 只是"占位符"，实际渲染需要 NVIDIA 库
   
5. **解决方案**：
   - 使用 `LD_PRELOAD` 强制优先加载 NVIDIA 库
   - 设置 `__EGL_VENDOR_LIBRARY_FILENAMES` 指向 NVIDIA 配置
   - 确保子进程也继承这些设置（多进程环境）

```bash
#!/bin/bash
# 强制使用 NVIDIA EGL 运行 Habitat-Sim

echo "=== 设置 NVIDIA EGL 环境 ==="

# 1. 强制指定 NVIDIA 的 EGL 配置
export __EGL_VENDOR_LIBRARY_FILENAMES=/usr/share/glvnd/egl_vendor.d/10_nvidia.json

# 2. 设置无头模式
export EGL_PLATFORM=surfaceless

# 3. 确保使用 NVIDIA 的 GL 库（GLX用于OpenGL）
export __GLX_VENDOR_LIBRARY_NAME=nvidia

# 4. 设置额外的 EGL 相关环境变量，确保子进程也能正确初始化
export EGL_DEVICE_ID=0
export __GL_SYNC_TO_VBLANK=0

# 5. 确保 NVIDIA 驱动相关的环境变量被设置
export __NVIDIA_BUG_REPORT=0

# 6. 使用 LD_PRELOAD 强制加载 NVIDIA 的 EGL 库，确保子进程也能正确初始化
# 查找 NVIDIA 库的实际位置
NVIDIA_EGL_LIB=""
NVIDIA_GLX_LIB=""

# 尝试多个可能的库位置
for egl_lib in /lib/x86_64-linux-gnu/libEGL_nvidia.so.0 /usr/lib/x86_64-linux-gnu/libEGL_nvidia.so.0; do
    if [ -f "$egl_lib" ]; then
        NVIDIA_EGL_LIB="$egl_lib"
        break
    fi
done

for glx_lib in /usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0 /usr/lib/libGLX_nvidia.so.0; do
    if [ -f "$glx_lib" ]; then
        NVIDIA_GLX_LIB="$glx_lib"
        break
    fi
done

# 如果找到了库，设置 LD_PRELOAD
if [ -n "$NVIDIA_EGL_LIB" ] && [ -n "$NVIDIA_GLX_LIB" ]; then
    export LD_PRELOAD="${NVIDIA_EGL_LIB}:${NVIDIA_GLX_LIB}"
    echo "  设置 LD_PRELOAD 强制使用 NVIDIA 库"
elif [ -n "$NVIDIA_EGL_LIB" ]; then
    export LD_PRELOAD="$NVIDIA_EGL_LIB"
    echo "  设置 LD_PRELOAD 强制使用 NVIDIA EGL 库"
fi

echo "环境变量设置："
echo "  __EGL_VENDOR_LIBRARY_FILENAMES=$__EGL_VENDOR_LIBRARY_FILENAMES"
echo "  EGL_PLATFORM=$EGL_PLATFORM"
echo "  EGL_DEVICE_ID=$EGL_DEVICE_ID"
echo "  LD_PRELOAD=$LD_PRELOAD"
echo "  __GLX_VENDOR_LIBRARY_NAME=$__GLX_VENDOR_LIBRARY_NAME"
echo ""

# 检查 NVIDIA 库是否存在
if [ -z "$NVIDIA_EGL_LIB" ]; then
    echo "警告：找不到 NVIDIA EGL 库，可能无法正常工作"
    echo "  请确保已安装 NVIDIA 驱动和相应的库文件"
fi

echo "开始运行命令..."
echo ""

# 获取 falcon conda 环境的 Python 路径
FALCON_PYTHON="/root/miniconda3/envs/falcon/bin/python"

# 如果第一个参数是 python，替换为 falcon 环境的 python
# 这样可以确保使用正确的 Python 环境和所有环境变量都被继承
if [ "$1" = "python" ] || [ "$1" = "python3" ]; then
    # 替换 python 为 falcon 环境的 python，保持其他参数
    shift
    exec "$FALCON_PYTHON" "$@"
else
    # 如果不是 python 命令，尝试使用 conda run（环境变量可能需要额外处理）
    # 这里我们假设用户知道自己在做什么
    exec "$@"
fi
```

**使用方法**：将脚本保存为 `run_with_nvidia_egl.sh` 并赋予执行权限，然后通过脚本运行评测命令：

```bash
./run_with_nvidia_egl.sh python -u -m habitat_baselines.eval \
    --config-name=social_nav_v2/falcon_hm3d.yaml \
    habitat_baselines.num_environments=1
```
这个脚本其实是最终版本了，在这之前还有下列一系列问题的，第一个问题：

```
ModuleNotFoundError: No module named 'habitat_baselines'
```

检查发现 `run_with_nvidia_egl.sh` 脚本使用了系统的 Python，而 `habitat_baselines` 模块安装在 `falcon` conda 环境中。

**修复方案**：修改 `run_with_nvidia_egl.sh`，自动使用 falcon conda 环境的 Python：

```bash
FALCON_PYTHON="/root/miniconda3/envs/falcon/bin/python"
if [ "$1" = "python" ] || [ "$1" = "python3" ]; then
    shift
    exec "$FALCON_PYTHON" "$@"
fi
```
修复 conda 环境后，程序可以导入模块，但在初始化多进程环境时崩溃：

```
malloc_consolidate(): unaligned fastbin chunk detected
ConnectionResetError: [Errno 104] Connection reset by peer
Aborted (core dumped)
```

问题在于多进程环境下，子进程无法正确继承 EGL 相关环境变量。虽然主进程设置了 `EGL_PLATFORM=surfaceless` 等环境变量，但子进程在初始化 EGL/OpenGL 时仍然失败。

**修复方案**：在 `run_with_nvidia_egl.sh` 中添加 `LD_PRELOAD` 强制加载 NVIDIA 库，并添加额外的 EGL 环境变量：

```bash
# 查找并设置 NVIDIA 库的 LD_PRELOAD
NVIDIA_EGL_LIB="/lib/x86_64-linux-gnu/libEGL_nvidia.so.0"
NVIDIA_GLX_LIB="/usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0"
export LD_PRELOAD="${NVIDIA_EGL_LIB}:${NVIDIA_GLX_LIB}"

# 额外的 EGL 环境变量确保子进程正确初始化
export EGL_DEVICE_ID=0
export __GL_SYNC_TO_VBLANK=0
export __NVIDIA_BUG_REPORT=0
```
修复 EGL 问题后，程序可以初始化 Simulator，但遇到数据集路径错误：

```
FileNotFoundError: Could not find dataset file `data/datasets/pointnav/social-hm3d/val`
```
坑爹的是，这个地方的报错和上面的一模一样，然后日志没有专门写出导一个文件中，轻易就超出了终端的最大长度，导致排查了好久，这里进行了最小元测试：
1. **test_egl_step_by_step.py**: 测试 EGL/OpenGL 初始化，验证环境变量、habitat_sim 导入、Simulator 创建、多进程环境
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
最小元测试：逐步验证每个配置环节
"""
import os
import sys
import json
import gzip

def test_step(step_num, step_name):
    """打印测试步骤标题"""
    print("\n" + "=" * 60)
    print(f"步骤 {step_num}: {step_name}")
    print("=" * 60)

def main():
    print("=" * 60)
    print("最小元测试：逐步验证配置")
    print("=" * 60)
    
    # 步骤1: 检查数据集文件是否存在
    test_step(1, "检查数据集文件")
    dataset_file = "data/datasets/pointnav/hm3d/val/val.json.gz"
    if not os.path.exists(dataset_file):
        print(f"  [FAIL] 数据集文件不存在: {dataset_file}")
        return False
    print(f"  [OK] 数据集文件存在: {dataset_file}")
    
    # 步骤2: 检查数据集文件内容
    test_step(2, "检查数据集文件内容")
    try:
        with gzip.open(dataset_file, 'rt') as f:
            data = json.load(f)
        print(f"  [OK] 数据集文件可以读取")
        print(f"  Episodes 数量: {len(data.get('episodes', []))}")
        
        if len(data.get('episodes', [])) == 0:
            print(f"  [WARN] 数据集文件为空（没有episodes）")
            return False
        
        # 获取第一个episode的场景路径
        first_episode = data['episodes'][0]
        scene_id = first_episode.get('scene_id', '')
        print(f"  第一个episode的场景ID: {scene_id}")
        
    except Exception as e:
        print(f"  [FAIL] 读取数据集文件失败: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 步骤3: 检查场景文件是否存在
    test_step(3, "检查场景文件路径")
    if not scene_id:
        print(f"  [FAIL] 场景ID为空")
        return False
    
    # 场景ID可能是相对路径或绝对路径
    print(f"  数据集中的场景路径: {scene_id}")
    
    # 尝试多个可能的路径
    possible_paths = [
        scene_id,  # 原始路径
        os.path.join(os.getcwd(), scene_id),  # 相对于当前目录
        scene_id.replace('data/', ''),  # 去掉data前缀
    ]
    
    scene_found = False
    for path in possible_paths:
        if os.path.exists(path):
            print(f"  [OK] 场景文件存在: {path}")
            print(f"  文件大小: {os.path.getsize(path) / (1024*1024):.2f} MB")
            scene_found = True
            break
    
    if not scene_found:
        print(f"  [FAIL] 场景文件不存在")
        print(f"  尝试过的路径:")
        for path in possible_paths:
            print(f"    - {path}")
        
        # 检查场景目录结构
        print(f"\n  检查场景目录结构...")
        scene_dir = os.path.dirname(scene_id)
        if os.path.exists(scene_dir):
            print(f"  [OK] 场景目录存在: {scene_dir}")
            files = os.listdir(scene_dir)
            print(f"  目录中的文件: {files[:10]}")
        else:
            print(f"  [FAIL] 场景目录不存在: {scene_dir}")
            
            # 检查hm3d目录结构
            hm3d_base = "data/scene_datasets/hm3d"
            if os.path.exists(hm3d_base):
                print(f"\n  检查 {hm3d_base} 目录结构...")
                subdirs = os.listdir(hm3d_base)
                print(f"  子目录: {subdirs}")
                
                # 检查val目录
                val_dir = os.path.join(hm3d_base, "val")
                if os.path.exists(val_dir):
                    print(f"  [OK] val目录存在")
                    val_subdirs = os.listdir(val_dir)[:5]
                    print(f"  val目录中的前5个条目: {val_subdirs}")
                else:
                    print(f"  [FAIL] val目录不存在")
        
        return False
    
    # 步骤4: 检查配置文件
    test_step(4, "检查配置文件")
    
    # 检查数据集配置
    dataset_config = "habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml"
    if os.path.exists(dataset_config):
        print(f"  [OK] 数据集配置文件存在")
        with open(dataset_config, 'r') as f:
            content = f.read()
            if 'hm3d' in content:
                print(f"  [OK] 配置使用hm3d路径")
    else:
        print(f"  [FAIL] 数据集配置文件不存在")
        return False
    
    # 检查任务配置
    task_config = "habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml"
    if os.path.exists(task_config):
        print(f"  [OK] 任务配置文件存在")
        with open(task_config, 'r') as f:
            content = f.read()
            if 'social_nav_v2: hm3d' in content or 'social_nav_v2:hm3d' in content.replace(' ', ''):
                print(f"  [OK] 任务配置引用hm3d数据集")
    else:
        print(f"  [FAIL] 任务配置文件不存在")
        return False
    
    # 步骤5: 测试实际加载数据集
    test_step(5, "测试加载数据集（使用habitat API）")
    try:
        import habitat
        from omegaconf import DictConfig
        
        # 创建配置
        dataset_cfg = {
            'type': 'PointNav-v1',
            'split': 'val',
            'data_path': 'data/datasets/pointnav/hm3d/{split}/{split}.json.gz'
        }
        
        print(f"  尝试加载数据集...")
        # 这里只是测试配置，不实际加载（因为可能需要更多依赖）
        print(f"  [OK] 数据集配置格式正确")
        
    except Exception as e:
        print(f"  [WARN] 无法测试habitat API: {e}")
    
    print("\n" + "=" * 60)
    print("测试完成")
    print("=" * 60)
    
    return True

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
```
2. **test_minimal_step_by_step.py**: 最小元测试，验证数据集文件、场景文件路径、配置文件
3. **test_scene_paths.py**: 专门诊断场景路径问题
4. **test_complete_flow.py**: 完整流程测试
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
完整流程测试：模拟实际运行环境
"""
import os
import sys

def main():
    print("=" * 60)
    print("完整流程测试")
    print("=" * 60)
    
    # 测试1: 场景文件
    print("\n1. 测试场景文件路径...")
    scene_path = "data/scene_datasets/hm3d/val/00808-y9hTuugGdiq/y9hTuugGdiq.basis.glb"
    if os.path.exists(scene_path):
        print(f"   [OK] 场景文件存在: {scene_path}")
        print(f"   文件大小: {os.path.getsize(scene_path) / (1024*1024):.2f} MB")
    else:
        print(f"   [FAIL] 场景文件不存在: {scene_path}")
        return False
    
    # 测试2: 数据集文件
    print("\n2. 测试数据集文件...")
    dataset_path = "data/datasets/pointnav/hm3d/val/val.json.gz"
    if os.path.exists(dataset_path):
        print(f"   [OK] 数据集文件存在: {dataset_path}")
        # 数据集文件可能为空，但这不是致命错误
        print(f"   [WARN] 数据集文件可能为空，但程序可能从其他地方获取场景信息")
    else:
        print(f"   [FAIL] 数据集文件不存在: {dataset_path}")
        return False
    
    # 测试3: 配置文件
    print("\n3. 测试配置文件...")
    configs = [
        "habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml",
        "habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml",
    ]
    for config in configs:
        if os.path.exists(config):
            print(f"   [OK] {config}")
        else:
            print(f"   [FAIL] {config}")
            return False
    
    print("\n" + "=" * 60)
    print("所有基本检查通过！")
    print("=" * 60)
    print("\n现在可以尝试运行评估命令:")
    print("  ./run_with_nvidia_egl.sh python -u -m habitat_baselines.eval \\")
    print("    --config-name=social_nav_v2/falcon_hm3d.yaml \\")
    print("    habitat_baselines.num_environments=1")
    
    return True

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
```

中间的两步一方面是最小元测试的过程中通过**排除法**检查发现配置文件引用了 `social-hm3d`，但实际数据集目录是 `hm3d`。

**修复方案**：
1. 创建新的数据集配置文件 `habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml`，设置 `data_path: data/datasets/pointnav/hm3d/{split}/{split}.json.gz`
2. 修改 `habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml`，将数据集引用从 `social-hm3d` 改为 `hm3d`
修复数据集配置后，程序可以加载数据集，但场景文件路径错误：

```
AssertionError: ESP_CHECK failed: No Stage Attributes exists for requested scene 
'data/scene_datasets/hm3d/val/00808-y9hTuugGdiq/y9hTuugGdiq.basis.glb'
```

检查发现：
- `data/scene_datasets/hm3d` 是指向 `/root/autodl-fs/habitat_data/versioned_data/hm3d-0.2/hm3d` 的符号链接
- 实际目录中只有 `minival` 子目录，没有 `val` 子目录
- 场景文件实际存储在 `minival` 目录下
**修复方案**：在场景数据实际目录中创建符号链接：

```bash
cd /root/autodl-fs/habitat_data/versioned_data/hm3d-0.2/hm3d
ln -sf minival val
```

这样 `data/scene_datasets/hm3d/val/` 就可以正确访问到场景文件了。

最后程序总算成功启动：
- EGL/OpenGL 初始化成功（检测到 NVIDIA RTX 4090，OpenGL 3.0.0）
- 场景文件加载成功
- 多进程环境正常工作
- 评估进度正常（2/1087 episodes）

**注意**：运行过程中会出现大量 Debug 日志 `Can't project end-point to navmesh`，这是正常的调试信息，表示某些 episode 的坐标无效，程序会自动跳过，不影响评估结果。

