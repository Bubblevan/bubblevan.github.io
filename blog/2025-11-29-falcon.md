---
slug: falcon
title: Falcon
authors: bubblevan
tags: [social-navigation, reinforcement-learning, falcon]
---

来拜读**梁俊卫老师**和**龚泽颖学长**的工作，没准之后红鸟面试还能用上。

## 论文研读


为解决**社交导航（SocialNav）**中机器人**"短视避障"**和现有评估基准不真实的问题，论文提出**Falcon 框架**（一种基于强化学习的未来感知社交导航架构），通过**社交认知惩罚（SCP）**（含障碍碰撞、人类距离、轨迹阻碍三类惩罚）和**时空预知模块（SPM）**（含人类计数估计、当前位置跟踪、未来轨迹预测三个辅助任务）实现主动避障与社交合规；同时构建**SocialNav 基准**，包含**Social-HM3D**（844 个场景）和**Social-MP3D**（72 个场景）两个高真实感室内数据集，平衡人类密度与自然运动模式。

**实验结果：** Falcon 在该基准上实现**55% 的任务成功率**，同时保持约**90% 的个人空间合规率**，显著优于 **A***、**ORCA** 等规则算法及 **Proximity-Aware** 等 RL 方法，且具备**零样本泛化能力**。

### 问题背景

**社交导航（SocialNav）**要求机器人在人类共享环境中遵守社交规范并安全导航，但现有方案存在两大关键问题：

#### 1. 算法短视性

传统 **RL 方法**（如 **Proximity-Aware**）仅依赖当前环境信息，易出现**"短视避障"**，无法预判人类未来轨迹，导致碰撞或效率低下；规则类算法（A*/ORCA）或依赖全局地图，或无法动态适应人类运动。

**传统 RL 方法的基本流程：**

1. **状态表示：** 通常使用当前时刻的传感器数据（如深度图像、激光雷达）和人类当前位置作为状态输入
2. **奖励设计：** 设计基于**当前距离**的奖励函数，例如：
   - 当机器人与人类距离过近时给予惩罚
   - 当机器人成功到达目标时给予奖励
   - 考虑当前时刻的碰撞风险
3. **策略学习：** 通过强化学习（如 **PPO**、**A3C**）训练策略网络，学习在当前状态下选择最优动作
4. **动作执行：** 策略网络直接输出下一步动作（如前进、转向、停止）

**传统方法的局限性：**

- **仅考虑当前状态：** 只基于当前时刻的人类位置和距离进行决策，无法预测人类未来的移动轨迹
- **反应式避障：** 当人类突然改变方向时，机器人只能被动反应，容易出现"短视避障"（即只关注眼前障碍，导致后续路径不佳）
- **缺乏前瞻性：** 无法提前规划路径以避免与人类未来轨迹发生冲突，导致效率低下或碰撞风险增加

**Proximity-Aware** 是一个典型的基于当前距离的 RL 方法：

- **状态空间：** $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前时刻机器人与人类的距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向
- **奖励函数：** $r_t = r_{goal} + \alpha \cdot r_{proximity}$，其中：
  - $r_{goal}$ 是到达目标的奖励
  - $r_{proximity} = -1/d_t$ 是基于当前距离的惩罚（距离越近惩罚越大）
- **问题：** 这个奖励函数只考虑**当前时刻**的距离 $d_t$，无法考虑人类未来可能移动到的位置
假设机器人在走廊中需要绕过前方正在移动的人类。**Proximity-Aware**可能：
- 看到人类在左侧，选择向右避让
- 但人类可能正在向左移动，导致机器人向右避让后反而与人类未来位置冲突
- 需要多次调整，效率低下

#### 2. 基准不真实性

现有数据集（如 **iGibson**、**Habicrowd**）场景类型单一、人类行为简化（随机行走 / 无动画）、人类密度失衡，且常假设机器人可获取全局信息，与真实场景脱节。

### 核心创新

因此 **Falcon 框架**的核心贡献在于是**首个融合显式人类轨迹预测的未来感知 RL 架构**，通过 **SCP** 和 **SPM** 实现主动避障与社交合规。

- **一方面**，该框架引入**"社交认知惩罚"**（含轨迹阻碍惩罚），鼓励智能体主动规避潜在碰撞并遵守社交礼仪
- **另一方面**，框架搭载**"时空预知模块"**，该模块融入包含轨迹预测在内的社交感知辅助任务，以增强智能体在训练过程中对未来动态的理解

> **注：** 这里提到社交导航最初是在 **iGibson 社交导航挑战赛**提出，是 **PointNav**基础上增加了移动人类这一元素，那这个领域真的非常新颖了，还是这篇文章首创的**Habitat替代这一静态人形模型**。

### 相关工作

根据 **Related Works**，往期人类轨迹预测方法可分为三类：

1. **基于物理学的方法：** 此类方法从牛顿运动定律中推导显式动力学模型，用于轨迹预测
2. **基于学习的方法：** 聚焦于从观测到的历史轨迹中学习运动模式
3. **基于规划的方法：** 其核心目标是推理理性智能体的运动意图，通过理解智能体的目标及其决策过程来预测轨迹

**Falcon**借鉴了这些研究思路，提出的方法不仅能预测人类轨迹，还能将社交感知信息融入智能体的导航策略，从而确保智能体在动态场景中实现安全、高效的导航。

### 方法论

考虑这样一个社交导航任务：机器人 $a$ 在存在 $N$ 个动态人类（记为 $i \in \{1, \ldots, N\}$）的环境中导航。机器人从初始位形 $q_a \in Q$ 出发，需持续选择动作以生成一条通往目标位形 $g_a \in Q$ 的路径 $\tau_a$，同时避免与静态障碍及动态人类发生碰撞。

其总体目标可建模如下：

$$
\begin{aligned}
\tau_a &= \arg \min_{\tau \in T} \left( c_a(\tau) + \lambda_a c_a^s(\tau, \tau_{1:N}) \right) \\
\text{s.t.} \quad &A_a(\tau_a) \notin C_{obs}, \quad A_a(\tau_a) \cap A_i(\tau_i) = \emptyset, \\
&\tau_a(0) = q_a, \quad \tau_a(T) = g_a
\end{aligned}
$$

其中：
- $c_a$ 为引导机器人前往目标的路径成本
- $c_a^s$ 为考虑社会规范的成本项
- $A(\tau)$ 表示轨迹 $\tau$ 所占据的体积
- $C_{obs}$ 表示静态障碍
- $T$ 为任务回合结束时间
- $\lambda_a$ 为权重因子

约束条件确保机器人在到达目标前，不会与静态障碍或人类发生碰撞。

![Falcon Overview](/blog/2025/falcon-overview.png)

如上图，**主策略网络（Main Policy Network）**在每个时间步以**深度图像（depth image）**和**点目标（point goal）**为输入，直接输出机器人下一步的动作。该网络的训练结合了**点目标导航（PointNav）**的奖励与本文提出的**社交认知惩罚（Social Cognition Penalty, SCP）**。此外，主策略网络还搭配了**时空预知模块（Spatial-Temporal Precognition Module）**，该模块在训练过程中支持多个辅助任务的执行。

### 主策略网络（Main Policy Network）

主策略网络包含两个核心组件：
1. **状态编码器（State Encoders）**：从观测中提取视觉与时序特征
2. **社交认知惩罚（SCP）**：促进社交合规性的惩罚机制

#### 网络架构

主策略网络的处理流程分为四个步骤：

##### 第一步：输入编码（Linear Encoder + Visual Encoder）

针对 3 个输入中的"点目标"和"深度图像"，分别用专用编码器处理（GPS+Compass 数据需先转化为与点目标匹配的坐标信号，再进入线性编码器）：

**点目标 + GPS+Compass → 线性编码器（Linear Encoder）：**
- 点目标的坐标（如 $g_a \in Q$）和 GPS 定位的机器人当前位置（$q_a \in Q$）
- 先计算"目标相对距离与方向"
- 通过线性变换转化为低维特征向量 $f_{goal}$
- **目的：** 将"目标信息"压缩成主网络可处理的格式

**深度图像 → 视觉编码器（Visual Encoder：ResNet-50）：**
- 深度图像包含环境的空间结构信息（如人类、障碍的位置）
- 通过预训练的 **ResNet-50 模型 [51]** 提取视觉特征 $f_{depth}$
- **ResNet-50 的作用：** "从像素级深度数据中，识别出人类、障碍等关键对象的特征"，避免机器人"只看像素，不懂语义"

##### 第二步：时序建模（Recurrent State Encoder：2 层 LSTM）

机器人的导航是"连续时序任务"（如"前一步看到人类向左走，这一步需要预判他下一步的位置"），因此需要"记忆"过往的环境变化——这一步由 **2 层 LSTM [52]** 实现。

**输入：** 将线性编码器输出的 $f_{goal}$ 和 ResNet-50 输出的 $f_{depth}$ 拼接，得到融合特征：
$$
f_{fusion} = [f_{goal}, f_{depth}]
$$

**输出：** LSTM 会输出两个关键结果：
1. **时序特征 $h_t$**：捕捉"前 t 步环境变化"的动态信息（如"人类过去 3 步的移动速度"）
2. **潜在变量 $\delta_R$**：LSTM 的隐藏状态，其核心作用是"将主网络的时序记忆传递给 SPM"，让 SPM 能基于主网络的"观察记忆"学习辅助任务

##### 第三步：输出决策（Actor Head + Value Head）

时序特征 $h_t$ 会输入到两个"头网络"，分别实现"动作决策"和"奖励预测"：

**Actor Head（动作头）：**
- 基于 $h_t$ 输出机器人的下一步动作（如"向前移动 0.5 米、右转 15°"）
- 这是机器人直接的"行为输出"，决定了机器人的实际导航路径 $\tau_a$

**Value Head（价值头）：**
- 基于 $h_t$ 预测当前动作的"预期奖励" $V(h_t)$
- **作用：** 辅助强化学习的训练（PPO 算法需要通过"预测奖励"与"实际奖励"的差异更新策略）

##### 第四步：主损失 $L_{main}$ 的来源

主策略网络的训练目标是"让机器人既快又安全地抵达目标"，其损失 $L_{main}$ 来自 **DD-PPO 算法 [53]** 的 PPO 损失。

**PPO 损失的核心逻辑：** "约束策略更新的幅度，避免更新过快导致不稳定"

其简化公式为：

$$
L_{PPO} = \mathbb{E}_{\hat{\pi}} \left[ \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a), \text{clip}\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}, 1-\epsilon, 1+\epsilon \right) A^{\pi_{\theta_{old}}}(s,a) \right) \right]
$$

其中：
- $\pi_{\theta}(a|s)$ 是当前策略（主网络）在状态 $s$ 下选择动作 $a$ 的概率
- $\pi_{\theta_{old}}(a|s)$ 是上一轮策略的概率（用于约束更新幅度）
- $A^{\pi_{\theta_{old}}}(s,a)$ 是优势函数（衡量"当前动作比平均动作好多少"，与奖励函数 $R_t^{socialnav}$ 直接相关，而 $R_t^{socialnav}$ 包含了 SCP 惩罚，因此 SCP 会间接影响 $L_{main}$）

**简言之，** $L_{main}$ 是"主策略网络决策质量的量化指标"：动作越接近"抵达目标 + 遵守社交规则"，$L_{main}$ 越小。

#### PointNav 奖励函数

训练过程中，策略的更新由一个鼓励"达成目标"行为的奖励函数引导。每个时间步 $t$ 采用经典的 **PointNav 奖励函数**：

$$
R_t^{pointnav} = -\beta_d \Delta_d - r_{slack} + \beta_{succ} \cdot I_{succ}
$$

其中：
- $\Delta_d$ 为机器人到目标的测地线距离变化量
- $r_{slack}$ 为防止不必要动作的步长惩罚项
- $I_{succ}$ 为导航成功的指示变量（成功时为 1，否则为 0）
- $\beta_d$ 与 $\beta_{succ}$ 为权重系数

#### 社交认知惩罚（SCP）

然而，**PointNav 奖励函数**未考虑动态环境与社交交互，无法满足社交导航（SocialNav）的需求。为此，本文引入**社交认知惩罚（SCP）**——一套用于促进机器人遵守社会规范的惩罚机制，具体包含以下三类惩罚：

##### 1. 障碍碰撞惩罚（Obstacle Collision Penalty）

该惩罚针对机器人与静态障碍或人类发生碰撞的行为，计算公式如下：

$$
r_{coll} = \beta_s \cdot I_{s\_coll} + \beta_h \cdot I_{h\_coll}
$$

其中：
- $I_{s\_coll}$ 和 $I_{h\_coll}$ 分别为表示"与静态障碍碰撞"和"与人类碰撞"的指示变量（发生碰撞时为 1，否则为 0）
- $\beta_s$ 和 $\beta_h$ 为对应的惩罚权重

##### 2. 人类距离惩罚（Human Proximity Penalty）

该惩罚确保机器人与人类保持安全距离，计算公式如下：

$$
r_{prox} = \sum_{i=1}^{N} \begin{cases}
\beta_{prox} \cdot \exp(-d_i^t) & \text{若 } d_i^t < 2.0 \text{ m} \\
0 & \text{若 } d_i^t \geq 2.0 \text{ m}
\end{cases}
$$

式中，$d_i^t = \|\tau_a(t) - \tau_i(t)\|$ 表示时间步 $t$ 时机器人与第 $i$ 个人类的欧氏距离。当 $d_i^t$ 减小时，惩罚呈指数增长，以促使机器人主动避开人类；当机器人距离目标不足 2.0 米时，该惩罚自动取消。

##### 3. 轨迹阻碍惩罚（Trajectory Obstruction Penalty）

该惩罚用于阻止机器人阻碍人类未来 $H$ 步的轨迹。它通过同时考虑"当前与未来位置"预判潜在阻碍，且对"早期轨迹重叠"的惩罚更重，计算公式如下：

$$
r_{traj} = \sum_{k=t+1}^{t+H} \sum_{i=1}^{N} \begin{cases}
\beta_{traj} \cdot \frac{1}{k-t+1} & \text{若 } d_{traj-i}^k < 0.05 \text{ m} \\
0 & \text{若 } d_{traj-i}^k \geq 0.05 \text{ m}
\end{cases}
$$

其中：
- $d_{traj-i}^k = \|\tau_a(k) - \tilde{\tau}_i(k)\|$ 表示时间步 $k$ 时机器人与第 $i$ 个人类"未来轨迹"的距离
- $\frac{1}{k-t+1}$ 为时间衰减因子，确保距离当前时间越近的轨迹重叠，惩罚权重越大
- 当机器人距离目标不足 2.0 米时，该惩罚同样取消

#### 总奖励函数

社交导航的总奖励函数为"目标导向奖励"与"社交认知惩罚"的差值：

$$
R_t^{socialnav} = R_t^{pointnav} - R_t^{scp}
$$

其中，$R_t^{scp}$ 为社交认知惩罚的总和：

$$
R_t^{scp} = r_{coll} + r_{prox} + r_{traj}
$$

主策略网络采用 **DD-PPO 算法**进行训练，优化目标为 PPO 损失 $L_{main}$。

### 时空预知模块（Spatial-Temporal Precognition Module）

**SPM 的核心作用：** "利用主网络的记忆（$\delta_R$），学习与人类相关的'经验'，辅助主网络更聪明地决策"。

根据论文第三章 3.3 节，SPM 包含 3 个辅助任务，每个任务的网络结构都是**"LSTM/BiLSTM + 自注意力（Self-Attention） + 分类器/回归器"**，且输入均依赖主网络的潜在变量 $\delta_R$。

#### 1. 人类计数估计（Human Count Estimation）——"数清楚周围有几个人"

**任务目标：** 估计场景中人类的总数

**输入：** 主网络的潜在变量 $\delta_R$（包含前几步的环境记忆，如"前 3 步看到 2 个人类轮廓"）

**网络结构：**
1. **LSTM 编码器：** 将 $\delta_R$ 编码为时序特征 $\Phi_R$
2. **自注意力层 [54]：** 以 $Q=K=V=\Phi_R$ 处理特征，目的是"聚焦与人类数量相关的关键记忆"（如"忽略墙的特征，重点关注人类轮廓的变化"），输出注意力特征 $A_t$
3. **分类器 $\phi_{count}$：** 基于 $A_t$ 预测"场景中人类数量为 $k$"的概率 $\hat{n}_k$：

$$
\hat{n}_k = \phi_{count}(A_t), \quad k \in \{0, 1, \ldots, M\}, M=6
$$

**损失函数：** 采用交叉熵损失（衡量"预测概率"与"真实数量"的差异）：

$$
L_{count} = -\sum_{k=0}^{M} n_k \log(\hat{n}_k)
$$

其中 $n_k$ 是"真实人类数量为 $k$"的指示变量（如真实有 2 人，则 $n_2=1$，其余 $n_k=0$）。

#### 2. 当前位置跟踪（Current Position Tracking）——"知道每个人在哪"

**任务目标：** 跟踪人类相对于机器人的二维位置

**输入：** 主网络的 $\delta_R$ + 场景中人类的真实数量 $N$（Oracle 信息，仅用于训练，推理时无需）

**网络结构：**
1. **LSTM 编码器：** 将 $\delta_R$ 和 $N$ 融合编码为特征 $\Phi_{R;N}$
2. **自注意力层：** 以 $Q=K=V=\Phi_{R;N}$ 处理，聚焦"人类位置相关的特征"（如"分辨'人类 A 在左前方'和'人类 B 在右后方'"），输出 $A_t$
3. **回归器 $\phi_{pos}$：** 基于 $A_t$ 预测第 $i$ 个人类的相对 2D 位置 $\hat{P}_i^t$：

$$
\hat{P}_i^t = \phi_{pos}(A_t)
$$

**损失函数：** 采用均方误差（MSE，衡量"预测位置"与"真实位置"的距离）：

$$
L_{pos} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^t - P_i^t\|^2
$$

其中：
- $P_i^t$ 是人类 $i$ 的真实位置
- $M$ 是"真实存在的人类"的掩码（如场景中只有 2 人，就只计算这 2 人的位置损失）

#### 3. 未来轨迹预测（Future Trajectory Forecasting）——"预判人类未来走哪"

**任务目标：** 预测人类未来多个时间步的轨迹

> **注：** 这是 SPM 中**最重要的任务**（实验证明其对性能提升最大），因"预测未来轨迹"需要处理更复杂的时序关系，所以用 **BiLSTM（双向 LSTM）**替代普通 LSTM。

**输入：** 主网络的 $\delta_R$ + 人类真实数量 $N$ + 当前人类位置 $P_i^t$

**网络结构：**
1. **BiLSTM 编码器：** 双向处理 $\delta_R$、$N$、$P_i^t$ 的融合信息，输出特征 $\Phi_{R;N;P}$（双向 LSTM 能同时利用"过去记忆"和"未来趋势"，更适合长时序预测）
2. **自注意力层：** 以 $Q=K=V=\Phi_{R;N;P}$ 处理，聚焦"人类运动趋势相关的特征"（如"人类 A 前 2 步朝电梯走，预判他会继续向电梯移动"），输出 $A_t$
3. **回归器 $\phi_{traj}$：** 基于 $A_t$ 预测未来 $H$ 步的人类轨迹 $\hat{P}_i^{t+1:t+H}$：

$$
\hat{P}_i^{t+1:t+H} = \phi_{traj}(A_t)
$$

**损失函数：** 采用 MSE（衡量"预测轨迹"与"真实轨迹"的差异）：

$$
L_{traj} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^{t+1:t+H} - P_i^{t+1:t+H}\|^2
$$

其中 $P_i^{t+1:t+H}$ 为第 $i$ 个人类未来 $H$ 步的真实轨迹。

#### 辅助损失与总损失

3 个任务的损失相加，得到 SPM 的辅助损失：

$$
L_{aux} = L_{count} + L_{pos} + L_{traj}
$$

训练过程中，主策略网络与辅助任务的优化同步进行，模型的总损失为"主策略损失"与"辅助损失"的加权和：

$$
L_{total} = \beta_{main} L_{main} + \beta_{aux} L_{aux}
$$

其中，$\beta_{main}$ 和 $\beta_{aux}$ 分别为主策略损失与辅助损失的权重系数。

### 实验与结果

![Falcon Experiment](/blog/2025/falcon-experiment.png)

#### 评估指标

**任务完成度指标：**
- **成功率（Success Rate, Suc.）**：机器人成功到达目标的比例
- **路径长度加权成功率（Success weighted by Path Length, SPL）**：考虑路径效率的成功率
- **时间长度加权成功率（Success weighted by Time Length, STL）**：考虑时间效率的成功率

**社交合规性指标：**
- **人机碰撞率（Human-Robot Collision Rate, H-Coll）**：机器人与人类发生碰撞的比例
- **个人空间合规率（Personal Space Compliance, PSC）**：机器人保持适当社交距离的比例

考虑到人类碰撞半径为 **0.3 米**、机器人碰撞半径为 **0.25 米**，本实验将个人空间合规（PSC）的距离阈值设定为 **1.0 米**。

#### 基线方法


##### Proximity-Aware

**Proximity-Aware** 是一个基于强化学习的社交导航方法，该方法通过两个辅助任务建模人类与机器人的距离和方向，能有效捕捉当前时刻的人机距离关系。

**核心思想：**
- 使用**当前时刻**的人机距离和相对方向作为状态输入
- 通过两个辅助任务学习人机距离关系：
  1. **距离预测任务**：预测机器人与人类的距离
  2. **方向预测任务**：预测人类相对于机器人的方向
- 奖励函数基于当前距离：$r_{proximity} = -1/d_t$，其中 $d_t$ 是当前时刻的距离

**工作原理：**
1. **状态表示：** $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向
2. **辅助任务：** 在训练过程中同时学习预测距离和方向，增强对当前人机关系的理解
3. **策略学习：** 使用 PPO 算法训练策略网络，学习在当前状态下选择最优动作

**局限性：**
- 只考虑**当前时刻**的距离和方向，无法预测人类未来的移动轨迹
- 当人类突然改变方向时，只能被动反应，容易出现"短视避障"
- 无法提前规划路径以避免与人类未来位置发生冲突

##### A* 算法

**A* 算法**是一个经典的静态路径规划算法，广泛应用于机器人导航和游戏AI中。

**核心思想：**
- 使用**启发式搜索**在静态地图上找到从起点到终点的最优路径
- 综合考虑**已走路径成本**（$g(n)$）和**预估剩余成本**（$h(n)$）
- 评估函数：$f(n) = g(n) + h(n)$，其中 $h(n)$ 通常是欧氏距离或曼哈顿距离

**工作原理：**
1. **初始化：** 将起点加入开放列表（open list）
2. **迭代搜索：**
   - 从开放列表中选择 $f(n)$ 值最小的节点
   - 将该节点移入关闭列表（closed list）
   - 检查该节点的所有邻居节点
   - 对于每个邻居节点，计算新的 $g$ 值，如果更优则更新
3. **终止条件：** 当目标节点被加入关闭列表时，回溯路径

**在社交导航中的应用：**
- 预先计算一条从起点到终点的**固定路径**
- 假设环境是**静态的**，不考虑动态人类的存在
- 当遇到人类时，需要重新规划路径

**局限性：**
- **无法适应动态环境**：预先确定的路径无法应对人类移动
- **易导致碰撞**：当人类移动到规划路径上时，机器人可能直接碰撞
- **需要全局地图**：算法需要完整的静态地图信息

##### ORCA 算法

**ORCA（Optimal Reciprocal Collision Avoidance）算法**是一个基于速度障碍的多智能体避障算法，可获取智能体的位置和速度"先知信息"（oracle access），以动态调整规划路径。

**核心思想：**
- 基于**速度障碍（Velocity Obstacle）**概念
- 每个智能体选择一个新的速度，使得在假设其他智能体也选择最优速度的情况下，避免碰撞
- 使用**线性规划**在速度空间中寻找可行速度

**工作原理：**
1. **速度障碍计算：**
   - 对于每个其他智能体，计算一个速度障碍区域
   - 该区域包含所有会导致碰撞的速度
   - 速度障碍是一个**圆锥形区域**

2. **ORCA 半平面：**
   - 将速度障碍转换为**ORCA 半平面**
   - 每个半平面定义了一个速度约束：$v \cdot n \geq u \cdot n$
   - 其中 $n$ 是半平面的法向量，$u$ 是参考速度

3. **线性规划求解：**
   - 在满足所有 ORCA 半平面约束的条件下，选择最接近期望速度的速度
   - 优化目标：$\min \|v - v_{pref}\|$，其中 $v_{pref}$ 是期望速度

**在社交导航中的应用：**
- 可以获取人类的位置和速度信息（oracle access）
- 实时计算速度障碍，动态调整机器人速度
- 假设所有智能体都遵循 ORCA 规则，实现**相互避让**

**局限性：**
- **假设运动不受限制**：ORCA 假设智能体可以在任意方向移动，但实际机器人可能有运动学约束（如非完整约束）
- **可能导致与静态障碍物碰撞**：算法主要关注动态避障，可能忽略静态障碍
- **需要精确的位置和速度信息**：在实际应用中，这些信息可能难以准确获取
- **计算复杂度**：当环境中智能体数量较多时，线性规划的计算成本较高


#### 实验设置

- **训练算法：** 强化学习智能体采用 **DD-PPO 算法 [53]** 训练，且所有模型使用相同超参数
- **随机性控制：** 每种算法均采用 3 个不同随机种子独立运行 3 次，最终结果取各指标的均值与标准差
- **模型初始化：** 模型初始化权重来自预训练的 **PointNav 模型 [57]**，并在社交导航任务上进行 **1000 万步微调**
- **训练硬件：** 训练过程使用 4 块 **Nvidia RTX 3090** 显卡，同时运行 8 个并行环境
- **数据集划分：** 模型在 **Social-HM3D 训练集**上训练，在 **Social-HM3D 测试集**和 **Social-MP3D 测试集**上测试
- **泛化评估：** **Social-MP3D** 的测试结果用于评估 Falcon 的**零样本泛化能力**

![Falcon Results](/blog/2025/falcon-results.png)

#### 实验结果与分析

##### 结论 1：具备未来感知能力的方法比静态方法和情境感知方法更高效、更安全

- **A*** 等静态路径规划算法会预先确定一条固定路径，无法适应动态环境，易导致人机碰撞（见图 4 (a)）
- **ORCA**、**Proximity-Aware** 等情境感知避障方法虽能通过调整路径避开当前人类与障碍物，但存在局限性：
  - 路径重规划需耗时，延迟响应会增加碰撞风险
  - **ORCA** 因假设运动不受限制，导致与静态障碍物碰撞（见图 4 (b)）
  - **Proximity-Aware** 因无法预判人类运动，短期调整失效，最终发生碰撞（见图 4 (c)）
- **Falcon** 能主动适应人类动态运动，高效抵达目标位置

##### 结论 2：辅助任务可提升模型性能，其中轨迹预测任务作用最显著

表 3 展示了不同辅助任务组合的实验结果。与 **PointNav 基线模型 [57]** 相比，单个辅助任务即可提升导航性能；其中，**轨迹预测任务（SPM.Traj）**效果最突出——将成功率从 **40.94%** 显著提升至 **54.00%**。这一结果证明，在社交导航中引入**显式轨迹预测**具有重要价值。

**表 3：Falcon 消融实验结果**

> **表格说明：** 仅使用 PointNav 算法 [57] 训练的模型作为基线；SPM.Count、SPM.Pos、SPM.Traj 分别指"人类计数估计""当前位置跟踪""未来轨迹预测"三个辅助任务；数据以百分比表示，"↑" 表示指标值越高越好，"↓" 表示指标值越低越好

| SPM. 计数（Count） | SPM. 位置（Pos） | SPM. 轨迹（Traj） | SCP | 成功率（Suc.）↑ | 路径长度加权成功率（SPL）↑ | 时间长度加权成功率（STL）↑ | 个人空间合规率（PSC）↑ | 人机碰撞率（H-Coll）↓ |
|-------------------|-----------------|------------------|-----|----------------|-------------------------|-------------------------|---------------------|---------------------|
| （无辅助任务） | （无辅助任务） | （无辅助任务） | 无 | 40.94 | 34.14 | 11.50 | 90.82 | 53.54 |
| ✓ | 无 | 无 | 无 | 51.43 | 51.42 | 51.16 | 90.53 | 46.46 |
| 无 | ✓ | 无 | 无 | 53.17 | 53.17 | 52.95 | 90.06 | 44.07 |
| 无 | 无 | ✓ | 无 | **54.00** | 53.99 | 53.92 | 89.46 | 43.88 |
| 无 | 无 | 无 | ✓ | 51.24 | 51.24 | 51.08 | 90.41 | 48.11 |
| ✓ | ✓ | ✓ | 无 | 53.63 | 53.63 | 53.40 | 89.33 | 44.89 |
| ✓ | ✓ | ✓ | ✓ | **55.15** | 55.15 | 54.94 | 89.56 | 42.96 |

##### 结论 3：SCP 与 SPM 协同互补，显著提升模型性能并加快训练收敛

表 3 显示，**SCP** 对模型性能提升至关重要，尤其在与 **SPM** 结合时效果更明显：

- **仅使用 SPM** 的三个辅助任务（计数、位置、轨迹）时，模型性能较单个辅助任务无显著提升（SPM 组合的成功率为 **53.63%**，而 SPM.Pos 为 **53.17%**、SPM.Traj 为 **54.00%**）
- **加入 SCP 后**，完整模型的成功率提升至 **55.15%**，显著优于单独使用 SPM 的情况
- 图 5 显示，同时包含 **SPM** 和 **SCP** 的模型在训练过程中收敛速度更快（**1400K 步前**即可体现）

这些结果表明，若缺乏 **SCP** 的引导，**SPM** 的辅助任务无法有效整合——**SCP** 能帮助模型平衡各项任务，更充分地利用现有信息。

#### 局限性

**Falcon** 虽能实现较高的成功率，但 **Proximity-Aware**（成功率约 20%）在避障方面表现更优。这一现象暴露了现有评估指标的局限性：

- 在拥挤环境中，指标可能过度优先考虑**"社交舒适度"**，而忽视**"任务完成度"**
- 本基准目前未涵盖**"礼让"**等更高阶的人类社交行为

## 在 Autodl 上复现 Falcon

```bash
conda create -n falcon python=3.9 cmake=3.14.0
conda activate falcon
```
