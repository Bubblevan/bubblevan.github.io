---
slug: 2025-11-19
title: 3D 场景理解工作
authors: [bubblevan]
tags: [depth-anything-3, 3d-vision, geometry-reconstruction, depth-map, ray-map, point-cloud, camera-pose, benchmark, bytedance]
---

# 3D 场景理解工作

## Depth Anything 3

[**Depth Anything 3 (DA3)**](https://github.com/ByteDance-Seed/depth-anything-3) 是字节跳动提出的 3D 视觉模型，核心目标是从**任意数量**（单张 / 多张 / 视频流）、**已知或未知相机姿态**的视觉输入中恢复空间一致的几何结构；其采用**单 Transformer 骨干网络**（如 vanilla DINOv2 编码器）和**深度-射线（depth-ray）表示**作为最小预测目标，避免复杂多任务学习，通过**师生训练范式**（教师模型用合成数据生成高质量伪标签指导学生模型）实现与 DA2 相当的细节度和泛化性；同时构建了涵盖相机姿态估计、任意视图几何重建、视觉渲染的 **Visual Geometry Benchmark**，在该基准上 DA3 刷新所有任务 SOTA，平均超越此前 SOTA 模型 VGGT **35.7%** 的相机姿态精度和 **23.6%** 的几何精度，且在单目深度估计任务上优于 DA2；此外，DA3 可通过微调扩展至前馈 **3D 高斯 splatting（3DGS）** 等下游任务，为通用 3D 感知提供基础模型支持。

<!-- truncate -->

### 架构

![DA3 Pipeline](/blog/2025/da3-pipeline.png)

| 模块 | 核心设计 | 作用 |
|------|---------|------|
| **单 Transformer 骨干** | 基于预训练 ViT（如 DINOv2），分 L_s（视图内注意力）和 L_g（交替跨视图 / 视图内）层，L_s:L_g=2:1 | 继承预训练特征，支持任意视图数（单图自动降级为单目） |
| **相机条件注入** | 每个视图前缀相机 token：已知姿态→MLP 编码（E_c (f,q,t)）；未知→共享可学习 token | 无缝处理有 / 无姿态输入，提供几何上下文 |
| **双 DPT 头（Dual-DPT）** | 共享重组模块（Reassemble），独立融合层 + 输出层，分别预测深度和射线 | 保证两任务特征交互，避免中间表示冗余，提升预测一致性 |

### 深度图（Depth Map）

在 **Depth Anything 3（DA3）** 的 3D 几何重建框架中，**深度图**是用于描述 "图像像素与相机之间物理距离" 的稠密矩阵，是构建 3D 结构的核心基础之一。

**定义与数学形式：** 深度图用符号 $D \in \mathbb{R}^{H \times W}$ 表示，其中 $H$ 和 $W$ 分别对应输入图像的高度和宽度，每个元素 $D(u,v)$ 代表图像中坐标 $(u,v)$ 的像素到相机光心的真实距离（单位通常为米），且与输入图像严格像素对齐。

**核心作用：** 深度图直接提供 "像素位置的远近信息"，是将 2D 图像映射到 3D 空间的关键桥梁 —— 结合相机姿态或射线信息，可通过 $P = t + D(u,v) \cdot d$（$t$ 为射线原点，$d$ 为射线方向）计算出该像素在世界坐标系中的 3D 坐标 $P$，进而生成完整点云。

**DA3 中的特性：** DA3 通过 **"师生训练范式"** 优化深度图质量 —— 教师模型（仅用合成数据训练）生成高质量伪深度标签，对齐真实场景的稀疏 / 噪声深度，确保深度图的细节完整性与几何一致性，避免近距区域（如物体表面）的距离预测偏差。


### 射线图（Ray Map）

**射线图**是 DA3 提出的创新表示方式，用于隐式编码 "相机观测方向与位置"，避免直接预测相机姿态（如旋转矩阵、平移向量）的复杂约束，是实现 **"最小建模"** 的关键设计。

**定义与数学形式：** 射线图用符号 $M \in \mathbb{R}^{H \times W \times 6}$ 表示，同样与输入图像像素对齐，6 个通道分为两组：
- **前 3 通道**（$M(:,:,:3)$）：**射线原点** $t \in \mathbb{R}^3$，代表该像素对应的相机光心在世界坐标系中的位置
- **后 3 通道**（$M(:,:,3:)$）：**射线方向** $d \in \mathbb{R}^3$，代表该像素从相机光心出发，指向世界空间目标点的单位向量（未归一化，保留投影尺度）

**核心作用：**
- **规避姿态预测约束：** 传统方法需直接预测相机外参（旋转矩阵 $R$ 需满足正交性），而射线图通过 "像素级射线" 隐式包含姿态信息，简化建模
- **反解相机参数：** 若需显式获取相机姿态，可从射线图推导 —— 相机中心 $t_c$ 为射线原点的平均值（$t_c = \frac{1}{H \times W} \sum M(h,w,:3)$），通过单应性矩阵 $H=KR$（DLT 算法求解）和 RQ 分解，进一步得到内参 $K$（上三角矩阵）和外参 $R$（正交矩阵）
- **生成 3D 点云：** 与深度图协同，通过 $P = t + D(u, v) \cdot d$ 直接计算 3D 点，无需额外任务目标（如点云、相机姿态），实现 "最小预测目标"

**DA3 中的特性：** 射线图与深度图通过 **"双 DPT 头"** 联合预测 —— 共享特征重组模块，独立融合层分别输出两者，确保射线方向与深度的空间一致性，避免多任务目标的纠缠（实验验证 "深度 + 射线" 组合的性能优于 "深度 + 点云 + 相机姿态" 等冗余方案）。

**详细解释：**

- **第一部分：射线原点（前 3 个通道）** 记录 "这条光线的起点"—— 也就是相机光心在 3D 空间中的位置（比如拍桌子时，手机所在的坐标：x=2 米、y=1 米、z=0.5 米）。所有像素的射线原点通常很接近（因为都来自同一台相机），所以射线图前 3 通道的数值差异很小，本质是 "相机位置的像素级体现"。

- **第二部分：射线方向（后 3 个通道）** 记录 "这条光线的指向"—— 比如 "桌子角像素" 的光线指向是 "向右下方 15°"，"桌子边缘像素" 的光线指向是 "向左下方 10°"。用 3 个数值（x/y/z 方向分量）描述这个指向，比如（0.2, -0.3, 0.1）就代表 "在 x 轴正方向、y 轴负方向、z 轴正方向有一定延伸"。

**为什么使用射线图？**

传统 3D 模型需要直接预测 "相机姿态"（比如相机朝哪个方向转、在哪个位置），但姿态计算有严格约束（比如旋转矩阵必须满足 "正交性"，算错一点就会严重偏差）。而射线图绕开了这个复杂问题：

- 它不直接算 "相机整体姿态"，而是给每个像素算 "专属光线"—— 这些光线天然包含了姿态信息（比如所有光线的原点平均下来就是相机中心，光线方向的规律就是相机朝向）
- 后续要显式获取相机姿态时，只需从射线图反推（比如用文档 1-44-46 节的方法：原点求平均得相机中心，方向算单应性矩阵得内参 / 外参），无需在训练时额外优化姿态目标

### 点云

**点云生成公式：** 3D 点坐标 = 射线原点 + 深度值 × 射线方向

**示例：** 比如某像素的射线原点是（2,1,0.5）、方向是（0.2,-0.3,0.1）、深度是 2 米，那么它的 3D 坐标就是（2+2×0.2, 1+2×(-0.3), 0.5+2×0.1）=（2.4, 0.4, 0.7）。所有像素的 3D 坐标拼起来，就是一张完整的点云（比如桌子的点云就是无数个 "桌子表面 3D 点" 组成的）。

**点云评估指标：** 把生成的点云和 "真实点云"（比如用 LiDAR 扫描的精准点云）对比：

- **Chamfer 距离**：点云之间的平均距离，越小越准
- **F1 分数**：点云的精度和召回率，越高越完整

### 实验

DA3 的实验核心围绕 **"统一视觉几何基准"** 展开，覆盖 **"相机姿态估计、几何重建、视觉渲染"** 三大任务，实验流程严格区分 **"数据准备、任务执行、指标评估"** 三阶段。

#### （1）实验基础：构建 Visual Geometry Benchmark

为避免不同任务评估标准不统一的问题，DA3 首先建立了标准化基准，明确实验的 **"输入 - 输出 - 评估"** 链路：

**基准覆盖范围：** 包含 5 个场景多样化的数据集（共 89+ 场景），涵盖合成数据、真实 LiDAR 数据、低清噪声数据，确保实验泛化性：

- **合成数据：** HiRoom（29 个室内场景，Blender 渲染，用于验证几何细节）
- **真实 LiDAR 数据：** ETH3D（11 个室内外场景，激光雷达采集，高分辨率）、DTU（22 个物体级场景，控制条件下的真实点云）、ScanNet++（20 个室内场景，iPhone LiDAR + 激光重建深度）
- **低清噪声数据：** 7Scenes（7 个室内场景，低分辨率 + 运动模糊，模拟真实复杂场景）

**基准流程：** 所有任务遵循 **"输入→模型预测→结果对齐→指标计算"** 的统一逻辑：

- **输入：** 随机采样图像（若图像数超 100 张，固定随机种子采样 100 张，保证实验可复现）
- **预测：** DA3 输出深度图、射线图、相机姿态（可选）
- **对齐：** 几何重建时用 RANSAC+evo 工具将预测姿态与真实姿态对齐，确保点云在同一坐标系
- **评估：** 按任务计算对应指标（姿态用 AUC3/AUC30，几何用 F1/Chamfer 距离，渲染用 PSNR/SSIM）

#### （2）核心任务实验流程（以 "几何重建" 为例）

用户关心的 **"图片流生成连续点云建模"**，正是 DA3 **"多视图 / 视频几何重建"** 任务的核心，实验流程在论文 7.1 节和 6.1 节明确：

**输入：** 支持 **"图片流（视频帧）"** 或 **"多视图图像集"**（无数量限制，单帧即单目，多帧即多视图）

**点云生成逻辑：**
- 模型先为每张图像预测 **"深度图 + 射线图"**（通过双 DPT 头联合输出，像素级对齐）
- 用公式 $P = t + D(u,v) \cdot d$（$t$ 为射线原点，$d$ 为射线方向，$D$ 为深度）计算每个像素的 3D 坐标，生成单帧稀疏点云
- 对图片流的连续帧，用 **TSDF 融合**（Truncated Signed Distance Function）将多帧稀疏点云合并为稠密、连续的场景点云（不同数据集的 TSDF 参数不同，如 HiRoom 体素大小 0.007m，ETH3D 为 0.039m）

**实验验证：** 论文 7.1 节用 ScanNet++、ETH3D 的图片流测试，**DA3-Giant** 生成的连续点云在 F1 分数上超 VGGT **23.6%**，且能保留桌角、墙面边缘等细粒度细节。

#### （1）评估阶段：用 "真实场景的标注数据" 作为 Ground Truth

实验中模型性能对比的是真实世界的物理测量结果，而非教师模型输出，具体来源：

- **姿态真值：** ETH3D、DTU 等数据集用 **COLMAP**（基于特征匹配的 SfM 方法）** 或激光跟踪仪获取相机姿态
- **几何真值：**
  - ETH3D/ScanNet++：用 TSDF 融合 LiDAR 采集的稠密深度图，生成真实场景点云
  - DTU：在实验室控制条件下，用高精度 3D 扫描仪获取物体的真实点云
- **渲染真值：** NVS 任务（视觉渲染）用 **"真实场景的多视图图像"** 作为渲染目标真值（如 DL3DV 的测试帧）

#### （2）训练阶段：用 "教师模型伪标签" 作为监督（非 Ground Truth）

教师模型的作用是 **"为真实噪声数据生成高质量监督信号"**，而非作为评估的真值：

- 真实数据（如 ARKitScenes、WildRGBD）的深度标注常稀疏 / 噪声大，无法直接用
- 教师模型（仅用合成数据训练）为这些真实图像生成 **"伪深度图"**，再通过 RANSAC 对齐真实稀疏深度，得到 **"干净的监督信号"**
- 论文 7.2.3 节验证：用教师伪标签训练的 DA3，深度图细节比无教师监督的版本丰富 **30%**

### Navigation/Tracking 适配

DA3 适配机器人 SLAM 避障需解决 **动态物体过滤、回环检测、语义融合** 三大核心问题。这些问题的本质是 **"DA3 作为'几何重建模型'的功能边界，与机器人'自主环境交互'的实际需求之间的差距"**——DA3 仅负责输出高精度几何信息（深度、点云），但机器人避障需要 **"干净的静态环境地图"**、**"长期建图的一致性"**、**"可解释的障碍决策依据"**，这三个适配问题正是填补这一差距的关键。以下结合你提供的新搜索资源（尤其是动态物体滤除、SLAM 实践相关内容），从 **"问题本质、DA3 局限、解决方案、落地细节"** 四个维度详细拆解：

#### 一、动态物体过滤：从 "无差别几何重建" 到 "精准静态环境提取"

**1. 问题本质与影响**

机器人在真实场景（如商场、街道）中，环境充满动态物体（行人、移动车辆、开合的门）。DA3 会将这些动态物体与静态场景（地面、墙壁）一同重建为点云，导致两个严重问题：

- **建图污染：** 动态物体的点云会被误判为静态障碍（如行人走过的区域，地图会残留 "人形障碍"），后续机器人再次经过时会误触发避障，无法通行
- **避障误判：** 若动态物体突然出现在路径上（如行人横穿），DA3 生成的点云包含该物体，但机器人无法区分 **"静态障碍"** 和 **"动态障碍"**，可能出现 **"过度避障"**（避开缓慢移动的行人导致绕远）或 **"避障不及时"**（未识别快速移动物体）

**2. DA3 当前的核心局限**

DA3 的设计目标是 **"恢复空间一致的几何结构"**，完全不区分 **"静态"** 与 **"动态"**：

- 输入图像中的动态物体，会被 DA3 当作 **"场景几何的一部分"**，输出对应的深度图和射线图，进而生成动态物体的点云
- 无任何时序动态检测模块（如光流跟踪、多帧运动一致性判断），无法从连续帧中识别 **"运动的物体"**

**3. 解决方案：结合 "几何 + 语义 + 时序" 多维度滤除，适配 DA3 输出**

根据搜索资源（摘要 3：动态物体滤除算法、摘要 6：Dynablox 几何方法），可将 DA3 的输出与以下技术结合，实现动态物体精准滤除：

| 技术路径 | 核心原理（结合 DA3） | 优势与优化方向（来自搜索资源） |
|---------|---------------------|---------------------------|
| **2D 语义分割反投影** | 1. DA3 输出 "RGB 图像 + 深度图"；<br></br>2. 用语义分割模型（如 Mask R-CNN、DeepLab，摘要 3）分割 RGB 图像中的动态物体（如 "行人""车辆"），得到动态区域掩码；<br></br>3. 将掩码反投影到 DA3 生成的 3D 点云中，滤除掩码对应的点（即动态物体点）。 | - **优势：** 直接利用 DA3 的 RGB + 深度输出，无需额外传感器；<br></br>- **优化：** 结合时序一致性融合（摘要 3），对连续 3-5 帧的分割结果取交集，减少单帧误分割（如将静态停靠的车辆误判为动态）。 |
| **光流跟踪 + 运动一致性** | 1. 对 DA3 处理的连续帧 RGB 图像，用光流模型（如 RAFT、FlowNet，摘要 3）计算像素运动向量；<br></br>2. 筛选运动向量大于阈值的像素（动态物体区域）；<br></br>3. 结合 DA3 的深度图，将动态像素反投影到点云滤除。 | - **优势：** 无需训练语义模型，适用于 "未知动态物体"（如未见过的包裹）；<br></br>- **优化：** 补偿相机运动（Ego-motion）（摘要 3），用 DA3 估计的相机姿态（extrinsics）修正光流，避免将 "相机移动导致的静态物体运动" 误判为动态。 |
| **纯几何时序检测（Dynablox）** | 1. 用 DA3 生成的连续帧点云构建 TSDF 体素地图（摘要 6）；<br></br>2. 跟踪每个体素的 "占用状态时序"（如 "最近 10 帧是否持续被占用"）；<br></br>3. 若体素从 "空闲" 突然变为 "占用"（如行人闯入），标记为动态体素，滤除对应点云。 | - **优势：** 无依赖外观，适用于黑暗、低纹理场景（DA3 深度图仍有效）；<br></br>- **优化：** 引入高置信度自由空间（摘要 6），通过激光射线穿透检测，排除 "静态物体遮挡导致的误判"（如行人挡住墙壁，墙壁仍为静态）。 |

**4. 落地示例**

机器人在商场走廊移动时：

- DA3 实时输出 640×480 分辨率的深度图 + RGB 图像（摘要 1、4 中 Jetson 平台配置）
- 同步运行 Mask R-CNN 分割 RGB 图像，得到 **"行人"** 掩码（动态区域）
- 将掩码与 DA3 的深度图结合，计算动态区域的 3D 坐标范围，从点云中删除这些坐标的点
- 同时用连续 5 帧的分割结果取交集，避免将 **"短暂停留的行人"** 误判为静态障碍，最终得到 **"仅含墙壁、地面的静态点云"**，供避障决策使用
#### 二、回环检测：从 "短期几何一致" 到 "长期地图全局一致"

**1. 问题本质与影响**

机器人在 **"长期建图"**（如探索整个办公楼）时，会因**位姿漂移**（相机姿态估计误差累积）导致地图扭曲：

- 例如机器人从 **"1 楼走廊"** 出发，绕一圈回到起点，若没有回环检测，DA3 生成的地图会出现 **"起点与终点不重合"**（如走廊两端错开 1 米），严重时会导致机器人 **"迷路"**（不知道自己的真实位置）
- DA3 仅能保证 **"单帧 / 短序列帧"** 的几何一致性（如连续 10 帧的点云对齐），但无法判断 **"当前场景是否与 10 分钟前访问过的场景相同"**—— 这一判断能力正是回环检测的核心

**2. DA3 当前的核心局限**

DA3 的定位是 **"几何重建模型"**，无任何回环检测与位姿修正模块：

- 仅能根据输入图像估计 **"相对姿态"**（相邻帧之间的姿态变化），无法估计 **"绝对姿态"**（相对于全局地图的位置）
- 长期运行后，相对姿态误差会累积，导致点云地图出现 **"漂移"**（如直线走廊建图后变成曲线），进而影响避障精度（如机器人认为前方有障碍，实际是地图漂移导致的虚警）

**3. 解决方案：DA3 作为 "几何 backbone"，集成到成熟 SLAM 框架**

根据搜索资源（摘要 5：DA3-Long 提升 SLAM 性能、摘要 3：SLAM 位姿优化），需将 DA3 的输出接入具备回环检测的 SLAM 框架，利用 SLAM 的回环模块修正漂移，具体流程如下：

- **选择 SLAM 框架：** 优先选择支持 **"深度图输入"** 的开源框架，如 **ORB-SLAM3**（适用于单目 / RGB-D）、**LDSO**（直接法 SLAM，高精度）、**VINS-Mono**（视觉惯性融合，抗抖动）

- **DA3 与 SLAM 的数据交互：**
  - DA3 将 **"深度图（prediction.depth）+ 相机内参（prediction.intrinsics）+ 相对姿态（prediction.extrinsics）"** 输出给 SLAM 框架
  - SLAM 框架将 DA3 的深度图作为 **"视觉观测数据"**，替代传统的 **"单目相机的三角化深度"** 或 **"LiDAR 点云"**，提升姿态估计精度（摘要 5 提到 DA3-Long 替换 VGGT 后，SLAM 漂移显著降低）

- **SLAM 回环检测模块修正漂移：**
  - 当 SLAM 框架检测到 **"当前场景与历史场景相似"**（如 ORB-SLAM3 通过 ORB 特征匹配判断回环），会触发 **"回环优化"**
  - 优化过程中，SLAM 会根据 DA3 生成的深度图重新计算 **"回环帧"** 的姿态，修正之前累积的漂移，并更新全局地图，确保长期建图的一致性

**4. 落地示例**

机器人探索办公楼时：

- DA3 在 Jetson AGX Orin 上实时输出深度图（6.35 FPS，摘要 1、4），并将数据传入 ORB-SLAM3
- ORB-SLAM3 用 DA3 的深度图优化相邻帧的姿态（减少初始漂移），同时提取图像的 ORB 特征，与历史帧特征库比对
- 当机器人回到 **"1 楼大厅"**（之前访问过的场景），ORB-SLAM3 检测到 ORB 特征匹配度超过阈值（回环触发），用 DA3 的深度图验证 **"当前深度与历史深度是否一致"**，若一致则优化全局位姿，修正之前的漂移
- 最终生成的地图无明显扭曲，机器人能准确判断自己的位置，避障决策更可靠
#### 三、语义融合：从 "纯几何点云" 到 "可解释的避障决策"

**1. 问题本质与影响**

DA3 生成的点云仅包含 **"3D 坐标 + 深度"** 等几何信息，但机器人避障需要语义理解—— 即 **"知道障碍物是什么，以及是否需要避开"**：

- 例如点云中的 **"地面点"**（可通行）和 **"桌子腿点"**（需避开），几何上都是 **"空间中的点"**，但对机器人的意义完全不同
- 若仅依赖几何信息，机器人可能出现 **"荒谬避障"**（如避开地面上的阴影点云）或 **"漏避障"**（如未识别细长的栏杆，几何上点云稀疏易被忽略）

**2. DA3 当前的核心局限**

DA3 是 **"纯几何模型"**，无任何语义输出能力：

- 无法给点云添加 **"类别标签"**（如 "地面""行人""栏杆"）
- 无法区分 **"可通行区域"**（如平坦地面）和 **"不可通行障碍"**（如台阶、桌椅），仅能通过 **"点云是否在路径上"** 判断是否避障，精度极低

**3. 解决方案：DA3 深度图 + 多模态语义分割，给点云 "贴标签"**

根据搜索资源（摘要 3：RGB-D 语义分割、多模态融合），可利用 DA3 的深度图生成 **"RGB-D 数据"**，输入语义分割模型，实现 **"几何 + 语义"** 融合：

| 技术路径 | 核心原理（结合 DA3） | 优势与优化方向（来自搜索资源） |
|---------|---------------------|---------------------------|
| **RGB-D 语义分割** | 1. DA3 输出 "RGB 图像 + 深度图"，组合为 RGB-D 数据（每个像素含 RGB 颜色 + 深度值）；<br></br>2. 将 RGB-D 数据输入专门的语义分割网络（如 Cylinder3D、RangeNet++，摘要 3）；<br></br>3. 分割网络输出 "语义标签图"（每个像素对应类别，如 "0 = 地面，1 = 行人，2 = 栏杆"）；<br></br>4. 将语义标签反投影到 DA3 生成的 3D 点云中，每个点云添加 "语义属性"。 | - **优势：** DA3 的深度图能弥补 RGB 图像的遮挡 / 光照问题（如阴影区域，深度图仍能区分地面与障碍）；<br></br>- **优化：** 多模态特征融合（摘要 3），将 DA3 的深度特征与 RGB 特征拼接输入分割网络，提升 "细长物体（栏杆）""低纹理物体（白色墙壁）" 的分割精度。 |
| **几何辅助语义修正** | 1. 用 DA3 的深度图计算 "点云曲率"（高曲率区域多为物体边缘，如桌子角）；<br></br>2. 结合语义分割结果，若 "语义标签为地面，但曲率过高"（如地面上的石头），修正标签为 "障碍"；<br></br>3. 若 "语义标签为障碍，但深度值过大"（如远处的树木，超出避障范围），标记为 "无需避障"。 | - **优势：** 利用 DA3 的几何精度修正语义分割的误判；<br></br>- **优化：** 高度阈值约束（摘要 3 提到的 ERASORS 算法），根据机器人高度（如 0.1-1.5 米为障碍范围），过滤掉 "过高（天花板）" 或 "过低（小石子）" 的点，减少无效避障。 |

**4. 落地示例**

服务机器人在办公室避障时：

- DA3 生成 RGB-D 数据（RGB 图像 + 深度图），输入 **Cylinder3D** 语义分割网络
- 分割网络输出语义标签图：**"地面（绿色）""办公桌（红色）""行人（蓝色）"**
- 点云被标记为三类：**绿色点**（可通行）、**红色点**（静态障碍，需绕开）、**蓝色点**（动态障碍，需实时跟踪）
- 机器人的避障算法优先避开红色 / 蓝色点，且仅在 **"障碍点位于机器人运动范围内（0.1-1.5 米高）"** 时触发避障，忽略地面小石子（过低）和天花板管道（过高），避免无效决策

## GPT4Scene
**3D 视觉-语言任务**旨在将 3D 场景理解与自然语言处理相融合。然而，我们希望更进一步：将 3D 内容融入大型语言模型（LLMs），以实现更自然的人机交互。这一方向的研究最初始于 **3D 点云大型语言模型（3D Point Cloud LLMs）**。此类模型以点云为输入，能够在 3D 场景中实现自然语言生成与交互。

早期的 3D 大型语言模型主要关注物体级别的几何结构与外观；后续研究扩展至室内场景，开始侧重物体间的空间关系及场景整体特征，通常会利用场景点云，并结合辅助性 2D 多视图图像。为了更精准地捕捉物体间关系，近期的 3D 大型语言模型会先对场景中的物体进行解耦，再将其输入至 LLM。此外，部分方法会更依赖视觉输入来判断场景上下文。

**[GPT4Scene](https://gpt4scene.github.io/)** 是旨在探索纯视觉输入的 VLM 是否能更有效地处理室内 3D 场景理解任务的框架，仅依赖视频输入（无需 3D 点云），核心通过 **3D 重建生成鸟瞰图（BEV）** 提供全局场景布局，以及 **时空对象标记（STO-markers）** 建立 BEV 与视频帧的全局-局部关联，解决 VLMs 缺全局表示、帧与时空上下文错位的问题；同时构建含 **165K 文本标注**的 **ScanAlign 数据集**，用于微调开源 VLMs。在零样本设置下，该框架显著提升 GPT-4o、Gemini-1.5-Pro 等大模型性能（如 GPT-4o 在 ScanQA 的 ROUGE 指标从 32.6 提升至 **37.7**）；微调后 **Qwen2-VL-7B** 在多任务达 SOTA，如 3D 问答（SQA3D）EM1 指标从 40.7 提升至 **60.7**（相对 **+48%**），超过此前 SOTA 模型 Chat-Scene **11.0%**，且能让 VLMs 形成内在 3D 理解能力，为 VLMs 扩展 3D 场景理解提供无缝方案。

![GPT4Scene Pipeline](/blog/2025/gpt4scene-pipeline.png)

### 整体工作流程

VLMs 直接处理视频时存在两个问题：

1. **缺全局场景表示**（第一视角视频看不到房间整体布局）
2. **帧与时空上下文错位**（不知道不同帧的同一物体是同一个）

因此，GPT4Scene 的框架设计围绕 **"补全局"**、**"建关联"** 展开：

- **"Reconstruct 路径"** 负责 **"补全局"**（生成 BEV）
- **"Partition 路径"** 负责 **"提细节"**（采样帧）
- **"STO-markers"** 负责 **"建关联"**（让 BEV 和帧的物体对应）

**起点：** 一段围绕室内场景拍摄的第一视角视频 $V = \{I_1, I_2, \ldots, I_N\}$（比如 1000 帧的房间漫游视频）。这段视频会被 **"复用"** 到两个预处理环节，而非拆分：

- **给 "Partition（采样）路径" 用：** 取 **"部分帧"**（采样），目的是减少 VLM 的输入 token 数量（避免 1000 帧计算量过大），同时保留场景局部细节（如物体外观、局部视角）
- **给 "Reconstruct（重建）路径" 用：** 用 **"完整帧"**，目的是通过连续时序的图像 + 相机外参，还原场景的 3D 结构（点云），进而生成全局布局（BEV）

![GPT4Scene Framework](/blog/2025/gpt4scene-framework.png)

如上图，给定一段围绕室内场景移动拍摄的视频 $V = \{I_1, \dots, I_N\}$，我们首先通过索引近似均匀地采样 $n$ 帧，采样公式为：

$$s_i = \lfloor (i-1) \frac{N}{n} \rfloor + 1$$

其中 $\forall i \in \{1, \dots, n\}$，由此形成采样后的视频帧集合 $V^* = \{I_{s_1}, \dots, I_{s_n}\}$。该策略在保留场景上下文且无显著信息损失的前提下，减少了视觉语言模型（VLMs）的令牌（token）数量和计算开销。随后，我们利用完整的时间序列进行 3D 场景重建，生成全局鸟瞰图（BEV）。通过后续的 3D 实例分割，可实现物体的精准定位；将该定位结果投影到 BEV 地图和 2D 视频帧上，即可建立时空对象标记（STO-markers）。

#### Reconstruct → 生成 "带 STO-markers 的 BEV 图像 $\mathcal{I}_b'$"

**核心作用：** 给 VLM 提供 **"全局场景布局"**（如房间里桌子、椅子的相对位置，墙壁的形状），解决 **"缺全局表示"** 问题，同时通过 STO-markers 与采样帧关联。

**步骤 1：3D 重建生成点云**

用 **"完整原始视频"**（而非采样帧）+ 每帧的 **"相机外参 $E = \{E_1, \ldots, E_N\}$"**（描述相机在现实中的位置和姿态），通过 3D 重建技术（如 BundleFusion）生成 3D 点云 $\mathcal{P}$：

$$\mathcal{P} = \mathcal{R}\left(\{(I_t, E_t)\}\right)$$

**步骤 2：点云生成 BEV 图像**

设定 **"俯视图相机外参 $E_{top}$"**（模拟从房间正上方往下拍的相机姿态，$E_{top} \in SE(3)$），通过渲染函数 $\mathcal{T}(\cdot)$ 将 3D 点云投影为 2D 的 BEV 图像 $\mathcal{I}_b$：

$$\mathcal{I}_b = \mathcal{T}(\mathcal{P}, E_{top})$$

**步骤 3：叠加 STO-markers**

与 **"路径 1"** 共享 3D 实例掩码 $M$，保证标记一致性：

1. 将 3D 掩码 $M$ 投影到 BEV 的 xy 平面（忽略 z 轴高度，只保留平面位置），提取每个物体投影后的 **"边界框中心坐标"** $C^{xy} = \{C_1^{xy}, \ldots, C_K^{xy}\}$
2. 用函数 $\mathcal{F}(\cdot)$ 将这些坐标作为标记叠加到 BEV 图像上，生成带 STO-markers 的 BEV $\mathcal{I}_b' = \mathcal{F}(\mathcal{I}_b, C^{xy})$

**关键：** BEV 上的标记 ID 与 **"路径 1"** 采样帧的标记 ID 完全一致（比如 BEV 上的 **"物体 3"** 和采样帧上的 **"物体 3"** 是同一个桌子），建立 **"全局布局"** 与 **"局部细节"** 的关联。

#### Partition（视频帧采样）→ 生成 "带 STO-markers 的采样帧 $\mathcal{V}^{* \prime}$"

**核心作用：** 给 VLM 提供 **"局部物体细节"**（如椅子的颜色、杯子的形状），并通过 STO-markers 标记物体 ID，解决 **"帧间物体错位"** 问题。

**步骤 1：帧采样**

按公式 $s_i = \lfloor (i-1)\frac{N}{n} \rfloor + 1$（$n$ 默认 8 帧）从原始视频中 **"均匀采样"** $n$ 帧，得到采样帧集合 $V^* = \{I_{s_1}, \ldots, I_{s_n}\}$。

**步骤 2：叠加 STO-markers**

这一步需要 **"Reconstruct 路径"** 的输出（3D 实例掩码）作为前提，并非独立完成：

1. 从 **"Reconstruct 路径"** 获取 3D 实例掩码 $M = \{M_1, ..., M_K\}$（每个 $M_k$ 对应 1 个物体的 3D 轮廓）
2. 根据每帧采样帧的 **"相机外参"**，将 3D 掩码 $M$ 投影到该 2D 帧上，提取每个物体的 **"2D 质心坐标"** $C_i^{uv}$（第 $i$ 帧中第 $k$ 个物体的标记是 $C_{i,k}^{uv}$）
3. 用函数 $\mathcal{F}(\cdot)$ 将这些坐标作为 **"标记"**（如数字 ID "物体 1""物体 2"）叠加到采样帧上，生成带 STO-markers 的采样帧 $\mathcal{V}^{* \prime} = \{\mathcal{F}(I_i, C_i^{uv})\}$

**关键：** 不同采样帧中，同一物体的标记 ID（如 **"物体 3"**）保持一致，解决 **"帧间物体错位"**（比如第 1 帧的 **"物体 3"** 和第 5 帧的 **"物体 3"** 都是同一个桌子）。

#### VLM 输入

**"The resulting frames and BEV image, enhanced with STO-markers, are inputs for VLM training and inference"**—— 即最终输入 VLM 的是两个核心组件：

- **$\mathcal{V}^{* \prime}$：** 带 STO-markers 的采样帧（提供 **"局部物体外观 + 帧间物体一致性"**）
- **$\mathcal{I}_b'$：** 带 STO-markers 的 BEV 图像（提供 **"全局场景布局 + 物体空间位置"**）

这两个组件结合，恰好解决了 VLMs 的 3D 理解缺陷：

- **缺全局？** BEV 提供房间整体布局
- **帧错位？** STO-markers 保证同一物体在 BEV 和所有采样帧中 ID 一致，让 VLM 能关联 **"局部看到的物体"** 和 **"全局中的位置"**