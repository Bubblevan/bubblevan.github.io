---
date: 2025-12-19
title: AI 上下游工作概念
authors: [bubblevan]
tags: [agent, pretraining, posttraining, planning]
---

[如何评价当前的 AI Agent 落地效果普遍不佳的问题？](https://www.zhihu.com/question/13476251758/answer/1914837861510934794?utm_psn=1985438588473741431)

这篇回答给我玩小黄文这一块，**"做AI"**是吧

不过也让我很好的了解到了从模型生产到部署AI各阶段具体干什么的概念：

### 1. Pre-training (预训练)：造脑工程

这是 AI 的 **"基建"阶段**，目标是从海量数据中学习通用知识。

#### 数据工程 (Data Curation)

- **清洗与去重**：处理成百上千 T 的互联网数据，剔除垃圾信息。
- **数据配比**：决定书本、代码、网页、数学题各自占多少比例（这是各家模型的核心秘密）。

#### 算力基础设施 (Infrastructure)

- **分布式训练**：如何让几万张显卡同时跑一个模型（**3D 并行**：数据并行、算力并行、流水线并行）。
- **算力优化**：提高显卡利用率（**MFU**），防止训练过程中突然崩溃（**Checkpoint 恢复**）。

#### 架构设计 (Architecture)

- **MoE (混合专家模型)**：像 **DeepSeek** 那样，让模型只激活部分参数以节省算力。
- **长文本窗口**：让模型一次能读完一整本书。

### 2. Post-training (后训练/对齐)：教育工程

预训练出的模型只是一个**"满腹经纶但满嘴胡话"的学者**，Post-training 是为了让它变乖、变有用。

- **SFT (有监督微调)**：喂给模型高质量的 Q&A 对，教会它听从指令。

#### Alignment (对齐/价值观)

- **RLHF (强化学习)**：让模型根据人类的打分来优化回答。
- **DPO (直接偏好优化)**：目前最流行的替代 RLHF 的方案，更高效。

#### 推理能力强化

- **思维链 (CoT) 激发**：通过特定的训练让模型学会**"想好了再说"**。
- **Reflection (反思)**：教会模型在输出前自我检查错误。
- **合成数据 (Synthetic Data)**：当人类数据不够用时，让模型生成高质量数据来训练模型自己。

### 3. Multimodal (多模态)：五官工程

让 AI 不仅能看懂文字，还能看图、听声音、甚至看视频。

- **模态对齐 (Modality Alignment)**：将图像编码器（如 **ViT**）捕捉到的特征，翻译成大语言模型能听懂的"语言"。
- **统一表示 (Unified Tokenization)**：尝试把声音、图像、文本都变成同一种数字序列进行处理（如 **Chameleon** 或 **GPT-4o** 的思路）。
- **时序理解**：针对视频流，如何让模型理解动作的先后顺序和逻辑。

### 4. Inference Optimization (推理优化)：落地工程

模型训练好后，如何让它运行得更快、更便宜、更稳。

- **量化 (Quantization)**：将 **16 位浮点数**压成 **8 位或 4 位**，模型体积缩小一倍，速度飞快，但精度损失很小。
- **算子优化**：比如 **FlashAttention**，通过底层数学技巧极大提升显卡的计算效率。
- **调度系统**：
  - **vLLM / TensorRT-LLM**：并发处理成千上万个请求，提高吞吐量。
  - **KV Cache 管理**：解决模型在生成长文本时内存占用过高的问题。

### 5. Agent 开发

虽然表面上是鄙视链最底层，很大程度上依赖**"调教 Prompt"**，不过下面这篇回答我觉得说的挺好：

[AI agent到底有多大创新？](https://www.zhihu.com/question/657739588/answer/1959347964674809996?utm_psn=1985429646507004959)

这个知乎提问主要谈到了 **AI agent** 的缺陷：

#### 1. Planning 阶段带来了巨大的耗时

当 tool 变多后，**turbo 系列模型**的准确率堪忧，因此不得不使用旗舰模型，这让延时进一步增加。

- **本质原因**：**组合优化问题**。工具多了以后，搜索空间呈指数级膨胀。弱模型搞不定，强模型 Token 多、推理慢。
- **解决方案**：
  - **分层治理（缩小搜索空间）**：意图分类 -> 路由到特定域（**Domain**） -> 仅暴露少量工具（类似 **MCP 协议**思路）。
  - **并行化（工程优化）**：将串行链改为 **DAG（有向无环图）**，无依赖的任务并行执行（参考 **LLMCompiler**）。
  - **路由策略（成本优化）**：简单任务给小模型（**SLM**）/硬编码，复杂任务给大模型（参考 **RouteLLM**）。

#### 2. Planning 的质量不够高

原来的 task bot 做任务所使用的 workflow 是人工决定的，现在改成了模型自助决定，从目前的测试来看，由模型构建的复杂工作流的可用率远远不及人类水平。简单工作流使用判别式小模型反而性能更好。

- **本质原因**：自然语言生成的计划缺乏**"可执行性"**和**"全局约束"**。模型线性思维（Step A->B->C）难以应对复杂多变场景。
- **解决方案**：
  - **解耦规划（HiPlan）**：战略（里程碑）与战术（执行细节）分离。
  - **结构化约束（Routine）**：强制输出 **DSL（领域特定语言）** 而非自然语言，由语法保证正确性。
  - **搜索式规划（LATS）**：引入 **MCTS（蒙特卡洛树搜索）**，不是赌一把，而是模拟多条路径+打分（**Verifier**）。
  - **多轮 RL 训练**：让模型在多轮交互中"学会"长程规划，而不是仅靠 Prompt。

#### 3. Reflection 是一种时间换准确度的策略

然而这个策略非常容易重复进行自我内耗，和死循环。

- **本质原因**：反馈信号太弱（**"我觉得不对"**太主观），缺乏明确的停机条件，导致错误假设被不断强化。
- **解决方案**：
  - **模型侧**：训练模型学会**"诊断错误"**并**"提出修复方案"**（**Failure Makes the Agent Stronger**）。
  - **工程侧（兜底）**：设置硬性上限（**Max rounds**）、状态去重（**State-hash**）、预算控制。

### 思考

这么看来，其实 agent 的 **Prompt Engineering** 已经臭了，应该转向成 **Flow Engineering**，使用 **HiPlan（分层）**、**DAG（并行）**、**Router（路由）** 等手段。**DSL（结构化语言）** 依然很重要，即输出 JSON 或特定代码，主要就看你一个 Schema 定义能力本身。然后 **MCP** 即插即用，**Multi-agent System (MAS)** 组成一组'专家 Agent'的协作网络。

## 相关论文

### 路由（Routing）

- [RouteLLM: Learning to Route LLMs with Preference Data](https://arxiv.org/pdf/2406.18665)
- [MoMA: Multimodal LLM Adapter for Mobile Agents](https://arxiv.org/pdf/2509.07571)

### 规划优化（Planning Optimization）

- [HiPlan: Hierarchical Planning for Complex Tasks](https://arxiv.org/pdf/2508.19076)
- [Routine: Structured Instruction for Agents](https://arxiv.org/pdf/2507.14447)
- [LATS: Language Agent Tree Search](https://arxiv.org/pdf/2310.04406)

### 多轮强化学习（Multi-turn RL）

- [RAGEN (StarPO-S)](https://arxiv.org/pdf/2504.20073)
- [AgentGym: Evolving Agents via RL](https://arxiv.org/pdf/2509.08755)
- [MUA-RL](https://arxiv.org/pdf/2508.18669)

### 反思与强化学习（Reflection & RL）

- [UFO: Unary Feedback as Observation](https://unary-feedback.github.io/assets/pdf/UFO_paper.pdf)
- [Failure Makes the Agent Stronger](https://www.arxiv.org/pdf/2509.1884)

