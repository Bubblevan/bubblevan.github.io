---
date: 2025-12-24
title: 足式运控综述（1）：从Moe架构与隐显双重感知表征开始
authors: [bubblevan]
tags: []
---

## 1. 绪论

### 1.1 足式机器人的发展背景与现状

**足式机器人（Legged Robots）**，作为**具身智能（Embodied AI）**的重要载体，近年来经历了从**"基于模型的控制（Model-Based Control）"**向**"基于学习的控制（Learning-Based Control）"**的深刻范式转移。传统的控制方法，如**模型预测控制（MPC）**和**全身控制器（WBC）**，依赖于精确的动力学建模和状态估计，虽然在平坦地形上表现出色，但在面对非结构化环境、复杂地形扰动以及高动态动作需求时，往往面临建模误差和实时算力瓶颈。

![](/blog/2025/legged-robot-structures.png)

在进入具体分类前，必须明确，足式与其他机器人有什么不同？

- **离散落地**：与轮式或履带式机器人的连续接触不同，足式机器人拥有离散的着地点
- **地形适应性**：这种结构使其能够适应复杂、动态且不确定的环境，如废墟、楼梯或崎岖的山路
- **高自由度 (DOF)**：冗余的肢体结构提供了极强的灵活性，能够完成跳跃、攀爬等动作

| 机器人类型 | 结构特征 | 优势 | 控制难点 | 代表型号 |
|-----------|---------|------|---------|---------|
| **双足<br>(Bipedal)** | 模拟人类解剖结构，通常具有10-30个DOF | 具有最高的灵活性，能使用人类工具，重心高，占地面积小 | 动态平衡极其困难，行走时通常只有一个支撑点 | Atlas, Optimus, Digit, Figure |
| **四足<br>(Quadrupedal)** | 左右对称结构，常采用"前肘后膝"配置以减小摩擦 | 兼顾稳定性和机动性，目前商业化最成熟 | 步态切换（如Trot到Gallop）的平滑控制 | Big Dog, Cheetah 3, 绝影 (Jueying) |
| **六足<br>(Hexapod)** | 肢体冗余度高，通常采用三角步态（Tripod Gait） | 极高的静态稳定性，即使失去一两条腿仍能行走 | 运动学求解复杂，硬件成本和能耗较高 | Genghis, SILVER2 (水下) |

> 其实还有更加仿生模仿蜘蛛的八足，不过这里不再赘述

![](/blog/2025/evolution-quadruped.png)

### 1.2 足式机器人的关键技术

#### 1.2.1 Perception

![](/blog/2025/perception-system-humanoid.png)

| 感知类型 | 传感器 | 技术原理 | 主要功能 |
|---------|--------|----------|----------|
| **外感知**<br>（环境感知） | **激光雷达（LiDAR）** | 通过发射激光脉冲并测量反射时间，计算目标物体的距离和方位角，生成高精度的三维点云数据 | 地图构建、自主定位、障碍物检测 |
| | **深度相机** | 利用结构光、飞行时间（ToF）或双目视觉技术，直接获取场景的深度信息，形成RGB-D图像 | 实时环境感知、物体识别、避障规划 |
| | **超声波传感器** | 发射超声波并接收回波，通过测量声波传播时间计算距离，成本低、响应快 | 近距离障碍物检测、防碰撞 |
| **内感知**<br>（状态估计） | **IMU<br>（惯性测量单元）** | 集成三轴陀螺仪和三轴加速度计，通过测量角速度和线性加速度，结合积分运算估计机器人的姿态、位置和速度 | 实时姿态估计、平衡控制、运动状态监测 |
| | **关节编码器** | 安装在机器人关节处的旋转或线性编码器，通过测量关节角度变化，精确获取各关节的位置和速度信息 | 关节位置反馈、运动学计算、轨迹跟踪 |
| | **力/力矩传感器** | 通常安装在足端或关节处，通过应变片或压电效应测量接触力和力矩，提供触觉反馈 | 足端接触力检测、平衡控制、步态调整 |

技术趋势上正在从传统的计算机视觉/语音识别，向基于深度学习和多模态融合的方向演进，以提高在复杂环境下的鲁棒性。

#### 1.2.2 Actuation

##### 1. 核心驱动方式对比

驱动器是机器人的"肌肉"，负责将电信号转换为机械运动，驱动关节转动或移动。不同类型的驱动器适用于不同的应用场景，各有优劣。

在工程应用中，主要有以下几类驱动器：

**电机驱动 (Motor Actuators)**
- **工作原理**：通过电磁感应原理，将电能转换为旋转力矩，类似于我们常见的电机（如风扇、电动玩具车）
- **优点**：目前最流行，具有**高效率**和**精确性**，控制简单，响应速度快
- **缺点**：负载能力相对较弱，难以承受突然的冲击力
- **应用场景**：大多数现代足式机器人（如波士顿动力的Spot、宇树科技的Go1）

    **SEA (串联弹性驱动器, Series Elastic Actuator)**
    - **工作原理**：在电机和输出端之间加入弹簧，就像在关节处加了一个"缓冲垫"。当机器人脚接触地面时，弹簧会先压缩，吸收冲击力，然后再传递给电机
    - **优点**：通过弹簧将减速器与连杆解耦，**吸收冲击**并提高力控带宽，显著增强机器人在接触地面时的**鲁棒性**。就像人类关节的软骨一样，起到缓冲作用
    - **缺点**：增加了系统复杂度，弹簧的刚度需要精心设计
    - **应用场景**：需要频繁接触地面的机器人，如MIT的Cheetah系列

    **PEA (本体感受驱动器, Proprioceptive Actuator)**
    - **工作原理**：在SEA基础上进一步优化，通过**高带宽力控**（能够快速响应力的变化）和**抗冲击设计**，使机器人能够感知自身的运动状态并快速调整
    - **优点**：近年来的热点，支持机器人实现**跑酷、跳跃**等高动态动作，能够像人类一样"感知"自己的动作
    - **缺点**：技术难度高，成本较高
    - **应用场景**：需要高动态性能的机器人，如Atlas的跑酷演示

**液压驱动 (Hydraulic Actuators)**
- **工作原理**：通过液压油在密闭管道中传递压力，驱动活塞运动，类似于挖掘机的液压臂
- **优点**：**力量大、输出功率高**，能够驱动大型、重型的机器人
- **缺点**：系统复杂（需要油泵、管道、阀门等），**噪声大**，维护困难，容易漏油
- **应用场景**：大型机器人（如早期Atlas、Big Dog），现在逐渐被电机驱动替代

**气压驱动 (Pneumatic Actuators)**
- **工作原理**：利用压缩空气驱动活塞运动，类似于气动工具
- **优点**：结构简单，成本低
- **缺点**：容易发热，控制精度较低，响应速度慢
- **应用场景**：应用相对较少，主要用于一些特殊场景

##### 2. 不同形态的自由度 (DOF) 配置

**什么是自由度（DOF）？** 自由度是指机器人关节能够独立运动的数量。每个关节通常对应一个自由度，自由度越多，机器人的灵活性越高，但控制难度也越大。

**驱动器的布局决定了机器人的灵活性**：

**双足机器人**
- **配置**：每条腿通常在**髋关节**（连接大腿和躯干）、**膝关节**（大腿和小腿之间）、**踝关节**（小腿和脚之间）配置驱动器，实现 **3 DOF**
- **为什么这样设计**：模拟人类腿部结构，髋关节负责前后摆动和侧向移动，膝关节负责弯曲，踝关节负责脚部姿态调整。这样的配置足以实现行走、跑步等基本动作

**四足机器人**
- **配置**：通常每条腿 **3-4 DOF**（是否带踝关节）
  - **3 DOF**：髋关节（2个：前后摆动+侧向摆动）+ 膝关节（1个）
  - **4 DOF**：在3 DOF基础上增加踝关节，提供更好的地形适应能力
- **为什么这样设计**：四足机器人需要平衡稳定性和灵活性。3 DOF已经足够实现基本的行走和跑步，增加踝关节可以更好地适应不平坦的地形

**六足机器人**
- **配置**：每条腿通常 **3 DOF**（基部关节、股部关节、胫部关节）
- **为什么这样设计**：六足机器人有6条腿，即使失去1-2条腿仍能行走，因此每条腿不需要太多自由度。3 DOF已经足够实现基本的移动和地形适应，过多的自由度会增加成本和复杂度

#### 1.2.3 Control

控制系统负责处理传感器收集的数据（如位置、速度、力），然后生成运动指令，告诉机器人如何移动。就像人类走路时，大脑会处理眼睛看到的路况和身体感受到的平衡信息，然后指挥腿部肌肉如何运动。

控制系统核心包含三个层次：

##### 1. 模型建立（Modeling）

**为什么需要模型？** 就像建筑师需要图纸来设计房子一样，机器人需要数学模型来描述它的运动规律。有了模型，我们才能预测机器人的行为，并设计控制算法。

**运动学模型（Kinematics）**
- **作用**：解决**空间位置与关节角度的映射关系**，不考虑运动过程中的受力分析
- **通俗理解**：就像问"如果我知道每个关节转了多少度，那么机器人的脚会落在哪里？"这就是**正运动学**。反过来，"如果我想让脚落在某个位置，每个关节应该转多少度？"这就是**逆运动学**
- **技术细节**：利用 **D-H 参数法**（Denavit-Hartenberg参数法）建立坐标系，通过数学公式计算脚端轨迹
- **类比**：就像用三角函数计算，如果知道手臂的长度和每个关节的角度，就能算出手的位置

<div style="display: flex; justify-content: space-around; align-items: center; gap: 10px; flex-wrap: wrap;">
  <div style="text-align: center; flex: 1; min-width: 200px;">
    <img src="/blog/2025/kinematic-bipedal.png" alt="双足机器人运动学模型" style="max-width: 100%; height: auto;">
    <p style="margin-top: 5px; font-size: 0.9em; color: #666;">双足机器人</p>
  </div>
  <div style="text-align: center; flex: 1; min-width: 200px;">
    <img src="/blog/2025/kinematic-quadruped.png" alt="四足机器人运动学模型" style="max-width: 100%; height: auto;">
    <p style="margin-top: 5px; font-size: 0.9em; color: #666;">四足机器人</p>
  </div>
  <div style="text-align: center; flex: 1; min-width: 200px;">
    <img src="/blog/2025/kinematic-hexapod.png" alt="六足机器人运动学模型" style="max-width: 100%; height: auto;">
    <p style="margin-top: 5px; font-size: 0.9em; color: #666;">六足机器人</p>
  </div>
</div>

**动力学模型（Dynamics）**
- **作用**：解决**力与运动的关系**
- **通俗理解**：运动学只关心"位置"，动力学关心"为什么会有这个位置"。就像物理课上，不仅要算物体在哪里，还要算需要多大的力才能让它到达那里
- **技术细节**：
  - **双足机器人**：多用**非线性微分方程**描述碰撞和摆动过程（因为双足机器人行走时会有单脚支撑和双脚支撑的切换，就像钟摆一样）
  - **四足/六足**：需考虑重力方向与机身坐标系的力矩平衡（就像桌子要稳定，四条腿的受力要平衡）
- **类比**：就像推一个箱子，不仅要算箱子会移动到哪里，还要算需要多大的力才能推动它

##### 2. 步态规划（Gait Planning）

**什么是步态？** 步态就是机器人走路的"节奏"，定义了**支撑腿**（接触地面的腿）与**摆动腿**（抬起的腿）的交替模式。就像人类走路时，左脚落地时右脚抬起，右脚落地时左脚抬起，这就是一种步态。

**传统方法**：

- **ZMP (零力矩点, Zero Moment Point)**
  - **原理**：经典的静态/准静态稳定性控制方法
  - **通俗理解**：计算一个"虚拟支撑点"，如果机器人的重心投影在这个点上方，就不会翻倒。就像走钢丝时，重心要在钢丝正上方
  - **优缺点**：方法成熟，但不太适合高速动态步态（因为假设机器人移动很慢，像慢动作一样）
  - **应用**：早期的双足机器人，如ASIMO

- **CPG (中枢模式发生器, Central Pattern Generator)**
  - **原理**：模仿生物节律，无需传感器反馈即可生成节奏性运动
  - **通俗理解**：就像人的心跳一样，有内在的节律，不需要大脑时刻指挥。机器人也有一个"节律发生器"，自动产生走路的节奏
  - **优缺点**：增强稳定性，但适应性较差
  - **应用**：一些仿生机器人

- **MPC (模型预测控制, Model Predictive Control)**
  - **原理**：通过在线优化解决有限时域内的最优控制问题
  - **通俗理解**：就像下棋时，高手会"往前看几步"，预测未来几步的情况，然后选择最优的走法。MPC也是这样，预测未来几秒机器人的状态，然后选择最优的控制策略
  - **优缺点**：是目前主流四足机器人的核心算法，效果好但计算量大
  - **应用**：MIT的Cheetah-III、波士顿动力的Spot等

- **强化学习 (RL, Reinforcement Learning)**
  - **原理**：让机器人通过试错学习环境与行为的关系
  - **通俗理解**：就像训练小狗，做对了给奖励，做错了不给奖励，久而久之小狗就学会了。机器人也是这样，在仿真环境中不断尝试，学会在复杂地形中行走
  - **优缺点**：在复杂地形（如楼梯、丛林）中表现出极强的适应力，但需要大量训练数据和时间
  - **应用**：OpenAI的机器人、DeepMind的机器人等

| 方法 | 优点 | 缺点 | 双足 | 四足 | 六足 |
|------|------|------|------|------|------|
| **ZMP<br>(零力矩点)** | 控制过程简单 | 不适用于动态步态控制 | ✓ | | |
| **CPG<br>(中枢模式发生器)** | 动态步态，提高稳定性 | 不适合复杂、崎岖地形 | ✓ | ✓ | ✓ |
| **SLIP<br>(弹簧负载倒立摆)** | 实现腿部末端缓冲和能量回收 | 控制复杂度高 | ✓ | | |
| **MPC<br>(模型预测控制)** | 将步态分为飞行相和摆动相，提高步态控制效率 | 求解过程复杂；在复杂地形中控制稳定性差 | | ✓ | |
| **WBC<br>(全身控制)** | 有效跟踪多任务运动轨迹 | 计算量大 | ✓ | ✓ | |
| **强化学习<br>(RL)** | 增强环境适应性 | 需要大量计算资源和数据 | ✓ | ✓ | ✓ |

##### 3. 稳定性与平衡策略（Stabilization）

**为什么需要稳定性控制？** 机器人不像轮式机器人那样有稳定的支撑，很容易翻倒。就像人类走路时，如果重心偏移太多，就会摔倒。机器人需要实时监测自己的稳定性，并及时调整。

**稳定性判据**：
- **静态稳定性**：通过计算**重心投影与支撑多边形的距离**来评估翻倒风险
  - **通俗理解**：想象机器人站在一个多边形区域内（支撑多边形），如果重心在这个多边形内，就不会翻倒。就像桌子，如果重心在四条腿围成的区域内，桌子就不会倒
- **动态稳定性**：通过**中心压力点**（CoP, Center of Pressure）来评估
  - **通俗理解**：机器人移动时，脚与地面的接触点会变化，需要实时计算压力中心，确保不会翻倒

**合规控制（Compliance Control）**：
- **原理**：利用弹簧机制或主动阻抗控制来缓冲地面冲击
- **通俗理解**：就像人类走路时，膝盖会自然弯曲来缓冲冲击。机器人也需要"软着陆"，不能硬邦邦地踩下去，否则会损坏硬件或失去平衡
- **作用**：保证足端落点精度，提高鲁棒性
- **类比**：就像汽车有减震器，机器人也需要"减震"机制

### 1.3 From ME to Learning-based

**上述方法的局限性**：

| 局限性类型 | 问题描述 | 通俗理解 |
|-----------|---------|---------|
| **建模之难** | 传统方法极度依赖精确的动力学模型。然而，当面对松软的泥土、湿滑的冰面或杂乱的灌木丛时，环境的物理特性极其复杂，很难用数学公式精确描述 | 就像要预测一个球在复杂地形上的滚动，如果地形是规则的（如光滑的斜坡），可以用物理公式精确计算。但如果地形是松软的沙地、湿滑的冰面，就很难用公式描述了 |
| **计算开销与延迟** | 虽然模型预测控制（**MPC**）很强大，但在实时处理高维非线性系统时，计算负担沉重，往往难以应对极高频率的突发扰动 | 就像下棋时，如果每一步都要"往前看很多步"，计算量会非常大。机器人控制也是这样，如果每一步都要预测未来很多步，计算时间会很长，无法实时响应 |
| **灵活性受限** | 传统控制器往往针对特定地形设计，缺乏"常识"，一旦环境超出预设范围，性能会迅速下降 | 就像只会走平路的机器人，遇到楼梯就不知道怎么办了。传统控制器就像"死记硬背"的学生，只会做见过的题目，遇到新情况就束手无策 |

学习方法之所以在近五年爆发，得益于以下四个关键因素：

1. **高性能仿真器（Simulators）**
   - **技术**：如 **Isaac Gym**、**MuJoCo**、**Habitat** 等
   - **优势**：让机器人能在几分钟内完成人类几年的行走练习
   - **通俗理解**：就像在游戏里练习开车，可以无限次重来，撞坏了也没关系。仿真器让机器人可以在虚拟世界中快速、安全地学习

2. **Sim-to-Real（从仿真到现实）技术**
   - **技术**：通过 **"域随机化"（Domain Randomization）** 等策略
   - **优势**：成功解决了仿真环境与现实世界之间的物理鸿沟
   - **通俗理解**：就像在游戏里学会了开车，但游戏和现实不一样（比如物理引擎不完美）。域随机化就像在游戏里随机改变路况、天气、摩擦力等，让机器人学会适应各种情况，这样到了现实世界也能用

3. **算力与算法的协同**
   - **技术**：**PPO（近端策略优化）** 等算法的成熟
   - **优势**：使得大规模并行训练变得稳定可行
   - **通俗理解**：就像以前只能一个人学习，现在可以同时训练成千上万个机器人，学习效率大大提高

4. **感控一体（End-to-End）**
   - **技术**：学习方法允许直接将视觉、深度图等原始感知数据输入网络，直接输出电机指令
   - **优势**：实现了感知与控制的深度融合，这是实现 **"具身智能"** 的关键一步
   - **通俗理解**：就像人类看到障碍物，大脑直接指挥腿避开，不需要先"识别障碍物"→"规划路径"→"执行动作"这样的多步骤过程。端到端学习让机器人也能做到"看到即行动"

**深度强化学习（DRL）成为热点的原因**：

所以最近的热点在于**DRL（深度强化学习）**，主要原因包括：

- **避开建模难题**：强化学习通过 **"试错"机制** ，避开了建立显式物理模型的难题。就像人类学走路，不需要理解复杂的物理公式，通过不断尝试就学会了

- **捕捉细微特征**：神经网络能够捕捉到人类难以建模的微小物理特征（如足端与地面的复杂摩擦力），从而产生比人工设计更鲁棒、更 **"仿生"** 的运动效果。就像人类走路时，脚底能感受到地面的细微变化并自动调整，神经网络也能学习到这些细微的适应能力

#### 1.3.1 MDP & RL

**什么是强化学习？** 几十年来，机器学习（ML）在计算机视觉、自然语言处理、金融、环境科学等众多领域展现出了解决复杂问题的潜力。在机器人学中，**强化学习（RL）**（Sutton and Barto, 2018）因其将机器人控制问题自然地视为**序贯决策问题**而成为热门选择。

强化学习是机器学习的一个分支，旨在在动态环境中开发智能体，以**最大化累积奖励**。就像训练小狗一样，做对了给奖励，做错了不给奖励，久而久之智能体就学会了最优策略。

**马尔可夫决策过程（MDP）**

强化学习通常使用 **马尔可夫决策过程（MDP）** 来建模控制问题。MDP定义为一个五元组 $(S, A, p, r, \gamma)$：

- **状态空间 $S$**：机器人所有可能的状态集合
- **动作空间 $A$**：机器人所有可能的动作集合
- **转移函数 $p$**：可以是确定性的 $s_{t+1} = f(s_t, a_t)$ 或随机性的 $p(s_{t+1}|s_t, a_t)$，描述在状态 $s_t$ 执行动作 $a_t$ 后转移到状态 $s_{t+1}$ 的概率
- **奖励函数 $r(s_t, a_t, s_{t+1})$**：评估在状态 $s_t$ 执行动作 $a_t$ 转移到 $s_{t+1}$ 的好坏
- **折扣因子 $\gamma$**：权衡即时奖励和未来奖励的重要性（通常 $\gamma \in [0,1]$）

通过在随机环境中执行策略 $\pi(a_t|s_t)$，我们可以生成状态和动作的轨迹 $\tau = (s_0, a_0, s_1, a_1, \ldots)$。由策略 $\pi$ 诱导的轨迹分布记为：

$$\rho_\pi(\tau) = p(s_0) \prod_t \pi_t(a_t|s_t) p(s_{t+1}|s_t, a_t)$$

我们的目标是找到最优策略 $\pi^*$，使得期望回报最大化：

$$J(\pi) = \mathbb{E}_{\tau \sim \rho_\pi} \left[ \sum_{t=0}^T r(s_t, a_t) \right] \tag{1}$$

研究人员开发了各种RL算法来解决MDP问题。早期算法可以大致分为**价值函数方法**和**策略迭代方法**：

- **价值函数方法**：以Q-learning（Watkins and Dayan, 1992）、SARSA（Rummery and Niranjan, 1994）或（拟合）价值迭代（Bellman, 1966; Boyan and Moore, 1994）为代表，旨在估计状态或状态-动作对的期望值，从而隐式定义最优策略
  - **通俗理解**：就像给每个状态打分，分数高的状态更好，然后选择能到达高分状态的动作

- **策略迭代方法**：无论是基于梯度的（Howard, 1960）还是无梯度的（Hansen et al., 2003），将给定的MDP框架化为优化问题，从策略的角度寻找最佳参数
  - **通俗理解**：直接学习一个"决策函数"，输入当前状态，输出应该执行的动作

![](/blog/2025/taxonomy-rl-algorithms.png)

尽管取得了有希望的结果，这些早期算法在解决复杂、大规模控制问题时遇到了挑战，主要是由于**可扩展性**和**收敛性**问题。然而，当与深度学习结合时，已建立的理论进展展现了显著的性能提升。

#### 1.3.2 DRL

深度学习（LeCun et al., 2015; Goodfellow et al., 2016）由多层神经元组成的**人工神经网络**被认为是**通用函数逼近器**（Hornik, 1991; Cybenko, 1989），能够学习任何多元函数。

**深度强化学习的诞生**

Mnih等人（2015）的开创性工作证明了强化学习也可以与深度学习结合，在Atari游戏中达到人类水平的性能。这一成就通过引入**经验回放缓冲区（replay buffers）**和**目标网络（target networks）**来稳定学习而得以实现。

**Deep Q Network (DQN)** 之后，出现了多项创新来改善收敛性和样本效率（Wang et al., 2016; Van Hasselt et al., 2016; Hessel et al., 2018）。

**连续动作空间**

虽然许多算法处理的是简单的离散动作（如游戏中的"上、下、左、右"），但机器人控制通常需要**连续动作空间**来生成电机指令（如"关节转30.5度"而不是"转30度或31度"）。

**深度确定性策略梯度（DDPG）**（Lillicrap et al., 2015）是最早支持连续动作的算法之一，随后出现了许多其他RL算法和变体，如：
- TD3（Fujimoto et al., 2018）
- SAC（Haarnoja et al., 2018b）
- A3C（Mnih et al., 2016）
- MPO（Abdolmaleki et al., 2018）
- TRPO和PPO（Schulman et al., 2015, 2017）

**基于模型 vs 无模型方法**

- **基于模型的RL（Model-based RL）**：通过利用学习到的世界模型（在状态空间或图像空间中）来达到最佳样本效率
  - **通俗理解**：就像下棋时，高手会在脑中"模拟"未来几步的走法，然后选择最优策略。基于模型的方法也是这样，先学习环境的模型，然后在这个模型上规划最优动作
  - **代表工作**：Nagabandi et al., 2018; Feinberg et al., 2018; Buckman et al., 2018; Chua et al., 2018; Janner et al., 2019（状态空间）；Ha and Schmidhuber, 2018; Hafner et al., 2019（图像空间）

- **无模型RL（Model-free RL）**：不学习环境模型，直接从经验中学习策略
  - **通俗理解**：就像直接通过"试错"学习，不需要理解环境的物理规律
  - **最新进展**：最近的无模型方法（Chen et al., 2021; Hiraoka et al., 2021）也展现了具有竞争力的样本效率

**可扩展性改进**

除了样本效率，研究人员还致力于提高RL算法的可扩展性，包括：
- **基于梯度的方法**：Espeholt et al., 2018; Barth-Maron et al., 2018; Wijmans et al., 2019
- **无梯度方法**：Mania et al., 2018

**足式运动中的算法选择**

在基于学习的足式运动领域，**同策略（on-policy）无模型算法**如：
- **TRPO（信任域策略优化）**（Schulman et al., 2015）
- **PPO（近端策略优化）**（Schulman et al., 2017）

通常是首选。这种偏好源于追求**最优且鲁棒的性能**，而对样本效率的关注较少。

**PPO**在足式运动社区中特别受欢迎，因为它具有：
- **优秀的收敛性**：训练过程稳定，不容易发散
- **对不同策略架构的适应性**：可以灵活应用于不同的网络结构

**足式运动中的主要RL算法分类**：

| 算法类别 | 代表算法 | 主要论文 |
|---------|---------|---------|
| **On-policy** | TRPO | Hwangbo et al. (2019); Lee et al. (2020); Yang et al. (2022b) |
| | PPO | Iscen et al. (2018); Tan et al. (2021); Margolis et al. (2022); Miki et al. (2022a); Zhuang et al. (2023); Escontrela et al. (2023); Liu et al. (2024) |
| **Model-based** | Dreamer | Xie et al. (2020); Kumar et al. (2021); Rudin et al. (2022b); Reske et al. (2024) |
| **Gradient-free** | ARS | Lee et al. (2020, 2021); Kumar et al. (2022) |
| **Behavior Cloning** | BC | Peng et al. (2020); Agarwal et al. (2021); Wu et al. (2022) |
| **GAIL** | GAIL | Yu et al. (2021) |

上方的算法分类图（Table 1）提供了最常见RL算法的详细分类，并标识了那些最常用于学习足式运动的算法。

**重要澄清：关于"无模型"的误解**

> 需要澄清一个常见的误解：当在Robotics DRL的语境中使用"无模型"这个术语时，虽然无模型DRL算法本身不需要世界的物理模型，但在实践中，这些策略是在**仿真环境**中训练的，而仿真环境本身就是物理模型。
> 这意味着无模型DRL方法的应用严重依赖于使用**第一性原理**开发的模型（仿真环境）。因此，在Robotics DRL的语境中，"无模型"这个术语应该谨慎理解——它们不需要显式地学习环境模型，但仍然依赖于仿真环境这个"隐式模型"。

#### 1.3.3 BC & IL

如前所述，强化学习的主要目标是以基于模型或无模型的方式找到最大化累积奖励的控制策略。无论哪种方式，我们都假设奖励函数是给定的，算法收集数据来学习模型（基于模型）或价值/策略函数（无模型）。

虽然这在实践中显示出很大的潜力，但它需要大量的**奖励工程**，特别是对于**稀疏奖励**问题。

**通俗理解**：
- **奖励工程**：就像训练小狗时，需要设计什么时候给奖励、给多少奖励。如果奖励设计不好，小狗就学不会。机器人也是这样，需要精心设计奖励函数
- **稀疏奖励问题**：就像考试，如果只有最后一道题做对了才给分，中间的过程都不给分，那么学习起来就很困难。机器人完成任务时，如果只有到达目标才给奖励，中间的过程没有奖励，学习效率就会很低

**模仿学习作为解决方案**

学习模仿专家人类演示或现有专家策略（Pomerleau, 1988）的行为可以被视为解决这个问题的方案。

**通俗理解**：就像学开车，与其自己摸索（强化学习），不如先看教练怎么开（模仿学习），然后模仿教练的动作。这样学习效率更高，不需要自己设计复杂的奖励函数。

**模仿学习的挑战**

然而，简单地模仿专家的动作往往会失败，原因包括：

1. **误差累积**：控制问题中误差的复合性质
   - **通俗理解**：就像学走路，如果第一步稍微偏了一点，第二步会偏得更多，第三步偏得更多，最后就完全走偏了。机器人模仿时，小的误差会不断累积，导致最终行为与专家相差很大

2. **分布不匹配**：训练时访问的状态与运行时看到的状态之间的分布不匹配
   - **通俗理解**：就像学开车时只在晴天练习，但实际开车时遇到雨天就不知道怎么开了。机器人训练时看到的状态和实际运行时看到的状态可能完全不同

**改进方法**

为了缓解这些问题，研究人员探索了开发鲁棒策略的替代方法：

- **DAgger（数据集聚合）**（Ross et al., 2011）
  - **原理**：让专家在机器人实际访问的状态下提供演示，而不是只在训练数据的状态下
  - **通俗理解**：就像学开车时，让教练在学员实际开车的路况下指导，而不是只在教练开车的路况下

- **GAIL（生成对抗模仿学习）**（Ho and Ermon, 2016）
  - **原理**：使用生成对抗网络（GAN）的思想，让一个判别器区分专家行为和机器人行为，机器人学习欺骗判别器
  - **通俗理解**：就像有一个"考官"来判断动作是否像专家，机器人学习如何让"考官"认为它的动作像专家

**特权学习（Privileged Learning）**

在基于学习的足式运动中，模仿学习的一个显著应用是**特权学习**（Chen et al., 2020）。

**工作原理**：
1. 首先学习一个**教师策略（teacher policy）**，使用**真值信息**（ground-truth information，如精确的位置、速度等）
2. 然后将这些行为复制到**学生策略（student policy）**中，使用**真实的传感器配置**（如只能看到RGB图像，不能直接获得精确位置）

**通俗理解**：就像学开车，先让一个"完美教练"（能看到所有信息，如精确的车速、位置）学会开车，然后让"学员"（只能看到普通摄像头画面）模仿"完美教练"的动作。这样学员就能在有限的感知能力下，学会复杂的驾驶技能。

**为什么重要**：在实际应用中，机器人往往无法获得完美的传感器信息（如精确的位置、速度），但我们可以先在仿真中训练一个能获得完美信息的教师，然后让学生模仿教师，这样就能在有限的感知能力下实现高性能。

### 1.4 预备知识

#### 1.4.1 Dynamics

**连续时间动力学模型**

足式机器人可描述为（通常是）确定性动力系统。设机器人与环境的状态为 $s \in \mathcal{S}$，机器人执行的动作（控制输入）为 $a \in \mathcal{A}$，则连续时间动力学方程表示为：

$$\dot{s} = f(s, a)$$

其中：
- $s$：状态向量，通常包含机器人各关节的位置 $q$、速度 $\dot{q}$、质心位置 $p$、质心速度 $\dot{p}$、姿态角 $\theta$、角速度 $\omega$ 等
- $a$：动作向量，通常为各关节的期望力矩或位置指令
- $f: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$：状态转移函数，描述在状态 $s$ 下执行动作 $a$ 后状态的变化率

**时间离散化**

为将其构建为 **MDP（马尔可夫决策过程）**，需对连续时间动力学进行时间离散化。设离散化步长为 $\Delta t$，则离散时间动力学方程为：

$$s_{t+1} = s_t + \int_{t}^{t+\Delta t} f(s(\tau), a_t) d\tau$$

在数值计算中，通常采用欧拉法或龙格-库塔法进行近似。最简单的一阶欧拉近似为：

$$s_{t+1} = s_t + f(s_t, a_t) \Delta t$$

**MDP 表示**

在 MDP 框架中，这一确定性过程通过随机转移函数 $p(s_{t+1} | s_t, a_t)$ 建模。对于确定性系统，转移函数为：

$$p(s_{t+1} | s_t, a_t) = \begin{cases}
1 & \text{if } s_{t+1} = s_t + f(s_t, a_t) \Delta t \\
0 & \text{otherwise}
\end{cases}$$

即概率质量（对于连续状态空间为概率密度）完全集中于由动力学方程确定的下一状态 $s_{t+1}$。

**训练数据生成**

状态转移数据元组 $(s_t, a_t, r_t, s_{t+1})$ 由动力学生成，其中：
- $s_t$：时刻 $t$ 的状态
- $a_t$：时刻 $t$ 执行的动作
- $r_t$：时刻 $t$ 获得的奖励
- $s_{t+1}$：时刻 $t+1$ 的状态（由 $s_t$ 和 $a_t$ 通过动力学方程计算得到）

这些数据元组用于强化学习算法的训练，使智能体学习最优策略 $\pi^*(a|s)$。

#### 1.4.2 Simulator
由于深度强化学习（DRL）训练策略需要大量样本，因此常用做法是在仿真中学习策略，再直接部署到现实世界中
主流仿真器普遍采用刚性接触假设（弹性或非弹性），通过互补条件结合摩擦锥约束（线性或非线性）建模接触行为

#### 1.4.3 Observation
**本体感受（Proprioception）**

提供机器人内部状态信息的传感器集合称为**本体感受传感器**，例如：
- **惯性测量单元（IMU）**：测量加速度和角速度
- **关节编码器**：测量关节位置和速度
- **接触传感器**：检测足端与地面的接触状态

**状态估计的必要性**

在腿部机器人领域，由于传感器存在潜在的**噪声**和**不准确性**，原始测量数据通常不会直接使用。相反，研究人员通常会采用**状态估计器**来推断机器人的关键状态，包括：
- **基座姿态**：机器人机身的空间位置和朝向
- **基座扭率**：机身的角速度
- **关节位置和速度**：各关节的状态信息

**观测信息的数学表示**

本体感受信息通常以高频（如 **50Hz-1000Hz**）获取，主要包括以下组成部分：

1. **关节状态**
   - **关节位置**：$q = [q_1, q_2, \ldots, q_n]^T \in \mathbb{R}^n$，其中 $n$ 为关节数量，$q_i$ 表示第 $i$ 个关节的角度
   - **关节速度**：$\dot{q} = [\dot{q}_1, \dot{q}_2, \ldots, \dot{q}_n]^T \in \mathbb{R}^n$，其中 $\dot{q}_i$ 表示第 $i$ 个关节的角速度

2. **机身姿态**
   通过 **惯性测量单元（IMU）** 获取：
   - **重力向量在机身坐标系下的投影**：$g^b = [g_x^b, g_y^b, g_z^b]^T \in \mathbb{R}^3$，用于估计机身的俯仰角和横滚角
   - **机身角速度**：$\omega^b = [\omega_x^b, \omega_y^b, \omega_z^b]^T \in \mathbb{R}^3$，表示机身绕三个轴的角速度

3. **速度估算**
   - **机身线性速度**：$v = [v_x, v_y, v_z]^T \in \mathbb{R}^3$，是控制的核心状态量
   - **噪声处理**：
     - 在**仿真训练**时，通常使用真实值（ground truth）$v_{\text{true}}$
     - 在**实机部署**时，由于传感器噪声，需依赖：
       - **滤波器**（如卡尔曼滤波器、扩展卡尔曼滤波器）
       - **专门的估算网络**（如基于神经网络的观测器）

4. **历史信息（Observation History）**
   由于传感器延迟和系统的**非马尔可夫特性**，现代算法倾向于输入过去 $k$ 个时刻的观测序列：
   
   $$\mathcal{H}_t = \{o_{t-k+1}, o_{t-k+2}, \ldots, o_t\}$$
   
   其中 $o_t$ 表示时刻 $t$ 的观测向量。历史信息有助于捕获：
   - **电机滞后**：电机响应指令的延迟特性
   - **地面摩擦力**：与地面接触时的动态摩擦特性
   - **其他动态特性**：无法从单时刻观测中获得的时序依赖关系

**完整观测向量的构成**

综合上述信息，时刻 $t$ 的本体感受观测向量可表示为：

$$o_t^{\text{proprio}} = [q_t, \dot{q}_t, g_t^b, \omega_t^b, v_t, \mathcal{H}_t]^T$$

其中各分量可能经过状态估计器的处理，以降低噪声影响并提高估计精度。

---

**外感受（Exteroception）**

机器人中用于获取周围环境信息的传感层通常称为**外感受传感器**。这类信息在机器人非平坦环境运动时尤为关键。

**显式映射方法（传统方法）**

腿部机器人的传统方法通常依赖**显式映射技术**，在控制器使用测量数据前对其进行预处理：

1. **高程图（Elevation Map）**
   - **定义**：二维网格地图 $\mathcal{E}: \mathbb{R}^2 \rightarrow \mathbb{R}$，其中每个网格单元 $(x, y)$ 存储该位置的地面高度值 $h(x, y)$
   - **数学表示**：$\mathcal{E} = \{h_{ij} | i \in [1, W], j \in [1, H]\}$，其中 $W$ 和 $H$ 分别为地图的宽度和高度
   - **应用**：早期感知型运动研究（Miki et al., 2022a; Xie et al., 2022）采用高程图方法，对机器人周围的高度值进行采样，并将其作为策略的观测输入
   - **观测向量**：$o_t^{\text{elevation}} = [h_{11}, h_{12}, \ldots, h_{WH}]^T \in \mathbb{R}^{WH}$

2. **体素映射（Voxel Map）**
   - **定义**：三维体素网格 $\mathcal{V}: \mathbb{R}^3 \rightarrow \{0, 1\}$，其中每个体素 $(x, y, z)$ 表示该位置是否被占用
   - **数学表示**：$\mathcal{V} = \{v_{ijk} \in \{0, 1\} | i \in [1, W], j \in [1, H], k \in [1, D]\}$，其中 $D$ 为深度维度

**原始传感数据（现代方法）**

近期研究则摒弃了此类显式映射，转而直接将**原始传感数据**作为策略输入：

1. **深度图像（Depth Image）**
   - **定义**：深度相机获取的距离图像 $D: \mathbb{R}^2 \rightarrow \mathbb{R}^+$，其中每个像素 $(u, v)$ 存储该像素对应的深度值 $d(u, v)$
   - **数学表示**：$D \in \mathbb{R}^{H \times W}$，其中 $H$ 和 $W$ 分别为图像的高度和宽度
   - **观测向量**：$o_t^{\text{depth}} = \text{vec}(D_t) \in \mathbb{R}^{HW}$，其中 $\text{vec}(\cdot)$ 表示将矩阵展平为向量

2. **点云（Point Cloud）**
   - **定义**：三维点的集合 $\mathcal{P} = \{p_i = (x_i, y_i, z_i)^T \in \mathbb{R}^3 | i = 1, 2, \ldots, N\}$，其中 $N$ 为点的数量
   - **数学表示**：$P \in \mathbb{R}^{N \times 3}$，每行表示一个三维点
   - **观测向量**：$o_t^{\text{pointcloud}} = \text{vec}(P_t) \in \mathbb{R}^{3N}$

**应用场景**：这一转变的核心驱动力是应对：
- **高度动态场景**：如跑酷（Zhuang et al., 2023）、避障（Yang et al., 2021）
- **高精度感知需求场景**：如垫脚石行走（Duan et al., 2022; Zhang et al., 2023）

<div style="display: flex; justify-content: space-around; align-items: center; gap: 10px; flex-wrap: wrap;">
  <div style="text-align: center; flex: 1; min-width: 300px;">
    <img src="/blog/2025/elevation-map.png" alt="高程图示例" style="max-width: 100%; height: auto;">
    <p style="margin-top: 5px; font-size: 0.9em; color: #666;">高程图（Elevation Map）</p>
  </div>
  <div style="text-align: center; flex: 1; min-width: 300px;">
    <img src="/blog/2025/depth-image.png" alt="深度图像示例" style="max-width: 100%; height: auto;">
    <p style="margin-top: 5px; font-size: 0.9em; color: #666;">深度图（Depth Image）</p>
  </div>
</div>

**RGB 数据**

**RGB 图像**可用于实现超越单纯几何信息的更高级场景感知，支持人行道导航和避障等功能（Sorokin et al., 2022）。

- **定义**：彩色图像 $I: \mathbb{R}^2 \rightarrow \mathbb{R}^3$，其中每个像素 $(u, v)$ 存储 RGB 三个通道的强度值 $(r, g, b)$
- **数学表示**：$I \in \mathbb{R}^{H \times W \times 3}$
- **观测向量**：$o_t^{\text{RGB}} = \text{vec}(I_t) \in \mathbb{R}^{3HW}$

**优势**：
- 包含更丰富的上下文信息，能够识别并响应仅靠几何信息无法捕捉的元素（如纹理和颜色）
- 通过学习场景语义提升导航效率：
  - Yang 等人 (2023c) 利用图像中的语义信息，自适应调整四足机器人的运动步态和速度以适应不同地形
  - Margolis 等人 (2023) 进一步将 RGB 数据与本体感受数据融合，以实现更精准的通行性估计

> RGB-D：同时包含彩色图像和深度信息的传感器数据

**压缩表示（Latent Space）**

除直接输入传感数据外，研究人员也可学习传感数据的**压缩表示**：

- **编码器-解码器架构**：
  - **编码器**：$z = E(I) \in \mathbb{R}^d$，将原始图像 $I \in \mathbb{R}^{H \times W \times 3}$ 编码为低维潜在表示 $z$，其中 $d \ll 3HW$
  - **解码器**：$\hat{I} = D(z) \in \mathbb{R}^{H \times W \times 3}$，从潜在表示 $z$ 重建原始图像
  - **无监督学习目标**：$\min_{E, D} \mathcal{L}_{\text{recon}}(I, D(E(I)))$，其中 $\mathcal{L}_{\text{recon}}$ 为重建损失（如均方误差）

- **应用**：Hoeller 等人 (2021) 和 Yang 等人 (2023b) 采用无监督学习获取能够压缩和重建原始相机图像的 latent 空间，从而使学习到的策略能够自动导航复杂地形

- **优势**：
  - **降维**：将高维图像数据压缩为低维表示，减少计算负担
  - **特征提取**：自动学习对任务有用的特征表示
  - **观测向量**：$o_t^{\text{latent}} = E(I_t) \in \mathbb{R}^d$，其中 $d$ 通常远小于原始图像维度

---

**任务相关信息（Task-Related Information）**

除本体感受和外感受信息外，与机器人任务相关的其他信息（包括速度、姿态等指令输入，或学习到的任务嵌入等更复杂的数据表示）也可作为策略输入。

> **说明**：需说明的是，这类输入更应被视为 **"目标"** 而非 **"观测"**，但为避免章节过于繁杂，本节将其归入观测部分一并讨论。

**指令输入（Command Input）**

1. **速度指令**
   - **定义**：期望的机器人运动速度 $v_{\text{cmd}} = [v_x^{\text{cmd}}, v_y^{\text{cmd}}, v_z^{\text{cmd}}]^T \in \mathbb{R}^3$
   - **应用**：速度指令是控制运动的常用输入，策略网络学习如何根据速度指令生成相应的控制动作
   - **观测向量**：$o_t^{\text{vel\_cmd}} = v_{\text{cmd}}$

2. **姿态指令**
   - **定义**：期望的机器人位置和姿态 $g_{\text{cmd}} = (p_{\text{cmd}}, R_{\text{cmd}})$，其中：
     - $p_{\text{cmd}} = [x_{\text{cmd}}, y_{\text{cmd}}, z_{\text{cmd}}]^T \in \mathbb{R}^3$ 为期望位置
     - $R_{\text{cmd}} \in \text{SO}(3)$ 为期望旋转矩阵
   - **应用**：Rudin 等人 (2022a) 的研究采用姿态指令，明确指定特定的位置或姿态目标
   - **观测向量**：$o_t^{\text{pose\_cmd}} = [p_{\text{cmd}}, \text{vec}(R_{\text{cmd}})]^T$

**任务嵌入（Task Embedding）**

除直接指令输入外，部分方法会使用学习到的**任务嵌入**：

1. **参考运动的 Latent 表示**
   - **定义**：将特定参考运动编码为低维潜在向量 $z_{\text{task}} \in \mathbb{R}^d$
   - **编码过程**：$z_{\text{task}} = E_{\text{task}}(\tau_{\text{ref}})$，其中 $\tau_{\text{ref}}$ 为参考运动轨迹
   - **应用**：Peng et al. (2022) 使用这种方法
   - **观测向量**：$o_t^{\text{task}} = z_{\text{task}}$

2. **引导行为的 Latent 空间**
   - **定义**：任何能够引导底层行为的潜在空间表示 $z_{\text{behavior}} \in \mathbb{R}^d$
   - **应用**：Haarnoja et al. (2018a) 使用这种方法
   - **观测向量**：$o_t^{\text{behavior}} = z_{\text{behavior}}$

**结构化动作空间的参数**

对于采用**结构化动作空间**的机器人系统（如基于中枢模式发生器（CPGs）或轨迹参数的系统），任务相关信息还可包括：

1. **期望相位（Desired Phase）**
   - **定义**：对于基于 CPG 的系统，相位 $\phi \in [0, 2\pi)$ 控制步态的节奏
   - **应用**：Iscen et al. (2018); Lee et al. (2020)
   - **观测向量**：$o_t^{\text{phase}} = \phi$

2. **轨迹模式参数（Trajectory Pattern Parameters）**
   - **定义**：描述期望轨迹的参数集合 $\theta_{\text{pattern}} \in \mathbb{R}^m$，如步长、步高、步频等
   - **应用**：Iscen et al. (2018); Lee et al. (2020)
   - **观测向量**：$o_t^{\text{pattern}} = \theta_{\text{pattern}}$

**未来参考轨迹（Future Reference Trajectory）**

部分方法会将**未来参考轨迹**作为策略输入：

1. **操作臂末端执行器轨迹**
   - **定义**：规划的操作臂末端执行器轨迹 $\tau_{\text{ee}} = \{p_{\text{ee}}^{(t+1)}, p_{\text{ee}}^{(t+2)}, \ldots, p_{\text{ee}}^{(t+H)}\}$，其中 $H$ 为预测时域
   - **应用**：Ma 等人 (2022) 将操作臂的规划末端执行器轨迹作为带操作臂四足机器人的额外输入，使策略能够预判操作臂运动并调整全身动作
   - **观测向量**：$o_t^{\text{ee\_traj}} = \text{vec}(\tau_{\text{ee}}) \in \mathbb{R}^{3H}$

2. **规划的落足点和轨迹**
   - **定义**：规划的足端落足点序列 $\mathcal{F} = \{f_1, f_2, \ldots, f_N\}$，其中 $f_i = (x_i, y_i, z_i)^T \in \mathbb{R}^3$ 为第 $i$ 个落足点的位置
   - **应用**：Gangapurwala 等人 (2022) 和 Jelenelten 等人 (2024) 提供规划的落足点和轨迹作为参考，用于训练跟踪控制器
   - **观测向量**：$o_t^{\text{footfall}} = \text{vec}(\mathcal{F}) \in \mathbb{R}^{3N}$

**方法对比**

这种由**基于模型的控制器**提供运动轨迹作为策略参考的方法：
- **优势**：能够为垫脚石导航等复杂任务提供更灵活的解决方案
- **劣势**：
  - 需要额外的规划步骤
  - 需要控制器间协调
  - 增加了系统复杂度和工程实现难度

因此，近年来许多动态运动相关研究更倾向于学习**无需任何参考轨迹的单一独立控制策略**，通过端到端学习直接从观测到动作的映射。

#### 1.4.4 Rewarding
在强化学习中，**奖励函数** $r(s_t, a_t, s_{t+1})$ 用于评估智能体在状态 $s_t$ 执行动作 $a_t$ 转移到状态 $s_{t+1}$ 的好坏。奖励函数的设计直接影响策略学习的效果。

**手动奖励设计（奖励工程）**

**手动奖励设计**（又称 **"奖励工程"**）指由工程师定义每个奖励组件 $r_i$ 并调整其权重 $c_i$。

**奖励函数的一般形式**：

$$r(s_t, a_t, s_{t+1}) = \sum_{i=1}^{n} c_i \cdot r_i(s_t, a_t, s_{t+1})$$

其中：
- $r_i$：第 $i$ 个奖励组件
- $c_i$：第 $i$ 个奖励组件的权重
- $n$：奖励组件的总数

**奖励组件类型**：

1. **跟踪奖励**：鼓励机器人跟踪期望的运动（如速度跟踪、姿态跟踪）
2. **正则化项**：惩罚不希望的行为（如过大的关节速度、加速度）
3. **物理约束**：将系统的物理约束作为代价项纳入奖励函数，例如限制运动过程中关节速度、加速度和基座姿态的幅值

**常用物理量表达式**：

| 物理量 | 表达式 | 说明 |
|--------|--------|------|
| **水平速度误差** | $v_{xy}^* - v_{xy}$ | 期望水平速度 $v_{xy}^*$ 与实际水平速度 $v_{xy}$ 的差值 |
| **偏航率误差** | $\omega_z^* - \omega_z$ | 期望偏航角速度 $\omega_z^*$ 与实际偏航角速度 $\omega_z$ 的差值 |
| **基座垂直速度** | $v_z$ | 基座在垂直方向的速度，用于惩罚跳跃或下沉 |
| **滚转与俯仰率** | $\omega_{xy}$ | 基座绕 $x$ 轴和 $y$ 轴的角速度，用于保持平衡 |
| **z 轴偏差** | $\|[0, 0, 1]^T - e_z^b\|$ | 基座 $z$ 轴方向 $e_z^b$ 与重力方向 $[0, 0, 1]^T$ 的偏差，用于保持姿态 |
| **关节速度** | $\sum_{i \in \text{joints}} \|\dot{q}_i\|$ | 所有关节速度的绝对值之和，用于平滑运动 |
| **关节加速度** | $\sum_{i \in \text{joints}} \|\ddot{q}_i\|$ | 所有关节加速度的绝对值之和，用于减少急动 |
| **关节扭矩** | $\sum_{i \in \text{joints}} \|\tau_i\|$ | 所有关节扭矩的绝对值之和，用于节能 |
| **关节机械功率** | $\sum_{i \in \text{joints}} \tau_i \cdot \dot{q}_i$ | 所有关节的机械功率之和，用于优化能耗 |
| **动作变化率** | $\|a_t - a_{t-1}\|$ | 相邻时刻动作的差值，用于平滑控制 |
| **动作平滑度** | $\|a_t - 2a_{t-1} + a_{t-2}\|$ | 动作的二阶差分，用于更平滑的控制 |

**奖励函数示例**：

一个典型的奖励函数可能包含：

$$r(s_t, a_t, s_{t+1}) = c_1 \|v_{xy}^* - v_{xy}\|^2 + c_2 \|\omega_z^* - \omega_z\|^2 - c_3 |v_z| - c_4 \|\omega_{xy}\| - c_5 \sum_{i} \|\dot{q}_i\| - c_6 \sum_{i} \|\tau_i\|$$

其中各项前的符号表示：
- **正项**（如速度跟踪）：鼓励机器人达到期望行为
- **负项**（如关节速度、扭矩）：惩罚不希望的行为

**模仿奖励（Imitation Reward）**

由于腿部机器人具有**受生物启发的特性**，我们通常对其运动方式有大致的认知。这些行为可从人类或动物的**运动捕捉数据**（或在线视频）中获取，利用这些信息设计奖励信号，能极大地减少所需的工程工作量。

**数学表示**：

模仿奖励通常定义为机器人当前状态与参考运动之间的相似度：

$$r_{\text{imitation}}(s_t, a_t) = -\alpha \cdot d(s_t, s_t^{\text{ref}})$$

其中：
- $s_t^{\text{ref}}$：参考运动在时刻 $t$ 的状态（从运动捕捉数据中获得）
- $d(\cdot, \cdot)$：状态距离度量函数（如欧氏距离、姿态误差等）
- $\alpha > 0$：权重系数

**优势**：
- **减少奖励工程**：不需要手动设计复杂的奖励函数
- **更自然的运动**：模仿生物运动，产生更符合人类直觉的行为
- **数据驱动**：直接从真实运动数据中学习，避免人工设计的偏差

#### 1.4.5 Action Space

在强化学习中，**动作空间** $\mathcal{A}$ 定义了智能体可以执行的所有可能动作的集合。对于足式机器人，动作空间的设计直接影响控制策略的学习效率和性能。

**底层关节指令**

大多数四足机器人学习相关研究将**关节目标位置**作为动作空间（即 **PD 控制策略**）。

**PD 控制策略（位置控制）**

对于机器人的每个关节 $i$，给定该关节的动作值 $a_i(s)$（目标关节角度），电机将尝试生成扭矩：

$$\tau_i = k_p (a_i(s) - \theta_i) - k_d \dot{\theta}_i$$

其中：
- $k_p > 0$：比例增益（Proportional gain）
- $k_d > 0$：微分增益（Derivative gain）
- $\theta_i$：测得的关节角度
- $\dot{\theta}_i$：测得的关节角速度
- $a_i(s)$：策略网络输出的目标关节角度

**完整动作向量**：对于 $n$ 个关节的机器人，动作空间为：

$$a(s) = [a_1(s), a_2(s), \ldots, a_n(s)]^T \in \mathbb{R}^n$$

**重要说明**：正如 Hwangbo 等人（2019）所阐释的，PD 控制策略与机器人领域传统使用的**位置控制**有所不同：
- **位置控制**：控制器会以大增益跟踪期望的时间索引轨迹
- **PD 控制策略**：并不跟踪任何期望轨迹（其期望速度为零），且运动过程中始终无法达到目标位置

**扭矩控制策略**

近年来，Chen 等人（2023）、Kim 等人（2023a）的研究实现了**直接输出扭矩**的四足和双足机器人学习行为（即**扭矩控制策略**）。

**数学表示**：

$$\tau = \pi(s) \in \mathbb{R}^n$$

其中策略网络 $\pi$ 直接输出关节扭矩向量 $\tau$，无需通过 PD 控制器。

**方法对比**：

| 特性 | PD 控制策略 | 扭矩控制策略 |
|------|------------|-------------|
| **动作空间** | 关节目标位置 $a(s) \in \mathbb{R}^n$ | 关节扭矩 $\tau \in \mathbb{R}^n$ |
| **控制结构** | 有预设结构（PD控制器） | 无预设结构 |
| **策略评估频率** | 较低（如 50Hz） | 较高（如 1kHz） |
| **Sim-to-Real 迁移** | 更易实现（Bogdanovic et al., 2020） | 相对困难 |
| **灵活性** | 受预设结构限制 | 不受限制 |
| **计算开销** | 较低（策略更新频率低） | 较高（策略更新频率高） |

**注意**：事实上，扭矩控制策略和 PD 控制策略在真实机器人上均用于**扭矩控制**，区别在于扭矩的来源：
- **PD 控制策略**：通过 PD 控制器根据目标位置计算扭矩
- **扭矩控制策略**：策略网络直接输出扭矩

**结构化动作空间**

除了通过关节空间直接控制机器人外，许多研究还探索在**任务空间**（例如足部）中控制机器人。

**任务空间控制**

- **定义**：在任务空间（如足端位置）中定义动作，然后通过逆运动学求解关节角度
- **数学表示**：
  - 任务空间动作：$a_{\text{task}} = [p_{\text{foot}}, R_{\text{foot}}]^T$（足端位置和姿态）
  - 通过逆运动学求解：$a_{\text{joint}} = \text{IK}(a_{\text{task}})$
- **应用**：Krishna 等人（2022）、Duan 等人（2021）、Castillo 等人（2023）
- **优势**：
  - 提升学习效率
  - 控制架构更简洁
- **劣势**：
  - 需要求解逆运动学，可能出现**奇点问题**
  - 通用性不如关节空间直接控制

**关节空间 vs 任务空间**：

| 特性 | 关节空间控制 | 任务空间控制 |
|------|------------|-------------|
| **动作定义** | 关节角度 $a \in \mathbb{R}^n$ | 任务空间坐标（如足端位置） |
| **通用性** | 更强 | 相对较弱 |
| **奇点问题** | 无 | 可能出现 |
| **学习效率** | 相对较低 | 相对较高 |
| **主流方案** | 是 | 较少 |

> 通过**残差强化学习**（Johannink 等人，2019），可将机器人运动的先验知识嵌入动作空间。这里不赘述，不然偏题了
> 
> $$a_t = \hat{a}_t + \pi(s_t)$$

#### 1.4.5 学习范式

![](/blog/2025/locomotion-learning-framework.png)
##### 1.4.5.1 端到端学习 (End-to-end Learning)

**端到端学习**直接将原始感知输入映射到底层电机控制指令，无需人工设计的中间环节。

**数学表示**：

$$a_t = \pi(o_t)$$

其中：
- $o_t$：原始感知输入（如传感器数据、深度图、RGB图像等）
- $\pi$：策略网络（通常是深度神经网络）
- $a_t$：底层电机控制指令（如关节角度或扭矩）

**映射关系**：$\pi: \mathcal{O} \rightarrow \mathcal{A}$，其中 $\mathcal{O}$ 为观测空间，$\mathcal{A}$ 为动作空间。

**优势**：
- **减少人工设计**：理论上能减少人工设计的中间环节（如特征提取、状态估计等）
- **挖掘潜力**：最大程度挖掘系统的动态潜力，学习到人类难以设计的复杂行为

**挑战**：
- **搜索空间巨大**：从原始感知到控制指令的映射空间非常大，需要大量训练数据
- **仿真器要求高**：对仿真器的精确度要求极高，仿真误差会直接影响学习效果
- **奖励函数设计**：对奖励函数（Reward Function）的设计要求极高，需要精心设计才能引导策略学习

##### 1.4.5.2 课程学习 (Curriculum Learning)

**核心逻辑**

针对足式运动中 **"奖励稀疏"** 的问题（例如机器人还没走两步就摔倒了，很难学到正面反馈），采用**由易到难**的策略。

**数学表示**：

定义难度序列 $\{\mathcal{M}_1, \mathcal{M}_2, \ldots, \mathcal{M}_K\}$，其中 $\mathcal{M}_i$ 表示第 $i$ 个难度级别的环境（任务分布），且难度递增：

$$\text{difficulty}(\mathcal{M}_1) < \text{difficulty}(\mathcal{M}_2) < \cdots < \text{difficulty}(\mathcal{M}_K)$$

策略 $\pi$ 在难度序列上逐步学习：

$$\pi_1 \xrightarrow{\mathcal{M}_1} \pi_2 \xrightarrow{\mathcal{M}_2} \cdots \xrightarrow{\mathcal{M}_{K-1}} \pi_K$$

**做法**：
1. **初始阶段**：在平地上训练，让机器人学习基本的行走技能
2. **逐步增加难度**：
   - 增加坡度
   - 添加障碍物
   - 引入外部扰动（推力）
3. **保持稳健**：让策略在逐步演化中保持稳健，避免在困难环境中直接训练导致的训练失败

**优势**：
- **解决奖励稀疏**：在简单环境中更容易获得正面反馈，加速学习
- **稳定训练**：避免在困难环境中直接训练导致的训练不稳定
- **渐进学习**：策略逐步适应更复杂的环境

##### 1.4.5.3 分层学习 (Hierarchical Learning)

**核心逻辑**

将控制分为**高层逻辑**（做什么）和**底层技能**（怎么走），形成分层控制架构。

**数学表示**：

**两层架构**：

1. **高层策略**（High-level Policy）：
   $$g_t = \pi_{\text{high}}(s_t^{\text{high}})$$
   其中 $g_t$ 为高层目标（如"前往坐标 A"、"执行 Trot 步态"），$s_t^{\text{high}}$ 为高层状态（如当前位置、目标位置）。

2. **底层策略**（Low-level Policy）：
   $$a_t = \pi_{\text{low}}(s_t^{\text{low}}, g_t)$$
   其中 $a_t$ 为底层动作（如关节角度或扭矩），$s_t^{\text{low}}$ 为底层状态（如关节位置、速度），$g_t$ 为高层目标。

**完整映射**：

$$a_t = \pi_{\text{low}}(s_t^{\text{low}}, \pi_{\text{high}}(s_t^{\text{high}}))$$

**价值**：
- **降低学习难度**：将复杂任务分解为简单的子任务，降低学习难度
- **增强可解释性**：高层对应"大脑"规划（做什么），底层对应"小脑"平衡（怎么走）
- **独立学习**：各层级可以独立学习，提高学习效率

**特点**：
- 高层任务的动作空间会作为底层技能的输入
- 各层级独立学习，可以并行训练
- 底层技能可以在不同高层任务间复用

##### 1.4.5.4 特权学习 (Privileged Learning / Teacher-Student)

**问题背景**

现实世界中的机器人任务本质上具有**部分可观测性**，例如：
- **有效载荷重量**：难以精确测量
- **摩擦系数**：无法精确估计
- **外部扰动**：无法直接观测

这是目前 **Sim-to-Real** 的主流范式。

**数学表示**：

**完整状态空间**：$s_t = (s_t^{\text{obs}}, s_t^{\text{priv}})$，其中：
- $s_t^{\text{obs}}$：可观测状态（如 IMU、编码器数据）
- $s_t^{\text{priv}}$：特权信息（如足端摩擦力、重心变化、外部推力）

**教师策略（Teacher Policy）**：

在仿真中可以"开挂"，获得所有特权信息：

$$a_t^{\text{teacher}} = \pi_{\text{teacher}}(s_t^{\text{obs}}, s_t^{\text{priv}})$$

**学生策略（Student Policy）**：

只能通过机载传感器观察到的信息，需要预测特权变量：

$$a_t^{\text{student}} = \pi_{\text{student}}(s_t^{\text{obs}})$$

**知识蒸馏过程**：

1. **教师训练**：在仿真中使用特权信息训练教师策略 $\pi_{\text{teacher}}$
2. **学生训练**：通过模仿学习，让学生策略 $\pi_{\text{student}}$ 学习教师的行为：

$$\min_{\pi_{\text{student}}} \mathbb{E}_{s_t \sim \rho} \left[ \mathcal{L}(\pi_{\text{student}}(s_t^{\text{obs}}), \pi_{\text{teacher}}(s_t^{\text{obs}}, s_t^{\text{priv}})) \right]$$

其中 $\mathcal{L}$ 为损失函数（如均方误差），$\rho$ 为状态分布。

**意义**：

通过这种**蒸馏过程**，使学生模型在不具备"上帝视角"的现实世界中，也能具备极强的环境适应力。学生策略学会了从有限的观测中推断出特权信息，从而做出与教师策略相似的控制决策。


#### 1.4.6 Sim2Real Transfer

基于强化学习（RL）的控制策略通常在物理仿真器（如 **Isaac Gym**、**MuJoCo**、**PyBullet**）中训练，因为仿真速度快且无损耗。但策略往往在真机上失败，主因在于：

- **物理建模不精**：仿真中对摩擦力、地面硬度、关节阻尼的数学建模与物理现实存在偏差。
- **传感器噪声与延迟**：真实传感器的读数不稳定，且从指令发送到电机响应存在仿真中难以完全模拟的延迟。

##### Good System Design

**Good System Design**是这是"磨刀不误砍柴工"的第一步，通过约束策略分布来缩小鸿沟：


- **奖励函数设计 (Reward Design)**：避免产生仿真器特有的非法行为。例如：惩罚关节加速度以减少抖动，奖励足部腾空时间（**Air time**）以避免拖脚，惩罚足部冲击力（**Impact**）以避免跺脚。
- **观测与动作空间设计**：选择合适的 **PD 控制器**增益（低增益有助于实现顺应性）和状态估计量（如机身速度），能显著增强策略对干扰的抵御能力。
- **引入领域知识 (Domain Knowledge)**：利用物理对称性约束、运动捕捉数据（**Style promotion**）或 **CPG（中枢模式发生器）**结构来引导策略生成更符合自然规律的步态。

##### System Identification (系统辨识)

核心思想是提高仿真器的精度，让"假戏真做"：


- **执行器建模 (Actuator Dynamics)**：这是误差的主要来源。可以通过数学解析模型或神经网络黑盒模型（如 Hwangbo 等人的工作）来模拟真实电机的带宽限制和非线性响应。
- **接触模型辨识 (Contact Model)**：传统的刚性接触仿真与现实有差异。最新的研究（如 Choi 等人）开始引入可变形/顺应性接触模型，专门模拟沙地、草地等软地面环境。

##### Domain Randomization (领域随机化)

这是目前最常用的策略，类似于"鲁棒控制"的思想：

- **核心逻辑**：在训练时随机改变仿真环境的参数（如质量、摩擦力、电机强度、通信延迟等）。如果一个策略能搞定成千上万个不同的仿真环境，那么它更有可能适应从未见过的现实环境。
- **视觉随机化**：除了物理参数，近期研究也开始对相机内外参、噪声模型进行随机化，以实现视觉驱动的越野避障。
##### Domain Adaptation (领域自适应)

相比随机化，自适应策略更"聪明"，它会实时识别当前环境并调整策略：

- **显式参数识别**：通过策略实时预测当前环境的物理参数（如地面摩擦力），并将其作为输入传给控制器。
- **隐式隐空间表示 (Implicit Latent Representation)**：这是目前最流行的方法（如 **RMA 算法**）。将高维环境参数压缩到低维隐向量（**Latent vector**）中，机器人根据历史运动数据"感知"环境属性，从而实现更精准的步态调整。
- **利用现实数据改进仿真**：通过真实采集的运动数据来修正仿真器的转移函数（**Transition function**），使仿真环境更趋向于真实物理环境。
> TBD......

#### 1.4.7 Combining Control and Learning
**OC（最优控制）** 具有物理可解释性、能显式处理安全约束；而 **RL** 鲁棒性强、实时推断延迟极低。

##### Learning Control Parameters

这一方向旨在利用学习手段解决传统控制中最头疼的"调参"问题：

- **自动化调参**：传统控制器（如 **MPC** 或轨迹优化）中的增益（**Gains**）和权重通常需要专家手动调节。本章提到利用 $PI^2$ 或 **贝叶斯优化（BO）** 等方法来自动搜索最优的追踪增益、成本权重或关节阻抗。
- **边界与约束学习**：通过学习来确定模型不确定性的边界，从而设计更稳健的 **MPC** 控制器。
- **降维探索**：针对高维参数空间，先进行维度缩减（**Dimensionality Reduction**），然后在低维隐空间中应用 **BO** 以提高搜索效率。
##### Learning a High-level policy

学习组件与底层控制器“协同作战”，让机器人更聪明：
- **足端/触地规划 (Contact Planner)**：在复杂地形上，给 **MPC** 寻找安全的落脚点是极大的挑战。研究者利用 **RL** 学习地形感知策略，输出落脚点位置，再交给底层的 **MPC** 去优化地面反作用力。
- **残差策略 (Residual Policy)**：当物理模型与实际环境存在偏差（**Model Mismatch**）时，学习一个"残差项"来修正传统控制器的输出。
- **多模态步态切换**：利用高层策略根据环境变化（如斜坡、障碍物）动态地在不同步态之间进行切换。

##### Learning for Efficient Model-based Control

解决传统方法“算不动”的问题，利用学习来加速计算：

- **离线缓存与预测**：模型控制（如行走规划）在线计算量巨大。通过学习**哈密顿函数（Hamiltonian）**或**价值函数（Value Function）**，可以极大地减少在线求解优化问题的负担。
- **热启动 (Warm-starts)**：利用大型轨迹优化数据集训练模型，为 **MPC** 实时求解非线性规划提供良好的初始值（即"热启动"），从而实现快速收敛。
- **前向模型学习**：学习一个快速运行的动力学前向模型，嵌入到基于模型的模块中，提高控制频率。

##### Model-based Control for Efficient Learning

利用控制理论来解决强化学习“采样效率低、漫无目的探索”的问题：

- **引导探索 (Guided Exploration)**：在稀疏奖励（**Sparse Rewards**）的环境下，利用最优控制生成的轨迹来引导 **RL** 寻找高奖励区域。
- **专家示范 (Demonstrations)**：使用 **OC** 生成的大量成功样本进行**模仿学习（IL）**或行为克隆，作为 **DRL** 训练的起点，大幅提升学习效率。
- **简化模型引导**：利用简化的低阶模型（**Reduced-order models**）进行宏观规划，同时让 **RL** 学习如何补偿简化模型与全量机器人模型之间的差异。 

> TBD......

### 1.5 Bottlenecks & Future Frontiers

#### 1.5.1 Limits

**结构与硬件的局限性**：目前足式机器人的机械结构往往缺乏刚柔耦合的平衡，导致在复杂地形搬运重物时重心（**CoM**）不稳定。同时，关节执行器的带宽直接限制了跳跃和奔跑等高动态动作的稳定性。

**控制算法的泛化性不足**：虽然现有技术能让机器人"走起来"，但在复杂、动态且不确定的环境中，缺乏足够的泛化能力和适应性。

**人机交互（HRI）的缺失**：目前机器人对人类意图的理解非常有限，反馈机制也不够友好和准确。

**Sim-to-Real Gap（仿真到现实的差距）**：
- **难点**：仿真器中的摩擦力、碰撞动力学和关节延迟很难百分之百模拟
- **后果**：在仿真中能跑酷的机器人，到了实验室可能因为一个微小的接触不确定性直接摔倒

**Reward Shaping 的复杂性**：
- **难点**：为了让机器人学会"像生物一样优雅地走"，研究者需要设计复杂的奖励函数（**Task objective + Regularization rewards**）
- **痛点**：奖励项太多会导致训练难以收敛，奖励太少会导致机器人出现"非人类"的奇怪姿态

**黑盒特性与安全性 (Safety & Interpretability)**：神经网络生成的策略缺乏物理上的可解释性，难以像传统方法那样提供数学上的稳定性证明（如 **Lyapunov 稳定性**）。

**感知与计算的挑战**：处理高维度的视觉信息（深度图、点云）在嵌入式端存在计算压力，导致感知集成存在延迟。

#### 1.5.2 Future works

**具身智能与持续学习 (Embodied Intelligence)**：未来的足式机器人不应只具备基础的感知和行为，而应在行走和工作过程中不断学习，逐步提升智能化水平。

**大模型（MLLM/Foundation Models）的深度集成**：
- 利用多模态大模型的先验知识，机器人可以做出更及时、准确的行为控制决策，并实现更自然的交互
- 利用 **VLM (Vision-Language Models)** 将语义理解引入运控。例如，不仅是"避障"，而是理解"前面是草地可以踩，前面是水坑不能踩"

**软体足式机器人 (Soft-bodied Robots)**：利用新材料、仿生学和柔性技术解决传统刚性结构的效率瓶颈，增强在不确定环境中的适应力。

**多机协作**：高智能水平的多个足式机器人之间的任务规划、分配与协调。

**Unsupervised Skill Discovery（无监督技能发现）**：让机器人自己在仿真中探索，无需人工设计步态，自动习得"跳跃"、"翻滚"等技能。

**Loco-manipulation（运控-操作一体化）**：不再把"走"和"拿东西"分开，而是像人类一样在行走的过程中协调肢体完成复杂任务。

**Generalist Policies（通用策略）**：训练一个"大脑"，能同时驱动四足、双足甚至是形态完全不同的机器人，实现跨平台的知识迁移。

## 2. 具体工作

**方法对比与区分**

下面这两篇论文在方法上很难严格区分，在高维度很难泾渭分明地去定义。

**Task Domain（切入点）**：
- **MoE-Loco**：主要涉及多任务与多形态适应
- **PIE**：在极限灵巧运动中（跑酷）

**方法上的创新点**：

- **PIE**：解决了显式感知（高程图）的"物理意义明确但脆弱"与隐式感知（端到端 **Latent**）的"鲁棒但不可控"之间的二元对立，加了个**双流估计器**，强制网络回归具有物理意义的状态（如脚下高度、速度），同时保留隐向量捕捉摩擦力等难以建模的属性。

- **MoE-Loco**：主要着眼解决**MTRL**的梯度冲突问题，因为动力学物理规律无法避免，面临"钻洞（低重心）"与"站立（高重心）"这类物理特性截然相反的任务时，不再强制一个网络学习所有参数，而是通过**门控网络**实现参数隔离。

### 2.1 PIE
- [**PIE: Parkour with Implicit-Explicit Learning Framework**](../../../papers/locomotion/pie-implicit-explicit), IEEE Robotics and Automation Letters (RA-L), 2024

### 2.2 MoE-Loco
- [**MoE-Loco: Mixture of Experts for Multitask Locomotion**](../../../papers/locomotion/moe-loco), preprint


## Reference

- Learning-based legged locomotion: state of the art and future perspectives, The International Journal of Robotics Research (IJRR), 2024.11.22
- A survey on legged robots: Advances, technologies and applications, Engineering Applications of Artificial Intelligence (EAAI), 2024.10
