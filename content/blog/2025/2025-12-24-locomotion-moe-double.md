---
date: 2025-12-24
title: 足式运控综述一：基于Moe架构与隐显双重感知表征的演进路径
authors: [bubblevan]
tags: []
---

## 1. 绪论

### 1.1 足式运控技术的发展背景与现状

足式机器人（Legged Robots），作为具身智能（Embodied AI）的重要载体，近年来经历了从"基于模型的控制（Model-Based Control）"向"基于学习的控制（Learning-Based Control）"的深刻范式转移。传统的控制方法，如模型预测控制（MPC）和全身控制器（WBC），依赖于精确的动力学建模和状态估计，虽然在平坦地形上表现出色，但在面对非结构化环境、复杂地形扰动以及高动态动作需求时，往往面临建模误差和实时算力瓶颈。

随着深度强化学习（Deep Reinforcement Learning, DRL）的引入，特别是 Proximal Policy Optimization (PPO) 等算法的成熟，足式机器人在鲁棒行走、抗扰动以及盲式地形适应方面取得了突破性进展。然而，随着研究的深入，当前的单一神经网络策略（Monolithic Policy）逐渐显露出两大瓶颈，这构成了本报告调研的核心动机：

**1. 多任务通用性与技能专精的矛盾（Versatility vs. Specialization Trade-off）**

传统的单一 MLP（多层感知机）策略在学习多种异构任务（如四足行走与双足站立，或平地奔跑与复杂越障）时，由于不同任务在参数空间中的梯度方向不一致，极易产生"梯度冲突（Gradient Conflict）"，导致"灾难性遗忘"或各任务性能平庸化。

**2. 非结构化环境感知的可靠性鸿沟（The Reliability Gap in Perception）**

在进行高难度跑酷（Parkour）时，机器人需要极高精度的环境感知。然而，传统的显式高程图（Elevation Mapping）易受传感器噪声和定位漂移影响，而纯隐式（End-to-End Implicit）方法虽然鲁棒但缺乏物理约束，导致 Sim-to-Real 迁移困难。

### 1.2 调研范围与对象

本技术报告基于"多任务架构"与"极限感知"两个维度，选取了代表当前最前沿技术路线的两篇核心文献进行深度剖析：

**架构创新方向：** "MoE-Loco: Mixture of Experts for Multitask Locomotion" (IROS 2025)。该工作通过引入大模型领域的混合专家（MoE）架构，解决了多任务强化学习中的梯度冲突问题，实现了单策略下四足与双足步态的共存及技能涌现。

**感知创新方向：** "PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots" (RA-L)。该工作提出了一种隐式与显式特征融合的感知框架，解决了低成本传感器下高动态跑酷的感知鲁棒性问题。

## 2. 预备知识

### 2.1 高程图（Elevation Map）

**定义：** 高程图是一种二维网格地图，其中每个网格单元存储该位置的地面高度值。它是足式机器人感知环境地形的主要方式之一。

**特点：** 高程图采用显式表示方式，直接存储高度信息，便于理解和可视化。其计算效率高，可以快速查询任意位置的高度。然而，高程图也存在局限性，易受传感器噪声、定位漂移和遮挡影响。

**应用场景：** 路径规划、地形适应、障碍物检测

### 2.2 混合专家模型（Mixture of Experts, MoE）

**定义：** MoE 是一种神经网络架构，通过多个"专家"网络（Expert Networks）和一个"门控网络"（Gating Network）来实现条件计算。

**核心思想：** MoE 通过门控网络决定每个专家的权重，使得不同的输入激活不同的专家网络。这种设计通过稀疏激活提高计算效率，同时可以在单一模型中实现多任务学习，避免梯度冲突。

### 2.3 隐式 vs 显式感知

**显式感知（Explicit Perception）：** 显式感知直接表示环境特征（如高程图、点云），具有可解释性强、便于调试的优点。然而，显式感知易受噪声影响，需要精确的传感器标定。

**隐式感知（Implicit Perception）：** 隐式感知通过端到端学习直接从原始传感器数据提取特征，具有鲁棒性强、适应性强的优点。然而，隐式感知缺乏物理约束，Sim-to-Real 迁移困难。

**混合方法：** 结合两者的优势，既保持鲁棒性又引入物理约束

## 3. 多任务强化学习中的梯度冲突

在深入 MoE-Loco 之前，必须厘清多任务强化学习（MTRL）在足式运控中面临的根本挑战。

### 3.1 梯度冲突的数学本质

在训练一个旨在掌握 $N$ 种不同运动技能（如爬楼梯、钻洞、双足站立）的单一策略网络 $\pi_\theta$ 时，网络的参数 $\theta$ 需要同时最小化所有任务的损失函数。总损失通常定义为加权和：

$$\mathcal{L}_{total}(\theta) = \sum_{i=1}^{N} w_i \mathcal{L}_i(\theta)$$

然而，不同任务对网络特征提取层的更新方向可能截然不同。例如，"钻洞（Baffle Crawling）"任务要求机器人极度降低重心，压缩关节空间；而"双足站立（Bipedal Standing）"任务则要求机器人极度抬高重心，伸展关节空间。

在反向传播过程中，任务 $i$ 的梯度 $g_i = \nabla_\theta \mathcal{L}_i$ 与任务 $j$ 的梯度 $g_j = \nabla_\theta \mathcal{L}_j$ 之间的余弦相似度可能为负：

$$\cos(g_i, g_j) < 0$$

这种现象被称为**梯度冲突（Gradient Conflict）**。

**梯度冲突的后果：**

在传统的共享参数架构中，这种冲突会导致参数震荡，即网络参数在优化过程中不断震荡，难以收敛。此外，还可能发生任务主导（Task Domination），即网络收敛到某个主导任务的局部最优解，牺牲其他任务的性能。最严重的情况是灾难性遗忘，即学习新任务时，旧任务的性能急剧下降。

**早期解决方案的局限性：**

早期的解决方案如 **PCGrad（Projecting Conflicting Gradients）** 试图通过将冲突梯度投影到法平面来缓解这一问题，但这增加了计算开销，未从根本上解决网络容量与表征能力的瓶颈，仍然受限于单一网络的表达能力。

### 3.2 混合专家模型（MoE）的引入逻辑

#### 3.2.1 MoE 在 NLP 领域的成功

混合专家模型（Mixture of Experts, MoE）最初在自然语言处理（NLP）领域大放异彩（如 GPT-4），其核心思想是将一个巨大的稠密网络拆分为多个并行的"专家（Experts）"子网络，通过一个"门控网络（Gating Network）"动态路由输入数据，使得每个专家专注于处理特定类型的输入模式。

**MoE 的优势：** MoE 通过稀疏激活实现了参数效率，可以在保持模型容量的同时减少计算量。每个专家可以专注于不同的数据模式或任务，实现专业化。此外，MoE 具有良好的可扩展性，可以轻松添加新的专家而不影响现有专家。

#### 3.2.2 MoE 在足式运控中的应用

将 MoE 引入足式机器人运控（如 MoE-Loco 所做的那样），本质上是在连续控制空间中构建了一种**动态的参数隔离机制**。

**核心机制：** MoE 通过让不同的专家专注于不同的运动模式（Locomotion Modes），在物理层面分离冲突的梯度流，实现参数隔离。同时，通过共享的底层特征保持任务间的正向迁移。门控网络根据当前状态和任务需求，动态选择激活哪些专家，实现动态路由。

**架构演进：** 这代表了足式机器人运控从"参数共享"向"动态路由"架构演进的重要一步。传统架构中，所有任务共享所有参数，容易产生梯度冲突；而 MoE 架构中，不同任务激活不同的专家，实现参数隔离，同时保持共享特征提取层。

### 3.3 梯度冲突的量化分析

为了更好地理解梯度冲突，我们可以通过以下指标来量化：

**梯度相似度（Gradient Similarity）：**

$$\text{Similarity}(g_i, g_j) = \frac{g_i \cdot g_j}{\|g_i\| \|g_j\|} = \cos(g_i, g_j)$$

当相似度大于 0 时，两个任务的梯度方向一致，可以协同优化；当相似度小于 0 时，两个任务的梯度方向相反，产生冲突；当相似度约等于 0 时，两个任务的梯度方向正交，互不影响。

**任务相关性矩阵：** 可以通过计算所有任务对之间的梯度相似度，构建任务相关性矩阵，用于识别哪些任务之间存在冲突，设计更合理的任务分组策略，以及优化专家分配方案。

### 3.4 MoE 架构的技术细节

#### 3.4.1 门控网络（Gating Network）

门控网络 $G(s)$ 根据当前状态 $s$ 输出每个专家的权重：

$$w = G(s) = [w_1, w_2, ..., w_E]$$

其中 $E$ 是专家的数量，$\sum_{i=1}^{E} w_i = 1$。

**门控网络的训练：** 门控网络与专家网络联合训练，通过端到端优化学习最优的路由策略。为了提高效率，可以使用 Top-K 路由（只激活前 K 个专家）。

#### 3.4.2 专家网络（Expert Networks）

每个专家 $E_i$ 是一个独立的神经网络，负责处理特定类型的输入：

$$y_i = E_i(s)$$

**专家的专业化：** 不同专家可以专注于不同的运动模式（如四足行走、双足站立、爬楼梯等）。通过训练，专家会自动学习到不同的特征表示。

#### 3.4.3 输出融合

最终的输出是所有专家输出的加权和：

$$y = \sum_{i=1}^{E} w_i \cdot E_i(s)$$

这种设计既保证了专业化，又保持了灵活性。

