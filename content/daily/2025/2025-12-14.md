---
slug: 14-dec-2025
title: 12-14
authors: [bubblevan]
tags: []
date: 2025-12-14
---

## 今日总结

另外又一个优秀的东南大学博主 [momo](https://xhslink.com/m/6aPeI4FNFV6)，其主要的笔记是一些 **SLAMer 拓展技术栈 Day n** 系列，我觉得这里的**三维重建**很有意思。同时他还是**智元实习生**！

还有这位 [laumy 的学习笔记](https://www.laumy.tech/)，非常牛逼，包括了一系列比如 **ROS2 实践**、**ACT 算法解析**的文章博客。

记录一下 **XLAB** 里面北美 **CS PhD** 对于 **agent** 的学术讨论，我看到这种交流真的是热泪盈眶：

> **再过两年现在的 agent 是不是都会废了？**  
> 模型自己就可以

> 我觉得 **agent 的定义**本身就还在演变，很多功能以前的 agent 需要，现在可能就不需要了。但 **agent 本身要做的事情应该和模型是区分开的**。  
> 现在**多模态模型**的未来预期是像人脑一样，能接受多模态的输入，能给出思考结果。这个结果不局限于文本，可能是 **action（VLA）**，可以是语音，图像（**omni model**），也可能是一个指令。  
> 而 **agent 更多侧重于 action**，拿到指令怎么去执行。  
> 过往的 agent 其实不少功能，比如**长期记忆**、**规划**、**自我反思**这些功能其实是之前**大模型**这几块能力不足的时候的一种过渡方案，之前的大模型不会思考，那我自己搭一个 agent 专门让模型只做思考等等，大模型的长期记忆不行，我要装个记忆模块，我要用 **RAG** 等等。但最近大模型的思考和记忆都在飞速提升，现在的 agent 也不是都需要一个记忆模块了，模型本身能力够用了。不过真的要执行指令，比如说**工具调用**、**图像标注**等都需要进一步的操作，这件事大模型本身可能并不会去做。**后续 agent 的定义可能也在演变**。

> 我觉得未来的**大模型本身可能是个很强的大脑**，他的泛化能力很强，是个全才，但不至于说什么都能做的很好。你要写代码，我要用专门的 **code agent**，比如 **Cursor**、**Claude Code**。你要用到**具身智能**里，要调用机械臂操控抓取，你要让大模型、**VLA** 输出 action。**agent 的概念感觉更偏下游**，至少我觉得模型本身再全能，应该也不至于全能到通杀所有领域，只要下游有需求就有专门为下游搭建一个 agent 框架的可能，你模型代码能力再强，能一口气输出再完美的代码，也还是要套个 agent 的壳子才能用的。

> 比如说现在很多技术解决**上下文不足**的问题，要是之后真的可以把整个代码仓库塞进去，那这些**记忆管理**之类的 trick 还有意义吗？

> 现在模型的**上下文长度已经很夸张了**（context_length 每年大概翻 30 倍）。  
> 但大多 paper 证明自己上下文能力大多就**大海捞针**说自己几百万上下文还能记住中间的某几个词。  
> 但实际应用上还是**效果说话**吧，记住了不等于效果好。  
> 比如现在大模型上下文很长，但很明显在对话一段时间后模型回复的质量会有所下降，如果一些**记忆 trick** 做出来的效果比没用 trick 好就很有意义。  
> **Claude Code** 本身也有一个超长下文对话后会主动对上下文做**压缩**的阶段。  
> 未来考虑到运行几天的 agent 模式，**无限上下文是发展趋势甚至说必然结果**的话，**记忆压缩大概率也还是有必要的**。  
> 用人脑对比，人能记住一大堆东西，但为了效率可能不如就记住今天要考试的关键几页内容，别的都忘了，**检索效率和回答效果都可能更好**。  
> **更好的记忆管理，或者压缩手段哪怕在无限上下文实现了我觉得也还是有意义的**，节省成本和提升效果能做到一个就 ok。

> 如果把 **LLM 视为人类的下位替代**的话，那所有人类遇到的记忆问题大模型最终也无法避免？

> 对于普通文本任务**上下文压缩有效**，但是目前来说遇到代码的话就失灵了……因为具体 **code 没法被"总结"**。

> 这个倒可以稍微引申一下，比如代码这种任务往往希望记忆里存储的是完整的代码信息，这个很难压缩，但除了代码领域同样也有很多需要**高精度记忆**的任务，这样的**类 code 领域的记忆压缩就有很多问题**。至少现在 **Gemini CLI** 的做法还是挺相对粗暴的，先对历史进行分割，调用大模型分块压缩，summary 总结。压缩用精心设计的 prompt。但这种方法直觉上说想一种**暂时的妥协**。

> 但是我觉得所谓的 **agent 的职能被慢慢统一到上游是可以遇见的**吧。

> 这个我觉得还是**具体问题具体分析**吧，我喜欢对比人脑单纯是便于我自己理解现在大模型能做什么不能做什么，然后给一种可能的解释，然后从人的思维角度思考有没什么解决方案。  
> 实际上很多任务 **LLM 都是人类是上位了**。单论记忆问题，LLM 可以一分钟记住数万字信息还能复述，人就不太行了。  
> 嗯，我觉得可能 **agent 的功能可能也随着 LLM 增强而改变**，agent 本身这个概念可能也在迭代。之前 LLM 不能做的要 agent 做的可能现在 LLM 能做了 agent 就不做了，比如说之前 agent 往往有专门的**思考模块**。

> 那我觉得要你从**学术角度**看还是从 **application 角度**看了。  
> 而且我觉得所谓的 agent 也不过就是一种加 **human prior** 的方式。

> 所以最后还是要看**工程实践**。  
> **数学/概念上等价的东西不代表实践中是等价的**。

> 那肯定是的。  
> 而且涉及到真的取不取代你还得看发展的多快，**旧东西会不会变成历史遗留问题**。

> 我理解 **Agent 是 LLM + tools / MCP**；**私有的 tools/MCP 不可能完全被大模型集成**，所以 agent 大概率还是会一直存在的。

> **agent 和 LLM 定义上最大的区别就是和环境的交互**吧，目前而言应该也没有必要让 LLM 往直接绕过 tool 之类的东西直接和环境交互方向发展。

> **GUI agent 算环境交互吗？**

> **GUI agent** 其实不太涉及到本地环境的操作，更多的是对打开的页面 / 手机的操作。  
> 我们组现在在搞的就是 **GUI agent**。  
> 目前实测下来基本不用自定义 **tools / MCP**。

> 以后会不会出现**面向 AI 的 UI 设计**？

> **GUI agent** 这个东西其实和爬虫差不多，鉴于目前已有的各种反爬措施，大概率不会有专门面向 GUI agent 的服务。  
> **更可能的是反 GUI agent**。

> 这你让我想起了**造机器人是因为现在环境是为人设计的**这个 story。

> 我**完全不 buy 这个 story**。

> 合理。  
> 前两天**豆包那个就被反了**。

> 一本浓厚 **CS PhD 气息**。

> 这个 **buy story 太典了**。

> 感觉 **GUI agent 有点多此一举的味道**。

> **UI 本来就是机器适应人类用的**。

## 技术笔记

见 [2025-12-14-habitat-common-sense](../blog/2025-12-14-habitat-common-sense.md)

## 明日计划
