---
slug: 31-dec-2025
title: 12-31
authors: [bubblevan]
tags: []
date: 2025-12-31
---
## 今日总结
今天有玉泉狂欢夜诶！感觉很棒，但是任何美景和活动其实归根到底还是要和人一起，才有意义。虽然也有一种意义在于，所谓“这是我在本科毕业前参与的最后一次跨年庆典了”，但实际上就算毕业了，回来用校友身份也能进校，也能看上。有人追问说那能一样吗？我说还真就一样，如果我还是一个人去看的话，无非是拍照游园逛吃，烤全羊还会破坏我的定量饮食规划。
好吧我是打大牌的胡适，我玩了4个游园会项目抽了一个三等奖雨伞，领了一个小之又小的烤全羊（盒装），猛干一碗羊骨汤（只吃了肉）和至少8串羊肉串（5RMB）+1串鸡腿肉（6RMB），还有一个烤红薯和一罐奶啤（没喝完），20RMB的车厘子，点了牛肉和牛肋骨吃不下去，晚上晚点再吃。

[这则帖子](https://www.xiaohongshu.com/explore/68e0ee380000000005012925?xsec_token=ABIlao1cbioCPIN2zjzTyhor-n-GMB8PaWHBz0xr8ajjw=&xsec_source=pc_search&source=unknown)描述的是在gpufree这个平台上进行Issac sim/lab仿真的一些技巧，因为这个平台已经把镜像给预装好了，回到AutoDL又要headless装好久并且还有100GB的assets要求（尽管gpufree暂时也没有）。然后这是这位科广爷的[Github](https://github.com/anlorla/EAI-World/blob/main/Codes/Codes.md)，奇怪的地方在于是Github里塞飞书，我现在已经不打算继续用飞书文档了，另外就是得亏某位科广老师的宣传，SocialNav概念每个人貌似都在搞。

投了一个Lins Lab的NUS的地方的科研实习，做的[LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating](https://social-nav.github.io/LISN-project/)，能中的话就去，线下也可以，我拾掇拾掇去新加坡；不行的话，我继续搁杭州待着找实习去。

话说回来我重新看了一眼之前那个AMAP-EAI的工作，他居然是反应式的吗？不过也不奇怪。

[SocialNav-SUB Benchmarking VLMs for Scene Understanding in Social Robot Navigation](https://github.com/LARG/SocialNavSUB)这篇不错，是CoRL2025关于VLM在SocialNav上评估的一个已开源的好工作，可以作为AMAP-EAI的代餐。
```
- **上下文增强实验**：当 **VLMs** 使用人类标注的**空间推理答案**作为上下文时，**社交推理 PA** 提升约 **24%**（如 **Gemini 2.0** 从 0.63→0.78）
- **指导意义**：实际应用可采用"**混合架构**"——用**专用感知模块**（如**3D 目标检测**、**轨迹预测**）处理空间/时空推理，将结果作为 **VLMs** 的输入上下文，重点发挥 **VLMs** 在**社交推理**上的优势，提升整体**导航决策**的准确性
```
WM 这种预测式方法可以作为一个idea，额外感知输入也可作为一个idea，或者这两个其实是一种？
意图预测还可以看看IAD或是MID。
现在idea不少，差的是实验。说到底Socialnav-Sub针对的是VLM本身的Bench，但是我加上Action head呢？我的实际场景中呢？

## 技术文档
### 今日论文
content\papers\vln\socialnav-sub.md
### 今日课程
content\docs\self-study\ai\rl\hands-on-rl\basic.md


## 明日计划

对了记得1.3玉泉Locomotion开会，这之前把新综述的看法整理一下
1.4本科毕设汇报，距离1.13只剩9天，得赶快搞上日程了。
