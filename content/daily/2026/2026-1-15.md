---
slug: 15-Jan-2026
title: 1-15
authors: [bubblevan]
tags: []
date: 2026-01-15
---

## 今日总结
今天听了[穆尧老师的分享](https://zju-xlab.feishu.cn/minutes/obcnan1ym8pg283wnf79k3j6?from_source=finish_recording)

有点感慨的是，如果没有一些基础的话，连问题都提不出来。


人在环（HITL）社交导航的最新基线可分为传统规则型、学习增强型、LLM融合型和专用HITL框架四类:

• ORCA (Optimal Reciprocal Collision Avoidance)

• A* + ORCA混合

• [CityWalker (CVPR 2025)](https://github.com/ai4ce/CityWalker)

• [Following the Human Thread in Social Navigation ( ICLR 2025 Spotlight)](https://github.com/L-Scofano/SDA)
- 该研究直面一个现实难题：在真实世界中，机器人无法总是精准、完整地预测每个人的未来轨迹。SDA的创新在于，它不依赖对未来轨迹的完美预测，而是训练机器人通过分析自己过去一系列的状态、动作和与人类的过往接触情况，来隐式地推断并适应周围人群的流动趋势与社会规则（即“社会动力学”）。这使得它在人类运动突然变化或部分被遮挡时更加鲁棒。
- 两阶段强化学习：阶段一使用特权信息学习；阶段二仅凭自身历史推断社会动态。
- 基于Habitat仿真器；在HSSD等社交导航数据集上验证。

| 对比维度 | **SDA (Social Dynamics Adaptation)** | **Falcon (ICRA 2025)** |
| :--- | :--- | :--- |
| **核心目标** | 在**无法获知或预测他人未来轨迹**的“盲”环境下，通过自身历史经验**自适应推断**社会动态。 | 在**能够观测**到行人的环境中，**显式预测**行人未来轨迹，实现主动、安全的规划。 |
| **技术焦点** | **社会动态的隐含推断**。属于两阶段强化学习，强调模型的**内省和适应**能力。 | **未来轨迹的显式预测**。将轨迹预测作为辅助任务集成到强化学习中，强调**前瞻规划**。 |
| **应用场景** | 更偏向**理论化、极限化**的挑战，适合**感知受限**或行为模式高度不确定的动态环境。 | 更接近**实用化、具身化**的机器人导航，需要机器人配备足以观测行人的传感器。 |

• [HA-VLN 2.0 (NIPS 2024)](https://github.com/F1y1113/HA-VLN)：离散-连续环境的人感知导航基准，含16,844条以人为中心的指令，提供HA-VLN-VL与HA-VLN-CMA基线实现（2025）
- 建立统一的人感知VLN基准测试，支持离散与连续环境。
  - HAPS 2.0数据集：提供了大量高质量的3D人体动作模型（如“打电话踱步”、“举杯庆祝”），并将其自然地置入多样的室内外3D场景中。
  - HA-R2R指令集：生成了上万条明确描述人类活动的导航指令（例如，“绕过那群正在干杯的朋友”）。
- 在此基础上，它提供了从视觉语言理解（HA-VLN-VL）到多模态决策（HA-VLN-CMA）的基线模型，让不同研究者的工作可以在同一标准下进行公平比较
- 覆盖90个场景；16,844条以人为中心的导航指令。
• Social-HM3D/Social-MP3D(Falcon)：高精度3D场景+自然人类行为，适配Falcon、SocialNav-Map等基线

• SocialNav-SUB：评估VLMs在社交导航中的场景理解能力，含人类基线与规则基线（2025, CoRL）

• [FreeAskWorld Simulator (AAAI26 Oral)](https://github.com/AIR-DISCOVER/FreeAskWorld)
- 它旨在解决传统导航任务中智能体被动接受指令的局限。其核心是创造一个能主动提问、理解人类模糊指引并实时调整计划的智能体。例如，当导航指令不明确时，智能体会像真人一样询问路人，并根据对方的手势或语言描述（如“在穿红衣服的人后面左转”）来行动。这通过LLM驱动的智能体、高度可定制的人类行为模型和一个闭环系统（智能体行动会实时改变环境状态）共同实现。
- 合成数据集（6.3万+帧）；“主动问路”任务基准。
• [Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments (ICRA 2024)](https://arxiv.org/pdf/2310.07335)

1.  **HA-VLN 2.0：统一的“考场”**
    它就是一个**基准测试平台**。它的核心贡献是提供了一个包含大规模场景、人类活动和指令的标准化测评体系（含数据、仿真器和评估指标），并提供了HA-VLN-VL和HA-VLN-CMA这两个**基线模型**（可以看作是“参考算法”）来定义任务的起跑线。

2.  **FreeAskWorld：交互的“新科目”**
    它是一个全新的**综合实验框架**。它不仅定义了一个叫“方向询问”的**新任务**（即“主动问路”），还为此构建了支持闭环交互的**仿真平台**，并发布了对应的**合成数据集**作为基准。在它的研究中，也包含了用于完成该任务的**智能体算法**（如基于LLM的规划器）。因此，它确实是“算法+基准+仿真器”的结合体。

3.  **SDA & Falcon：应试的“解法”**
    它们是针对不同“考题”（社交导航的特定子问题）提出的不同**算法**。无论是HA-VLN 2.0还是FreeAskWorld这样的基准平台，都可以用来测试和比较像SDA、Falcon以及其他算法的性能。

当然最后我还是倾向做的像InternVLA-N1和NaVILA那种双系统
另外InternUtopia 说是也支持GRResidents：LLM 驱动的 NPC 系统，支持社交交互、任务生成 / 分配，模拟社会场景来着？

---
CUHK毕业时间调研
[一个日历年中的学位授予日期——2024-25学年及以后毕业的学生](https://www.gs.cuhk.edu.hk/page/Graduating2024-25)
[毕业流程](https://www.gs.cuhk.edu.hk/page/Graduation#G1)
[常见问题（日历年中的学位授予日期——2024-25及以后毕业的学生）](https://www.gs.cuhk.edu.hk/page/Graduating2024-25_FAQ?f_link_type=f_linkinlinenote&flow_extra=eyJkb2NfcG9zaXRpb24iOjAsImRvY19pZCI6IjBhZGU0YjBiMWY2MzQzMTktNGRlMTA4ZWYwMjdhNDVkMyIsImlubGluZV9kaXNwbGF5X3Bvc2l0aW9uIjowfQ%3D%3D)

## 技术文档
content\docs\web\languages\python3\algorithms\stack\_index.md

## 明日计划

预约下周一去解放路还是哪里打激光吧，周末就算了
然后我比较希望的点是
