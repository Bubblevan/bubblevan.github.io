---
slug: 7-Jan-2026
title: 1-7
authors: [bubblevan]
tags: []
date: 2026-01-07
---

## 今日总结

### 给个交代
| Title | Arxiv | Summary |
| --- | --- | --- |
| SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning | https://arxiv.org/abs/2512.14757v1 | 为解决服务机器人在医疗、餐饮等人类密集环境导航中，现有方法或侧重安全忽视社交合规性（如人际距离、通行礼仪），或学习型方法（RL/IL）依赖大规模高质量数据集且泛化性差，传统大视觉语言模型（VLM）参数规模大（百亿级）、推理延迟高、能耗高难以适配资源受限机器人实时部署的问题，研究提出了面向社交合规导航的混合专家（MoE）架构小VLM——SocialNav-MoE，其通过强化微调（RFT）实现“精度-效率平衡”，还设计了基于BERTScore-F1的语义相似性奖励（SSR），以解决传统RFT中硬奖励和字符级奖励的刚性缺陷，同时系统分析了小语言模型（SLM）类型、专家数量/路由策略、视觉编码器等关键组件的影响，为社交合规导航提供设计指南；该模型采用三阶段训练流程，第一阶段监督微调（SFT）通过图像-文本对齐优化视觉与文本编码器一致性，并基于SNEI数据集（含“场景感知-人类行为预测-导航决策”多轮对话标注，共325张图像，训练集265张增强后为530张、测试集60张）学习“多模态输入→导航动作”映射，第二阶段强化微调（RFT）采用Group Sequence Policy Optimization（GSPO）框架，利用SSR捕捉语义级匹配以稳定MoE模型训练，第三阶段MoE微调（MoEFT）通过Top-k路由（激活得分最高的k个专家）进行多轮对话微调，提升上下文连贯性与社交行为稳定性；实验中，模型最优配置为SLM选用Phi-2-2.7B（在BERTScore、SBERT等指标优于Qwen-1.8B、StableLM-1.6B）、视觉编码器采用冻结训练的SigLIP（比CLIP性能高1.8%且避免过拟合）、MoE配置为4个专家+Top-k=1路由（SMS比1个专家提升10.6%），训练参数上初始化学习率1×10⁻³训练1周期，SFT学习率2×10⁻⁵训练20周期，RFT和MoEFT学习率均为2×10⁻⁶分别训练3 epoch和20 epoch，评估从效率（参数量、FPS）和精度（BERTScore、SBERT、SMS）维度展开，结果显示，SocialNav-MoE参数量仅5.74B（为GPT-4o的2.9%、Claude的3.3%），FPS达1.709（为GPT-4o的8.1倍、Claude的19.6倍），SMS比GPT-4o高46.5%、比Claude高32.1%，消融实验进一步验证了Phi-2-2.7B、4专家+Top-k=1、冻结SigLIP、SSR及多轮对话+RFT的优势；不过，该研究存在社会规范“模糊性”问题（部分场景中模型输出与ground truth均符合社交合规性，缺乏统一评估标准），未来计划将模型部署到真实机器人平台验证鲁棒性，探索更轻量化模型，并建立统一的社交合规导航评估基准；综上，SocialNav-MoE通过“MoE架构轻量化+SSR增强RFT+系统组件优化”，首次实现小参数规模VLM在社交合规导航中的高效应用，既解决了传统大模型的实时性问题，又弥补了学习型方法的泛化缺陷，为服务机器人实际部署提供了可行方案。 |
| Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception - Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track | https://arxiv.org/abs/2510.07871v1 | 小米EV-AD VLA团队针对IROS 2025 RoboSense挑战赛社交导航赛道开展研究，该赛道要求开发基于RGBD感知的导航系统，使自主智能体在动态人类密集的室内环境中，仅依赖第一视角传感器数据（RGBD图像、里程计）、无全局地图或特权信息的情况下，实现安全、高效且符合社交规范（如保持安全距离、避撞）的导航，团队最终基于Falcon框架改进的方案获16支参赛队伍中的第2名，在Social-HM3D基准测试中表现优异；在相关工作方面，社交导航从早期简化人类模型发展到基于Habitat 3.0等平台的真实感模拟，但现有方法多依赖全局环境信息或室内避撞能力不足，导航辅助学习通过多种辅助任务提升性能却存在泛化性受限问题，轨迹预测的三类主流方法（物理模型、数据驱动模型、意图推理模型）则多聚焦户外自动驾驶，室内场景适配性不足。团队的技术方案先将任务数学定义为智能体从初始状态生成轨迹到达目标，需满足无碰撞约束并优化目标成本与社交合规成本的平衡，随后介绍了Falcon框架基础，其主策略架构以RGBD图像和相对目标坐标为输入，通过ResNet-50提取视觉特征、线性层处理目标信息、2层LSTM捕捉时序依赖，奖励函数结合基础奖励与社交惩罚（碰撞、距离过近、路径阻塞惩罚），同时通过人数估计、位置估计、轨迹预测三个辅助任务提升时空理解能力并以对应损失函数优化；为解决Falcon框架轨迹预测未量化碰撞风险、社交惩罚信号稀疏的不足，团队提出核心创新——主动风险感知模块，该模块以主策略网络的LSTM隐藏态为输入，通过轻量神经网络输出人类风险评分，按智能体与人类距离将风险分为危险区（<2m，评分1.0）、警告区（2-4m，渐变评分）、安全区（≥4m，评分0.0），并将其整合到总损失函数中（设置β_risk=0.1平衡风险感知与其他目标），且推理阶段无额外计算开销。实验验证使用含844个真实室内场景的Social-HM3D数据集，该数据集具有目标导向人类轨迹、自然运动动力学、按场景面积校准人类密度的特点，数据划分为训练集、验证集（约1000个场景）、测试集（约500个未见过场景），评估指标包括成功率（SR）、路径效率（SPL）、社交空间合规性（PSC）、人类碰撞率（H-Coll）及由前三者加权计算的总分（决定排名），实验采用DD-PPO算法，基于Habitat平台的PointNav预训练权重微调，在4×NVIDIA A40 GPU上训练约7500万步，竞赛结果显示团队方案的SR为0.6560、SPL为0.5958、PSC为0.8608、H-Coll为0.3300、总分为0.6994，较基线Falcon（总分0.6248）显著提升，与第一名（总分0.7022）差距仅0.0028。结论指出，团队提出的基于Falcon框架并加入主动风险感知模块的方案，实现了碰撞风险量化与密集监督，有效提升了社交导航能力，在Social-HM3D基准上验证了有效性并获挑战赛第2名，且方案仅依赖第一视角传感器、无额外计算开销，具备真实场景部署潜力。 |
| UrbanVLA: A Vision-Language-Action Model for Urban Micromobility | https://arxiv.org/abs/2510.23576 | 针对城市微移动机器人（如配送机器人、辅助轮椅）在大规模动态非结构化城市环境中实现长距离导航的核心挑战——现有传统SLAM方法依赖高精度地图维护成本高、基于学习的点目标导航方法（如CityWalker）易出现路线与物理世界错位、现有VLA模型无法处理长距离导航中的路线噪声与交通规则遵守等问题，研究团队提出了路线条件约束的视觉-语言-动作（VLA）框架UrbanVLA，旨在整合高层导航工具指引与视觉-语言策略学习，实现最长超500米的鲁棒、安全长距离导航。该框架以预训练导航基础模型NavFoM为骨干，先将高层目标路线（2D坐标序列）编码为结构化“roadbook”指令（模拟场景中通过角点检测分割路线生成转向/距离指令，真实场景直接调用高德地图API获取），再利用DINOv2和SigLIP预训练编码器处理多相机RGB图像序列，经网格池化降维和跨模态投影器映射后得到视觉tokens，与“roadbook”的语言tokens一同输入Qwen2 LLM骨干，通过语言头完成VideoQA任务以增强真实场景理解、通过MLP动作头输出含2D位置与朝向的导航轨迹；训练上采用“监督微调（SFT）-强化微调（RFT）”两阶段策略：SFT阶段结合MetaUrban模拟器的PPO专家轨迹与Sekai网页视频解析的人类轨迹构建数据，通过启发式轨迹提升（HTL）算法（经Savitzky-Golay滤波去噪、角点检测分割、添加高斯噪声、平滑重采样）处理轨迹噪声，生成（高层路线、视觉观测、真实轨迹）三元组，以MSE损失优化模型构建基础导航能力；RFT阶段采用隐式Q学习（IQL），以LLM第17层隐藏表示为状态、轨迹向量化结果为动作，设计“完成度增量奖励-碰撞惩罚-过度偏离惩罚”的复合奖励函数（λ_comp=0.5、λ_coll=1、λ_dev=1），利用2400个模拟episode（约40小时）与8小时人类远程操控真实数据（Unitree Go2机器人）提升模型安全性与真实适应性。实验验证方面，模拟实验在MetaUrban的PointNav和SocialNav任务中，仅用RGB输入便显著超越所有LiDAR输入基线，如PointNav Test集成功率94%（基线最高66%）、SPL 0.91（基线最高0.64），Unseen集成功率97%（基线最高60%）、SPL 0.95（基线最高0.57），SocialNav Test集成功率91%（基线最高36%）、SNS 0.87（基线最高0.67），Unseen集成功率88%（基线最高32%）、SNS 0.85（基线最高0.64）；真实实验中，基于配备4个前后左右相机的Unitree Go2机器人、远程NVIDIA RTX 4090服务器及GNSS定位模块，调用高德地图API获取路线，在天桥、人行横道、密集障碍环境中实现2Hz频率的平滑导航，能适应光照变化与夜间场景，完成500米以上长距离任务；消融实验进一步证实，HTL算法可使真实场景路线完成率从42%提升至100%，RFT阶段能使SocialNav Unseen集成功率从82%提升至88%、碰撞成本从0.98降至0.82。该研究的核心贡献在于提出首个面向城市微移动的路线条件VLA框架、构建模拟-真实迁移 pipeline、通过IQL强化微调提升安全关键行为，为动态城市环境中embodied智能部署提供实用框架，未来计划探索LiDAR、IMU等更多模态 cues，优化对复杂路口、极端天气等场景的适应性。 |
| MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning | https://arxiv.org/abs/2512.21722 | MAction-SocialNav由日本北海道大学与美国乔治梅森大学团队合作完成，于2025年12月发表于arXiv平台（cs.RO领域），其核心是解决移动机器人在人类环境中社交合规导航的关键问题——传统方法默认单一“正确”动作忽略社交场景歧义性，且现有基于视觉语言模型（VLMs）的方案存在大规模模型推理延迟高、小语言模型（SLMs）社交推理能力未被挖掘的矛盾；为此，研究提出以SLM（NVILA-Lite2B，参数<30亿）为骨干的MAction-SocialNav框架，通过全参数监督微调（SFT）训练，核心创新是元认知提示（MCP），该提示由元认知自我评估（S_meta，通过内部递归优化按“安全+社交合规”评分，最低90分通过并迭代优化）与竞争刺激推理（S_com，含与人类、其他AI、自身竞争三种模式，最优组合为“自我评估+与其他AI竞争”）构成，能生成按“可行性→社交规范→效率”层级排序的6个离散基础动作（前进、前左、前右、左转、右转、停止），而非单一动作；为支撑研究，团队构建了含789个样本（710个训练、79个测试，均含双人类标注）的多动作社交导航数据集，每个样本包含“视觉观测+三回合对话”，并设计了Pred@1（顶级动作准确性）、Pred@n（动作匹配度）、APG（严格合规性）、MAA（带权重多动作准确性）、ER（无效动作比例）5个评估指标；实验在4×NVIDIA RTX 8000 GPU环境下开展，结果显示，带MCP的MAction-SocialNav在各指标上显著优于无MCP及带因果推理的变体（如Pred@1 0.760 vs 0.734/0.747，ER 0.264 vs 0.306/0.325），且对比零样本GPT-4o和Claude，在决策质量（APG 0.595 vs 0.000/0.025）、安全性（ER 0.264 vs 0.642/0.668）和效率（1.524 FPS，约为GPT-4o的5倍、Claude的3.4倍）上均大幅领先，消融实验验证了MCP组件协同的有效性，不同场景难度测试则表明其在复杂环境中鲁棒性更强（难场景Pred@1反超中场景）；研究结论指出MAction-SocialNav首次明确建模社交导航动作歧义性，兼顾高性能与实时效率，同时也提及当前动作空间粗粒度、需与低层运动规划器结合的局限性，未来计划整合高层多动作推理与低层运动规划，在真实场景中评估碰撞率、路径长度等指标。 |
> 上面是极新环节

| Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation | https://arxiv.org/abs/2503.20425 | 该论文由葡萄牙里斯本理工大学的Kevin Alcedo、Pedro U. Lima及法国图卢兹ANITI研究所的Rachid Alami共同撰写，核心问题是机器人在人类环境中导航需考虑他人隐藏信念（如意图、目标），传统马尔可夫决策过程（MDP）无法处理“部分可观测性”，而直接用部分可观测马尔可夫决策过程（POMDP）计算复杂度极高，为此提出两大核心贡献：一是构建神经符号模型基强化学习（MBRL）架构以解决部分可观测环境中的信念追踪问题，二是设计基于“基于影响的抽象（IBA）”框架的视角转换算子，实现“心智理论（ToM）”计算化以估计他人信念。在背景与挑战方面，社交导航需平衡机器人自身目标与人类体验，关键在于安全性、社交清晰度和场景适应性，而传统模型无关RL数据采集成本高、传统MBRL无法处理隐藏状态、纯符号方法泛化性差、纯神经方法缺乏结构化推理，且POMDP虽为部分可观测环境提供理论框架但多智能体场景下计算不可行，神经符号架构和IBA框架则为解决这些问题提供了基础。预备知识部分，影响增强局部模型（IALM）将全局环境拆解为局部模型与影响点，无需追踪全局状态即可实现“局部计算+全局效果”，局部因子化POMDP（Local-FPOMDP）则将全局状态空间分解为局部因子，维护各因子子分布以降低计算开销。核心方法上，神经符号世界模型基于变分自编码器（VAE）结合Gumbel-Softmax重参数化，学习离散符号化latent因子，包含预测下一时刻latent因子的前向模型、推断动作的逆模型及重构原始观测的观测解码器，损失函数由VAE标准损失、前向模型预测与真实因子的KL散度、逆模型推断与真实动作的均方误差（MSE）组成；视角转换算子通过将人类状态视为局部目标状态，模拟“想象导航”以估计人类视角下的信念，实现计算化心智理论；影响增强策略学习基于深度Q网络（DQN），以机器人自身因子化信念和视角转换算子估计的人类信念为输入，输出离散运动原语的Q值，降低信念更新计算复杂度。实验方面，以Minigrid部分可观测网格世界为环境，设计人员跟随任务（机器人需跟随人类至未知目标并遵守社交距离），使用3000个混合随机与专家策略（A*规划）的episode（约30万步）训练模型，对比“随机影响”（下界）和“完全信息”（上界）基线，结果显示所提视角转换方法显著优于随机基线，性能接近完全信息上界，且世界模型能学习到有意义的因子化信念，验证了社交推理能力。结论指出该神经符号MBRL架构结合IBA框架成功将社交导航建模为因子化POMDP，解决了信念追踪计算复杂度问题，视角转换算子有效提升了社交导航的安全性与适应性，未来计划扩展至动态人群、3D场景，建模更精细社交动态，融合符号推理与传统信念空间规划，并增强模型可解释性与形式化验证。 |
| Social World Models | https://arxiv.org/abs/2509.00559 | 该论文针对AI系统难以自动构建和推理隐含社交情境的问题，提出了一种通用的结构化社交世界表示形式（S³AP），其基于POMDP设计，能将对话、小说、会议记录等自由文本叙事自动转化为包含环境状态（Eᵗ）、多智能体联合记忆（Mᵗ）、联合观测（Oᵗ，含外部环境信息与内省的信念/目标/情绪）及联合动作（Aᵗ）的结构化元组，有效解决了静态文本存在的报告偏差、心理状态隐含等问题，为构建社交世界模型（SWM）奠定基础。论文首先开发了由LLM驱动的S³AP-Parser，该解析器无需依赖任务问题即可生成结构化表征，能显著提升不同规模LLM（从o3-mini、Llama 4到GPT-4.1、o1）在5类社交推理任务（如ToMi的心理理论推理、ParaToMi的转述信念追踪、FANToM的多主体对话心智推理等）的性能，平均提升幅度显著（如FANToM任务平均提升11.1个百分点），甚至让小模型突破能力瓶颈，还发现社交推理包含“社交世界表征构建”与“基于表征的心理状态推理”两个独立部分，弱推理能力的模型也能生成有效的S³AP表征。在此基础上，论文基于S³AP构建了生成式社交世界模型（SWM），并提出“Foresee and Act”推理算法——让智能体通过SWM模拟候选动作对社交状态的演化影响（如他人的心理反应、环境变化），再迭代优化决策；在SOTOPIA-hard交互式基准测试中，配备SWM的智能体目标完成分数普遍提升（如o3-mini提升1.1分），且在竞争场景（如议价）中提升更明显，同时发现强社交智能体未必能构建更优SWM，需匹配智能体的决策整合能力。此外，论文也指出局限性，如S³AP解析器可能难以处理文化歧义场景、多智能体场景下计算复杂度随规模增长、未完全解决LLM偏见传递问题等，最终强调S³AP作为通用社交世界表征的潜力，可支撑静态第三方推理与动态第一方交互，为开发更具社交感知的AI系统提供框架。 |
| Social World Model-Augmented Mechanism Design Policy Learning | https://arxiv.org/pdf/2510.19270 | 本文发表于NeurIPS 2025，由北京大学、北京通用人工智能研究院等机构团队提出社会世界模型增强的机制设计策略学习方法（SWM-AP），核心是解决传统机制设计在异构多智能体系统中面临的“潜在特质建模难、系统动态复杂、样本效率低”三大挑战，通过协调自利智能体行为实现集体最优，并在税收政策设计、团队协作优化、设施选址三大场景验证了优越性；传统机制设计常忽略智能体不可观测的持久潜在特质（如技能、偏好）、难以捕捉多智能体交互产生的非线性涌现行为，且现有模型基强化学习（MBRL）的世界模型未考虑智能体异构性，SWM-AP基于MBRL框架，将问题形式化为“主-从episodic马尔可夫博弈”（主为机制设计者，目标是设计机制最大化累积社会福利，从为具有固定潜在特质的背景智能体，特质不可观测但决定行为响应），核心包含两大互联模块：一是社会世界模型（SWM），通过后验特质追踪器从完整训练轨迹无监督推断智能体潜在特质$\hat{m}_{post}$，再结合当前可观测状态、部署机制预测下一状态与即时社会福利，通过最小化预测误差与KL正则项优化；二是机制设计策略，基于PPO训练，借助先验特质追踪器从部分历史轨迹实时推断特质$\hat{m}_{prior}$（以后验追踪器结果为监督），并通过“真实交互收集轨迹+SWM模拟生成想象轨迹”提升样本效率，同时通过证据下界（ELBO）推导证明了联合学习特质与系统动态的可行性。实验中，设施选址场景下SWM-AP达到PPO最终性能所需步数最少且最终奖励最高，团队结构优化场景下能准确预测智能体资源导向移动（而MBPO预测错误），税收调整场景下在保持与PPO相当生产力的同时显著提升平等性（优于MBPO“平等提升但生产力大幅下降”的情况）；补充实验显示32智能体大规模设施选址场景中，SWM-AP仍保持最优最终奖励与样本效率，且计算成本可控（32智能体每100k步训练时间5.25小时、内存5104MB）。论文总结SWM-AP在“奖励性能、样本效率、预测精度”上全面优于模型基/无模型基线的贡献，同时指出其在超大规模系统可扩展性与潜在特质可解释性上的局限，未来将通过结构化先验架构提升可扩展性、通过解纠缠表示学习增强特质可解释性，附录还明确了各场景超参数配置（如SWM动态预测模块用MLP/GCN、策略折扣因子γ=0.99），并验证高观测模糊度场景下SWM特质推断更准确。 |
> 上面是WM环节，是从某个[awesome](https://github.com/leofan90/Awesome-World-Models)里来的，但是有点非原教旨主义的WM

下面的文献稍微看一看：

#### 1. 数据集 / 仿真平台 / Benchmark / 基础模型

这类文献主要提供**工具和资源**：仿真环境、benchmark、数据集、通用模型（如 Social Force）。

- SocNavBench: A Grounded Simulation Testing Framework for Evaluating Social Navigation  
- SocialGym: A Framework for Benchmarking Social Robot Navigation  
- SocNavGym: A Reinforcement Learning Gym for Social Navigation  
- SEAN 2.0: Formalizing and generating social situations for robot navigation  
- Social Force Model for Pedestrian Dynamics  

如果你想单独分一类“经典基础模型”，可以把 Social Force Model 单拎出来作为“手工/物理模型基线”。

#### 2. 经典模型/优化/规划视角的社会导航方法

这类多是从**优化、MPC、约束规划、图模型等规划视角**出发，未必完全依赖深度强化学习。  

- Social Robot Navigation through Constrained Optimization  
- Social Robot Navigation Adapted to Time-dependent Affordance Spaces  
- DR-MPC: Deep Residual Model Predictive Control for Real-World Social Navigation  
- Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation（偏表征和规划结合）  
- In Search of a Lost Metric: Human Empowerment as a Pillar of Socially Conscious Navigation（偏指标/代价设计，也可以视为“指标与伦理”小类）  
- Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning（奖励建模介于规划与RL之间）  
- A graph neural network to model disruption in human-aware robot navigation  
- Relational graph learning for crowd navigation  

如果想再细一点，可以在这一大类里分成：  
- 「基于优化/MPC/约束」：Constrained Optimization, DR-MPC, Affordance Spaces  
- 「图模型 / GNN / 表征」：Relational graph learning, GNN disruption, latent social motion space, Gaussian reward（表征/代价是核心）

#### 3. 深度强化学习 / 学习型社会导航方法

这类以**RL 或深度学习**为主线去学导航策略，通常强调 crowd navigation / social-aware policy。

- Collision Avoidance in Pedestrian-Rich Environments with Deep Reinforcement Learning  
- Uncertainty-Aware Deep Reinforcement Learning for Crowd Navigation  
- Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning  
- SESNO: Sample Efficient Social Navigation from Observation  
- SACSoN: Scalable Autonomous Control for Social Navigation  
- All Aware Robot Navigation in Human Environments Using Deep Reinforcement Learning  
- HeR-DRL: Heterogeneous Relational Deep Reinforcement Learning for Decentralized Multi-Robot Crowd Navigation  
- Risk-aware deep reinforcement learning for robot crowd navigation  
- SRL-ORCA: A Socially Aware Multi-Agent Mapless Navigation Algorithm in Complex Dynamic Scenes  
- Multi-robot social-aware cooperative planning in pedestrian environments using attention-based actor-critic  
- Frozone: Freezing-free, pedestrian-friendly navigation in human crowds  
- Intention aware robot crowd navigation with attention-based interaction graph  
- Occlusion-aware crowd navigation using people as sensors  
- Learning a group-aware policy for robot navigation  
- ST2: Spatial-Temporal State Transformer for Crowd-Aware Autonomous Navigation  
- SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning  

其中还可以再细分几个“子簇”方便你写 related work：  
- 「基于注意力 / 图 / Transformer 的 crowd navigation」：Relational graph learning, intention-aware with attention graph, ST2, HeR-DRL（异构关系图），Multi-robot attention-based actor-critic  
- 「风险/不确定性/安全」：Risk-aware DRL, Uncertainty-Aware DRL, Frozone, Occlusion-aware  
- 「多机器人 / 协作」：HeR-DRL, Multi-robot social-aware cooperative planning  
- 「人类在环 / 反馈」：SRLM（LLM + DRL 的交互式导航），也可以单独成一小类  

#### 4. 社会信号 / 交互 / 指标与人因

这类重点是**人类因素、社会线索、沟通或评价指标**，而不是单纯的运动规划。

- You Are In My Way: Non-verbal Social Cues for Legible Robot Navigation Behaviors（非语言线索、可读性行为）    
- In Search of a Lost Metric: Human Empowerment as a Pillar of Socially Conscious Navigation（以 “empowerment / human empowerment” 为核心指标）  
- Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation（人类感知 / social latent space）  
- Exploiting Proximity-Aware Tasks for Embodied Social Navigation（接近度、任务与社会行为的关系）  

这些可以作为“社会信号与人因维度”的一类，与纯技术型 RL/规划论文区分开。

---

我现在需要做几个事情。先前是把一些内容记录到blog里，最后blog脏了；后来记录到了daily里，现在daily也带来了知识遗忘的风险，我得定时定期的做转移，daily不应该serve as a database，而是一个planner。
另外整理一下播客，收集一下几个优秀博主的collection，From xhs to zhihu，and wechat。实时去关注，还要善用好Huggingface与Arxiv。
DP说的这一点倒是很对，就是不宜看的太杂了，本科阶段最重要的还得是面试时能够深入聊下去的成果。然后大一大二果然搞不定所以他亲自开始写MPC了，另外视觉可能也会来交给我去做


## 技术文档

今天通过让trae给我如这样的[Python讲义文件](https://github.com/walter201230/Python/blob/master/Article/PythonBasis/python4/Dict.md)读取它的内容然后出练习题的方式，感觉效率挺高的，可以拿去学其他语言！
我觉得可能比较关键的是node，javascript、Typescript之类的，有没有类似的仓库？

可以用同样的套路学前端/Node，这类“按知识点分章节的 Markdown 讲义仓库”不少:

### JavaScript / Node 基础讲义

- [javascript-tutorial/en.javascript.info](https://github.com/javascript-tutorial/en.javascript.info) – 系统的 JS 教程，按“教程 → 章节 → 小节”结构排列，每个小节都是一份 Markdown/HTML 文档，难度从基础语法到异步、模块、浏览器 API 逐步上升，很适合一节一节丢给模型做问答和练习题。
- [The-JS-Guide](https://github.com/Aahil13/The-JS-Guide) – “The JavaScript Guide Docs”，仓库里是按主题拆好的 JS 指南 Markdown 文件，内容偏语言与日常开发，用法类似你现在的 Python 讲义：读完一节（如数组、对象、闭包），让模型基于该文件出一组练习题。

Node 本身可以配合 JS 教程用，很多 Node 教程是直接在 README 或 docs 目录里用 Markdown 编排章节，这类仓库可以按关键字再扩展一轮，例如搜索 `node 教程 markdown site:github.com`，挑结构清晰、有目录的即可。

### TypeScript 中文讲义

- [zhongsp/TypeScript](https://github.com/zhongsp/TypeScript) – 《TypeScript 使用手册》中文翻译，结构完全按官方 Handbook 来，章节拆得细（基础类型、接口、类、泛型、高级类型等），内容就是一章一章的 Markdown，非常适合作为“逐章出题”的语料。
- [oyyd/typescript-handbook-zh](https://github.com/oyyd/typescript-handbook-zh) – TypeScript 手册中文版（另一套翻译整理），仓库里是分章节 Markdown/GitBook 结构，你可以根据目录精细控制要学的章节，然后让模型出习题或小项目题目。
- [DavidCai1111/typescript-handbook](https://github.com/DavidCai1111/typescript-handbook) – 也是基于官方 TS Handbook 的中文整理，章节化 Markdown，内容覆盖度、组织方式跟上面两份类似，可以当交叉参考或补充材料。
- [The Concise TypeScript Book](https://github.com/gibbok/typescript-book) – “精简版 TypeScript 书”，英文为主，仓库里有按章节拆开的 Markdown，可以先用它快速跑一遍 TS 思维，再用中文版仓库补细节+出练习题。

### Vue：官方文档 Markdown 仓库

- [vuejs/docs](https://github.com/vuejs/docs) – Vue 3 官方文档仓库，所有内容都是 Markdown，主要在 `src/guide`、`src/api` 等目录下，按照“基础 → 进阶 → 工具”的难度曲线排好，中文文档也在这个 repo 里，非常适合按章节丢给 Trae 出题。
- [vuejs/vitepress](https://github.com/vuejs/vitepress) – Vue 官方的静态站点生成器，本身就是用 Vue + Markdown 做文档站的，`docs` 目录里有不少范例文档，可以用来做 Vue 生态相关的补充练习材料。

### NestJS：官方文档 Markdown 仓库

- [nestjs/docs.nestjs.com](https://github.com/nestjs/docs.nestjs.com) – NestJS 官方文档仓库，说明里写得很清楚：用 markdown 源文档编译成 docs 网站，章节包括 Fundamentals、Techniques、Security、GraphQL 等，难度非常循序渐进，直接拿 `content` 里的各个 `.md` 文件给 Trae 出“章节小测 + 小项目题”就行。

### React / MDX：React 组件 + MDX 文档示例

React 官方文档的源码拆散在多个内部仓库，结构不如上面两个那么一眼就能拿来逐章喂，但你可以用一些“React + MDX 文档”项目当讲义源：

- [jxnblk/mdx-docs](https://github.com/jxnblk/mdx-docs) – 用 MDX 记录 React 组件文档的示例项目，仓库里有一整套 React 组件 + MDX 文档结构，很适合作为“React + 文档驱动开发”的进阶练习语料，让 Trae 读 MDX 文件出题或出“实现组件/补全文档”的任务。

如果你想针对“React 本身的语法/思想”逐章学，比较现实的做法是：
- 直接抓 React 官方文档当前小节的 HTML/MDX 文本给 Trae，让它总结 + 出题（而不是依赖一个集中式 Markdown 仓库）；
- 或者把 `mdx-js/mdx`、Docusaurus 这类 MDX 文档项目（里面有大量 React + MDX 示例）当成“中高级 React 练习材料”。

按“循序渐进程度 + 适合出题”的优先级，大概是：**NestJS docs.nestjs.com → Vue 3 docs → 再用 React/MDX 项目做进阶**。如果你告诉想先啃哪一条线（比如先 Vue 还是先 NestJS），可以进一步帮你点名具体子目录和推荐的章节顺序，直接配一套“第几天看哪个 md、做哪类题”的路线。


## 明日计划

记得给PPT来一个讲稿！
