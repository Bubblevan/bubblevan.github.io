---
date: 2026-01-14
title: 足式运控综述（3）：初稿一
authors: [bubblevan]
tags: []
---

## 3. 强化学习本体感知
### 3.1 开山之作
苏黎世联邦理工学院（ETH Zurich）与英特尔（Intel）团队在《Learning Agile and Dynamic Motor Skills for Legged Robots》中提出的混合仿真-强化学习框架，是首个在复杂四足机器人（ANYmal）上实现高动态运动技能“零样本迁移”的里程碑研究，其核心突破在于系统性解决了强化学习（RL）在足式机器人落地中最关键的“仿真-现实鸿沟（Sim-to-Real Gap）”问题。

在2019年该研究发表前，足式机器人的运动控制长期被两类方法主导

一类是基于模板动力学的模块化控制（如Bellicoso等人2018年提出的在线非线性运动优化框架），这类方法需将机器人抽象为“单质点+无质量肢体”的简化模型，通过手动拆解“ foothold规划-足端轨迹生成-PID跟踪”等子模块实现控制，但其对模型简化的强依赖导致在高动态场景（如速度＞1.0m/s的trot步态）下，躯干姿态误差会放大至±8°，且需工程师花费数月手工调参以适配单一步态；

另一类是在线轨迹优化方法（如Neunert等人2017年的接触轨迹优化方案），这类方法虽能通过数值优化显式处理摩擦力锥等约束，但需预设接触时序（如trot步态的对角足接触周期），且在ANYmal等搭载串联弹性执行器（SEA）的机器人上，因难以建模执行器的非线性动态（如齿轮摩擦的速度依赖性、弹簧形变的迟滞效应），导致轨迹跟踪误差超30%，无法实现稳定 locomotion。

RL方法虽在理论上具备处理高维非线性系统的能力，但此前在四足机器人上的应用几乎局限于仿真环境2018年Tan等人在Minitaur机器人上的研究虽实现了Sim-to-Real迁移，但其采用的直接驱动执行器结构简单，可通过解析模型精准建模，而ANYmal搭载的12个SEA关节（每个关节含"无刷电机-谐波减速器-线性弹簧-双编码器"四级传动链），其动态特性需上百个参数描述，且存在多重未建模动态
控制信号经“PD扭矩转换-PID电流调节-磁场定向控制（FOC）”三级处理后，会引入约8ms的通信延迟；
关节扭矩输出随弹簧形变呈现迟滞效应，传统解析模型（如Gehring等人2016年提出的SEA模型）的扭矩预测误差达3.55Nm，远超ANYmal关节控制允许的1Nm阈值。

这些问题导致此前的RL策略在ANYmal真机上启动时，关节常出现15-20Hz的高频震荡，扭矩超调达额定值（40Nm）的1.8倍，触发3秒内停机保护，无法完成单次步态循环。

因此，Hwangbo等摒弃了“通过复杂奖励函数诱导步态”的传统思路，转而从“仿真保真度”入手，构建“刚体动力学+数据驱动执行器模型”的混合仿真架构。一方面，通过硬接触模型还原足端与地面的真实交互特性，解决传统平滑接触模型“力延迟”问题；另一方面，通过自监督学习训练执行器网络（Actuator Net），端到端建模SEA的非线性动态，将扭矩预测误差降至0.74Nm。这种架构使仿真环境在动力学层面与真机高度一致，最终实现trot步态（0-1.5m/s）、gallop步态（0-2.5m/s）、30°斜坡攀爬及摔倒后3秒翻正等动作的零样本迁移

![](/paper/anymal-agile-overview.png)
*整体训练流程。先做物理参数辨识与不确定性建模，再训练执行器网络拟合执行器/软件动态，最后在前两步的模型上训练策略并直接上机。*

#### 3.1.1 执行器网络（Actuator Net）

ANYmal的SEA关节动态特性包含三重强非线性电机铜损随转速呈二次函数变化，减速器齿隙导致的摩擦力矩在关节反转时突变，弹簧形变的迟滞效应使相同形变下的扭矩输出差异达2Nm。

Gehring等人2016年提出的解析模型虽包含近120个参数（如弹簧刚度、摩擦系数、延迟时间），但需通过实验逐一校准，且在动态步态下（如足端离地瞬间，关节角速度从-5rad/s骤升至8rad/s），参数耦合导致误差放大至5Nm以上，无法满足控制精度需求。

为此，Hwangbo团队提出Actuator Net，通过一种基于数据驱动的方式直接学习“关节指令-扭矩输出”的映射关系，将误差降至 0.74Nm，规避了解析建模的复杂度。

![](/paper/anymal-agile-sim.png)
*仿真中训练控制器的闭环。策略网络输出关节位置目标，执行器网络基于关节历史状态把“位置目标 + 历史”映射为关节扭矩，扭矩驱动刚体仿真得到下一状态。*

Actuator Net的网络结构设计紧扣SEA的动态特性，采用3层全连接网络
输入层维度 24（每个关节的输入为 “位置误差 + 角速度” 的 3 个时间步，共 6 个特征 / 关节，12 个关节即 72 维），隐藏层设 3 层，每层 32 个神经元，激活函数选用 softsign 而非 tanh
> 原文通过对比实验发现，softsign 的计算复杂度更低（12 个关节推理时间 12.2μs，tanh 需 31.6μs），且其 “两端渐进饱和” 特性可匹配 SEA 的 “高误差时扭矩饱和” 行为（如关节超限时扭矩不再线性增长），避免输出过冲。
输出层维度 12，对应12个关节的“扭矩修正值$\Delta \tau$”，激活函数为线性（确保扭矩输出的连续性），最终真机扭矩指令为“传统解析模型输出$\tau_{ana}$+$\Delta \tau$”，形成“解析模型粗调+数据模型精修”的混合控制逻辑

训练过程中，团队采用加权均方误差（WMSE）作为损失函数，对动态步态样本赋予1.5倍权重
$L=\frac{1}{N}\sum_{i=1}^N [w_i \cdot (\tau_{act,i} - (\tau_{ana,i}+\Delta \tau_i))^2]$（$w_i$在“足端接触 / 离地瞬态”“手动扰动” 等动态场景取1.5，其余平稳运动场景取1.0）。

这种加权设计的原因是，动态场景是解析模型误差最大的区域（如足端离地时，解析模型易低估摩擦扭矩），需优先拟合。

同时为提升模型鲁棒性，训练中对样本加入 ±5% 的随机噪声（模拟传感器测量误差），并随机偏移关节惯性参数 ±10%（模拟不同机器人个体的硬件差异）。最终，Actuator Net 在验证集上的平均扭矩预测误差 RMS 误差为 0.74Nm，接近扭矩传感器的分辨率（0.2Nm），较传统解析模型的3.55Nm降低79.1%；即便在测试集（采用训练后的 locomotion 策略采集）中误差仅 0.966Nm，远低于理想模型的 5.74Nm，证明其对动态场景的适配性，有效解决了SEA关节建模精度不足的问题。

#### 3.1.2 刚体动力学与硬接触求解器的工程优化
传统RL仿真多采用“弹簧-阻尼平滑接触模型”（$F=k\delta + c\dot{\delta}$，$\delta$为足端穿透深度），这种模型虽计算高效，但一方面存在“力延迟”，足端接触地面后需产生一定穿透深度才会输出接触力，导致仿真中足端落地时的冲击力比真机低40%；二是通过连续函数描述库仑摩擦，无法还原足端从“静止”到“滑动”的摩擦力突变特性，导致仿真中稳定的步态在真机上出现足端打滑。

针对这些接触动力学的问题，Hwangbo团队基于此前提出的“逐接触迭代法”（Per-contact Iteration Method），设计了硬接触求解器，核心设计围绕“瞬时接触特性”展开。

接触检测采用分离轴定理（SAT），实时检测足端（简化为长方体）与地面（三角网格模型）的接触状态，检测频率与刚体动力学求解频率保持一致（2000Hz），高于机器人控制频率（400Hz）5倍，从而精准捕捉足端撞击地面时的瞬时接触；

力计算逻辑摒弃弹簧模型，转而基于“动量守恒”反推接触力。假设足端穿透地面的最大深度$\delta \leq 0.5mm$（匹配ANYmal足端的弹性形变极限），通过$\Delta v = F \cdot \Delta t / m$（$\Delta v$为足端速度变化量，$\Delta t$为求解时间步长）计算接触力$F$，避免平滑模型的“力延迟”问题；

库仑摩擦约束则通过“离散化摩擦锥”实现，将连续的摩擦锥（$f_x^2 + f_y^2 \leq \mu f_z$，$\mu$为摩擦系数）离散为16个方向的摩擦向量，每个向量对应一个可能的摩擦力方向，求解时通过“最大静摩擦判断-滑动状态切换”逻辑，还原足端滑动瞬间的摩擦力突变，$\mu$的取值范围设为0.3-1.2，覆盖草地、水泥地、碎石地等常见场景。

在此基础之上，为平衡动力学精度与计算效率，Hwangbo等对ANYmal的刚体模型进行针对性简化将机器人抽象为12个刚体（躯干+4条腿，每条腿含髋外展/内收（HAA）、髋屈/伸（HFE）、膝屈/伸（KFE）3个关节），忽略电机外壳等细小部件，但保留关键惯性参数（如躯干质心位置$(x=0.12m,y=0,z=0.35m)$、每条腿的转动惯量$I_{leg}=[0.05,0.03,0.04]kg·m^2$），并通过真机称重与惯性测量实验校准参数，确保惯性力计算误差≤3%。

求解器采用半隐式欧拉法求解刚体动力学方程（$\dot{\mathbf{v}} = M^{-1}(\mathbf{F}_{ext} - C\mathbf{v})$，$M$为惯性矩阵，$C$为科里奥利力与离心力矩阵），并通过"稀疏矩阵分解"优化$M^{-1}$的计算效率因ANYmal的刚体连接呈树状结构，惯性矩阵具有稀疏性，分解后可将矩阵求逆时间从10μs压缩至0.5μs/步，单机器人实例的动力学求解时间仅0.5μs，支持单GPU同时运行100个机器人实例的并行仿真。

在与Actuator Net的融合逻辑上，系统采用“多步累积延迟”策略以处理推理延迟。刚体动力学求解器每5μs（2000Hz）输出一次“关节期望角速度$\dot{q}_{des}$”，Actuator Net接收“$\dot{q}_{des}$与实际关节状态的误差”后输出“修正扭矩$\Delta \tau$”，但因Actuator Net存在12.2μs的推理延迟，求解器每4个求解步长（20μs）更新一次Actuator Net的输出，既保证信号同步，又避免延迟导致的动力学发散。验证显示，该策略可将延迟引发的速度跟踪误差控制在0.02m/s以内，远低于控制需求的0.1m/s。

仿真效率方面，该系统在"Intel i7-8700K CPU + NVIDIA GTX 1080Ti GPU"的普通PC上，可实现90万时间步/秒的求解效率，即单GPU同时运行100个ANYmal实例时，每个实例的仿真速度仍可达真实时间的2倍（Real-Time Factor=2）。这种效率突破使大规模随机化训练成为可能，后续RL策略训练需1000万+仿真步长，单PC仅需12小时即可完成，无需依赖多CPU/多GPU服务器，大幅降低了工程化应用的硬件门槛。

#### 3.1.3 TRPO
在控制框架上，该算法采用了信赖域策略优化（TRPO），并以关节位置目标（Joint Position Targets）作为动作输出，配合低层 PD 控制器执行
\[
\tau = K_p(\phi^{*}-\phi) + K_d(\dot{\phi}^{*}-\dot{\phi})
\]

策略网络的结构设计重点关注"时序特征提取"。

足式机器人的步态控制需依赖历史状态判断接触相位（如足端是否离地），因此输入层维度设为60，包含"当前周期+过去4个周期（12.5ms）的本体状态"，每个周期的状态含12个关节的"位置$q$、速度$\dot{q}$、扭矩$\tau$"与IMU的"三轴加速度$(a_x,a_y,a_z)$、三轴角速度$(\omega_x,\omega_y,\omega_z)$"，共42个特征，通过"按腿分组降维"（计算每条腿的关节角度均值、速度方差）压缩至60维，既减少冗余信息，又保留关键时序特征。

隐藏层设2层，单元数分别为256和128，激活函数采用tanh因关节位置指令需在±0.5rad范围内，tanh的输出范围[-1,1]便于后续缩放，且作为有界激活函数，可避免ReLU等无界函数在未访问状态下的动作过冲。

输出层维度12，对应12个关节的"位置指令增量$\Delta q_{cmd}$"，每个增量限制在±0.01rad（控制周期内的关节最大允许转动角度），最终指令为$q_{cmd,t} = q_{cmd,t-1} + \Delta q_{cmd}$，确保动作平滑性。

奖励函数设计采用"线性复合"形式，未引入复杂的多阶段奖励，核心逻辑是"以任务目标为导向，避免奖励函数本身引入偏差"。

$R = w_1 R_{vel} + w_2 R_{pose} + w_3 R_{torque} + w_4 R_{fall}$

速度跟踪奖励$R_{vel}$采用指数函数（$R_{vel} = \exp(-5|v_{des} - v_{act}|)$），$w_1=2.0$，这种设计在速度接近目标时给予更高奖励，鼓励步态稳定；

姿态稳定奖励$R_{pose}$以躯干横滚角（$\theta_{roll}$）和俯仰角（$\theta_{pitch}$）为约束（$R_{pose} = \exp(-10(\theta_{roll}^2 + \theta_{pitch}^2))$），$w_2=1.5$，惩罚躯干倾斜；

扭矩平滑奖励$R_{torque}$通过约束相邻周期的扭矩变化（$R_{torque} = \exp(-0.1|\tau_t - \tau_{t-1}|)$），$w_3=0.5$，减少机械损耗；

摔倒惩罚$R_{fall}$则在躯干接触地面时给予-100的强惩罚，$w_4=1.0$，确保策略优先保证机器人不摔倒。

训练过程中，团队引入"课程学习"以解决"局部最优陷阱"传统RL训练中，若对关节扭矩和速度的惩罚过高，策略易收敛到"站立不动"的局部最优（因站立状态的扭矩消耗低，惩罚小）；若惩罚过低，则会出现不自然的关节运动。

为此，团队定义"课程因子$k_c$"，初始值$k_0=0.3$（对应低难度），通过$k_{c,j+1} = (k_{c,j})^{k_d}$（$k_d=0.997$为推进速率）逐步提升至1（最终难度），所有惩罚项（如扭矩、关节速度惩罚）均乘以$k_c$，仅任务目标项（如速度跟踪、姿态稳定）保持不变。

这种设计使策略先学习"如何实现运动目标"，再逐步适应"扭矩平滑、速度约束"等要求，避免初始阶段陷入站立局部最优。

折扣因子$\gamma$的选择则根据任务特性调整。

命令跟随与高速 locomotion 任务采用$\gamma=0.9988$（半衰期5.77s），因这类任务需长期稳定跟踪速度，长半衰期可鼓励策略关注长期收益；

摔倒恢复任务采用$\gamma=0.993$（半衰期4.93s），因恢复动作需快速完成，短半衰期可提升策略的响应速度。

原文实测显示，TRPO在800万仿真步长时达到稳定性能（奖励值波动≤5%），较DDPG算法（需1200万步）收敛速度提升33.3%；训练过程中"机器人摔倒次数"仅120次，为DDPG（480次）的1/4，策略稳定性显著更优。

零样本迁移验证中，该策略在ANYmal真机上表现出卓越性能命令跟随任务中，平均线性速度误差0.143m/s，较Bellicoso等人的模块化控制器（0.231m/s）降低37.2%；平均扭矩消耗8.23Nm，较传统方法（11.7Nm）减少30%；

高速 locomotion任务中，机器人达到1.5m/s的实测速度，突破此前ANYmal的1.2m/s速度记录，且关节扭矩与速度均达到硬件极限（40Nm、12rad/s），证明策略可充分挖掘硬件潜力；

摔倒恢复任务中，机器人在9种随机初始姿态（包括近乎倒立的姿态）下，均能在3秒内翻正，这是传统控制方法无法实现的动态技能这些结果充分验证了TRPO框架在高维连续动作空间的稳定性，以及混合仿真架构的Sim-to-Real迁移能力。

该工作打破了"强化学习仅能停留在仿真"的刻板印象，其开发的频率高达 400Hz 的控制环路在机载 CPU 上的占用率仅为 0.1%，证明了深度神经网络在实时控制中的高效性。更重要的是，它确立了后续四足机器人研究的基本范式高保真执行器建模 + 动力学随机化 + 端到端策略训练。


### 3.2 师生网络与 PMTG
3.1虽首次验证了端到端强化学习在足式机器人平坦地形盲走中的可行性，但其控制策略仅能在实验室可控的平坦或轻度纹理化地面稳定运行，一旦迁移至真实自然环境中的复杂地形（如泥泞、积雪、碎石堆、密集植被区等），便会暴露三大核心瓶颈:

其一，监督信号稀疏导致训练效率低下，纯 RL 训练过程中，机器人在复杂地形中易因平衡失稳、足端碰撞或打滑而提前终止 episode，有效训练样本占比极低，策略难以收敛至鲁棒状态；

其二，显式环境状态估计模块的脆性，传统 RL 或模型驱动控制器依赖 “足地接触状态”“打滑程度” 等显式估计（如基于关节力矩阈值或足端力传感器的判断逻辑），但在变形地形（如 mud 导致足端下陷、snow 覆盖地形轮廓）中，本体感知信号受扰动剧烈，预设阈值频繁失效，进而引发控制指令失准；

其三，仿真真实泛化鸿沟显著，前序工作的仿真训练环境仅包含刚性、规则地形（如固定高度台阶、平滑斜坡），而真实环境中普遍存在的 “地形可变形性”“动态支撑（如滚动碎石）”“地面障碍（如缠绕植被）” 等物理现象，受限于当时机器人仿真框架（如 Bullet、ODE）的计算效率与建模精度，无法被精确复现，导致仿真训练的策略部署到真机后性能骤降。

正是在这一技术背景下，ETH Zurich 与 Intel 继续联合提出了 “师生网络 + PMTG（Policies Modulating Trajectory Generators）控制架构 + 自适应地形课程” 的三位一体解决方案《Learning Quadrupedal Locomotion over Challenging Terrain》（Science Robotics, 2020）。

该工作首次突破了强化学习盲走控制器在真实复杂地形中的零样本泛化难题（前一代零样本泛化只能在平稳地形），成功将两代 ANYmal 机器人（ANYmal B 与 ANYmal C）部署于雪山斜坡（坡度≈45°）、溪流（流速 0.5m/s）、森林泥泞地、DARPA 地下挑战赛城市赛道（18cm 台阶 descent）等前序工作无法企及的场景，且在所有部署中无需任何现场参数调优，60 分钟连续任务零失败。


#### 3.2.1 师生特权学习
![](/paper/anymal-tcn-overview.png)
*方法总览。(A) 两阶段训练流程先在仿真中用特权信息训练 Teacher，再让仅有本体感受的 Student 模仿 Teacher。(B) 自适应地形课程用粒子滤波动态调整地形参数，确保难度适中。(C) 控制器架构策略网络调制基础运动原语，最后输出给关节 PD 控制器。*

师生特权学习（Two Stage Privileged Learning）是该工作为解决“复杂地形RL稀疏奖励”与“隐式环境状态推断”难题提出的核心训练范式，其设计灵感源于“Learning by Cheating”框架，但创新性地摒弃了对专家演示数据的依赖，转而通过“仿真环境特权信息”构建教师策略，再将教师的“环境理解能力”蒸馏给仅依赖本体感知的学生策略。

##### 3.2.1.1 教师策略
教师策略以马尔可夫决策过程（MDP） 为数学框架，且假设 “环境对教师完全可观测”。
从技术实现来看，基于多层感知器构建的教师策略的核心作用是利用仿真环境中可获取的“全局真实信息”快速收敛至最优动作基准。

**状态空间**

一是**机器人本体可观测状态 \(o_t\)**，具体涵盖运动指令向量（目标水平方向 \((_{IB}^{B}\hat{v}_T)_{xy}\) 与转向方向 \((\hat{\omega}_T)_z\)）、基座姿态与角速度（由IMU测量）、12个关节的位置与速度（关节编码器数据）、4条腿的相位变量 \(\phi_i\)（通过 \(<\cos\phi_i, \sin\phi_i>\) 平滑编码，避免角度周期性歧义）、腿频率偏移 \(f_i\) 及历史足端位置目标；

二是**仿真特权信息 \(x_t\)**，仿真中可直接获取的地面真实状态，具体包括地形真实高程（以每个足端为中心、10cm半径圆上9个采样点的高度）、足地接触状态（布尔值）与接触力大小（物理引擎直接输出）、地面摩擦系数（随机化范围 \(U(0.4,1.0)\)）、外部扰动力（训练中施加的侧向50N力等）。

**动作空间**

- **4条腿的频率偏移 \(f_i\)**：每条腿1个维度，用于调整单条腿的相位更新速率（\(\phi_i = (\phi_{i,0} + (f_0 + f_i)t) \mod 2\pi\)）——例如，当前腿遇障碍时，减小 \(f_i\) 可延长摆动相，为避障争取时间；  
- **4条腿的3维足端位置残差 \(\Delta r_{f_i,T}\)**：每条腿3个维度（x/y/z轴），用于在FTG生成的基础轨迹 \(F(\phi_i)\) 上叠加微调量——例如，遇台阶时增加z轴残差以抬高足端，遇碎石时增加x/y轴残差以侧向避障，最终目标足端位置为 \(r_{f_i,T} = F(\phi_i) + \Delta r_{f_i,T}\)。

教师策略的输出分为两部分，一是用于表征环境与接触特征的 latent 向量 \(\bar{l}_t\)（由MLP编码器对 \(x_t\) 单独编码得到，仅包含地形与接触相关信息，可驱动“根据地形调整足间隙”等自适应行为）；二是16维动作向量 \(\bar{a}_t\)（4条腿的频率偏移 \(f_i\) 与3维足端位置残差 \(\Delta r_{f_i,T}\)）。该 16 维残差调制向量并非直接控制关节，而是通过 “调制 PMTG 架构中的足轨迹生成器（FTG）” 实现运动。

训练过程采用 TRPO 算法，奖励函数设计兼顾“运动性能”与“硬件安全”。
\[ R = 0.05r_{lv} + 0.05r_{av} + 0.04r_b + 0.01r_{fc} + 0.02r_{bc} + 0.025r_s + 2 \times 10^{-5}r_\tau \]  

| 奖励项 | 核心目的 | 计算逻辑 |
|--------|----------|----------|
| 线性速度奖励 \(r_{lv}\) | 最大化指令方向的推进速度 | 定义 \(v_{pr}\) 为基座线速度在指令方向的投影，若 \(v_{pr} \geq 0.6m/s\)（ANYmal平坦地形最大速度），则 \(r_{lv}=1.0\)；否则按高斯函数 \(exp(-2.0(v_{pr}-0.6)^2)\) 衰减，停止指令时 \(r_{lv}=0\) |
| 角速度奖励 \(r_{av}\) | 最大化转向速度（当有转向指令时） | 定义 \(ω_{pr}\) 为基座角速度与转向指令的乘积，若 \(ω_{pr} \geq 0.6rad/s\)，则 \(r_{av}=1.0\)；否则按 \(exp(-1.5(ω_{pr}-0.6)^2)\) 衰减 |
| 基座稳定奖励 \(r_b\) | 抑制基座的不稳定运动 | 由两部分组成：① 正交速度惩罚 \(exp(-1.5v_o^2)\)（\(v_o\) 为垂直于指令方向的速度）；② 姿态角速率惩罚 \(exp(-1.5\|(_{IB}^{B}\omega)_{xy}\|^2)\)（抑制roll/pitch方向的剧烈晃动） |
| 足间隙奖励 \(r_{fc}\) | 避免摆动相足端碰撞地形 | 定义 \(F_{clear}\) 为“足端高度 > 周围9个地形采样点最大高度”的摆动腿集合，\(r_{fc} = \sum_{i \in I_{swing}} \mathbb{1}_{F_{clear}}(i) / |I_{swing}|\)（取值范围[0,1]） |
| 身体碰撞惩罚 \(r_{bc}\) | 避免机器人躯干与地形碰撞（保护硬件） | 若躯干与地形发生非预期接触（如腹部擦地），则 \(r_{bc}=0\)；否则 \(r_{bc}=1.0\) |
| 轨迹平滑奖励 \(r_s\) | 避免足端轨迹突变（减少关节冲击） | 惩罚足端目标位置的二阶有限差分导数（导数越大，\(r_s\) 越小） |
| 关节扭矩惩罚 \(r_\tau\) | 避免执行器过载（降低能耗） | 线性惩罚所有关节的扭矩绝对值：\(r_\tau = -\sum_{i \in joints} |\tau_i|\)（负奖励，扭矩越大，惩罚越重） |

**网络架构**

其网络结构的第一层特权信息编码器将 36 维的 \(x_t\) （含 4 条腿 ×9 个地形采样点 + 接触状态 + 接触力 + 摩擦系数）映射至 64 维 latent 向量 \(\bar{x}_t\)。

第二层则将 \(\bar{x}_t\) 与 \(o_t\) 拼接后，最终输出 16 维动作向量 \(\bar{a}_t\)。

##### 3.2.1.2 学生策略
学生策略是最终部署到真机的核心，其设计目标是“仅通过本体感知时序信号，复现教师策略的环境适应能力”，因此输入严格限制为机器人实际可获取的本体感知数据（无任何特权信息），且采用**时间卷积网络（TCN）** 替代教师的MLP架构。

TCN通过“因果膨胀卷积”结构可捕捉长达2秒的本体感知历史（采样间隔0.02s，即100个时间步，记为TCN 100），能够有效识别“足陷阱”“渐进式打滑”等需依赖历史信息判断的事件（如“某关节力矩持续增大但足端位移微小→判断为足被植被缠绕→触发抬足反射”）。

学生策略的训练采用模仿学习范式，损失函数设计为“动作损失+latent表示损失”的组合
\(\mathcal{L} = \left(\bar{a}_t(o_t,x_t) - a_t(o_t,H)\right)^2 + \left(\bar{l}_t(o_t,x_t) - l_t(H)\right)^2\)，其中第一项确保学生输出的动作与教师对齐，第二项则强制学生的TCN中间层隐向量与教师的 \(\bar{l}_t\) 匹配。

这一设计的创新性在于，学生不仅模仿教师的“表层动作”，更学习教师对“环境状态的理解方式”，从而实现“从特权信息依赖到本体感知推断”的能力迁移。消融实验清晰验证了该范式的必要性，直接用TRPO训练TCN 20策略（无师生蒸馏），在斜坡行走与台阶测试中完全失败，训练过程中 episode 长度始终较短（表明频繁失稳），而经师生蒸馏的同架构TCN 20策略，奖励值可快速逼近教师策略，且在16cm台阶测试中成功率达80%以上。  

此外，TCN的时序建模能力对学生策略的鲁棒性至关重要。通过对比不同记忆长度的TCN（TCN 1对应20ms、TCN 20对应0.4s、TCN 100对应2s）发现，在均匀斜坡场景中，记忆长度对性能影响较小，但在台阶场景中，TCN 100可处理高达20.1cm的台阶，而TCN 1仅能处理10cm以下台阶，且当后腿接触台阶时，TCN 1的失败率达75%（因无法利用前腿接触台阶的历史信息调整后腿轨迹），在50N侧向扰动测试中，TCN 100的运动方向偏差比TCN 1降低35.5%，证明长时序记忆可有效提升抗扰动能力。

同时，原文通过训练解码器网络（从TCN中间层重构特权信息 \(x_t\)）发现，学生策略能从本体感知时序中隐式推断出地形高程（红色椭球表示估计地形形状与不确定性）、足地接触状态与外部扰动力，例如在湿滑白板上行走时，解码器估计的摩擦系数会从0.8快速降至0.4，并在返回正常地面2秒后回升，这表明TCN确实学习到了“本体感知 环境状态”的映射关系，而非简单的动作记忆。


#### 3.2.2 PMTG 控制架构
PMTG（Policies Modulating Trajectory Generators）控制架构是该工作为平衡“训练稳定性”与“地形适应性”提出的核心控制范式，其本质是将“模型驱动的运动先验”与“数据驱动的策略优化”深度融合，在3.1前一章的端到端RL控制器直接输出关节位置或力矩，训练初期易因动作混乱导致机器人快速摔倒，而PMTG通过预定义的“足轨迹生成器（FTG）”提供步态周期性先验，同时允许策略网络通过“残差调制”适应复杂地形，既保证了训练初期的稳定性，又保留了对未知环境的灵活响应能力。  

PMTG架构的底层核心是**足轨迹生成器（FTG）**，其设计基于“周期性腿相位”机制，为每条腿定义相位变量 \(\phi_i \in [0, 2\pi)\)，其中 \(\phi_i \in [0, \pi)\) 对应接触相（足端接地，提供支撑），\(\phi_i \in [\pi, 2\pi)\) 对应摆动相（足端抬起，避免碰撞），相位更新公式为 \(\phi_i = (\phi_{i,0} + (f_0 + f_i)t) \mod 2\pi\)。

式中 \(\phi_{i,0}\) 为初始相位（训练中随机采样自 \(U(0,2\pi)\)，确保步态多样性），
\(f_0 = 1.25Hz\) 为基础频率（与ANYmal传统trot步态控制器一致，保证步态自然性），
\(f_i\) 为策略网络输出的频率偏移（用于调整单条腿的步态节奏，如遇障碍时减慢摆动腿频率）。

FTG的核心功能是生成基础足端轨迹，当 \(k \in [0,1]\) 时（摆动相初期），轨迹为 \(F(\phi_i) = (h(-2k^3 + 3k^2) - 0.5)^{H_i}z\)；
当 \(k \in [1,2]\) 时（摆动相后期），轨迹为 \(F(\phi_i) = (h(2k^3 - 9k^2 + 12k - 4) - 0.5)^{H_i}z\)；
其余情况为 \(-0.5^{H_i}z\)（接触相足端高度），其中 \(k = 2(\phi_i - \pi)/\pi\) 为归一化相位，\(h = 0.2m\) 为最大足端高度，\(H_i\) 为“水平帧”坐标系，这一坐标系设计是FTG的关键创新，\(H_i\) 固连于第i条腿的髋关节下方（距离等于腿的名义reach），其z轴平行于重力向量，x轴为基座x轴在水平面的投影（与基座保持相同偏航角），roll与pitch角则与基座完全解耦。

这种设计的优势在于一是大幅降低基座姿态波动对足端轨迹的影响（如基座倾斜时，足端仍能保持水平方向的稳定运动），使训练初期策略不易因基座晃动而失稳，二是可在训练中对策略的动作分布进行分解，在侧向方向施加更大探索噪声，促进机器人沿地面的横向移动能力。  

PMTG架构的上层是**策略网络的残差调制模块**，其核心逻辑是“策略不直接生成足端轨迹，而是在FTG基础轨迹上叠加微调量”，具体而言，策略网络输出两类调制参数一是4条腿的频率偏移 \(f_i\)（调整单条腿的相位更新速率，如当前腿遇障碍时，减小 \(f_i\) 以延长摆动相，为避障争取时间），二是4条腿的3维足端位置残差 \(\Delta r_{f_i,T}\)（在FTG生成的 \(F(\phi_i)\) 基础上，沿x/y/z轴叠加微调，实现“抬高足端过台阶”“侧向偏移避碎石”等自适应行为），最终目标足端位置为 \(r_{f_i,T} = F(\phi_i) + \Delta r_{f_i,T}\)。

这种“先验+残差”的设计，使得策略网络的优化空间被限制在“合理步态范围内”，避免了端到端RL中常见的“动作探索混乱”问题，同时残差调制的灵活性又能应对复杂地形的不确定性，
机器人遇到16.8cm台阶时（高于平坦地形12.9cm的最大足间隙），策略网络通过输出正的z向残差 \(\Delta r_{f_i,T,z}\)，将前腿（LF/RF）的最大足间隙提升至22.5cm/18.5cm，后腿（LH/RH）提升至16.6cm/15.9cm，形成“足陷阱反射”；更重要的是，该反射无需任何显式触发条件（如足端接触信号），当小腿中部在摆动相碰撞障碍时（图3C），策略仍能通过分析关节力矩与位置的时序变化，输出合适的残差调整足端轨迹，而传统脚本控制器（依赖足端接触触发）则会因未检测到足端碰撞而失效。  

PMTG架构的运动跟踪环节则采用“解析逆运动学=+关节位置PD控制”的组合，首先将 \(H_i\) 坐标系下的目标足端位置 \(r_{f_i,T}\) 转换为基座坐标系下的坐标（通过坐标变换矩阵），再利用ANYmal腿的3自由度结构（髋屈/伸、髋内收/外展、膝屈/伸）求解解析IK，得到各关节的位置目标；
随后，关节位置PD控制器跟踪这些目标，其动力学模型采用“learned actuator model”，该模型通过训练学习了关节PD控制器的滞后特性与非线性，输入为当前及历史0.01s、0.02s的关节位置误差与速度，输出为关节力矩，这一设计大幅提升了sim to real的迁移精度，仿真训练的策略部署到真机后，无需调整PD参数即可稳定运行，在平坦地形全向测试中，速度跟踪误差小于0.05m/s，航向误差始终控制在10°以内，而传统模型驱动控制器（Bellicoso 2018）在侧向运动时航向误差达30°，且加载10kg payload（占机器人重量22.7%）后完全失效。


#### 3.2.3 自适应地形课程
自适应地形课程（Adaptive Terrain Curriculum）是该工作突破 sim2real 的关键创新，其核心思想是“动态生成‘中等难度’的训练地形，确保策略始终在‘可学习区间’内进化”，前一章的工作采用“随机地形采样”（如均匀采样台阶高度、斜坡角度），易导致训练过程中“地形难度与策略能力不匹配”，过易地形无法提供有效训练信号，过难地形则导致策略频繁失败、样本效率低下，而自适应课程通过粒子滤波动态调整地形参数分布，使训练难度随策略性能提升而逐步增加，最终让策略学习到“覆盖真实环境多样性的通用适应规律”，而非对特定训练地形的过拟合。  

技术实现分为“地形参数化”“粒子滤波维护难度分布”“动态更新地形”三个核心步骤。

首先是**地形参数化**，原文将训练地形分为三类具有代表性的刚性地形（无需建模变形，降低仿真复杂度），每类地形通过一组可调节参数描述。
一是Hills地形（模拟自然斜坡与起伏地面），基于Perlin噪声生成，参数包括粗糙度（\(c_{T,1}\)，控制地形高度的随机波动范围，采样自 \(U(0.02,0.15)\)）、Perlin噪声频率（\(c_{T,2}\)，控制地形起伏的密集程度，采样自 \(U(0.1,1.2)\)）、噪声振幅（\(c_{T,3}\)，控制地形起伏的最大高度，采样自 \(U(0.5,2.0)\)），其高度图表达式为 \(hm[i,j] = Perlin(c_{T,2},c_{T,3})[i,j] + U(-c_{T,1},c_{T,1})\)，策略在该地形上主要学习“平滑斜坡行走”与“应对轻微足滑”；
二是Steps地形（模拟离散高程变化，如岩石堆），由多个 \(c_{T,1} \times c_{T,1}\) 的方形块组成（\(c_{T,1}\) 为台阶宽度，采样自 \(U(0.1,0.5)m\)），每个块的高度采样自 \(U(0, c_{T,2})\)（\(c_{T,2}\) 为台阶高度，采样自 \(U(0.05,0.3)m\)），策略在此学习“足端抬高过障碍”与“适应离散高度变化”；
三是Stairs地形（模拟规则楼梯，如DARPA挑战赛中的台阶），参数为台阶高度（\(c_{T,1}\)，采样自 \(U(0.02,0.2)m\)）与宽度（固定为0.3m），机器人初始位置设置在楼梯中间的平坦段，策略在此学习“连续台阶上下行”的步态协调。所有地形参数的范围均基于ANYmal的运动学极限设定（如台阶高度不超过腿长的1/3），确保策略有能力通过合理调整实现 traversal。  

其次是**粒子滤波维护难度分布**，该过程通过“粒子”表征地形参数组合，动态筛选“策略可通过但需付出努力”的中等难度地形初始化阶段，为每类地形采样 \(N_{particle}=10\) 个粒子（每个粒子对应一组地形参数 \(c_T\)），粒子均匀分布于参数空间；
训练迭代过程中，每完成 \(N_{evaluate}=10\) 次策略更新（视为一个评估周期），对每个粒子对应的地形，运行 \(N_{traj}=6\) 条轨迹，计算该地形的“可通行性” \(Tr(c_T,\pi)\)——定义为“成功遍历时间（无失稳）与总任务时间（10s）的比值”；随后，根据 \(Tr\) 筛选粒子，仅保留 \(Tr \in [0.5,0.9]\) 的粒子（\(Tr<0.5\) 为过难地形，策略频繁失败；
\(Tr>0.9\) 为过易地形，无训练价值），并计算每个粒子的权重 \(w_j = P(y_j^k | c_{T,j}^k)\)，其中 \(P(y_j^k | c_{T,j}^k)\) 为粒子 \(c_{T,j}^k\) 属于“中等难度”的概率，近似为 \(N_{traj}\) 条轨迹中 \(Tr \in [0.5,0.9]\) 的比例；
最后，基于权重进行粒子重采样，同时引入 \(P_{replay}=0.05\) 的概率从“历史优秀粒子回放内存”中采样，避免粒子分布退化（如因策略性能提升导致所有当前粒子均变为过易地形），并以 \(p_{transition}=0.8\) 的概率将重采样后的粒子向参数空间相邻区域轻微移动（随机游走），保证地形多样性。  

最后是**动态更新地形**，随着策略训练的推进，中等难度地形的参数分布会逐步向“更具挑战性”的区域偏移，例如Hills地形的粒子分布，从训练初期的“低粗糙度（0.02）、低频率（0.1）、低振幅（0.5）”（易地形），逐步演变为“高粗糙度（0.15）、高频率（1.2）、高振幅（2.0）”（中等难度地形）；Stairs地形的粒子分布则从“宽台阶（0.5m）、低高度（0.05m）”逐步转向“窄台阶（0.1m）、高高度（0.2m）”。这种动态调整确保了“策略每一步学习都能获得有效反馈”，避免了传统随机采样的低效性，消融实验对比了“自适应课程”与“均匀随机采样”的训练效果，在Hills地形测试中，无自适应课程的教师策略成功率仅为40%，而有课程的达85%；训练奖励方面，无课程的策略在5000次迭代后即进入平台期（奖励≈0.15），而有课程的策略可持续提升至0.25；平均 episode 长度方面，无课程的策略因频繁遇到过难地形，episode 长度始终低于200s，而有课程的策略则稳定在350s以上，有效训练样本量提升近一倍。  

值得注意的是，该自适应课程的核心价值在于“无需建模变形地形，却能让策略适应变形地形”，通过在刚性地形中覆盖“足够广的难度与多样性”，策略学习到的是“本体感知信号与动作调整”的通用映射（如“关节力矩骤增→足端受阻→抬高足端”“基座倾斜→调整对侧腿支撑力→恢复平衡”），而非对特定地形的记忆。原文真机实验验证了这一点，仅在刚性地形训练的策略，部署到泥泞地时，能通过关节力矩的时序变化判断足端下陷，输出更慢的步态与更大的足端接地面积；部署到雪地时，能通过基座角速度的微小波动感知打滑，调整足端落地速度以增大摩擦力，最终实现零样本泛化，这一结果彻底颠覆了“仿真必须精确复现真实地形物理特性”的传统认知，为后续足式机器人强化学习控制器的高效开发提供了关键方法论。


总结下来，该工作证实了 “无需在仿真中精确建模真实世界的复杂性，仅通过‘时序本体感知 + 知识蒸馏 + 自适应训练课程’，即可让策略学习到通用的环境适应规律”，这一认知打破了 “仿真复杂度必须匹配真实复杂度” 的固有范式，为后续强化学习盲走控制器的工程化落地奠定了核心架构基准。

### 3.3 地形隐空间向量
### 3.3.1 双模块与隐空间
### 3.3.2 双阶段训练

### 3.4 质心线速度估计

### 3.5 复杂地形梯度冲突

## 4. 强化学习视觉感知

### 4.1 高程图
在强化学习与视觉感知结合的足式机器人运动控制研究中，ETH Zurich 与 Intel 团队于 2022 年又一次提出的 “基于机器人中心高程图与注意力循环编码器的端到端感知 - 运动框架”，《Learning robust perceptive locomotion for quadrupedal robots in the wild》，是首个实现四足机器人在多季节、多场景野外环境中 “高速 - 鲁棒” 双目标统一的里程碑式成果。

该研究的核心价值在于，打破了此前 “盲走” 策略与传统视觉感知策略之间的性能割裂 —— 前者依赖本体感知（关节编码器、IMU）虽能保证基础鲁棒性，但因缺乏地形预判能力而陷入 “速度天花板” 与 “越障上限低” 的困境，后者虽能利用视觉信息提升运动效率，却因传感器依赖性强、对野外感知噪声鲁棒性差而难以落地。

此前的盲走策略（即本文3.2节提出的纯本体感知控制器）需通过 “足端物理接触反馈” 调整步态，导致平地运动速度被限制在 0.6 m/s 以下，面对 20 cm 以上台阶时前腿易卡滞、后腿同步抬高失败，成功率骤降；
而传统视觉感知策略（如 Boston Dynamics Spot 机器人）虽能通过深度传感器识别地形，但需人工切换 “楼梯模式” 且仅支持正向行走，更关键的是，当传感器面临雪地高反光、洞穴低光、植被遮挡等野外典型干扰时，深度图易出现 “伪凹陷”“数据缺失” 等问题，此时控制器若无有效退化机制，极易引发步态崩溃。

为解决上述痛点，该研究构建了以 “机器人中心高程图（Robot-Centric Elevation Map）为感知抽象层、注意力循环编码器为多模态融合核心、特权学习为训练范式” 的三层技术架构。

#### 4.1.1 机器人中心高程图

#### 4.1.2 注意力循环编码器

> 特权学习的部分与 3.2.1 师生特权学习需不需要做赘述？

### 4.2 深度图
