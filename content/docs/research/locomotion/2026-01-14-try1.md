---
date: 2026-01-14
title: 足式运控综述（3）：初稿一
authors: [bubblevan]
tags: []
---

## 3. 强化学习本体感知
### 3.1 开山之作
苏黎世联邦理工学院（ETH Zurich）与英特尔（Intel）团队在《Learning Agile and Dynamic Motor Skills for Legged Robots》中提出的混合仿真-强化学习框架，是首个在复杂四足机器人（ANYmal）上实现高动态运动技能“零样本迁移”的里程碑研究，其核心突破在于系统性解决了强化学习（RL）在足式机器人落地中最关键的“仿真-现实鸿沟（Sim-to-Real Gap）”问题。

在2019年该研究发表前，足式机器人的运动控制长期被两类方法主导

一类是基于模板动力学的模块化控制（如Bellicoso等人2018年提出的在线非线性运动优化框架），这类方法需将机器人抽象为“单质点+无质量肢体”的简化模型，通过手动拆解“ foothold规划-足端轨迹生成-PID跟踪”等子模块实现控制，但其对模型简化的强依赖导致在高动态场景（如速度＞1.0m/s的trot步态）下，躯干姿态误差会放大至±8°，且需工程师花费数月手工调参以适配单一步态；

另一类是在线轨迹优化方法（如Neunert等人2017年的接触轨迹优化方案），这类方法虽能通过数值优化显式处理摩擦力锥等约束，但需预设接触时序（如trot步态的对角足接触周期），且在ANYmal等搭载串联弹性执行器（SEA）的机器人上，因难以建模执行器的非线性动态（如齿轮摩擦的速度依赖性、弹簧形变的迟滞效应），导致轨迹跟踪误差超30%，无法实现稳定 locomotion。

RL方法虽在理论上具备处理高维非线性系统的能力，但此前在四足机器人上的应用几乎局限于仿真环境2018年Tan等人在Minitaur机器人上的研究虽实现了Sim-to-Real迁移，但其采用的直接驱动执行器结构简单，可通过解析模型精准建模，而ANYmal搭载的12个SEA关节（每个关节含"无刷电机-谐波减速器-线性弹簧-双编码器"四级传动链），其动态特性需上百个参数描述，且存在多重未建模动态
控制信号经“PD扭矩转换-PID电流调节-磁场定向控制（FOC）”三级处理后，会引入约8ms的通信延迟；
关节扭矩输出随弹簧形变呈现迟滞效应，传统解析模型（如Gehring等人2016年提出的SEA模型）的扭矩预测误差达3.55Nm，远超ANYmal关节控制允许的1Nm阈值。

这些问题导致此前的RL策略在ANYmal真机上启动时，关节常出现15-20Hz的高频震荡，扭矩超调达额定值（40Nm）的1.8倍，触发3秒内停机保护，无法完成单次步态循环。

因此，Hwangbo等摒弃了“通过复杂奖励函数诱导步态”的传统思路，转而从“仿真保真度”入手，构建“刚体动力学+数据驱动执行器模型”的混合仿真架构。一方面，通过硬接触模型还原足端与地面的真实交互特性，解决传统平滑接触模型“力延迟”问题；另一方面，通过自监督学习训练执行器网络（Actuator Net），端到端建模SEA的非线性动态，将扭矩预测误差降至0.74Nm。这种架构使仿真环境在动力学层面与真机高度一致，最终实现trot步态（0-1.5m/s）、gallop步态（0-2.5m/s）、30°斜坡攀爬及摔倒后3秒翻正等动作的零样本迁移

![](/paper/anymal-agile-overview.png)
*整体训练流程。先做物理参数辨识与不确定性建模，再训练执行器网络拟合执行器/软件动态，最后在前两步的模型上训练策略并直接上机。*

#### 3.1.1 执行器网络（Actuator Net）

ANYmal的SEA关节动态特性包含三重强非线性电机铜损随转速呈二次函数变化，减速器齿隙导致的摩擦力矩在关节反转时突变，弹簧形变的迟滞效应使相同形变下的扭矩输出差异达2Nm。

Gehring等人2016年提出的解析模型虽包含近120个参数（如弹簧刚度、摩擦系数、延迟时间），但需通过实验逐一校准，且在动态步态下（如足端离地瞬间，关节角速度从-5rad/s骤升至8rad/s），参数耦合导致误差放大至5Nm以上，无法满足控制精度需求。

为此，Hwangbo团队提出Actuator Net，通过一种基于数据驱动的方式直接学习“关节指令-扭矩输出”的映射关系，将误差降至 0.74Nm，规避了解析建模的复杂度。

![](/paper/anymal-agile-sim.png)
*仿真中训练控制器的闭环。策略网络输出关节位置目标，执行器网络基于关节历史状态把“位置目标 + 历史”映射为关节扭矩，扭矩驱动刚体仿真得到下一状态。*

Actuator Net的网络结构设计紧扣SEA的动态特性，采用3层全连接网络
输入层维度 24（每个关节的输入为 “位置误差 + 角速度” 的 3 个时间步，共 6 个特征 / 关节，12 个关节即 72 维），隐藏层设 3 层，每层 32 个神经元，激活函数选用 softsign 而非 tanh
> 原文通过对比实验发现，softsign 的计算复杂度更低（12 个关节推理时间 12.2μs，tanh 需 31.6μs），且其 “两端渐进饱和” 特性可匹配 SEA 的 “高误差时扭矩饱和” 行为（如关节超限时扭矩不再线性增长），避免输出过冲。
输出层维度 12，对应12个关节的“扭矩修正值$\Delta \tau$”，激活函数为线性（确保扭矩输出的连续性），最终真机扭矩指令为“传统解析模型输出$\tau_{ana}$+$\Delta \tau$”，形成“解析模型粗调+数据模型精修”的混合控制逻辑

训练过程中，团队采用加权均方误差（WMSE）作为损失函数，对动态步态样本赋予1.5倍权重
$L=\frac{1}{N}\sum_{i=1}^N [w_i \cdot (\tau_{act,i} - (\tau_{ana,i}+\Delta \tau_i))^2]$（$w_i$在“足端接触 / 离地瞬态”“手动扰动” 等动态场景取1.5，其余平稳运动场景取1.0）。

这种加权设计的原因是，动态场景是解析模型误差最大的区域（如足端离地时，解析模型易低估摩擦扭矩），需优先拟合。

同时为提升模型鲁棒性，训练中对样本加入 ±5% 的随机噪声（模拟传感器测量误差），并随机偏移关节惯性参数 ±10%（模拟不同机器人个体的硬件差异）。最终，Actuator Net 在验证集上的平均扭矩预测误差 RMS 误差为 0.74Nm，接近扭矩传感器的分辨率（0.2Nm），较传统解析模型的3.55Nm降低79.1%；即便在测试集（采用训练后的 locomotion 策略采集）中误差仅 0.966Nm，远低于理想模型的 5.74Nm，证明其对动态场景的适配性，有效解决了SEA关节建模精度不足的问题。

#### 3.1.2 刚体动力学与硬接触求解器的工程优化
传统RL仿真多采用“弹簧-阻尼平滑接触模型”（$F=k\delta + c\dot{\delta}$，$\delta$为足端穿透深度），这种模型虽计算高效，但一方面存在“力延迟”，足端接触地面后需产生一定穿透深度才会输出接触力，导致仿真中足端落地时的冲击力比真机低40%；二是通过连续函数描述库仑摩擦，无法还原足端从“静止”到“滑动”的摩擦力突变特性，导致仿真中稳定的步态在真机上出现足端打滑。

针对这些接触动力学的问题，Hwangbo团队基于此前提出的“逐接触迭代法”（Per-contact Iteration Method），设计了硬接触求解器，核心设计围绕“瞬时接触特性”展开。

接触检测采用分离轴定理（SAT），实时检测足端（简化为长方体）与地面（三角网格模型）的接触状态，检测频率与刚体动力学求解频率保持一致（2000Hz），高于机器人控制频率（400Hz）5倍，从而精准捕捉足端撞击地面时的瞬时接触；

力计算逻辑摒弃弹簧模型，转而基于“动量守恒”反推接触力。假设足端穿透地面的最大深度$\delta \leq 0.5mm$（匹配ANYmal足端的弹性形变极限），通过$\Delta v = F \cdot \Delta t / m$（$\Delta v$为足端速度变化量，$\Delta t$为求解时间步长）计算接触力$F$，避免平滑模型的“力延迟”问题；

库仑摩擦约束则通过“离散化摩擦锥”实现，将连续的摩擦锥（$f_x^2 + f_y^2 \leq \mu f_z$，$\mu$为摩擦系数）离散为16个方向的摩擦向量，每个向量对应一个可能的摩擦力方向，求解时通过“最大静摩擦判断-滑动状态切换”逻辑，还原足端滑动瞬间的摩擦力突变，$\mu$的取值范围设为0.3-1.2，覆盖草地、水泥地、碎石地等常见场景。

在此基础之上，为平衡动力学精度与计算效率，Hwangbo等对ANYmal的刚体模型进行针对性简化将机器人抽象为12个刚体（躯干+4条腿，每条腿含髋外展/内收（HAA）、髋屈/伸（HFE）、膝屈/伸（KFE）3个关节），忽略电机外壳等细小部件，但保留关键惯性参数（如躯干质心位置$(x=0.12m,y=0,z=0.35m)$、每条腿的转动惯量$I_{leg}=[0.05,0.03,0.04]kg·m^2$），并通过真机称重与惯性测量实验校准参数，确保惯性力计算误差≤3%。

求解器采用半隐式欧拉法求解刚体动力学方程（$\dot{\mathbf{v}} = M^{-1}(\mathbf{F}_{ext} - C\mathbf{v})$，$M$为惯性矩阵，$C$为科里奥利力与离心力矩阵），并通过"稀疏矩阵分解"优化$M^{-1}$的计算效率因ANYmal的刚体连接呈树状结构，惯性矩阵具有稀疏性，分解后可将矩阵求逆时间从10μs压缩至0.5μs/步，单机器人实例的动力学求解时间仅0.5μs，支持单GPU同时运行100个机器人实例的并行仿真。

在与Actuator Net的融合逻辑上，系统采用“多步累积延迟”策略以处理推理延迟。刚体动力学求解器每5μs（2000Hz）输出一次“关节期望角速度$\dot{q}_{des}$”，Actuator Net接收“$\dot{q}_{des}$与实际关节状态的误差”后输出“修正扭矩$\Delta \tau$”，但因Actuator Net存在12.2μs的推理延迟，求解器每4个求解步长（20μs）更新一次Actuator Net的输出，既保证信号同步，又避免延迟导致的动力学发散。验证显示，该策略可将延迟引发的速度跟踪误差控制在0.02m/s以内，远低于控制需求的0.1m/s。

仿真效率方面，该系统在"Intel i7-8700K CPU + NVIDIA GTX 1080Ti GPU"的普通PC上，可实现90万时间步/秒的求解效率，即单GPU同时运行100个ANYmal实例时，每个实例的仿真速度仍可达真实时间的2倍（Real-Time Factor=2）。这种效率突破使大规模随机化训练成为可能，后续RL策略训练需1000万+仿真步长，单PC仅需12小时即可完成，无需依赖多CPU/多GPU服务器，大幅降低了工程化应用的硬件门槛。

#### 3.1.3 TRPO
在控制框架上，该算法采用了信赖域策略优化（TRPO），并以关节位置目标（Joint Position Targets）作为动作输出，配合低层 PD 控制器执行
\[
\tau = K_p(\phi^{*}-\phi) + K_d(\dot{\phi}^{*}-\dot{\phi})
\]

策略网络的结构设计重点关注"时序特征提取"。

足式机器人的步态控制需依赖历史状态判断接触相位（如足端是否离地），因此输入层维度设为60，包含"当前周期+过去4个周期（12.5ms）的本体状态"，每个周期的状态含12个关节的"位置$q$、速度$\dot{q}$、扭矩$\tau$"与IMU的"三轴加速度$(a_x,a_y,a_z)$、三轴角速度$(\omega_x,\omega_y,\omega_z)$"，共42个特征，通过"按腿分组降维"（计算每条腿的关节角度均值、速度方差）压缩至60维，既减少冗余信息，又保留关键时序特征。

隐藏层设2层，单元数分别为256和128，激活函数采用tanh因关节位置指令需在±0.5rad范围内，tanh的输出范围[-1,1]便于后续缩放，且作为有界激活函数，可避免ReLU等无界函数在未访问状态下的动作过冲。

输出层维度12，对应12个关节的"位置指令增量$\Delta q_{cmd}$"，每个增量限制在±0.01rad（控制周期内的关节最大允许转动角度），最终指令为$q_{cmd,t} = q_{cmd,t-1} + \Delta q_{cmd}$，确保动作平滑性。

奖励函数设计采用"线性复合"形式，未引入复杂的多阶段奖励，核心逻辑是"以任务目标为导向，避免奖励函数本身引入偏差"。

$R = w_1 R_{vel} + w_2 R_{pose} + w_3 R_{torque} + w_4 R_{fall}$

速度跟踪奖励$R_{vel}$采用指数函数（$R_{vel} = \exp(-5|v_{des} - v_{act}|)$），$w_1=2.0$，这种设计在速度接近目标时给予更高奖励，鼓励步态稳定；

姿态稳定奖励$R_{pose}$以躯干横滚角（$\theta_{roll}$）和俯仰角（$\theta_{pitch}$）为约束（$R_{pose} = \exp(-10(\theta_{roll}^2 + \theta_{pitch}^2))$），$w_2=1.5$，惩罚躯干倾斜；

扭矩平滑奖励$R_{torque}$通过约束相邻周期的扭矩变化（$R_{torque} = \exp(-0.1|\tau_t - \tau_{t-1}|)$），$w_3=0.5$，减少机械损耗；

摔倒惩罚$R_{fall}$则在躯干接触地面时给予-100的强惩罚，$w_4=1.0$，确保策略优先保证机器人不摔倒。

训练过程中，团队引入"课程学习"以解决"局部最优陷阱"传统RL训练中，若对关节扭矩和速度的惩罚过高，策略易收敛到"站立不动"的局部最优（因站立状态的扭矩消耗低，惩罚小）；若惩罚过低，则会出现不自然的关节运动。

为此，团队定义"课程因子$k_c$"，初始值$k_0=0.3$（对应低难度），通过$k_{c,j+1} = (k_{c,j})^{k_d}$（$k_d=0.997$为推进速率）逐步提升至1（最终难度），所有惩罚项（如扭矩、关节速度惩罚）均乘以$k_c$，仅任务目标项（如速度跟踪、姿态稳定）保持不变。

这种设计使策略先学习"如何实现运动目标"，再逐步适应"扭矩平滑、速度约束"等要求，避免初始阶段陷入站立局部最优。

折扣因子$\gamma$的选择则根据任务特性调整。

命令跟随与高速 locomotion 任务采用$\gamma=0.9988$（半衰期5.77s），因这类任务需长期稳定跟踪速度，长半衰期可鼓励策略关注长期收益；

摔倒恢复任务采用$\gamma=0.993$（半衰期4.93s），因恢复动作需快速完成，短半衰期可提升策略的响应速度。

原文实测显示，TRPO在800万仿真步长时达到稳定性能（奖励值波动≤5%），较DDPG算法（需1200万步）收敛速度提升33.3%；训练过程中"机器人摔倒次数"仅120次，为DDPG（480次）的1/4，策略稳定性显著更优。

零样本迁移验证中，该策略在ANYmal真机上表现出卓越性能命令跟随任务中，平均线性速度误差0.143m/s，较Bellicoso等人的模块化控制器（0.231m/s）降低37.2%；平均扭矩消耗8.23Nm，较传统方法（11.7Nm）减少30%；

高速 locomotion任务中，机器人达到1.5m/s的实测速度，突破此前ANYmal的1.2m/s速度记录，且关节扭矩与速度均达到硬件极限（40Nm、12rad/s），证明策略可充分挖掘硬件潜力；

摔倒恢复任务中，机器人在9种随机初始姿态（包括近乎倒立的姿态）下，均能在3秒内翻正，这是传统控制方法无法实现的动态技能这些结果充分验证了TRPO框架在高维连续动作空间的稳定性，以及混合仿真架构的Sim-to-Real迁移能力。

该工作打破了"强化学习仅能停留在仿真"的刻板印象，其开发的频率高达 400Hz 的控制环路在机载 CPU 上的占用率仅为 0.1%，证明了深度神经网络在实时控制中的高效性。更重要的是，它确立了后续四足机器人研究的基本范式高保真执行器建模 + 动力学随机化 + 端到端策略训练。


### 3.2 师生网络与 PMTG
3.1虽首次验证了端到端强化学习在足式机器人平坦地形盲走中的可行性，但其控制策略仅能在实验室可控的平坦或轻度纹理化地面稳定运行，一旦迁移至真实自然环境中的复杂地形（如泥泞、积雪、碎石堆、密集植被区等），便会暴露三大核心瓶颈:

其一，监督信号稀疏导致训练效率低下，纯 RL 训练过程中，机器人在复杂地形中易因平衡失稳、足端碰撞或打滑而提前终止 episode，有效训练样本占比极低，策略难以收敛至鲁棒状态；

其二，显式环境状态估计模块的脆性，传统 RL 或模型驱动控制器依赖 “足地接触状态”“打滑程度” 等显式估计（如基于关节力矩阈值或足端力传感器的判断逻辑），但在变形地形（如 mud 导致足端下陷、snow 覆盖地形轮廓）中，本体感知信号受扰动剧烈，预设阈值频繁失效，进而引发控制指令失准；

其三，仿真真实泛化鸿沟显著，前序工作的仿真训练环境仅包含刚性、规则地形（如固定高度台阶、平滑斜坡），而真实环境中普遍存在的 “地形可变形性”“动态支撑（如滚动碎石）”“地面障碍（如缠绕植被）” 等物理现象，受限于当时机器人仿真框架（如 Bullet、ODE）的计算效率与建模精度，无法被精确复现，导致仿真训练的策略部署到真机后性能骤降。

正是在这一技术背景下，ETH Zurich 与 Intel 继续联合提出了 “师生网络 + PMTG（Policies Modulating Trajectory Generators）控制架构 + 自适应地形课程” 的三位一体解决方案《Learning Quadrupedal Locomotion over Challenging Terrain》（Science Robotics, 2020）。

该工作首次突破了强化学习盲走控制器在真实复杂地形中的零样本泛化难题（前一代零样本泛化只能在平稳地形），成功将两代 ANYmal 机器人（ANYmal B 与 ANYmal C）部署于雪山斜坡（坡度≈45°）、溪流（流速 0.5m/s）、森林泥泞地、DARPA 地下挑战赛城市赛道（18cm 台阶 descent）等前序工作无法企及的场景，且在所有部署中无需任何现场参数调优，60 分钟连续任务零失败。


#### 3.2.1 师生特权学习
![](/paper/anymal-tcn-overview.png)
*方法总览。(A) 两阶段训练流程先在仿真中用特权信息训练 Teacher，再让仅有本体感受的 Student 模仿 Teacher。(B) 自适应地形课程用粒子滤波动态调整地形参数，确保难度适中。(C) 控制器架构策略网络调制基础运动原语，最后输出给关节 PD 控制器。*

师生特权学习（Two Stage Privileged Learning）是该工作为解决“复杂地形RL稀疏奖励”与“隐式环境状态推断”难题提出的核心训练范式，其设计灵感源于“Learning by Cheating”框架，但创新性地摒弃了对专家演示数据的依赖，转而通过“仿真环境特权信息”构建教师策略，再将教师的“环境理解能力”蒸馏给仅依赖本体感知的学生策略。

##### 3.2.1.1 教师策略
教师策略以马尔可夫决策过程（MDP） 为数学框架，且假设 “环境对教师完全可观测”。
从技术实现来看，基于多层感知器构建的教师策略的核心作用是利用仿真环境中可获取的“全局真实信息”快速收敛至最优动作基准。

**状态空间**

一是**机器人本体可观测状态 \(o_t\)**，具体涵盖运动指令向量（目标水平方向 \((_{IB}^{B}\hat{v}_T)_{xy}\) 与转向方向 \((\hat{\omega}_T)_z\)）、基座姿态与角速度（由IMU测量）、12个关节的位置与速度（关节编码器数据）、4条腿的相位变量 \(\phi_i\)（通过 \(<\cos\phi_i, \sin\phi_i>\) 平滑编码，避免角度周期性歧义）、腿频率偏移 \(f_i\) 及历史足端位置目标；

二是**仿真特权信息 \(x_t\)**，仿真中可直接获取的地面真实状态，具体包括地形真实高程（以每个足端为中心、10cm半径圆上9个采样点的高度）、足地接触状态（布尔值）与接触力大小（物理引擎直接输出）、地面摩擦系数（随机化范围 \(U(0.4,1.0)\)）、外部扰动力（训练中施加的侧向50N力等）。

**动作空间**

- **4条腿的频率偏移 \(f_i\)**：每条腿1个维度，用于调整单条腿的相位更新速率（\(\phi_i = (\phi_{i,0} + (f_0 + f_i)t) \mod 2\pi\)）。例如，当前腿遇障碍时，减小 \(f_i\) 可延长摆动相，为避障争取时间；  
- **4条腿的3维足端位置残差 \(\Delta r_{f_i,T}\)**：每条腿3个维度（x/y/z轴），用于在FTG生成的基础轨迹 \(F(\phi_i)\) 上叠加微调量。例如，遇台阶时增加z轴残差以抬高足端，遇碎石时增加x/y轴残差以侧向避障，最终目标足端位置为 \(r_{f_i,T} = F(\phi_i) + \Delta r_{f_i,T}\)。

教师策略的输出分为两部分，一是用于表征环境与接触特征的 latent 向量 \(\bar{l}_t\)（由MLP编码器对 \(x_t\) 单独编码得到，仅包含地形与接触相关信息，可驱动“根据地形调整足间隙”等自适应行为）；二是16维动作向量 \(\bar{a}_t\)（4条腿的频率偏移 \(f_i\) 与3维足端位置残差 \(\Delta r_{f_i,T}\)）。该 16 维残差调制向量并非直接控制关节，而是通过 “调制 PMTG 架构中的足轨迹生成器（FTG）” 实现运动。

训练过程采用 TRPO 算法，奖励函数设计兼顾“运动性能”与“硬件安全”。
\[ R = 0.05r_{lv} + 0.05r_{av} + 0.04r_b + 0.01r_{fc} + 0.02r_{bc} + 0.025r_s + 2 \times 10^{-5}r_\tau \]  

| 奖励项 | 核心目的 | 计算逻辑 |
|--------|----------|----------|
| 线性速度奖励 \(r_{lv}\) | 最大化指令方向的推进速度 | 定义 \(v_{pr}\) 为基座线速度在指令方向的投影，若 \(v_{pr} \geq 0.6m/s\)（ANYmal平坦地形最大速度），则 \(r_{lv}=1.0\)；否则按高斯函数 \(exp(-2.0(v_{pr}-0.6)^2)\) 衰减，停止指令时 \(r_{lv}=0\) |
| 角速度奖励 \(r_{av}\) | 最大化转向速度（当有转向指令时） | 定义 \(ω_{pr}\) 为基座角速度与转向指令的乘积，若 \(ω_{pr} \geq 0.6rad/s\)，则 \(r_{av}=1.0\)；否则按 \(exp(-1.5(ω_{pr}-0.6)^2)\) 衰减 |
| 基座稳定奖励 \(r_b\) | 抑制基座的不稳定运动 | 由两部分组成：① 正交速度惩罚 \(exp(-1.5v_o^2)\)（\(v_o\) 为垂直于指令方向的速度）；② 姿态角速率惩罚 \(exp(-1.5\|(_{IB}^{B}\omega)_{xy}\|^2)\)（抑制roll/pitch方向的剧烈晃动） |
| 足间隙奖励 \(r_{fc}\) | 避免摆动相足端碰撞地形 | 定义 \(F_{clear}\) 为“足端高度 > 周围9个地形采样点最大高度”的摆动腿集合，\(r_{fc} = \sum_{i \in I_{swing}} \mathbb{1}_{F_{clear}}(i) / |I_{swing}|\)（取值范围[0,1]） |
| 身体碰撞惩罚 \(r_{bc}\) | 避免机器人躯干与地形碰撞（保护硬件） | 若躯干与地形发生非预期接触（如腹部擦地），则 \(r_{bc}=0\)；否则 \(r_{bc}=1.0\) |
| 轨迹平滑奖励 \(r_s\) | 避免足端轨迹突变（减少关节冲击） | 惩罚足端目标位置的二阶有限差分导数（导数越大，\(r_s\) 越小） |
| 关节扭矩惩罚 \(r_\tau\) | 避免执行器过载（降低能耗） | 线性惩罚所有关节的扭矩绝对值：\(r_\tau = -\sum_{i \in joints} |\tau_i|\)（负奖励，扭矩越大，惩罚越重） |

**网络架构**

其网络结构的第一层特权信息编码器将 36 维的 \(x_t\) （含 4 条腿 ×9 个地形采样点 + 接触状态 + 接触力 + 摩擦系数）映射至 64 维 latent 向量 \(\bar{x}_t\)。

第二层则将 \(\bar{x}_t\) 与 \(o_t\) 拼接后，最终输出 16 维动作向量 \(\bar{a}_t\)。

##### 3.2.1.2 学生策略
学生策略是最终部署到真机的核心，其设计目标是“仅通过本体感知时序信号，复现教师策略的环境适应能力”，因此输入严格限制为机器人实际可获取的本体感知数据（无任何特权信息），且采用**时间卷积网络（TCN）** 替代教师的MLP架构。

TCN通过“因果膨胀卷积”结构可捕捉长达2秒的本体感知历史（采样间隔0.02s，即100个时间步，记为TCN 100），能够有效识别“足陷阱”“渐进式打滑”等需依赖历史信息判断的事件（如“某关节力矩持续增大但足端位移微小→判断为足被植被缠绕→触发抬足反射”）。

学生策略的训练采用模仿学习范式，损失函数设计为“动作损失+latent表示损失”的组合
\(\mathcal{L} = \left(\bar{a}_t(o_t,x_t) - a_t(o_t,H)\right)^2 + \left(\bar{l}_t(o_t,x_t) - l_t(H)\right)^2\)，其中第一项确保学生输出的动作与教师对齐，第二项则强制学生的TCN中间层隐向量与教师的 \(\bar{l}_t\) 匹配。

这一设计的创新性在于，学生不仅模仿教师的“表层动作”，更学习教师对“环境状态的理解方式”，从而实现“从特权信息依赖到本体感知推断”的能力迁移。消融实验清晰验证了该范式的必要性，直接用TRPO训练TCN 20策略（无师生蒸馏），在斜坡行走与台阶测试中完全失败，训练过程中 episode 长度始终较短（表明频繁失稳），而经师生蒸馏的同架构TCN 20策略，奖励值可快速逼近教师策略，且在16cm台阶测试中成功率达80%以上。  

此外，TCN的时序建模能力对学生策略的鲁棒性至关重要。通过对比不同记忆长度的TCN（TCN 1对应20ms、TCN 20对应0.4s、TCN 100对应2s）发现，在均匀斜坡场景中，记忆长度对性能影响较小，但在台阶场景中，TCN 100可处理高达20.1cm的台阶，而TCN 1仅能处理10cm以下台阶，且当后腿接触台阶时，TCN 1的失败率达75%（因无法利用前腿接触台阶的历史信息调整后腿轨迹），在50N侧向扰动测试中，TCN 100的运动方向偏差比TCN 1降低35.5%，证明长时序记忆可有效提升抗扰动能力。

同时，原文通过训练解码器网络（从TCN中间层重构特权信息 \(x_t\)）发现，学生策略能从本体感知时序中隐式推断出地形高程（红色椭球表示估计地形形状与不确定性）、足地接触状态与外部扰动力，例如在湿滑白板上行走时，解码器估计的摩擦系数会从0.8快速降至0.4，并在返回正常地面2秒后回升，这表明TCN确实学习到了“本体感知 环境状态”的映射关系，而非简单的动作记忆。


#### 3.2.2 PMTG 控制架构
PMTG（Policies Modulating Trajectory Generators）控制架构是该工作为平衡“训练稳定性”与“地形适应性”提出的核心控制范式，其本质是将“模型驱动的运动先验”与“数据驱动的策略优化”深度融合，在3.1前一章的端到端RL控制器直接输出关节位置或力矩，训练初期易因动作混乱导致机器人快速摔倒，而PMTG通过预定义的“足轨迹生成器（FTG）”提供步态周期性先验，同时允许策略网络通过“残差调制”适应复杂地形，既保证了训练初期的稳定性，又保留了对未知环境的灵活响应能力。  

PMTG架构的底层核心是**足轨迹生成器（FTG）**，其设计基于“周期性腿相位”机制，为每条腿定义相位变量 \(\phi_i \in [0, 2\pi)\)，其中 \(\phi_i \in [0, \pi)\) 对应接触相（足端接地，提供支撑），\(\phi_i \in [\pi, 2\pi)\) 对应摆动相（足端抬起，避免碰撞），相位更新公式为 \(\phi_i = (\phi_{i,0} + (f_0 + f_i)t) \mod 2\pi\)。

式中 \(\phi_{i,0}\) 为初始相位（训练中随机采样自 \(U(0,2\pi)\)，确保步态多样性），
\(f_0 = 1.25Hz\) 为基础频率（与ANYmal传统trot步态控制器一致，保证步态自然性），
\(f_i\) 为策略网络输出的频率偏移（用于调整单条腿的步态节奏，如遇障碍时减慢摆动腿频率）。

FTG的核心功能是生成基础足端轨迹，当 \(k \in [0,1]\) 时（摆动相初期），轨迹为 \(F(\phi_i) = (h(-2k^3 + 3k^2) - 0.5)^{H_i}z\)；
当 \(k \in [1,2]\) 时（摆动相后期），轨迹为 \(F(\phi_i) = (h(2k^3 - 9k^2 + 12k - 4) - 0.5)^{H_i}z\)；
其余情况为 \(-0.5^{H_i}z\)（接触相足端高度），其中 \(k = 2(\phi_i - \pi)/\pi\) 为归一化相位，\(h = 0.2m\) 为最大足端高度，\(H_i\) 为“水平帧”坐标系，这一坐标系设计是FTG的关键创新，\(H_i\) 固连于第i条腿的髋关节下方（距离等于腿的名义reach），其z轴平行于重力向量，x轴为基座x轴在水平面的投影（与基座保持相同偏航角），roll与pitch角则与基座完全解耦。

这种设计的优势在于一是大幅降低基座姿态波动对足端轨迹的影响（如基座倾斜时，足端仍能保持水平方向的稳定运动），使训练初期策略不易因基座晃动而失稳，二是可在训练中对策略的动作分布进行分解，在侧向方向施加更大探索噪声，促进机器人沿地面的横向移动能力。  

PMTG架构的上层是**策略网络的残差调制模块**，其核心逻辑是“策略不直接生成足端轨迹，而是在FTG基础轨迹上叠加微调量”，具体而言，策略网络输出两类调制参数一是4条腿的频率偏移 \(f_i\)（调整单条腿的相位更新速率，如当前腿遇障碍时，减小 \(f_i\) 以延长摆动相，为避障争取时间），二是4条腿的3维足端位置残差 \(\Delta r_{f_i,T}\)（在FTG生成的 \(F(\phi_i)\) 基础上，沿x/y/z轴叠加微调，实现“抬高足端过台阶”“侧向偏移避碎石”等自适应行为），最终目标足端位置为 \(r_{f_i,T} = F(\phi_i) + \Delta r_{f_i,T}\)。

这种“先验+残差”的设计，使得策略网络的优化空间被限制在“合理步态范围内”，避免了端到端RL中常见的“动作探索混乱”问题，同时残差调制的灵活性又能应对复杂地形的不确定性，
机器人遇到16.8cm台阶时（高于平坦地形12.9cm的最大足间隙），策略网络通过输出正的z向残差 \(\Delta r_{f_i,T,z}\)，将前腿（LF/RF）的最大足间隙提升至22.5cm/18.5cm，后腿（LH/RH）提升至16.6cm/15.9cm，形成“足陷阱反射”；更重要的是，该反射无需任何显式触发条件（如足端接触信号），当小腿中部在摆动相碰撞障碍时（图3C），策略仍能通过分析关节力矩与位置的时序变化，输出合适的残差调整足端轨迹，而传统脚本控制器（依赖足端接触触发）则会因未检测到足端碰撞而失效。  

PMTG架构的运动跟踪环节则采用“解析逆运动学=+关节位置PD控制”的组合，首先将 \(H_i\) 坐标系下的目标足端位置 \(r_{f_i,T}\) 转换为基座坐标系下的坐标（通过坐标变换矩阵），再利用ANYmal腿的3自由度结构（髋屈/伸、髋内收/外展、膝屈/伸）求解解析IK，得到各关节的位置目标；
随后，关节位置PD控制器跟踪这些目标，其动力学模型采用“learned actuator model”，该模型通过训练学习了关节PD控制器的滞后特性与非线性，输入为当前及历史0.01s、0.02s的关节位置误差与速度，输出为关节力矩，这一设计大幅提升了sim to real的迁移精度，仿真训练的策略部署到真机后，无需调整PD参数即可稳定运行，在平坦地形全向测试中，速度跟踪误差小于0.05m/s，航向误差始终控制在10°以内，而传统模型驱动控制器（Bellicoso 2018）在侧向运动时航向误差达30°，且加载10kg payload（占机器人重量22.7%）后完全失效。


#### 3.2.3 自适应地形课程
自适应地形课程（Adaptive Terrain Curriculum）是该工作突破 sim2real 的关键创新，其核心思想是“动态生成‘中等难度’的训练地形，确保策略始终在‘可学习区间’内进化”，前一章的工作采用“随机地形采样”（如均匀采样台阶高度、斜坡角度），易导致训练过程中“地形难度与策略能力不匹配”，过易地形无法提供有效训练信号，过难地形则导致策略频繁失败、样本效率低下，而自适应课程通过粒子滤波动态调整地形参数分布，使训练难度随策略性能提升而逐步增加，最终让策略学习到“覆盖真实环境多样性的通用适应规律”，而非对特定训练地形的过拟合。  

技术实现分为“地形参数化”“粒子滤波维护难度分布”“动态更新地形”三个核心步骤。

首先是**地形参数化**，原文将训练地形分为三类具有代表性的刚性地形（无需建模变形，降低仿真复杂度），每类地形通过一组可调节参数描述。
一是Hills地形（模拟自然斜坡与起伏地面），基于Perlin噪声生成，参数包括粗糙度（\(c_{T,1}\)，控制地形高度的随机波动范围，采样自 \(U(0.02,0.15)\)）、Perlin噪声频率（\(c_{T,2}\)，控制地形起伏的密集程度，采样自 \(U(0.1,1.2)\)）、噪声振幅（\(c_{T,3}\)，控制地形起伏的最大高度，采样自 \(U(0.5,2.0)\)），其高度图表达式为 \(hm[i,j] = Perlin(c_{T,2},c_{T,3})[i,j] + U(-c_{T,1},c_{T,1})\)，策略在该地形上主要学习“平滑斜坡行走”与“应对轻微足滑”；
二是Steps地形（模拟离散高程变化，如岩石堆），由多个 \(c_{T,1} \times c_{T,1}\) 的方形块组成（\(c_{T,1}\) 为台阶宽度，采样自 \(U(0.1,0.5)m\)），每个块的高度采样自 \(U(0, c_{T,2})\)（\(c_{T,2}\) 为台阶高度，采样自 \(U(0.05,0.3)m\)），策略在此学习“足端抬高过障碍”与“适应离散高度变化”；
三是Stairs地形（模拟规则楼梯，如DARPA挑战赛中的台阶），参数为台阶高度（\(c_{T,1}\)，采样自 \(U(0.02,0.2)m\)）与宽度（固定为0.3m），机器人初始位置设置在楼梯中间的平坦段，策略在此学习“连续台阶上下行”的步态协调。所有地形参数的范围均基于ANYmal的运动学极限设定（如台阶高度不超过腿长的1/3），确保策略有能力通过合理调整实现 traversal。  

其次是**粒子滤波维护难度分布**，该过程通过“粒子”表征地形参数组合，动态筛选“策略可通过但需付出努力”的中等难度地形初始化阶段，为每类地形采样 \(N_{particle}=10\) 个粒子（每个粒子对应一组地形参数 \(c_T\)），粒子均匀分布于参数空间；
训练迭代过程中，每完成 \(N_{evaluate}=10\) 次策略更新（视为一个评估周期），对每个粒子对应的地形，运行 \(N_{traj}=6\) 条轨迹，计算该地形的“可通行性” \(Tr(c_T,\pi)\)定义为“成功遍历时间（无失稳）与总任务时间（10s）的比值”；随后，根据 \(Tr\) 筛选粒子，仅保留 \(Tr \in [0.5,0.9]\) 的粒子（\(Tr<0.5\) 为过难地形，策略频繁失败；
\(Tr>0.9\) 为过易地形，无训练价值），并计算每个粒子的权重 \(w_j = P(y_j^k | c_{T,j}^k)\)，其中 \(P(y_j^k | c_{T,j}^k)\) 为粒子 \(c_{T,j}^k\) 属于“中等难度”的概率，近似为 \(N_{traj}\) 条轨迹中 \(Tr \in [0.5,0.9]\) 的比例；
最后，基于权重进行粒子重采样，同时引入 \(P_{replay}=0.05\) 的概率从“历史优秀粒子回放内存”中采样，避免粒子分布退化（如因策略性能提升导致所有当前粒子均变为过易地形），并以 \(p_{transition}=0.8\) 的概率将重采样后的粒子向参数空间相邻区域轻微移动（随机游走），保证地形多样性。  

最后是**动态更新地形**，随着策略训练的推进，中等难度地形的参数分布会逐步向“更具挑战性”的区域偏移，例如Hills地形的粒子分布，从训练初期的“低粗糙度（0.02）、低频率（0.1）、低振幅（0.5）”（易地形），逐步演变为“高粗糙度（0.15）、高频率（1.2）、高振幅（2.0）”（中等难度地形）；Stairs地形的粒子分布则从“宽台阶（0.5m）、低高度（0.05m）”逐步转向“窄台阶（0.1m）、高高度（0.2m）”。这种动态调整确保了“策略每一步学习都能获得有效反馈”，避免了传统随机采样的低效性，消融实验对比了“自适应课程”与“均匀随机采样”的训练效果，在Hills地形测试中，无自适应课程的教师策略成功率仅为40%，而有课程的达85%；训练奖励方面，无课程的策略在5000次迭代后即进入平台期（奖励≈0.15），而有课程的策略可持续提升至0.25；平均 episode 长度方面，无课程的策略因频繁遇到过难地形，episode 长度始终低于200s，而有课程的策略则稳定在350s以上，有效训练样本量提升近一倍。  

值得注意的是，该自适应课程的核心价值在于“无需建模变形地形，却能让策略适应变形地形”，通过在刚性地形中覆盖“足够广的难度与多样性”，策略学习到的是“本体感知信号与动作调整”的通用映射（如“关节力矩骤增→足端受阻→抬高足端”“基座倾斜→调整对侧腿支撑力→恢复平衡”），而非对特定地形的记忆。原文真机实验验证了这一点，仅在刚性地形训练的策略，部署到泥泞地时，能通过关节力矩的时序变化判断足端下陷，输出更慢的步态与更大的足端接地面积；部署到雪地时，能通过基座角速度的微小波动感知打滑，调整足端落地速度以增大摩擦力，最终实现零样本泛化，这一结果彻底颠覆了“仿真必须精确复现真实地形物理特性”的传统认知，为后续足式机器人强化学习控制器的高效开发提供了关键方法论。

总结下来，该工作证实了 “无需在仿真中精确建模真实世界的复杂性，仅通过‘时序本体感知 + 知识蒸馏 + 自适应训练课程’，即可让策略学习到通用的环境适应规律”，这一认知打破了 “仿真复杂度必须匹配真实复杂度” 的固有范式，为后续强化学习盲走控制器的工程化落地奠定了核心架构基准。

### 3.3 地形隐空间向量
前两项工作虽提升了策略的鲁棒性，但本质上仍属于被动适应。3.1 节虽通过域随机化在仿真中覆盖了部分环境变异，但策略的泛化能力完全依赖训练阶段的环境分布，一旦部署到未见过的复杂地形（如沙地、泥泞、高草覆盖区域），由于无法实时感知地面摩擦系数、刚度、负载变化等关键物理参数，极易出现步态失稳或摔倒。

而 3.2 节利用仿真中的特权信息（如地形高度、摩擦系数）提升策略适应性，但该方法仍未解决 “部署阶段环境参数不可得” 的核心问题。教师网络在仿真中可直接调用全局环境参数，但学生网络在真实世界仅能获取本体感知数据，若要适配新环境，需线下采集 3–5 分钟的运动数据进行微调，在实际中可能并不可行。
在这一背景下，Kumar 等人 2021 年提出的 RMA 首次为盲走控制器引入了显式的、在线运行的环境自适应能力，实现了部署阶段的零样本、亚秒级在线自适应。
### 3.3.1 双模块与隐空间
#### 3.3.1.1 基础策略
基础策略 π 是 RMA 的 “运动控制中枢”，其核心功能是基于机器人实时状态与环境特征，输出适配的关节动作指令。
![](/paper/rma-overview.png)
输入包含三类信息：
- 当前状态 \(x_t \in \mathbb{R}^{30}\)：30 维状态空间包含机器人质心（COM）位置、 12 个关节角度、IMU 测量的滚转 / 俯仰角、线性速度与角速度，全面反映机器人实时运动状态；
- 先前动作 \(a_{t-1} \in \mathbb{R}^{12}\)：12 维动作空间对应 12 个关节的上一时刻期望位置（通过 PD 控制器转换为关节扭矩），引入历史动作可避免当前动作与前序运动脱节，提升步态平滑性；
- 外部参数向量 \(z_t \in \mathbb{R}^{8}\)：\(z_t\) 并非直接使用高维环境参数 \(e_t \in \mathbb{R}^{17}\)（含地面摩擦系数、负载质量、电机强度等 17 项物理参数），而是通过环境因子编码器 \(\mu\) 对 \(e_t\) 进行低维非线性投影得到（\(z_t = \mu(e_t)\)）。\(z_t\) 的 8 维设计旨在规避高维 \(e_t\) 的参数耦合与可辨识性问题，其中部分参数（如地面刚度与摩擦系数）对步态的影响存在叠加，直接估计易导致精度损失，而 \(z_t\) 通过端到端优化，仅保留影响动作决策的关键环境特征，既降低计算复杂度，又确保策略能快速适配环境变化。
π 的输出为当前时刻 12 个关节的期望位置 \(a_t\)（\(a_t = \pi(x_t, a_{t-1}, z_t)\)），最终通过 PD 控制器转换为关节扭矩，实现从算法指令到硬件执行的落地。其采用 3 层多层感知器（MLP）架构，隐藏层维度为 128，其设计重点在于将环境隐向量 \(z_t\) 融入动作决策，使策略能根据不同环境特征动态调整步态：例如，当 \(z_t\) 表征低摩擦地面时，π 会输出更大的关节扭矩以抑制打滑；当 \(z_t\) 表征高负载时，π 会优化关节运动轨迹以降低电机功耗。

#### 3.3.1.2 自适应模块
基础策略 π 依赖 \(z_t\)（\(e_t\)）实现环境适配，但现实部署中 \(e_t\)（高维物理参数）无法直接观测，这是强化学习盲走的核心部署瓶颈。为此，RMA 设计自适应模块 \(\varphi\)，其功能是从机器人可观测的状态–动作历史中在线估计 \(z_t\) 的近似值 \(\hat{z}_t\)，解决环境参数不可得的问题。

\(\varphi\) 的输入为近期状态–动作历史序列：具体为过去 \(k\) 步的状态 \(x_{t-k:t-1}\) 与动作 \(a_{t-k:t-1}\)，原文实验中 \(k = 50\)（对应 0.5 秒，因为 π 的运行频率为 100 Hz，50 步即 0.5 秒），可以覆盖完整步态周期。

输出为 \(\hat{z}_t \in \mathbb{R}^{8}\)（\(z_t\) 的估计值），运行频率设定为 10 Hz，采用 1D CNN 实现，通过捕捉时序数据中的局部关联特征，相比 MLP 能更高效地从 50 步历史中提取环境影响的时序模式，提升 \(\hat{z}_t\) 的推断精度。例如，当机器人在沙地行走时，足端会因地面变形而出现期望关节位置 \(a\) 与实际位置 \(x\) 偏差增大的现象，\(\varphi\) 通过 CNN 捕捉这一时序偏差模式，即可推断出对应的 \(\hat{z}_t\)，进而指导 π 调整步态以适应沙地凹陷。

传统系统识别方法试图在线估计精确的高维 \(e_t\)，但既无必要又存在难度，因为 \(e_t\) 中部分参数对动作决策的影响微小，且参数间的耦合会导致估计误差放大，最终影响策略性能；而 \(z_t\) 是仅编码动作调整需求的低维特征，无需精确匹配物理参数，只需确保 \(\hat{z}_t\) 能引导 π 输出适配动作即可，大幅降低了推断难度与计算复杂度。

#### 3.3.1.3 异步部署
RMA 的最终目标是从仿真全流程训练过渡到现实直接部署。为此需要通过双模块频率拆分，确保 RMA 能在机器人有限的机载算力上稳定运行，同时不损失控制实时性。

基础策略 π 的运行频率为 100 Hz，与 A1 机器人的 PD 控制器扭矩更新频率一致，确保从动作指令到硬件执行的实时性。足端接触地面的瞬间（约 0.01 秒）需快速调整扭矩以维持平衡，100 Hz 的频率可满足这一需求。

自适应模块 \(\varphi\) 的运行频率为 10 Hz（每 0.1 秒更新一次 \(\hat{z}_t\)），因为 \(\varphi\) 需处理 50 步历史数据（计算复杂度高于 π），10 Hz 的频率可平衡推断精度与算力消耗。

π 在运行时无需等待 \(\varphi\) 的计算结果，只需调用 \(\varphi\) 最新输出的 \(\hat{z}_t\)（即便是 0.1 秒前的估计值），因为环境参数 \(z_t\) 的变化频率远低于 π 的动作更新频率。例如，机器人从水泥地走到沙地，\(z_t\) 的变化是渐进的（需 0.5–1 秒才能完全体现），0.1 秒更新一次 \(\hat{z}_t\) 足以覆盖这种变化，不会影响 π 的动作决策精度。

### 3.3.2 双阶段训练
RMA 的双阶段训练流程是确保 “双模块协同工作” 与 “Sim-to-Real 无缝迁移” 的关键，两个阶段均在仿真环境中完成，无需任何真实世界数据预采集，彻底规避了传统方法的部署风险。其核心逻辑是 “先训练‘环境感知 - 动作决策’的关联能力，再训练‘时序推断 - 隐向量估计’的适配能力”，两阶段的衔接通过 “隐空间向量 z” 实现。

第一阶段为 “基础策略 π 与环境因子编码器 \(\mu\) 的 PPO 联合训练”，目标是让 π 学会根据 \(z_t\) 适配不同环境，同时让 \(\mu\) 学会将高维环境参数 \(e_t\) 编码为对 π 有用的低维 \(z_t\)。
训练的核心是 “生物能量学启发的奖励函数”，奖励函数设计并非依赖人工经验（如 “足间隙奖励”），而是基于生物运动的能量效率原则，最小化电机功与地面冲击：
| 奖励/惩罚类型       | 数学表达式                          | 缩放系数 | 核心作用                                                                 |
|--------------------|-------------------------------------|----------|--------------------------------------------------------------------------|
| 前向速度奖励       | $\min (v_t^x, 0.35)$                | 20       | 鼓励机器人以0.35m/s的最优速度移动，避免过快导致失稳或过慢降低效率         |
| 横向与旋转惩罚     | $-\|v_t^y\|^2 - \|\omega_t^{yaw}\|^2$ | 21       | 抑制横向偏移与航向偏转，确保运动方向稳定                                 |
| 电机功惩罚         | $-|\tau^T \cdot (q_t - q_{t-1})|$   | 0.002    | 最小化关节扭矩与角度变化的乘积，降低能量消耗                             |
| 地面冲击惩罚       | $-\|f_t - f_{t-1}\|^2$              | 0.02     | 减少足端接触地面时的反作用力突变，避免硬件损伤                           |
| 扭矩平滑性惩罚     | $-\|\tau_t - \tau_{t-1}\|^2$        | 0.001    | 避免关节扭矩骤变，提升步态流畅性                                         |
| 动作幅度惩罚       | $-\|a_t\|^2$                        | 0.07     | 限制关节期望位置的最大范围，防止超出行程                                 |
| 关节速度惩罚       | $-\|\dot{q}_t\|^2$                  | 0.002    | 抑制关节超速，保护电机                                                   |
| 姿态惩罚           | $-\|\theta_t^{roll,pitch}\|^2$      | 1.5      | 维持机身滚转/俯仰角稳定，避免倾倒                                         |
| Z轴加速度惩罚      | $-\|v_t^z\|^2$                      | 2.0      | 减少垂直方向颠簸，提升运动平稳性                                         |
| 足端打滑惩罚       | $-\|\text{diag}(g_t) \cdot v_{ft}\|^2$ | 0.8   | 通过二进制接触向量$g_t$（1表示接触地面，0表示悬空）抑制足端滑动，确保抓地稳定 |

这种端到端训练避免了先训练 \(\mu\) 再训练 π 的两阶段偏差，\(\mu\) 的编码方向由 π 的动作需求引导，确保 \(z_t\) 的每一个维度都与动作调整强相关，而非单纯的环境参数压缩。

为避免策略因惩罚项过强而停滞不动，原文设计了渐进式训练课程（Curriculum）：初始时惩罚项的乘数 \(k_0\) 设为 0.03，随后每迭代一次按
\[
k_{t+1} = k_t^{0.997}
\]
指数递增，同时逐步增加环境扰动（如负载、摩擦、电机强度的随机化范围），确保策略从简单步态逐步过渡到复杂环境适配。
此外，训练环境采用 “分形地形生成器”，随机生成不平坦地形，使策略在仿真中就能习得对地形起伏的自适应能力，为后续真实世界部署奠定基础。

---

第二阶段为自适应模块 \(\varphi\) 的监督学习训练，目标是让 \(\varphi\) 学会从状态–动作历史序列中准确推断 \(z_t\)，替代部署时不可得的 \(e_t\)。自适应模块需从 50 步状态–动作历史中提取时序关联，推断 8 维 \(z_t\)，因此采用 “MLP 嵌入 + 1D CNN 时序特征提取” 的混合架构，兼顾特征抽象与时序捕捉。

该阶段采用仿真内有监督学习。\(\varphi\) 的优化目标是最小化估计值 \(\hat{z}_t\) 与真实值 \(z_t\) 的均方误差（MSE），优化器采用 Adam。训练数据来自第一阶段 π 运行时的 “状态–动作历史–真实 \(z_t\)” 三元组，即 on-policy 数据（与 π 运行时的轨迹完全同步的数据），可避免数据分布偏移。若采用 off-policy 数据，\(\varphi\) 学到的时序模式可能与 π 的动作决策逻辑不匹配，导致部署时 \(\hat{z}_t\) 的推断精度下降。

1. 初始化 \(\varphi\) 为随机参数，用该 \(\varphi\) 预测 \(\hat{z}_t\)；
2. 让 π 基于 \(\hat{z}_t\)（而非真实 \(z_t\)）执行动作，生成非最优的探索轨迹（因 \(\hat{z}_t\) 存在误差，机器人会出现步态波动甚至短暂失衡）；
3. 收集该轨迹的状态–动作历史与真实 \(z_t\)，构成训练样本（历史序列映射到 \(z_t\)）；
4. 用这些样本更新 \(\varphi\) 的参数，最小化 MSE；
5. 重复步骤 1–4，直至 \(\varphi\) 收敛。
同期的环境自适应方法（如用策略梯度优化隐变量、贝叶斯优化、元学习初始化策略等）均需 “在现实世界中进行多次轨迹滚动（Roll-out）” 以调整参数，而 RMA 的 φ 完全在仿真中训练，无需任何现实数据，彻底规避了 “现实滚动导致的机器人摔倒风险”（尤其对轻量机器人 A1 而言，摔倒易造成硬件损坏）。

通过双阶段训练，RMA 最终实现了 “π 负责实时动作输出、\(\varphi\) 负责在线环境推断” 的协同模式，且两者均无需真实世界微调即可直接部署，为后续真实环境中的高成功率奠定了算法基础。

总的来说，RMA 通过基础策略 π（100 Hz 实时控制）和自适应模块 \(\varphi\)（10 Hz 在线推断）的异步架构，实现了对多样地形（如不平坦泡沫、床垫、碎石地、楼梯）的动态适配，无需离线微调，无需示范或预定义运动模板。仅通过自然约束下的超大规模仿真训练，系统便可凭借本体感知历史实时推断环境隐向量 \(z_t\)，让策略快速调整关节动作以适应环境。

### 3.4 质心线速度估计
RMA等前序工作虽赋予了策略在线环境自适应的能力，但其“自适应模块”与“基础策略”的交互存在一个关键限制：自适应模块主要推断的是外部环境属性（如摩擦、刚度），而对机器人自身的核心状态——尤其是质心线速度——的估计，仍依赖于传统或独立的观测器。在崎岖地形中，这种解耦的估计方式容易因足端滑移、碰撞等动态接触而产生显著误差，进而误导策略决策。
DreamWaQ的核心突破在于，它将状态估计与环境推断深度融合，提出了一个统一的“隐式地形想象”框架。通过非对称Actor-Critic架构，策略（Actor）在仅有本体感知的条件下，被价值函数（Critic）引导去隐式地学习和推断包括地形高程在内的特权信息，从而能更精准地预测身体运动状态。。
因此，DreamWaQ在继承RMA在线自适应思想的基础上，将研究的焦点从“感知环境”深化到了“通过环境来理解自身状态”，实现了更鲁棒、更精确的盲走控制，为在极度复杂地形中实现长期、稳定的自主运动提供了关键解决方案。

#### 3.4.1 隐式地形想象
前序研究采用的师生范式存在两大缺陷：一是性能上限受限，行为克隆（BC）机制使学生策略只能模仿教师策略的 “最优动作”，无法突破教师的性能边界；二是数据效率低下，教师与学生网络需序贯训练，且学生无法探索教师在早期强化学习中接触的 “失败状态”（如足端绊脚、打滑），这些失败状态的经验对复杂地形鲁棒性至关重要，但 BC 仅提供 “成功动作” 的监督，导致学生策略在未知地形中适应性不足。

为此，DreamWaQ 采用非对称 Actor–Critic 架构，通过 Actor 与 Critic 的差异化输入与协同学习，实现隐式地形想象，仅通过本体感知就能隐式推断出地形的特权信息（如高度图 \(h_t\)、干扰力 \(d_t\)），而无需依赖显式的外感传感器。
![](/paper/dreamwaq-overview.png)
Actor 的核心是基于部分观测做决策。其输入为时序部分观测 \(o_t^H\)、由 CENet（3.4.2 节会展开）估计的身体速度 \(\mathbf{v}_t\) 与上下文向量 \(z_t\)，输出为 12 维关节角度偏差 \(a_t\)：
\[
o_{t}=\left[\begin{array}{lllllll}
\omega_{t} & g_{t} & c_{t} & \theta_{t} & \dot{\theta}_{t} & a_{t-1}
\end{array}\right]^{T}.
\]
其中，身体角速度 \(\omega_t\) 反映姿态稳定性，重力向量 \(g_t\) 辅助判断身体倾斜方向，速度指令 \(c_t\) 明确运动目标，关节状态 \((\theta_t,\dot{\theta}_t)\) 反映执行器当前状态，前一时刻动作 \(a_{t-1}\) 确保步态连续性。为降低学习难度并适配硬件，策略网络并非直接输出绝对关节角，而是学习围绕静止姿态 \(\theta^{\text{stand}}\) 的偏差，最终期望关节角通过
\[
\theta^{\text{des}} = \theta^{\text{stand}} + a_t
\]
计算，并由 PD 控制器（\(K_p = 28, K_d = 0.7\)）以 200 Hz 频率跟踪，确保关节运动平滑且符合硬件动力学限制。

Critic 的核心是基于完全状态做评估。其输入为包含特权信息的完全状态 \(s_t\)：
\[
s_{t}=\left[\begin{array}{llll}
o_{t} & v_{t} & d_{t} & h_{t}
\end{array}\right]^{T}.
\]
与 Actor 网络不同，价值网络能获取本体感知之外的全局信息，如随机干扰力 \(d_t\)（如地面对机器人的冲击力）、地形高度图 \(h_t\)（如台阶高度、斜坡倾角）。其作用是通过评估当前状态–动作对的长期价值 \(V(s_t)\)，间接引导策略网络学习如何从本体感知中推断这些特权信息。例如，当机器人踏上斜坡时，价值网络会根据 \(h_t\) 评估当前步态是否会导致失稳，并通过价值信号反馈给策略网络，让策略逐渐学会通过 IMU 的重力向量变化 \(g_t\) 推断斜坡倾角，从而实现隐式地形想象。

为强化隐式地形想象的学习效果，论文还在训练策略上针对奖励函数与课程学习做了优化。
奖励函数包含任务奖励与稳定性奖励两类。前者聚焦速度跟踪，例如前向速度跟踪奖励
\[
\exp\bigl(-4 (v_{xy}^{\text{cmd}} - v_{xy})^2\bigr),
\]
通过指数函数惩罚大误差、鼓励精准跟踪；后者则覆盖关节加速度、身体高度等，避免步态剧烈波动。

比较突破性的一点是功率分配奖励 \(-\operatorname{var}(\tau \cdot \dot{\theta})^2\)（权重约为 \(-10^{-5}\)）。前序方法仅追求总功率最小化，易导致部分电机长期高负载过热，而该奖励通过惩罚各电机功率的方差，平衡关节功率消耗，为长距离行走（如 465 m 丘陵地形）提供硬件鲁棒性支撑。

#### 3.4.2 上下文辅助估计器网络
质心线速度的准确估计需解决两大核心问题：一是如何结合环境上下文修正估计偏差，避免前序方法 “状态 - 地形解耦” 导致的动态误差；二是如何协调估计器与策略网络的训练节奏，避免早期估计噪声对策略学习的干扰。针对这两点，DreamWaQ 提出上下文辅助估计器网络（CENet），通过 “共享编码器 + 多任务解码” 与 “自适应引导（AdaBoot）” 机制，实现质心线速度与环境上下文的联合估计，同时动态优化估计器对策略训练的支撑时机。

前序研究在状态估计与环境表征 \(z_t\) 的协同上存在明显不足：要么像 AdaptationNet 与 Margolis 等工作那样估计了地形隐空间向量 \(z_t\)，但未将其与质心线速度 \(v_t\) 等身体状态关联，导致地形信息无法为状态估计提供修正；要么像 EstimatorNet 那样虽实现策略与估计器的并发训练，但仅聚焦于质心线速度的显式估计，缺乏环境上下文的约束。在楼梯等动态地形中，当机器人足端绊脚时，估计误差会急剧增大，甚至引发灾难性失稳。

![](/paper/dreamwaq-cenet.png)

CENet 的核心是共享编码器。它以时序部分观测 \(o_t^H\) 为输入，通过 128×64×19 的网络结构将多步本体感知编码为统一的特征表示，再分支出两个解码器头：
- 质心线速度估计头：直接输出身体速度估计值 \(\tilde{v}_t\)，作为策略网络判断运动状态的依据（如通过 \(\tilde{v}_t\) 调整步长以匹配速度指令 \(c_t\)）；
- 观测重构头：基于 \(\beta\)-变分自编码器（\(\beta\)-VAE）架构，输出重构的下一时刻观测 \(\tilde{o}_{t+1}\) 与上下文向量 \(z_t\)。其中，\(z_t\) 是环境状态的潜在表示（如地形摩擦、高度变化），而 \(\tilde{o}_{t+1}\) 的重构任务强制编码器学习本体感知的时序关联性（如通过历史关节角速度 \(\dot{\theta}_{t-1}\) 预测 \(\dot{\theta}_t\)），间接为 \(\tilde{v}_t\) 的估计提供动力学约束，避免 \(\tilde{v}_t\) 脱离物理规律，如出现身体速度突变。
> 普通VAE损失是 “重建损失” 和 “KL 散度” 的加权和，默认权重都是 1，但潜在变量的维度是 “纠缠” 的，一个维度对应多个语义特征，导致潜在空间完全没有 “可解释性”，无法通过控制单个维度来精准操控生成结果。
> β-VAE 通过给 KL 散度乘以一个参数 β，调整 KL 散度在总损失中的权重，强制潜在变量的各个维度变得独立，从而让潜在空间从 “黑箱” 变成 “可解释、可操控的工具”
这种架构的优势在于，共享编码器能同时学习身体动力学与环境对观测的影响，而观测重构任务通过强制编码器捕捉时序观测的内在关联（如关节运动与质心速度的映射关系、地形扰动对 IMU 数据的影响），间接为质心线速度估计提供动力学约束，避免估计值脱离物理规律。同时，\(z_t\) 作为环境上下文的隐式表征（如地形摩擦、高度变化），能为质心线速度估计提供环境层面的修正。例如在湿滑地形中，\(z_t\) 会编码低摩擦信息，使编码器在估计 \(\tilde{v}_t\) 时，对 IMU 检测到的瞬时速度波动进行平滑处理，避免策略因误判速度而调整过度。

CENet 的优化通过混合损失函数实现：
\[
L_{\text{CE}} = L_{\text{est}} + L_{\text{VAE}}.
\]
其中，\(L_{\text{est}}\) 为质心线速度估计损失，采用均方误差（MSE）计算估计值 \(\tilde{v}_t\) 与模拟器真值 \(v_t\) 的偏差；
\(L_{\text{VAE}}\) 为 \(\beta\)-VAE 损失，包含两部分：一是观测重构损失，采用 MSE 计算重构观测 \(\tilde{o}_{t+1}\) 与真实观测 \(o_{t+1}\) 的偏差，强制编码器学习时序观测的预测规律；二是潜在空间损失，采用 KL 散度计算后验分布 \(q(z_t \mid o_t^H)\) 与先验分布 \(p(z_t)\) 的差异，其中 \(p(z_t)\) 设为标准正态分布（因所有观测数据均归一化为零均值、单位方差），通过 \(\beta\) 参数（实验中设为 1.0）控制潜在空间的离散度，确保 \(z_t\) 既能有效表征环境信息，又具备良好的泛化性。

从实验结果来看，在平坦地形的正常行走中，CENet 凭借观测重构任务带来的前向–后向动力学学习，估计误差已显著低于 EstimatorNet；而在楼梯环境中，当机器人足端绊脚时，EstimatorNet 的质心线速度估计误差骤增至 0.1 m/s 以上，而 CENet 的误差仍稳定在 0.05 m/s 以内，这一优势直接保障了策略网络能及时调整步态，避免失稳。

其本质是 CENet 通过 \(z_t\) 隐式推断台阶高度变化，提前修正了质心线速度的估计偏差，体现了上下文辅助的核心价值。 
#### 3.4.3 自适应引导机制
前序研究在训练时，通常以固定概率使用估计器的输出（如 \(\tilde{v}_t\)）引导策略学习，但这种方式存在明显缺陷：训练初期，CENet 尚未收敛，\(\tilde{v}_t\) 的估计噪声较大，此时强制引导会导致策略学习错误的动作模式（如因 \(\tilde{v}_t\) 高估而缩小步长，进而无法稳定行走）；训练后期，估计器精度提升，若仍维持较低引导概率，则无法充分利用估计器的优势来提升策略鲁棒性。

为解决估计器与策略训练节奏不协调的问题，DreamWaQ 进一步提出自适应引导（AdaBoot）机制。AdaBoot 的核心思想是基于策略训练状态动态调整引导概率，其具体实现依赖多域随机环境下 episodic reward 的变异系数（CV）。

CV 定义为 \(m\) 个域随机环境（实验中 \(m = 4096\)）的 episodic reward 标准差与均值的比值，反映策略在不同环境中的性能稳定性。
- 当 CV 较大时，说明策略尚未在多样环境中稳定，此时估计器噪声对策略的干扰风险高，应降低引导概率；
- 当 CV 较小时，说明策略已具备一定鲁棒性，此时利用估计器引导能进一步提升策略对估计误差的容忍度。

引导概率 \(p_{\text{boot}}\) 的计算公式为
\[
p_{\text{boot}} = 1 - \tanh(\text{CV}(R)),
\]
其中 \(R\) 为 \(m\) 个环境的 episodic reward 向量，\(\tanh\) 函数的作用是将 CV 值平滑约束在 \([0, 1]\) 范围内，避免引导概率为负或超过 1。

实验验证表明，在命令跟踪任务中，DreamWaQ w/ AdaBoot 的前向速度跟踪误差 \(v_x^{e}\)、侧向速度跟踪误差 \(v_y^{e}\) 与偏航角速度跟踪误差 \(\omega_z^{e}\) 均显著低于 DreamWaQ w/o AdaBoot，且通过配对 t 检验（\(p < 10^{-4}\)）验证了差异的统计显著性。

在鲁棒性测试中，DreamWaQ w/ AdaBoot 能承受的最大推力达 \(1.121 \pm 0.164\ \text{m/s}\)，30 分钟随机行走的存活率达 \(95.23\% \pm 1.61\%\)，均显著高于无 AdaBoot 的版本（最大推力 \(1.015 \pm 0.121\ \text{m/s}\)，存活率 \(90.71\% \pm 1.25\%\)），充分证明了该机制在协调估计器与策略训练、提升鲁棒性方面的价值。

这些结果表明，CENet 通过上下文联合估计与自适应引导，有效解决了前序质心线速度估计方法的局限，成为 DreamWaQ 实现长距离复杂地形盲走的核心技术支柱。

归根到底，DreamWaQ 本质是一个 “稳健的四足机器人运动控制框架”，核心目标是解决 “非结构化地形行走” 这一核心难题，在 “随机不连续地形”（混合 10-20cm 台阶、5-10cm 沟壑）中，DreamWaQ 的 “步态稳定性指标”相比传统方法提升有显著提升，证明其速度估计不依赖特定地形，而是能通过 “地形想象” 自适应不同起伏。

其后续 DreamWaQ++ 工作更是进一步引入“多模态混合器”（融合 3D 点云外感数据），将 “隐式地形想象” 升级为 “显式地形建模”，在质心速度估计中加入 “外感地形高度先验”（如前方 1m 处的台阶高度），进一步在“完全未知的大障碍”场景下将速度估计误差降低。

### 3.5 复杂地形梯度冲突
尽管 RMA 通过‘地形隐空间向量’实现了单一策略对多地形的适配，DreamWaQ 通过‘质心线速度估计’提升了盲走的抗扰动性，但这些工作均局限于‘单一步态（四足）+ 有限地形’的单任务场景。然而，足式机器人在实际非结构化环境中，需同时应对‘杆 / 坑 / 台阶 / 挡板’等异质地形，且需切换‘四足 / 二足’双步态（如狭窄空间用二足站立跨越，开阔地形用四足高效移动），这要求‘单一盲走策略’具备多任务泛化能力。

但传统多任务强化学习（MTRL）采用‘单神经网络架构’时，存在梯度冲突的致命瓶颈。其本质是不同任务（如四足跨越坑需‘腿部伸展 + 躯干稳定’，二足上台阶需‘重心前移 + 单腿支撑’）对网络参数的更新方向相反 。例如，四足任务要求关节角度参数向‘大伸展范围’更新，而二足任务要求同一参数向‘小摆动范围’更新，导致梯度相互抵消，不仅训练效率骤降（收敛速度慢 30%+），更会引发策略发散（如部分任务成功率趋近于 0，如表 1 中 RMA 在二足台阶任务的成功率为 0）。此前的工作虽尝试用分层 RL 解决多地形问题，但仅支持单一步态且未部署实机，未能突破这一核心瓶颈。

MoE-Loco 则首次将混合专家（Mixture of Experts, MoE）架构引入多任务盲走，通过‘动态任务路由 + 专家专精化’机制，从架构层面化解梯度冲突，实现了‘单一策略覆盖 9 种多地形 + 双步态任务’的通用盲走能力。

#### 3.5.1 基于混合专家模型的多任务运动学习

首先我们把多任务移动形式化为一个马尔科夫决策过程，最终目的是学习一种单一的通用策略。

该策略通过最大化目标函数

\[
J(\pi) = \mathbb{E}\left[\sum_{\tau} \sum_{t=0}^{\infty} \gamma^{t} R_{\tau}\bigl(s_{t}, a_{t}, s_{t+1}\bigr)\right]
\]

使其能够在各种任务中进行泛化。

![](/paper/moe-loco-pipeline.png)

为适配多地形与双步态（二足与四足）任务，状态空间包含四类观测信息。

第一类是本体感知信息 \(p_t\)，这是盲走的核心输入，涵盖了惯性测量单元（IMU）提供的投影重力与基座角速度、所有关节的位置与速度，以及上一时刻的动作指令，这些信息直接反映机器人的实时运动状态，为底层平衡控制提供基础；

第二类是显式特权状态 \(e_t\)，包含基座线速度（因 IMU 测量的线速度噪声过高而单独提取）与地面摩擦系数，这类信息在训练阶段辅助策略学习环境物理属性的适配规律，为后续盲走阶段的隐式推断提供先验；

第三类是隐式特权状态 \(i_t\)，具体为机器人各连杆的接触力数据。由于接触力直接关联地形交互特性（如台阶高度、地面硬度），但原始数据维度高且易受仿真与真实环境差异影响，因此通过一个编码器网络将其编码为低维 latent 表示，有效缓解了 sim-to-real 鸿沟；

第四类是指令信息 \(c_t\)，由速度指令 \(\mathbf{v} = (v_x, v_y, v_{\text{yaw}})\) 与步态类型的 one-hot 向量 \(g\) 组成（\(g=0\) 代表四足步态，\(g=1\) 代表二足步态），为策略提供明确的运动目标与步态切换信号，确保多任务场景下的指令响应准确性。

动作空间则被定义为 \(a_t \in \mathbb{R}^{12}\)，对应机器人 12 个关节的期望关节位置。通过低层级的 PD 控制器（设置参数 \(K_p = 40.0\)、\(K_d = 0.5\)）将关节位置指令转换为实际力矩输出，既保证了关节运动的平滑性，又避免了直接控制力矩带来的稳定性风险。

奖励函数充分考虑了四足与二足步态的运动特性差异，通过差异化的奖励项引导策略学习符合步态物理规律的运动模式。

对于四足步态（\(g=0\)），总奖励记为 \(r_{\text{quad}} = r_{\text{track}}^{\text{quad}} + r_{\text{reg}}^{\text{quad}}\)。其中跟踪奖励 \(r_{\text{track}}^{\text{quad}}\) 采用指数函数形式 \(\exp\bigl(-\sigma \lVert \mathbf{v}_{v,x,y} - \mathbf{v}_{b,x,y} \rVert^2\bigr)\)（权重设为 7.0）与 \(\exp\bigl(-\sigma (\kappa_{x_m} - \omega_{x_m})^2\bigr)\)（权重设为 2.5），分别用于惩罚线速度与角速度跟踪误差，确保机器人按指令速度运动；正则化奖励 \(r_{\text{reg}}^{\text{quad}}\) 则包含关节位置、速度、加速度的惩罚项（权重分别为 \(-0.05\)、\(-0.002\)、\(-2\times10^{-6}\)）、基座高度偏离惩罚（权重 \(-0.1\)）、足端离地状态惩罚（权重 \(-0.05\)）等，防止机器人出现关节超限、躯干倾斜或足端拖拽等不稳定行为。

对于二足步态（\(g=1\)），总奖励记为 \(r_{\text{bip}} = r_{\text{track}}^{\text{bip}} + r_{\text{stand}}^{\text{bip}} + r_{\text{reg}}^{\text{bip}}\)。在四足步态奖励的基础上新增了站立奖励 \(r_{\text{stand}}^{\text{bip}}\)，通过 \((0.5\cos\theta + 0.5)^2\)（权重 1.0）惩罚躯干姿态偏离垂直方向的误差，并通过线性函数 \(\min\left\{\max\left(\frac{T_{\max} - T_{\min}}{R_{\max} - R_{\min}}, \Pi\right), 1\right\}\)（权重 0.8）鼓励基座维持合适高度，确保二足站立与行走的稳定性；同时，二足步态的正则化奖励还强化了对后腿离地状态的约束（权重 \(-0.5\)）与碰撞惩罚（权重 \(-2.0\)），避免行走过程中腿部与地面或自身结构发生干涉。

Actor 网络与 Critic 网络均采用 MoE 模块作为核心决策层，且二者共享同一个门控网络（Gating Network），以保证策略评估与动作生成的一致性。

对于 Actor MoE 模块，其工作机制首先是低层级 LSTM 模块对拼接后的状态特征（包含编码后的隐式特权状态、显式特权状态、本体感知与指令信息）进行时序信息整合，输出历史状态特征 \(h_t\)；随后，门控网络（采用 128 维 MLP 结构）对 \(h_t\) 进行处理，通过 softmax 函数计算每个专家网络的权重

\[
\hat{g}_i = \operatorname{softmax}(g(h_t))[i],
\]

其中 \(i\) 代表第 \(i\) 个专家，实验验证了专家数量 \(N_{\text{exp}} = 6\) 时性能最优；最后，动作输出由所有专家网络的输出加权求和得到

\[
a_t = \sum_{i=1}^{6} \hat{g}_i f_i(h_t),
\]

其中 \(f_i(\cdot)\) 为每个专家的网络结构（采用 256-128-128 维 MLP）。

这种设计的核心优势在于，门控网络会根据当前任务类型（如四足跨杆、二足上台阶）动态分配高权重给专精于该任务的专家，使得不同任务的梯度仅定向更新对应专家的参数，避免了传统单网络架构中不同任务梯度相互抵消的问题。例如，原文通过专家权重分析发现，四足跨杆任务中第 3 个专家的平均权重占比达 45%（专精于 “腿部抬高跨越障碍” 动作），而二足平衡任务中第 5 个专家的平均权重占比达 52%（专精于 “躯干姿态稳定” 控制），这种明确的专家专精化特性从架构层面切断了跨任务梯度干扰的路径。

Critic MoE 的设计则与 Actor MoE 相呼应，同样通过门控网络为不同专家分配权重，其核心作用是更精准地估计多任务场景下的状态价值函数，从而为 PPO 算法的策略更新提供可靠的价值基准。由于不同任务的奖励结构差异显著（如四足任务更关注速度跟踪，二足任务更关注站立稳定），传统单 Critic 网络难以准确拟合所有任务的价值函数，而 Critic MoE 则通过专家的专精化特性，让每个专家专注于拟合某一类任务的价值函数，再通过门控权重融合得到最终的价值估计。这种设计不仅提升了价值估计的准确性，还进一步辅助缓解了梯度冲突，因为 Critic 网络的梯度更新同样被限制在对应专家范围内，不会对其他任务的价值拟合产生干扰。

为实现从 “特权信息辅助训练” 到 “纯本体感知盲走” 的过渡，原文设计了两阶段训练流程，每阶段的目标与操作细节均围绕 MoE 架构的适配性展开。

第一阶段为 “Oracle 策略训练阶段”，此时策略可访问完整的状态信息 \([p_t, e_t, i_t, c_t]\)，核心目标是训练门控网络的任务路由精度与专家网络的任务专精性。在该阶段，首先对隐式特权状态 \(i_t\) 进行编码，得到低维 latent 表示 \(\mathrm{Enc}(i_t)\)，并与显式特权状态 \(e_t\)、本体感知 \(p_t\) 拼接形成双状态表示：

\[
l_t = [\mathrm{Enc}(i_t), e_t, p_t]
\]

随后，LSTM 模块对 \(l_t\) 与指令 \(c_t\) 进行时序建模，输出 \(h_t\) 并输入 MoE 模块；同时，原文还在该阶段预训练一个估计器模块（采用 256 维 LSTM 结构），通过 \(L_2\) 重建损失 \(L_{\text{recon}}\) 让估计器仅从本体感知 \(p_t\) 与指令 \(c_t\) 中重构出 \([\mathrm{Enc}(i_t), e_t]\)，为后续盲走阶段的特权状态隐式推断奠定基础。该阶段的整体优化目标为

\[
L_{\text{surro}} + L_{\text{value}} + L_{\text{recon}},
\]

其中 \(L_{\text{surro}}\) 与 \(L_{\text{value}}\) 分别为 PPO 算法的代理损失与价值损失，\(L_{\text{recon}}\) 则确保估计器的重构精度，三者共同作用使 MoE 架构在充分利用特权信息的前提下，学习到多任务的核心运动规律。

第二阶段为 “盲走适配训练阶段”，此时策略仅能访问本体感知 \(p_t\) 与指令 \(c_t\)，模拟真实盲走场景下的信息限制。该阶段的参数初始化策略至关重要。原文直接将第一阶段训练完成的估计器、低层级 LSTM 与 MoE 模块的参数复制到本阶段，仅保留门控网络与估计器的参数可更新，专家网络的参数则被冻结，这种设计的目的是保护已形成的专家专精化特性不被盲走场景的信息缺失破坏。为进一步降低从 “Oracle 策略” 到 “盲走策略” 的性能退化，原文引入了概率退火选择（PAS）机制，通过一个逐渐变化的概率参数 \(P_t = \alpha\)（\(\alpha\) 随训练迭代次数逐渐减小）控制 “Oracle 状态” \(l_t\) 与 “估计状态” \(\hat{l}_t = [\mathrm{Estimator}(p_t, c_t), p_t]\) 的使用比例：

\[
\bar{l}_t = \mathrm{ProbabilitySelection}(P_t, \hat{l}_t, l_t)
\]

在训练初期更多依赖 Oracle 状态保证稳定性，随着训练推进逐渐增加估计状态的占比，最终使策略完全依赖本体感知实现盲走。原文通过实验验证，这种两阶段训练流程能使盲走策略的性能保留 Oracle 策略的 90% 以上，远高于直接训练盲走策略的性能保留率（约 65%）。

IsaacGym 仿真中，四足跨杆与二足上坡度任务中，MoE 策略的梯度余弦相似度为 0.278，而传统单网络策略（Ours w/o MoE）仅为 -0.132（负数值表明梯度方向完全相反，冲突剧烈），证明了 MoE 架构在化解跨步态、跨地形梯度冲突上的有效性。

实机部署验证方面，在 20 次混合地形测试中，MoE 策略的平均成功率达 85%，而传统单网络策略（Ours w/o MoE）的成功率仅为 40%，RMA 策略则完全无法完成混合地形任务（成功率为 0），证明了其作为 “通用多任务盲走策略” 的实用价值。

#### 3.5.2 技能分解与组合

MoE 架构天然具备 “技能分解与组合” 的能力，通过专家网络的自动专精化，策略可将复杂的多任务运动技能拆解为若干个基础子技能，再通过调整门控权重实现基础子技能的重组，从而快速生成新的运动技能或适配新的任务场景，这种特性突破了传统强化学习策略 “黑箱化” 的局限，为足式机器人运动技能的复用与扩展提供了全新范式。

从技能分解的内在机制来看，MoE 架构中的每个专家网络会在训练过程中自发学习并专精于某一类基础运动子技能，这种专精化并非通过人工设计的标签引导，而是由多任务训练中的梯度流向与奖励信号自然塑造。

在专家权重分布分析实验中，原文针对 9 个典型任务（5 个四足任务、4 个二足任务），统计了每个专家在任务执行过程中的平均权重占比，结果显示不同任务对专家的依赖度呈现显著差异：例如，第 1 个专家在 “四足爬挡板” 与 “四足上台阶” 任务中的平均权重占比分别为 41% 与 38%，且在这两个任务中表现出对 “前腿抬高与躯干前倾” 动作的强控制能力，表明其专精于 “障碍攀爬” 子技能；第 2 个专家在 “四足跨坑” 与 “二足下坡度” 任务中的平均权重占比分别为 39% 与 43%，核心作用是 “控制后腿伸展以维持落地稳定性”，对应 “跨越与下坡平衡” 子技能。这种专家与子技能的一一对应关系，证明 MoE 架构能够自动完成复杂运动技能的分解，将多任务需求拆解为可复用的基础子技能模块。

t-SNE 特征可视化实验则从特征空间层面进一步验证了技能分解的有效性。原文对门控网络输出的专家权重向量（即每个任务对应的 6 维权重分布）进行 t-SNE 降维，结果显示所有任务在特征空间中形成了明显的聚类结构，其中二足任务（站立、平地走、上坡度、下台阶）聚为一个大类，四足任务聚为另一个大类，且两个大类在特征空间中距离较远，表明门控网络能够清晰区分双步态的任务差异；在四足任务大类内部，“障碍攀爬” 相关任务（爬挡板、上台阶）、“跨越” 相关任务（跨杆、跨坑）、“基础移动” 相关任务（平地走、坡度走）又分别形成了三个子聚类，每个子聚类对应的专家权重向量与该类任务的核心子技能高度匹配（如 “障碍攀爬” 子聚类中第 1 个专家的权重占比显著高于其他专家）；二足任务大类内部则根据 “静态平衡”（站立）与 “动态行走”（平地走、上坡度、下台阶）分为两个子聚类，其中 “动态行走” 子聚类中第 5 个专家（专精二足平衡）与第 2 个专家（专精下坡平衡）的权重占比协同提升，体现了子技能的组合使用特性。这种清晰的聚类结构表明，门控网络输出的权重向量已成为任务与子技能的 “特征指纹”，进一步证明 MoE 架构的技能分解具备明确的规律性与可解释性。

![](/paper/moe-loco-tsne.png)

另外该工作还发现，通过手动或自动调整专家的权重占比能够实现基础子技能的重组，从而快速生成新的运动技能，无需重新训练完整策略。其形式可写为

\[
g^i = w[i]\cdot \operatorname{softmax}(g(h_t))[i],
\]

其中 \(w[i]\) 为专家 \(i\) 的权重调整系数，可根据新技能的需求手动设定或通过小型神经网络动态生成。例如 “障碍攀爬” 子技能（第 1 个专家，负责前腿抬高）与 “二足平衡” 子技能（第 5 个专家，负责躯干稳定）的组合可实现 “前腿踢球 + 维持平衡” 的动作，因此设置 \(w[1]=2.0\)（加倍第 1 个专家的权重，增强前腿抬高幅度与频率）、\(w[5]=1.5\)（提升第 5 个专家的权重，增强平衡控制），同时将其他专家的权重设为 0，最终零样本生成了带球运球步态。

“二足上 45° 陡坡” 这一未在训练集中出现的新任务，传统方法需重新训练 48 小时完整策略（约 100,000 次迭代）最终新任务成功率为 75%；而 MoE-Loco 仅需调整 “二足平衡” 专家（第 5 个）与 “障碍攀爬” 专家（第 1 个）的权重（设置 \(w[5]=1.8\)、\(w[1]=1.2\)），并进行 4 小时 8,000 次迭代的门控网络微调成功率达 82%，且在训练过程中未出现策略发散现象，充分证明，MoE-Loco 的技能分解与组合机制不仅具备可解释性，还在训练效率、泛化性与任务适配灵活性上显著优于传统方法。

综上，通过 MoE 架构的自动技能分解，复杂运动任务被拆解为可解释、可复用的基础子技能；通过门控权重调整的技能组合，基础子技能可快速重组为新的运动技能，大幅提升了策略的灵活性、可扩展性与开发效率。这种 “分解 - 组合” 的范式，不仅解决了传统强化学习策略的黑箱化与复用性差的问题，还为足式机器人运动控制的模块化发展提供了新的思路。

## 4. 强化学习视觉感知

### 4.1 高程图
在强化学习与视觉感知结合的足式机器人运动控制研究中，ETH Zurich 与 Intel 团队于 2022 年提出的 “基于机器人中心高程图与注意力循环编码器的端到端感知 - 运动框架”，《Learning robust perceptive locomotion for quadrupedal robots in the wild》，是首个实现四足机器人在多季节、多场景野外环境中 “高速 - 鲁棒” 双目标统一的里程碑式成果。

该研究的核心价值在于，打破了此前 “盲走” 策略与传统视觉感知策略之间的性能割裂，前者依赖本体感知（关节编码器、IMU）虽能保证基础鲁棒性，但因缺乏地形预判能力而陷入 “速度天花板” 与 “越障上限低” 的困境，后者虽能利用视觉信息提升运动效率，却因传感器依赖性强、对野外感知噪声鲁棒性差而难以落地。

此前的盲走策略（即本文3.2节的纯本体感知控制器）需通过 “足端物理接触反馈” 调整步态，导致平地运动速度被限制在 0.6 m/s 以下，面对 20 cm 以上台阶时前腿易卡滞、后腿同步抬高失败，成功率骤降；
而传统视觉感知策略（如 Boston Dynamics Spot 机器人）虽能通过深度传感器识别地形，但需人工切换 “楼梯模式” 且仅支持正向行走，更关键的是，当传感器面临雪地高反光、洞穴低光、植被遮挡等野外典型干扰时，深度图易出现 “伪凹陷”“数据缺失” 等问题，此时控制器若无有效退化机制，极易引发步态崩溃。

为解决上述痛点，该研究构建了以 “机器人中心高程图（Robot-Centric Elevation Map）为感知抽象层、注意力循环编码器为多模态融合核心、特权学习为训练范式” 的三层技术架构。
![](/paper/anymal-transformer-pipeline.png)
![](/paper/anymal-transformer-components.png)
#### 4.1.1 机器人中心高程图
作为连接多模态传感器与运动控制器的核心抽象层，机器人中心高程图的设计初衷在于解决传统视觉感知策略中 “传感器依赖性强” 与 “野外感知噪声鲁棒性差” 的双重痛点。该高程图并非直接采用原始点云或单模态深度图，而是以机器人本体为坐标原点，构建 2.5D 地形高度表征，通过统一的几何信息格式实现对不同深度传感器的解耦，同时通过概率化更新机制降低野外环境中感知不确定性的影响。该高程图的构建流程可拆解为 “点云融合 - 概率更新 - 高度采样” 三个关键步骤。

在点云融合阶段，研究团队为 ANYmal-C 机器人配置了两种典型传感器方案，并通过 GPU 加速的点云处理流水线实现多传感器数据的实时整合。与传统 CPU 处理方案相比，GPU 并行计算能够支撑 20Hz 的地图更新频率，这一频率与控制器 50Hz 的运动指令输出频率形成匹配，地图每更新一次，控制器可基于最新地形信息生成 2-3 组关节目标位置，既避免了因感知滞后导致的步态失配，又不会因地图更新过于频繁而占用过多计算资源。
该高程图的更新借鉴了卡尔曼滤波框架，但额外增加了 “漂移补偿” 与 “射线投射” 模块，前者用于修正因足端打滑（如雪地、冰面）导致的机器人位姿估计误差，后者则通过模拟激光束传播路径填补因传感器视野遮挡（如上坡时的地形盲区、植被覆盖）造成的地图空洞，这两项改进使得高程图在地下洞穴、森林等复杂场景中的一致性提升了 30% 以上。

在概率更新阶段，高程图并非简单存储单一高度值，而是为每个网格单元维护一个包含 “均值 - 方差” 的概率分布，用于量化地形高度的不确定性。例如，当立体相机面对雪地高反光表面时，深度测量易出现离散型异常值，此时卡尔曼滤波会通过方差阈值过滤这些异常点，避免地图中出现 “伪凹陷” 或 “伪凸起”；而当 LiDAR 在低光环境（如地下洞穴）中采样密度下降时，系统会通过相邻网格的高度均值进行插值，并提高对应网格的方差，以提示后续控制器 “该区域地形信息可信度较低”。例如，在悬垂树枝场景中，尽管 2.5D 高程图无法完全区分 “悬垂物” 与 “地面障碍”，但概率分布的方差会提示控制器 “该区域地形存在不确定性”，为后续注意力编码器的权重调节提供依据。

在高度采样阶段，为适配运动控制器的输入需求，研究团队设计了 “以足端为中心的环形采样模式”，而非对整个高程图进行全局特征提取。具体而言，针对机器人的四条腿，每条腿周围均设置 5 个不同半径的采样圆环（半径分别为 0.08m、0.16m、0.26m、0.36m、0.48m），每个圆环上的采样点数随半径增大而增加（分别为 6、8、10、12、16 个点），四条腿总计形成 208 维的高度采样特征向量。
这种采样模式的优势在于：一方面，采样点集中在足端运动范围内，能够为步态规划提供最相关的地形信息（如即将接触的地面高度、台阶边缘位置），避免全局采样带来的冗余计算；另一方面，多半径采样能够兼顾 “近场精度” 与 “远场预判”—— 小半径（0.08-0.16m）采样点用于判断当前足端下方的地面稳定性，大半径（0.36-0.48m）采样点则用于提前感知 1-2 步后的地形变化（如前方台阶、斜坡）。当机器人面对 30.5cm 高的台阶时，大半径采样点能够提前 0.5-0.8 秒感知到台阶边缘，使得控制器有充足时间调整躯体前倾角度（前倾 15°-20°）与腿抬高高度（较平地提升 50%），从而避免盲走策略中 “前腿卡滞” 的问题，实现 100% 的越障成功率。

更关键的是，该机器人中心高程图的设计彻底打破了传统视觉感知策略的 “传感器绑定” 困境。无论是使用 LiDAR 还是立体相机，只需将传感器输出的点云数据接入相同的高程图构建流水线，无需对后续控制器进行任何参数微调或模型重训，机器人即可保持一致的运动性能。LiDAR 的优势在于远距离采样精度高（10m 范围内误差 < 2cm），适合开阔的高山场景；立体相机则在近距离纹理丰富区域表现更优（如城市楼梯的砖石纹理），适合复杂结构化环境，而高程图通过统一的 “高度 - 概率” 表征，将两种传感器的优势融合，同时规避了 LiDAR 在高反光场景中的噪声问题与立体相机在低纹理场景中的匹配失效问题。

#### 4.1.2 注意力循环编码器
若说机器人中心高程图解决了 “感知信息如何可靠输入” 的问题，那么注意力循环编码器则回答了 “本体感知与外感知如何动态融合” 的核心命题。该编码器是连接感知层与控制层的关键，其设计突破了传统多模态融合中 “固定权重” 或 “启发式规则” 的局限，通过端到端学习实现 “外感知可靠时主动利用、外感知失效时无缝退化” 的自适应融合机制，这也是该研究能够在野外环境中兼顾 “高速” 与 “鲁棒” 的核心技术壁垒。从原文的模型架构与训练细节来看，注意力循环编码器可从 “网络结构设计”“多模态融合逻辑”“训练范式适配” 三个维度展开。

在网络结构设计上，该编码器以门控循环单元（GRU）为核心，叠加 “注意力门控模块” 与 “信念状态解码器”，形成 “编码 - 调节 - 解码” 的闭环结构，其输入包括本体感知数据（o_t^p）、带噪声的高程图高度采样（n (o_t^e)）以及上一时刻的隐藏状态（h_t），输出则是整合了多模态信息的信念状态（b_t）。
相较于传统循环神经网络（RNN）易出现的梯度消失问题，以及长短期记忆网络（LSTM）复杂的门控结构带来的计算负担，GRU 通过 “更新门” 与 “重置门” 的简化设计，在保证时序信息建模能力的同时，将单次前向传播时间缩短了 25%。同时，研究团队采用 “双堆叠 GRU 层”（每层 50 个隐藏单元），而非单一层级，目的是增强对长时序依赖的建模能力。例如，当机器人踩上柔软泡沫块时，第一次接触时足端反馈的 “地面塌陷” 信息需要与前 0.5 秒内高程图采样的 “地面坚硬” 信息形成对比，双堆叠 GRU 能够更好地捕捉这种 “感知冲突”，并快速修正内部信念。

注意力门控模块是该编码器的创新核心，其作用是动态生成注意力向量 α（维度与外感知特征向量 l_t^e 一致，均为 96 维），通过 Hadamard 乘积（⊙）调节外感知信息进入信念状态的权重。
首先，本体感知数据 o_t^p、外感知特征 l_t^e（由高程图高度采样经 MLP 编码器 g_e 压缩得到）与隐藏状态 h_t 共同输入 GRU，生成中间信念状态 b_t' 与新的隐藏状态 h_{t+1}；随后，中间信念状态 b_t' 通过全连接网络 g_a（两层隐藏层，各 64 个单元，激活函数为 sigmoid）计算得到注意力向量 α，α 的每个元素取值范围为 [0,1]，代表对应外感知特征的 “可信度”；最终，信念状态 b_t 由两部分组成：g_b (b_t')（中间信念状态经全连接网络 g_b 处理后的结果，代表本体感知主导的基础信念）与 l_t^e ⊙ α（注意力加权后的外感知特征，代表外感知补充的增量信息）。这种设计的关键在于，α 并非人工预设，而是通过端到端学习从数据中自动习得。
当外感知信息可靠时（如晴天平整路面，高程图高度采样与真实地形误差 < 3cm），α 的多数元素趋近于 1，外感知特征被充分融入信念状态，控制器可利用地形预判提前调整步态（如看到前方 20cm 台阶时，提前 0.3 秒抬高腿部至 25cm 高度）；当外感知信息不可靠时（如传感器被遮挡，高程图采样为随机噪声），α 的多数元素趋近于 0，外感知特征的影响被最小化，信念状态主要由本体感知数据主导，控制器退化为类似 “盲走” 的鲁棒模式，但区别于纯盲走策略，此时编码器仍会利用历史时序信息（如前一步的足端力反馈）修正当前信念，避免 “硬着陆” 或 “步态紊乱”。

从训练范式适配的角度来看，注意力循环编码器的学习过程同样为特权学习，通过 “教师 - 学生” 双阶段训练确保其在真实环境中的泛化能力。在教师政策训练阶段（第一阶段），研究团队利用仿真环境（RaiSim）提供的 “特权信息”（如无噪声的真实地形高度、地面摩擦系数、外部扰动大小），通过 PPO 算法训练出一个 “最优参考控制器”，能够基于完美信息做出理想决策（如面对 30cm 台阶时，精确计算躯体前倾角度与腿部抬高高度的匹配关系），其输出的动作（教师动作 a_t^teacher）与特征向量（外感知 latent l_t^e + 特权信息 latent l_t^priv）成为学生政策的学习目标。
在学生政策训练阶段（第二阶段），学生政策（即注意力循环编码器所在的模型）仅能获取与真实机器人一致的 “带噪声信息”（本体感知 o_t^p + 经噪声模型扰动的高度采样 n (o_t^e)），其训练目标通过两个损失函数实现：一是行为克隆损失（L_bc），即最小化学生动作（a_t^student）与教师动作（a_t^teacher）的平方距离，确保学生政策的动作输出接近最优；二是重构损失（L_re），即通过信念状态解码器（由外感知解码器与特权信息解码器组成），从信念状态 b_t 中重构出无噪声的高度采样（o_t^e）与特权信息（s_t^p），最小化重构结果与真实值的平方距离，这一损失函数强制信念状态必须包含足够的环境信息，避免学生政策 “机械模仿” 教师动作而缺乏对环境变化的适应能力。

该编码器的落地价值还体现在其对 “控制实时性” 的兼顾上，编码器的前向传播与控制器的动作生成被整合到同一计算流水线中，整体延迟控制在 20ms 以内（满足 50Hz 的控制频率要求），这得益于两方面优化：一是网络参数的轻量化设计，GRU 的隐藏单元数量（50 个 / 层）与全连接网络的隐藏层规模（64 个单元 / 层）均经过多次迭代验证，在保证性能的同时最小化计算量；二是与高程图采样的协同优化，高度采样向量（208 维）经外感知编码器（g_e）压缩为 96 维 latent 特征后再输入 GRU，避免高维输入带来的计算负担。


综上，注意力循环编码器通过 “GRU 时序建模 + 注意力动态调节 + 特权学习适配” 的三重设计，解决了传统多模态融合中 “融合权重固定”“鲁棒性差”“泛化能力弱” 的痛点，其与机器人中心高程图的协同作用，共同构建了 “感知可靠输入 - 动态融合 - 鲁棒控制输出” 的完整链路，为四足机器人从 “实验室演示” 走向 “野外实用” 提供了核心技术支撑。

### 4.2 深度图

在 4.1 节所述的 “高程映射到落足点规划” 范式中，其核心瓶颈始终难以适配小型化、低成本足式机器人的复杂地形需求。

这类方法不仅依赖多传感器融合（如 ANYmalC 配备 4 个深度相机与 2 个激光雷达，Spot 搭载 5 个环绕式深度相机）推高硬件成本，更因多帧深度图融合需通过视觉 / 惯性里程计估计相机相对位姿，导致位姿漂移引入的噪声直接影响高程图精度，最终在间隙、垫脚石等需精确落足的场景中频繁失效。

更关键的是，这种 “建图到规划再到控制” 的分解流程与生物运动机制相悖。因为人类通过第一视角视觉与运动控制的直接耦合实现高效移动，无需复杂的中间建图步骤。

为此，本文提出首个适用于四足机器人的端到端视觉运动控制系统，仅以 Unitree A1 机器人头部的单前置深度相机与本体感知数据（关节角度、关节速度、IMU 测量的滚转 / 俯仰 / 偏航角速度及姿态）为输入，直接输出 50Hz 频率的目标关节角度，彻底跳过高程图构建与落足点规划环节，同时通过两阶段训练策略解决深度图仿真渲染效率低的核心难题，实现 “仿真高效训练到实机无微调迁移” 的闭环。

![](/paper/vision-locomotion-pipeline.png)

#### 4.2.1 基于扫描点的强化训练

Phase 1 的核心目标是规避深度图渲染效率瓶颈，通过低成本地形表征扫描点（Scandots）训练具备复杂地形适应能力的 “教师策略 \(\pi_1\)”。原文将扫描点定义为 “机器人参考系（robot’s frame of reference）下一组固定 \((x,y)\) 坐标点”，每个控制时间步会查询这些坐标处的地形高度并作为观测值，其计算成本仅为深度图渲染的 1/10，可支撑强化学习对海量样本的需求。

此阶段的输入观测向量 \(o_t\) 由三部分构成：扫描点地形高度数据 \(m_t\)、本体感知 \(x_t\) 与期望运动指令 \(u_t^{\text{cmd}} = (v_x^{\text{cmd}}, \omega_z^{\text{cmd}})\)。

其中，本体感知 \(x_t\) 涵盖机载传感器实测的 12 个关节角度（左髋 + 0.1 偏移、右髋 - 0.1 偏移、前大腿 + 0.8 偏移等）、关节速度、机器人角速度（滚转 / 俯仰 / 偏航）、姿态（roll/pitch）及上一时刻动作 \(a_{t-1}\)，全面反映机器人动态状态；

期望运动指令则根据场景动态调整，如复杂地形中 \(v_x^{\text{cmd}}\) 固定为 0.35m/s、航向角在 \([-10^\circ, 10^\circ]\) 波动，平地则随机切换曲线跟随（\(v_x^{\text{cmd}} \in [0.2, 0.75]\) m/s）、原地转弯（\(v_x^{\text{cmd}} = 0\)）与完全停止模式，模拟真实运行中的指令分布。

| 观测项 | 缩放系数 \(a\) | 偏置项 \(b\) | 噪声标准差 \(\sigma\) |
| :--- | :--- | :--- | :--- |
| 左髋关节角度 | 1.0 | 0.1 | 0.01 |
| 右髋关节角度 | 1.0 | -0.1 | 0.01 |
| 前大腿关节角度 | 1.0 | 0.8 | 0.01 |
| 后大腿关节角度 | 1.0 | 1.0 | 0.01 |
| 小腿关节角度 | 1.0 | -1.5 | 0.01 |
| 线速度 | 0.05 | 0.0 | 0.05 |
| 角速度 | 0.25 | 0.0 | 0.05 |
| 姿态 (Orientation) | 1.0 | 0.0 | 0.02 |
| Scandots 高度 | 5.0 | 0.0 | 0.07 |
| Scandots 水平位置 | – | – | 0.01 |

为平衡 “表征解耦性” 与 “架构简洁性”，Phase 1 采用两种并行策略架构：

其一为 RMA（Rapid Motor Adaptation）架构，该架构通过模块化设计实现地形几何与环境参数的分离表征。首先，利用单隐藏层 GRU 对扫描点序列 \(m_t\) 进行时序建模，输出地形几何隐向量

\[
\gamma_t = \mathrm{GRU}_t(m_t)
\]

该向量可编码楼梯高度、间隙宽度等关键地形特征；其次，通过含 2 个隐藏层的 MLP 处理 “特权信息” \(e_t\)（仿真中可直接获取的机器人质心位置、地面摩擦系数 \(\in [0.3, 1.25]\)、电机强度 \(\in [90\%, 110\%]\)），生成环境参数隐向量

\[
z_t = \mathrm{MLP}(e_t)
\]

最终，将 \(x_t\)、\(\gamma_t\)、\(z_t\) 与 \(u_t^{\text{cmd}}\) 共同输入基础 MLP，输出目标关节角度

\[
a_t = \mathrm{MLP}(x_t, \gamma_t, z_t, u_t^{\text{cmd}})
\]

这种解耦设计的核心优势在于，后续 Phase 2 蒸馏时仅需替换 \(\gamma_t\) 的估计来源（从扫描点改为深度图），即可复用 Phase 1 训练完成的基础 MLP，大幅降低迁移成本。

其二为 Monolithic 架构，该架构以简洁性为目标：先通过 MLP 将扫描点 \(m_t\) 压缩为地形特征向量

\[
\gamma_t = \mathrm{MLP}(m_t)
\]

再将 \(\gamma_t\)、\(x_t\) 与 \(u_t^{\text{cmd}}\) 输入 stateful GRU（标注 “t” 表示保留时序状态），直接输出

\[
a_t = \mathrm{GRU}_t(x_t, \gamma_t, u_t^{\text{cmd}})
\]

虽无需处理特权信息 \(e_t\)，但地形几何与环境参数的表征隐含于 GRU 权重中，后续传感器替换需重新训练全网络，因此仅作为 RMA 架构的对比基准。

Phase 1 采用 PPO 算法优化，结合截断 24 步的时间反向传播（BPTT）平衡长序列依赖建模与训练稳定性，在 IsaacGym 仿真平台的单 NVIDIA RTX 3090 显卡上同步运行 4096 个机器人实例，以 2e5 时间步 / 秒的吞吐量完成 150 亿样本训练。

为引导策略自主涌现适配小型机器人的步态，奖励函数设计摒弃所有预编程步态约束，采用 “能量效率 + 硬件保护 + 运动跟踪” 的多目标优化：

### 奖励/惩罚项定义表

| 惩罚/奖励项名称 | 数学表达式 | 作用说明 |
|----------------|------------|----------|
| 绝对功惩罚 | \(-|\boldsymbol{\tau} \cdot \dot{\boldsymbol{q}}|\) <br> 其中 \(\boldsymbol{\tau}\) 为关节力矩 | 采用绝对值计算，避免策略利用接触仿真的误差获取正向奖励 |
| 指令跟踪项 | \(v_{\text{cmd},x} - |v_{\text{cmd},x} - v_x| - |\omega_{\text{cmd},z} - \omega_z|\) <br> 其中 \(v_x\) 为机器人前进方向线速度，\(\omega_z\) 为偏航角速度 <br>（\(x\)、\(z\) 为固连于机器人的坐标系坐标轴） | 激励机器人跟踪期望的前进线速度与偏航角速度指令 |
| 足部加速度突变惩罚 | \(\sum_{i\in F}\|f_{t}^{i} - f_{t-1}^{i}\|\) <br> 其中 \(f_{t}^{i}\) 为 \(t\) 时刻第 \(i\) 个刚体足部的受力，\(F\) 为足部刚体索引集合 | 抑制足部受力的突变，防止电机产生较大间隙冲击 |
| 足部拖曳惩罚 | \(\sum_{i\in F} \mathbb{I}[f_{z}^{i} \ge 1\ \text{N}] \cdot (|v_{x}^{i}| + |v_{y}^{i}|)\) <br> 其中 \(\mathbb{I}[\cdot]\) 为指示函数（条件满足时取1，否则取0），\(v_{x}^{i}\)、\(v_{y}^{i}\) 为第 \(i\) 个足部刚体的水平方向速度 | 当足部与地面接触（法向力≥1N）时，惩罚其水平方向的速度，避免足部拖曳地面造成损伤 |
| 碰撞惩罚 | \(\sum_{i\in C\cup T} \mathbb{I}[f^{i} \ge 0.1\ \text{N}]\) <br> 其中 \(C\) 为小腿刚体索引集合，\(T\) 为大腿刚体索引集合 | 惩罚大腿、小腿部位的接触受力（≥0.1N），避免其在楼梯或离散障碍物边缘发生剐蹭 |
| 生存奖励 | 每个时间步给予恒定值 \(1\) | 在复杂场景下，优先保障机器人的生存，而非单纯执行指令 |

这种设计使髋高仅 28cm 的 A1 机器人自主涌现 “大髋外展步态”，成功攀爬 25cm 高楼梯（相对高度 89%），远超依赖预编程步态的现有方法。

#### 4.2.2 基于深度图的监督蒸馏

Phase 2 的核心是将 \(\pi_1\) 的地形适应能力迁移至实机可获取的 “深度图 + 本体感知” 输入，通过监督学习训练 “学生策略 \(\pi_2\)”，解决深度图直接用于 RL 训练效率低的问题。由于监督学习的样本效率比 RL 高多个数量级，整个阶段仅需在单 GPU 上以 500 时间步 / 秒的吞吐量（256 个并行环境）完成 600 万样本训练（耗时 6 小时，原文 C.2 节），即可实现实机部署。

深度图预处理环节需适配实机硬件限制，首先裁剪 D435 输出图像左侧 200 个无效像素（被机器人头部遮挡），采用最近邻插值填补深度空洞，下采样至 58×87 分辨率以降低计算量；随后通过含 3 个卷积层（输出通道 16 到 32 再到 64，3×3 卷积核 + 2×2 最大池化）的 ConvNet 压缩为特征向量 \(\tilde{d}_t\)，并通过 UDP socket 从 Jetson NX（运行 ConvNet，0.8 TFLOPS 算力）传输至 UPboard（运行基础策略，Intel Atom X5-8350 处理器）。

\(\pi_2\) 的架构设计针对性解决了 “前向相机无法直接观测后脚地形” 的难题，利用 RNN 的时序建模能力，结合深度图历史序列与本体感知推断后脚下方地形（如 \(t-3\) 时刻观测的前方地形，在 \(t\) 时刻随机器人移动至后脚位置）。

对于 Monolithic 架构，\(\pi_2\) 复用 Phase 1 的 GRU 结构，仅将输入中的 \(m_t\) 替换为 \(\tilde{d}_t\)，通过 DAgger 算法在仿真中展开 24 步，以 \(\pi_1\) 输出的 \(a_t\) 为标签，最小化预测动作 \(\hat{a}_t\) 与 \(a_t\) 的均方误差（\(\lVert \hat{a}_t - a_t \rVert^2\)）。

对于 RMA 架构，\(\pi_2\) 无需重新训练基础 MLP（参数冻结），仅训练 \(\gamma_t\) 与 \(z_t\) 的估计器：地形几何隐向量 \(\hat{\gamma}_t\) 由 \(x_t\)、\(u_t^{\text{cmd}}\) 与 \(\tilde{d}_t\) 输入 GRU 生成（最小化 \(\lVert \hat{\gamma}_t - \gamma_t \rVert^2\)），环境参数隐向量 \(\hat{z}_t\) 由 \(x_t\) 与 \(u_t^{\text{cmd}}\) 输入另一个 GRU 生成（最小化 \(\lVert \hat{z}_t - z_t \rVert^2\)），最终通过基础 MLP 输出 \(a_t = \mathrm{MLP}(x_t, u_t^{\text{cmd}}, \hat{\gamma}_t, \hat{z}_t)\)，这种设计使参数更新量减少 60%，进一步提升训练效率。

为保障 \(\pi_2\) 的性能下限，原文从马尔可夫决策过程框架提供理论证明：设 \(M = (S, A, P, R, \gamma)\) 为标准 MDP，若 Phase 1 的 \(\pi_1\) 价值函数 \(V^1\) 与最优价值函数 \(V\) 的误差 \(< \epsilon\)（\(\forall s \in S\)，\(|V(s) - V^1(s)| < \epsilon\)），\(\pi_2\) 与 \(\pi_1\) 的动作误差 \(< \eta\)（\(\forall s\)，\(|\pi^1(s) - \pi^2(f(s))| < \eta\)，\(f\) 为状态空间映射），且奖励函数 \(R\)、转移函数 \(P\) 满足 Lipschitz 连续，则 \(\pi_2\) 的价值函数 \(V^{\pi_2}\) 与 \(V\) 的差距被边界 \(\frac{2\epsilon\gamma + \eta c}{1-\gamma}\) 限制（\(c\) 为与 \(V\) 相关的有界常数）。这一证明从数学上消除了 “动作模仿导致性能退化” 的担忧，为实机迁移的可靠性提供保障。

实机测试结果进一步验证 \(\pi_2\) 的鲁棒性：在 17cm 高楼梯场景中，\(\pi_2\) 实现 100% 成功登顶（平均完成 13 级台阶），而仅依赖本体感知的 Blind 基线平均仅能走 2.2 级；26cm 宽间隙场景中，\(\pi_2\) 成功率 100%，Blind 基线则完全失效；30cm 宽、15cm 间距的垫脚石场景中，\(\pi_2\) 成功率 94%（平均踩中 9.4 块石头），且在户外岩石地、沙滩等非结构化地形中，能应对滑倒、足部卡缝等扰动。

值得注意的是，Blind 基线虽能 100% 完成下楼任务，但通过 “每步高冲击跌倒 - 再稳定” 的步态实现，导致实机测试中 A1 的右后髋关节脱位，而 \(\pi_2\) 因学习到平滑的落足策略，无任何硬件损伤。

该研究的核心创新在于将 “生物运动观察” 深度融入技术设计：借鉴人类 “通过短期记忆利用前方视觉指导落足” 的机制，以 RNN 实现地形历史信息的存储与推断；摒弃预编程步态，通过强化学习让策略自主涌现适配小型机器人的运动模式。这种 “视觉 - 运动紧密耦合” 的端到端范式，不仅以单相机、低成本硬件实现了超越多传感器方案的复杂地形适应能力，更通过两阶段训练与理论证明，为足式机器人从 “仿真训练” 到 “实机实用” 的跨越提供了可复现的技术路径，\(\pi_2\) 在斜坡、楼梯、离散障碍等地形的平均位移与跌倒时间均超越基线 60%-90%，实机部署时无需任何微调即可稳定运行，为四足机器人在室内巡检、野外勘探等场景的实用化奠定了关键基础。