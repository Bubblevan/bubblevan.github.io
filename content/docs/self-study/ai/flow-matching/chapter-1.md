# Generative AI with SDEs

## Generative Modeling As Sampling

1. **图像**：考虑具有\(H \times W\)像素的图像，其中H表示图像的高度，W表示图像的宽度，每个图像都有三个颜色通道（RGB）。对于每个像素和每个颜色通道，我们得到一个\([0,1]\)中的强度值。因此，图像可以用元素\(z \in \mathbb{R}^{H \times W \times 3}\)来表示。
2. **视频**：视频本质上是一系列随时间变化的图像。如果我们有T个时间点或帧，那么视频就会因此可以用元素\(z \in \mathbb{R}^{T \times H \times W \times 3}\)来表示。
3. **分子结构**：一种简单的方法是用矩阵\(z=(z^{1}, ..., z^{N}) \in \mathbb{R}^{3 \times N}\)来表示分子结构，其中N是分子中的原子数量，每个\(z^{i} \in \mathbb{R}^{3}\)描述该原子的位置。当然，还有其他更复杂的方法来表示这类分子。

**Key Idea 1 (Objects as Vectors)**

我们将生成的对象标识为向量\(z \in \mathbb{R}^{d}\)

> 上述情况的一个显著例外是**文本数据**，它通常通过自回归语言模型（如ChatGPT）被建模为**离散对象**。虽然已经开发出了用于离散数据的流模型和扩散模型，但本课程仅专注于其在**连续数据**中的应用。

让我们来定义"**生成**"某物意味着什么。例如，假设我们想要生成一张狗的图像，然而并不存在唯一"最佳"的狗的图像。相反，存在一系列拟合程度有好有坏的图像。

在机器学习中，通常将这种可能图像的多样性视为一种**概率分布**。我们称之为**数据分布**，并将其表示为\(p_{data }\)。因此，在狗的图像示例中，这种分布会给看起来更像狗的图像赋予更高的可能性。

因此，一个图像/视频/分子的拟合程度"有多好"——这是一个相当主观的说法——被其在**数据分布**\(p_{data }\)下的"可能性有多大"所取代。如此一来，我们就可以用数学方式将生成任务表示为从（未知的）分布\(p_{data }\)中进行**采样**：

**Key Idea 2 (Generation as Sampling)**

生成对象z被建模为从**数据分布**\(z \sim p_{data }\)中采样。

**Key Idea 3 (Dataset)**

数据集由有限数量的样本\(z_{1}, ..., z_{N} \sim p_{data }\)组成。

在许多情况下，我们希望根据某些数据y生成一个对象。例如，我们可能希望根据\(y=" a\)"一只狗在积雪覆盖的山上奔跑，背景是山脉"生成一幅图像。我们可以将其重新表述为从**条件分布**中采样：

**Key Idea 4 (Conditional Generation)**

**条件生成**涉及从\(z \sim p_{data }(\cdot | y)\)中采样，其中y是一个**条件变量**。

我们将\(p_{data }(\cdot | y)\)称为**条件数据分布**。条件生成建模任务通常包括学习以任意而非固定的y选择作为条件。以我们之前的例子来说，我们希望以不同的文本提示为条件，例如\(y=" a\)"一只猫吹生日蜡烛的逼真图像"。因此，我们寻求一个单一模型，该模型可以以任何此类y的选择为条件。事实证明，**无条件生成技术**很容易推广到有条件的情况。因此，在前3节中，我们将几乎完全专注于**无条件情况**（同时记住，我们正在朝着有条件生成的方向努力）。

到目前为止，我们已经讨论了**生成建模**的内容：从\(p_{data }\)生成样本。在这里，我们将简要讨论其方法。为此，我们假设可以获取一些易于从中采样的**初始分布**\(p_{init }\)，例如**高斯分布**\(p_{init }=N(0, I_{d})\)。生成建模的目标是将来自\(x \sim p_{init }\)的样本转换为来自\(p_{data }\)的样本。我们注意到，\(p_{init }\)不必像高斯分布那样简单。正如我们将看到的，利用这种灵活性有一些有趣的用例。尽管如此，在大多数应用中，我们将其视为简单的高斯分布，**记住这一点很重要**。

## Flow and Diffusion Models

### Flow Models

#### ODE 三要素

**流模型**的本质是通过**常微分方程（ODE）**，将简单初始分布（如高斯噪声\(p_{init}\)）的样本，逐步"推"到目标数据分布（\(p_{data}\)），整个过程是**确定性的**（给定初始点，轨迹唯一）

- **轨迹（Trajectory）**：\(X: [0,1] \to \mathbb{R}^d\)（记为\(X_t\)）可以理解为"空间中的运动路径"—— 时间t从 0 到 1，\(X_t\)表示"时刻t在\(\mathbb{R}^d\)空间中的位置"（比如\(d=2\)时就是平面上的点）。
- **向量场（Vector Field）**：\(u: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d\)（记为\(u_t(x)\)）是 ODE 的"指挥棒"—— 对每个时刻t、每个位置x，给出一个"速度向量"，规定"此刻在此位置该往哪个方向、以多大速度移动"（图 1 中蓝色箭头就是 2 维空间的向量场）。
- **ODE 方程与初始条件**：\(\frac{d}{dt}X_t = u_t(X_t),\quad X_0 = x_0\)

**核心规则**：轨迹\(X_t\)的瞬时变化率（导数）= 向量场在当前位置\(X_t\)给出的速度；且必须从初始点\(x_0\)（比如噪声样本）出发。

![](/img/flow-matching/f1.png)

**图1**：流\(\psi_{t}: \mathbb{R}^{d} \to \mathbb{R}^{d}\)（红色方格）由速度场\(u_{t}: \mathbb{R}^{d} \to \mathbb{R}^{d}\)（用蓝色箭头可视化）定义，该速度场规定了其在所有位置（此处为\(d=2\)）的瞬时运动。我们展示了三个不同的时间t。可以看出，流是一种"扭曲"空间的**微分同胚**。

#### 流：ODE 的解

**流（Flow）**：ODE 的解定义：\(\psi: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d\)（记为\(\psi_t(x_0)\)）是"初始点→时刻t位置"的映射 —— 输入"初始点\(x_0\)和时间t"，输出"从\(x_0\)出发，沿向量场运动t时间后的位置"。满足：\(\psi_0(x_0)=x_0\)（时刻 0 还在初始点），且\(\frac{d}{dt}\psi_t(x_0)=u_t(\psi_t(x_0))\)（符合向量场指挥）。

**Theorem 3 (Flow existence and uniqueness)**

如果\(u: \mathbb{R}^{d} \times[0,1] \to \mathbb{R}^{d}\)是**连续可微的**且具有**有界导数**，那么式（2）中的常微分方程（ODE）有一个由流\(\psi_{t}\)给出的**唯一解**。在这种情况下，对于所有t，\(\psi_{t}\)都是一个**微分同胚**，即\(\psi_{t}\)是连续可微的，具有连续可微的逆映射\(\psi_{t}^{-1}\)。

让我们考虑一个向量场\(u_{t}(x)\)的简单例子，它是x的**简单线性函数**。对于\(\theta>0\)，给定向量场（速度场）\(u_{t}(x)=-\theta x\)。这是一个**线性向量场**，其作用是让所有点都朝着原点方向移动（方向与位置相反）。

我们提出一个候选的流函数：

\[\psi_{t}(x_{0})=e^{-\theta t} x_{0} \quad (3)\]

这表示从初始点\(x_0\)出发，经过时间\(t\)后的位置。要验证它确实是ODE的解，需要验证两点：

**1. 验证初始条件**：\(\psi_{0}(x_{0})=e^{0} x_{0}=x_{0}\) ✓（满足初始条件）

**2. 验证满足ODE方程**：需要证明\(\frac{d}{dt}\psi_{t}(x_{0}) = u_{t}(\psi_{t}(x_{0}))\)

计算左边（对时间求导，由于\(x_0\)是常数，使用链式法则）：

\[\frac{d}{d t} \psi_{t}\left(x_{0}\right) = \frac{d}{d t}\left(e^{-\theta t} x_{0}\right) = x_{0} \cdot \frac{d}{d t}(e^{-\theta t}) = x_{0} \cdot (-\theta) e^{-\theta t} = -\theta e^{-\theta t} x_{0}\]

计算右边（将\(\psi_t(x_0)\)代入向量场）：

\[u_{t}\left(\psi_{t}\left(x_{0}\right)\right) = u_{t}(e^{-\theta t} x_{0}) = -\theta \cdot (e^{-\theta t} x_{0}) = -\theta e^{-\theta t} x_{0}\]

左边等于右边，因此\(\psi_t(x_0) = e^{-\theta t} x_0\)确实是该ODE的解。

**直观效果**：所有初始点都会**指数级收敛到原点**。衰减速度由\(\theta\)控制（\(\theta\)越大，衰减越快）。例如，当\(\theta=0.5\)，\(t=10\)时，位置约为初始点的\(e^{-5} \approx 0.0067\)，即约为初始位置的0.67%。

一般来说，如果\(u_{t}\)不像线性函数那样简单，就无法**显式计算**流\(\psi_{t}\)。大多数向量场（如**神经网络参数化的**）没有显式解，在这些情况下，人们会使用**数值方法**来模拟常微分方程。

#### ODE的数值模拟

其中最简单且最直观的方法之一是**欧拉方法**。在欧拉方法中，我们用\(X_{0}=x_{0}\)进行初始化，并通过

\[X_{t+h}=X_{t}+hu_{t}(X_{t}) \quad (t=0,h,2h,3h,...,1-h) \quad (4)\]

进行更新，其中\(h=n^{-1}>0\)是一个**步长超参数**，且满足\(n \in \mathbb{N}\)。对于本课程而言，欧拉方法已经足够好用了。

**欧拉法（Euler method）**：最基础，一步一步"走直线"，步长\(h=1/n\)（n为总步数），更新规则：\(X_{t+h} = X_t + h \cdot u_t(X_t)\)。通俗说：当前位置 + "步长 × 当前速度" = 下一步位置（比如\(h=0.1\)，每步走 0.1 个单位时间的距离）。

> 为了领略一种更精确的方法，我们来考虑**休恩方法（Heun's method）**。欧拉法只使用当前位置的速度来预测下一步，而休恩法通过**预测-校正**两步过程来提高精度。
>
> **休恩法的两步过程**：
>
> **步骤1：预测（初始猜测）**
> \[X'_{t+h} = X_t + hu_t(X_t) \quad (5a)\]
> 
> 使用欧拉法的公式，基于当前位置\(X_t\)的速度\(u_t(X_t)\)，先做一个初步预测，得到猜测位置\(X'_{t+h}\)。
> 
> **步骤2：校正（用平均速度更新）**
> \[X_{t+h}=X_{t}+\frac{h}{2}\left(u_{t}\left(X_{t}\right)+u_{t+h}\left(X_{t+h}'\right)\right) \quad (5b)\]
> 
> 使用当前速度\(u_t(X_t)\)和**猜测位置处的速度**\(u_{t+h}(X'_{t+h})\)的平均值来更新位置。这样可以修正初始猜测的误差。
> 
> **为什么更精确？**
> 
> 欧拉法的问题在于，它假设在整个步长\(h\)内，速度始终保持为\(u_t(X_t)\)不变。但实际上，当我们从\(X_t\)移动到\(X_{t+h}\)时，速> 度会变化（因为向量场依赖于位置）。休恩法通过考虑终点位置的速度，使用**平均速度**来估计，更接近真实的积分过程。
> 
> **直观理解**：
> - 欧拉法：像"盲走"，只看当前方向就一直走下去
> - 休恩法：像"先看一步再调整"，先预测会走到哪里，然后看那里的方向，最后用两个方向的平均来走，这样更准确
> 
> 这类似于数值积分中的**梯形法则**，使用起点和终点的平均值比只用起点值更精确。

#### 流模型的最终构建

**核心思想**：用**神经网络参数化向量场**（记为\(u_t^\theta(x)\)，\(\theta\)是网络参数），让 ODE 的终点\(X_1 = \psi_1^\theta(X_0)\)服从目标分布\(p_{data}\)，从而实现从初始分布到目标分布的转化。

流模型由常微分方程描述：

\[X_0 \sim p_{init} \quad \text{（随机初始化）}\]

\[\frac{d}{d t} X_{t}=u_{t}^{\theta}\left(X_{t}\right) \quad \text{（ODE）}\]

其中向量场\(u_{t}^{\theta}\)是一个带有参数\(\theta\)的**神经网络**，它决定了轨迹在每个位置的瞬时速度（即轨迹的导数）。目前，我们将\(u_{t}^{\theta}\)称为**通用神经网络**，即一个带有参数\(\theta\)的连续函数\(u_{t}^{\theta}: \mathbb{R}^{d} \times[0,1] \to \mathbb{R}^{d}\)。

**训练目标**：训练神经网络，使得从\(X_0\)出发、沿着向量场走到\(X_1\)时，\(X_1\)的分布接近\(p_{data}\)。用数学语言表达，即：

\[X_{1} \sim p_{data } \Leftrightarrow \psi_{1}^{\theta}\left(X_{0}\right) \sim p_{data }\]

其中\(\psi_{t}^{\theta}\)描述了由\(u_{t}^{\theta}\)引起的流。

**但请注意**：尽管它被称为流模型，但**神经网络参数化的是向量场，而不是流**。具体来说：
- 神经网络学习的是"速度场"\(u_t^\theta\)（在每个位置的移动方向和速度）
- 流\(\psi_t\)是从起点到终点的完整路径，需要通过数值方法求解 ODE 才能得到
- 不能直接用神经网络输出流，而是先学习向量场，再通过 ODE 数值积分得到流

在算法1中，我们总结了如何从流模型中采样的过程。

**Algorithm 1 Sampling from a Flow Model with Euler method**

**Require**: Neural network vector field \(u_t^\theta\), number of steps \(n\)

```
1: Set t = 0
2: Set step size h = 1/n
3: Draw a sample X₀ ~ p_init
4: for i = 1, ..., n-1 do
5:     X_{t+h} = X_t + h·u_t^θ(X_t)
6:     Update t ← t + h
7: end for
8: return X₁
```

### Diffusion Models

#### SDE 与布朗运动
**ODE** 的轨迹是**确定性的**（给定初始点，轨迹唯一，比如"从 A 到 B 的直线路径"）；而 **SDE** 通过加入由**布朗运动**驱动的随机项，让轨迹变成**随机的**（同一初始点，每次模拟路径都不同，比如"从 A 到 B 但偶尔随机绕小弯"）—— 这正是**扩散模型**"生成多样性样本"的关键

\(X_{t}\) 是随机变量，对于每个 \(0 \leq t \leq 1\)，\(X:[0,1] \to \mathbb{R}^{d}\)，\(t \mapsto X_{t}\) 是随机轨迹，对于X的每次抽取。

**布朗运动**（也称**维纳过程**\(W_t\)）是 **SDE** 的核心，本质是"**连续的随机游走**"，定义满足 3 个条件：

- **初始条件**：\(W_0=0\)（起点固定在原点）
- **轨迹连续**：可以一笔画完，不会跳跃
- **两大核心性质**（决定随机性）：
  - **正态增量**：任意两个时刻\(s<t\)，位置差\(W_t - W_s \sim N(0, (t-s)I_d)\)—— 差值是**高斯分布**，方差随时间线性增加（时间越长，随机波动越大）
  - **独立增量**：对于任意\(0 \leq t_{0}<t_{1}<\cdots<t_{n}=1\)，增量\(W_{t_{1}}-W_{t_{0}}, ..., W_{t_{n}}-W_{t_{n-1}}\)是**独立随机变量**，所谓不同时间段的位置差互不影响（比如"0~0.2 秒的波动"和"0.3~0.5 秒的波动"没关系）

我们可以通过设置\(W_{0}=0\)并更新

\[W_{t+h}=W_{t}+\sqrt{h} \epsilon_{t}, \quad \epsilon_{t} \sim \mathcal{N}\left(0, I_{d}\right) \quad (t=0, h, 2h, ..., 1-h) \quad (5)\]

很容易地用步长\(h>0\)近似模拟布朗运动。每步先保持当前位置，再加一个"**缩放后的高斯噪声**"——\(\sqrt{h}\)是为了让方差随步长（时间）线性增加，符合正态增量性质。

![](/img/flow-matching/f2.png)

**图2**：在维度\(d=1\)中使用公式（5）模拟的布朗运动\(W_{t}\)的样本轨迹。

#### From ODEs to SDEs

让我们重写 **ODE** 的轨迹\((X_{t})_{0 \leq t \leq 1}\)，从两个角度来表达：

**角度1：通过导数表达**
\[\frac{d}{d t} X_{t}=u_{t}\left(X_{t}\right)\]

这表示轨迹的瞬时变化率等于向量场给出的速度。

**角度2：通过离散更新表达**

我们可以将导数形式改写为离散更新形式：
\[\frac{1}{h}(X_{t+h}-X_{t}) = u_{t}(X_{t}) + R_{t}(h) \Leftrightarrow X_{t+h} = X_{t} + hu_{t}(X_{t}) + hR_{t}(h)\]

其中\(R_{t}(h)\)是一个对于小的\(h\)**可忽略的函数**，即\(\lim _{h \to 0} R_{t}(h)=0\)（这来自于导数的定义）。上述推导简单地重述了我们已知的内容：**ODE** 的轨迹\((X_{t})_{0 \leq t \leq 1}\)在每个时间步长，沿方向\(u_{t}(X_{t})\)迈出一小步。

**从 ODE 到 SDE：加入随机性**

现在我们可以修改最后一个方程使其随机化，得到 **SDE** 的更新规则。**SDE** 的轨迹\((X_{t})_{0 \leq t \leq 1}\)在每个时间步长，沿方向\(u_{t}(X_{t})\)迈出一小步，再加上**布朗运动**的随机贡献：

\[X_{t+h}=X_{t}+\underbrace{hu_{t}(X_{t})}_{\text{确定项}}+\sigma_{t}\underbrace{\left(W_{t+h}-W_{t}\right)}_{\text{随机项}}+\underbrace{hR_{t}(h)}_{\text{误差项}} \quad (6)\]

其中：
- \(\sigma_{t} \geq 0\)是**扩散系数**，控制随机性的强度
- \(R_{t}(h)\)是一个随机误差项，使得标准差\(\mathbb{E}[\left\|R_{t}(h)\right\|^{2}]^{1/2} \to 0\)在\(h \to 0\)时趋于零（可以忽略）

上述描述了**随机微分方程（SDE）**。通常用以下符号记号表示：

\[dX_{t}=u_{t}(X_{t})dt+\sigma_{t}dW_{t} \quad (7a)\]

\[X_{0}=x_{0} \quad \text{（初始条件）} \quad (7b)\]

**但请注意**：上述\(dX_{t}\)记号只是公式(6)的纯符号表示。不幸的是，**SDE 不再有流映射**\(\phi_{t}\)。这是因为值\(X_{t}\)不再完全由\(X_{0} \sim p_{init }\)确定，因为演化本身是随机的（即使给定初始点，每次模拟的轨迹都不同）。尽管如此，与 ODE 一样，我们有：

**Theorem 5 (SDE Solution Existence and Uniqueness)**

如果\(u: \mathbb{R}^{d} \times[0,1] \to \mathbb{R}^{d}\)是**连续可微的**且具有**有界导数**，且\(\sigma_{t}\)是**连续的**，那么式（7）中的随机微分方程存在由满足式（6）的**唯一随机过程**\((X_{t})_{0 \leq t \leq 1}\)给出的解。

#### 奥恩斯坦-乌伦贝克（OU）过程

让我们考虑一个**恒定的扩散系数**\(\sigma_{t}=\sigma \geq 0\)和一个**恒定的线性漂移**\(u_{t}(x)=-\theta x\)，对于\(\theta>0\)，得到随机微分方程

\[dX_{t}=-\theta X_{t}dt+\sigma dW_{t} \quad (8)\]

上述随机微分方程的一个解\((X_{t})_{0 \leq t \leq 1}\)被称为**奥恩斯坦-乌伦贝克（OU）过程**。我们在图3中对其进行了可视化。向量场\(-\theta x\)将该过程推回其中心0（就像我总是朝着与我所在位置相反的方向移动一样），而**扩散系数**\(\sigma\)则总是会增加更多噪声。如果我们对其模拟\(t \to \infty\)，这个过程会收敛到一个**高斯分布**\(N(0, \frac{\sigma^{2}}{2\theta})\)。请注意，对于\(\sigma=0\)，我们有一个具有线性向量场的流，这一点我们已在方程（3）中研究过。

![](/img/flow-matching/f3.png)

**图3**：在维度\(d=1\)中，针对\(\theta=0.25\)以及不同\(\sigma\)值（从左到右逐渐增大）的奥恩斯坦-乌伦贝克过程（式（8））示意图。对于\(\sigma=0\)，我们得到一个流（平滑的确定性轨迹），其随着\(t \to \infty\)收敛到原点。对于\(\sigma>0\)，我们有随机路径，其随着\(t \to \infty\)收敛到高斯\(N(0, \frac{\sigma^{2}}{2 \theta})\)。

#### SDE 的模拟：欧拉-马尔可夫法

和 **ODE** 的欧拉法对应，**欧拉-马尔可夫法**是 **SDE** 最基础的数值模拟方法（式 9）：

\[X_{t+h} = X_t + h u_t(X_t) + \sqrt{h}\sigma_t \epsilon_t \quad (9)\]

其中\(\epsilon_t \sim N(0,I_d)\)，\(h=1/n\)为步长。

- 从当前位置\(X_t\)出发，沿向量场\(u_t(X_t)\)走一小步（**确定部分**）
- 采样一个高斯噪声\(\epsilon_t\)，用\(\sqrt{h}\sigma_t\)缩放后，加到位置上（**随机部分**）
- **核心**：噪声缩放因子\(\sqrt{h}\sigma_t\)—— 保证随机项的方差随时间/步长线性增加，符合布朗运动的性质

#### 扩散模型的构建（用 SDE 做生成模型）

和流模型逻辑一致，只是把 **ODE** 换成 **SDE**。**核心组件**：

- **神经网络**\(u_t^\theta(x)\)：参数化 SDE 的向量场（学习"确定步"的方向）
- **固定扩散系数**\(\sigma_t\)：控制"随机步"的噪声强度

一个扩散模型因此由以下公式给出：

\[dX_{t}=u_{t}^{\theta}(X_{t})dt+\sigma_{t}dW_{t} \quad \text{（SDE）}\]

\[X_{0} \sim p_{init} \quad \text{（随机初始化）}\]

**但请注意**：尽管它被称为扩散模型，但**神经网络参数化的是向量场，而不是随机过程**。为了计算随机过程，我们需要使用数值方法模拟 SDE。在算法2中，我们总结了如何从扩散模型中采样的过程。

**Algorithm 2 Sampling from a Diffusion Model (Euler-Maruyama method)**

**Require**: Neural network \(u_t^\theta\), number of steps \(n\), diffusion coefficient \(\sigma_t\)

```
1: Set t = 0
2: Set step size h = 1/n
3: Draw a sample X₀ ~ p_init
4: for i = 1, ..., n-1 do
5:     Draw a sample ε ~ N(0, I_d)
6:     X_{t+h} = X_t + h·u_t^θ(X_t) + σ_t·√h·ε
7:     Update t ← t + h
8: end for
9: return X₁
```

其中第6行的更新规则对应数学公式：\(X_{t+h} = X_t + h \cdot u_t^\theta(X_t) + \sigma_t \sqrt{h} \cdot \epsilon\)，其中\(\epsilon \sim \mathcal{N}(0, I_d)\)

> A diffusion model with \(\sigma_{t}=0\) is a flow model.