---
id: rl-chapter-5
title: 第5章 蒙特卡洛方法
sidebar_label: 第5章
---

# Monte Carlo Methods

> **写在前面**：在前面的章节中，我们学习了价值迭代和策略迭代，它们都假设我们**已知环境模型**（即状态转移概�?`p(s'|s,a)` 和奖励概�?`p(r|s,a)`）。但在很多实际场景中，我�?*没有环境模型**，只有智能体与环境交互产生的**数据**（episode）。这一章我们要学习如何"**用数据替代模�?*"�?> 
> 蒙特卡洛方法（Monte Carlo Methods）就是解决这�?无模�?（model-free）问题的经典方法。它的核心思想很简单：**通过收集大量�?episode 数据，用经验平均来估计动作价值，然后基于这些估计改进策略**�?> 
> 通过这一章的学习，你会看到：即使不知道环境的"内部机制"，我们也能通过"试错"�?经验总结"来学习最优策略。这就是蒙特卡洛方法的魅力所在�?
> **核心思想**：If we do not have a model, we must have some data. If we do not have data, we must have a model. If we have neither, then we are not able to find optimal policies.
> 
> The "data" in reinforcement learning usually refers to the agent's interaction experiences with the environment

## 从有模型到无模型：为什么需要蒙特卡洛方法？

在深入算法细节之前，让我们先思考一个问题：**为什么我们需要无模型的方法？**

想象一下，你要训练一个机器人学习走路。在价值迭代和策略迭代中，你需要知�?如果机器人向前迈一步，它会以多大的概率摔�?（这就是环境模型）。但在现实中，你可能根本不知道这个概率是多少，你只能让机器人**实际去走**，然后观察它是否摔倒�?
这就是无模型强化学习的核心场景：**我们不知道环境的内部机制，但我们有大量的交互数据**�?
蒙特卡洛方法就是利用这些数据来学习的方法。它的名字来源于著名�?蒙特卡洛赌场"——就像通过大量随机试验来估计概率一样，蒙特卡洛方法通过大量 episode 来估计动作价值�?
## MC Basic

现在让我们看看如何将策略迭代"改�?成无模型版本。MC Basic 算法可以看作是策略迭代的"无模型版�?�?
> **关键区别**：策略迭代需�?解方�?（贝尔曼方程），MC Basic 只需�?算平均�?（经验平均）。这就是为什�?MC 方法更实�?—�?因为它不需要知道环境的内部机制�?
**核心思路**：既然我们不知道环境模型，那就用"经验"来替�?理论"。具体来说，我们通过收集大量�?episode，用这些 episode 的回报平均值来估计动作价值，而不是通过求解贝尔曼方程�?
### 算法描述

让我们先看看算法的整体框架，然后再通过例子深入理解�?
#### MC Basic (a model-free variant of policy iteration)

- **Initialization**: Initial guess `π₀`. Goal: Search for an optimal policy.
- **For the kth iteration** (`k = 0, 1, 2, ...`), **do**:
  - **For every state** `s �?S`, **do**:
    - **For every action** `a �?A(s)`, **do**:
      - Collect sufficiently many episodes starting from `(s, a)` by following `πₖ`
  - **Policy evaluation**:
    - `q_π�?s, a) �?q�?s, a) =` the average return of all the episodes starting from `(s, a)`
  - **Policy improvement**:
    - `a*�?s) = arg max_a q�?s, a)`
    - `πₖ₊�?a|s) = 1` if `a = a*�?s)`, and `πₖ₊�?a|s) = 0` otherwise

 

> **关键理解**：与策略迭代不同，MC Basic 不需要求解贝尔曼方程，而是通过**收集 episode 数据**，用**经验平均**来估计动作价值。这就是"无模�?的含义�?
### 通过例子理解 MC Basic：从经验中学�?
理论说再多，不如看个例子。让我们通过一个具体例子来理解 MC Basic 算法是如何工作的�?
> **学习策略**：通过这个例子，你会看�?MC 方法如何�?经验"来估�?理论�?。虽然估计可能不完美，但足够指导策略改进�?
我们重点分析"状�?`s₁` �?5 个动作（`a₁` �?`a₅`�?，每个动作对应不同的 episode 轨迹，进而推�?Q 值�?
![An example for illustrating the MC Basic algorithm](/img/rl/f5-1.png)
#### 1. 动作 `a₁`：循环在 `s₁`，拿负奖�?
**步骤 1：确�?`a₁` �?episode 轨迹**

�?`(s�? a�?` 出发，根据初始策�?`π₀`，动�?`a₁` 会让智能体一直停留在 `s₁`（即循环：`s�?→[a₁] s�?→[a₁] s�?→[a₁] ...`）�?
所以每一步的奖励 `R_{t+1} = R_{t+2} = ... = -1`（对�?`r_boundary = -1`）�?
**步骤 2：写出回�?`G_t` 的表达式**

根据回报定义，从 `s₁` �?`a₁` 的回报是�?
`G_t = R_{t+1} + γ R_{t+2} + γ² R_{t+3} + ...`

代入每一步的奖励 `R = -1`，得到：

`G_t = (-1) + γ(-1) + γ²(-1) + γ³(-1) + ...`

**步骤 3：用等比级数求和�?Q �?*

这是一�?首项 `a = -1`，公�?`r = γ = 0.9`"的无穷等比级数。根据求和公式：

`q_{π₀}(s�? a�? = G_t = a/(1 - r) = (-1)/(1 - 0.9) = (-1)/0.1 =` ****

#### 2. 动作 `a₂`：到达目标状�?
�?`(s�? a�?` 出发，episode 轨迹为：`s�?→[a₂] s�?→[a₃] s�?→[a₃] ...`（即智能体从 `s₁` �?`a₂` �?`s₂`，再�?`a₃` 到目标状�?`s₅`，之后在 `s₅` 中循环）�?
该动作的动作值等于该 episode 的折扣回报，计算如下�?
`q_{π₀}(s�? a�? = 0 + γ·0 + γ²·0 + γ³(1) + γ�?1) + ... = γ³/(1 - γ)`

#### 3. 动作 `a₃`：另一条到达目标的路径

�?`(s�? a�?` 出发，episode 轨迹为：`s�?→[a₃] s�?→[a₂] s�?→[a₃] ...`（即智能体从 `s₁` �?`a₃` �?`s₄`，再�?`a₂` 到目标状�?`s₅`，之后在 `s₅` 中循环）�?
该动作的动作值等于该 episode 的折扣回报，计算如下�?
`q_{π₀}(s�? a�? = 0 + γ·0 + γ²·0 + γ³(1) + γ�?1) + ... = γ³/(1 - γ)`

#### 4. 动作 `a₄`：回�?`s₁` 并循�?
�?`(s�? a�?` 出发，episode 轨迹为：`s�?→[a₄] s�?→[a₁] s�?→[a₁] ...`（即智能体从 `s₁` �?`a₄` 回到 `s₁`，之后经 `a₁` �?`s₁` 中循环）�?
该动作的动作值等于该 episode 的折扣回报，计算如下�?
`q_{π₀}(s�? a�? = -1 + γ(-1) + γ²(-1) + ... = (-1)/(1 - γ)`

#### 5. 动作 `a₅`：延迟回�?`s₁` 并循�?
�?`(s�? a�?` 出发，episode 轨迹为：`s�?→[a₅] s�?→[a₁] s�?→[a₁] ...`（即智能体从 `s₁` �?`a₅` 回到 `s₁`，之后经 `a₁` �?`s₁` 中循环）�?
该动作的动作值等于该 episode 的折扣回报，计算如下�?
`q_{π₀}(s�? a�? = 0 + γ(-1) + γ²(-1) + ... = (-γ)/(1 - γ)`

#### 策略改进：选择最优动�?
通过比较上述 5 个动作的动作值，我们发现�?
`q_{π₀}(s�? a�? = q_{π₀}(s�? a�? = γ³/(1 - γ) > 0`

这两个动作值是最大的。因此，可得到新策略�?
`π�?a₂|s�? = 1` �?`π�?a₃|s�? = 1`

> **重要说明**：对于每个动作，理论上需要收集足够多且足够长�?episode，才能有效近似其动作值。但由于本示例中�?策略"�?环境模型"均为确定性的（即每次执行相同动作都会产生相同轨迹），多次运行也会生成完全一致的轨迹，因此每个动作值的估计仅需 1 �?episode 即可�?
> **实际应用**：在真实场景中，环境通常是随机的，所以需要收集多�?episode 来平均，以减少随机性的影响�?
### Episode 长度的影响：为什么需要足够长的轨迹？

通过上面的例子，我们已经理解�?MC Basic 的基本工作原理。但这里有一个关键问题：**episode 需要多长？**

在实际应用中，episode 的长度对算法的性能有重要影响。太短了，智能体来不及到达目标；太长了，计算成本又太高。让我们通过一个综合例子来理解这一点�?
> **核心问题**：episode 太短，智能体来不及到达目标；episode 太长，计算成本高。如何找到平衡点�?
![A comprehensive example: Episode length and sparse rewards](/img/rl/f5-2.png)

#### 1. Episode 长度对策略和值估计的影响

在深入分析之前，让我们先明确两个核心概念�?
| 概念 | 定义（通俗版） | 示例中的具体表现�?.4.3�?|
| --- | --- | --- |
| **step（步�?* | 智能体与环境�?一次完整交�?�?. 智能体在当前状�?`s` 选动�?`a`�?. 环境给奖�?`r`，并切换到新状�?`s'`；这 3 个元素（`s �?a �?r �?s'`）构�?1 �?step�?| 1 �?step = 智能体在网格某格（如 `s₁`）选动作（�?`a₂`）→ 环境给奖励（�?0）→ 走到新格子（�?`s₂`）�?|
| **episode（情节）** | �?起始状�?动作�?开始，�?结束条件"为止的所�?step 的序列。（注：若环境无明确"结束"，会�?固定步数"作为 episode 的终止条件） | 示例中无明确结束（目标状态可循环停留），故用"100 万步"作为 1 �?episode 的长�?�?1 �?episode = 100 万个连续�?step�?|
首先我们将证明：**episode 长度对最终得到的最优策略有显著影响**。具体而言，上图展示了蒙特卡洛基础算法在不�?episode 长度下的最终结果：

- **�?episode 长度过短�?*，无论是策略还是值估计都无法达到最优（见上�?(a)~(d)）。在"episode 长度 = 1"的极端情况下，只有与目标状态相邻的状态才有非零值，其他所有状态的值均�?0——这是因�?episode 太短，智能体无法到达目标状态或获得正奖励�?- **随着 episode 长度的增�?*，策略和值估计会逐渐趋近于最优�?
#### 2. Episode 长度与状态值的空间分布规律

随着 episode 长度增加，会出现一个有趣的空间分布规律�?*距离目标状态越近的状态，越早出现非零�?*�?
**原因分析**：从任意状态出发，智能体需要至少一定步数才能到达目标状态并获得正奖励；�?episode 长度小于这个"最小必要步�?，则该状态的回报必然�?0，其估计值也会为 0�?
在本示例中，episode 长度至少需要达�?****—�?5 是从左下角状态出发到达目标状态所需的最小步数�?
#### 3. Episode 长度�?非无限�?要求

尽管上述分析表明 episode 需�?足够�?，但**并非要求"无限�?**。如图所示，�?episode 长度 = 30 时，算法已能找到最优策略（尽管此时的值估计尚未完全最优）�?
#### 4. 与稀疏奖励问题的关联

上述分析涉及一个重要的奖励设计问题—�?*稀疏奖励（sparse reward�?*，即"只有到达目标状态才能获得正奖励"的场景�?
**稀疏奖励的问题**�?- 稀疏奖励设置要�?episode 必须足够长（以确保智能体能到达目标）
- 但当状态空间较大时，这一要求很难满足
- 最终会导致算法的学习效率下�?
**解决方案：设计非稀疏奖�?*

解决稀疏奖励问题的一个简单方法是**设计非稀疏奖�?*。例如，在上述网格世界示例中，我们可以重新设计奖励规则：让智能体到达"目标附近的状�?时也能获得少量正奖励�?
这样一来，目标状态周围会形成一�?**吸引域（attractive field�?*"，帮助智能体更轻松地找到目标�?
> **实践建议**：在实际应用中，如果遇到稀疏奖励问题，可以考虑�?> 1. 设计中间奖励（如距离目标的负距离�?> 2. 使用奖励塑形（reward shaping�?> 3. 使用课程学习（curriculum learning）逐步增加难度

## MC Exploring Starts

通过前面的学习，我们已经掌握�?MC Basic 的基本原理。但你可能已经注意到一个问题：**MC Basic 的效率似乎不太高**�?
确实，在实际应用中，我们不仅希望算法能找到最优策略，还希望它�?*高效地利用样�?*�?*及时地更新策�?*。让我们看看如何改进 MC Basic 算法�?
> **改进方向**：我们可以从两个角度来提升效率：
> 1. **更高效地利用样本**：一�?episode 可能访问多个状�?动作对，如何充分利用这些信息�?> 2. **更及时地更新策略**：是否必须等到所�?episode 收集完毕才能更新�?
### 更高效地利用样本：一�?episode 的多种用�?
让我们先思考一个问题：**一�?episode 只能用来估计一个状�?动作对吗�?*

答案是否定的。蒙特卡洛（MC）强化学习的一个重要方面，就是如何更高效地利用样本。具体来说，假设我们遵循某一策略 `π`，得到了如下一�?episode 的样本：

`s�?→[a₂] s�?→[a₄] s�?→[a₂] s�?→[a₃] s�?→[a₁] ...` (5.3)

其中，下标代表状态或动作的索引，而非时间步�?
> **关键概念**：在一�?episode 中，某一状�?动作对每出现一次，就称为对该状�?动作对的一�?**访问（visit�?*"�?
**观察**：这�?episode 不仅访问了起始状�?动作�?`(s�? a�?`，还访问�?`(s�? a�?`、`(s�? a�?`、`(s�? a�?` 等多个其他状�?动作对。我们能否利用这些信息来估计更多状�?动作对的动作值呢�?
#### 访问策略：三种不同的样本利用方式

我们可以采用不同的策略来利用这些访问信息。在深入细节之前，让我们先明确三种访问策略的定义�?
> **Q：什么是初始访问策略（initial-visit）、首次访问策略（first-visit）和每次访问策略（every-visit）？**
> 
> **A�?* 它们是利�?episode 中样本的不同策略。一�?episode 可能会访问多个状�?动作对，三种策略的定义如下：
> 
> - **初始访问策略**：仅利用整个 episode 来估�?episode 起始状�?动作�?的动作�?> - **每次访问策略**：每当某个状�?动作对在 episode 中被访问时，就利用该访问之后�?episode 轨迹来估计该状�?动作对的动作值，能充分利用样�?> - **首次访问策略**：仅在某个状�?动作�?第一次被访问"时，利用该访问之后的 episode 轨迹来估计其动作值，同样能高效利用样�?
**第一种策略：初始访问（initial visit�?*

即一�?episode 仅用于估计该 episode 起始的状�?动作对的动作值。以式（5.3）的 episode 为例，初始访问策略仅会估计状�?动作�?`(s�? a�?` 的动作值�?
蒙特卡洛基础算法（MC Basic）采用的就是初始访问策略。然而，这种策略的样本效率较�?—�?因为�?episode 还访问了 `(s�? a�?`、`(s�? a�?`、`(s�? a�?` 等多个其他状�?动作对，这些访问信息同样可用于估计对应状�?动作对的动作值�?
**第二种策略：每次访问（every-visit�?*

我们可以将式�?.3）的 episode 分解为多�?**子情节（subepisode�?*"�?
![子情节分解示意图](/img/rl/f5-3.png)

某一状�?动作对被访问后所生成的轨迹，可视为一段新�?episode。这些新 episode 能用于估计更多状�?动作对的动作值，从而更高效地利用原�?episode 中的样本�?
**第三种策略：首次访问（first-visit�?*

此外，某一状�?动作对在一�?episode 中可能被访问多次。例如，在式�?.3）的 episode 中，状�?动作�?`(s�? a�?` 就被访问了两次�?
- **首次访问策略（first-visit strategy�?*：若仅统计该状�?动作对在 episode 中的"第一次访�?
- **每次访问策略（every-visit strategy�?*：若统计该状�?动作对在 episode 中的"每一次访�?

> **效率对比**：从样本利用效率来看，每次访问策略是最优的。若一�?episode 足够长，能多次访问所有状�?动作对，那么仅通过这一�?episode，采用每次访问策略就可能完成对所有状�?动作对动作值的估计�?
> **注意事项**：每次访问策略所获取的样本存在相关�?—�?因为从某状�?动作�?第二次访�?开始的轨迹，本质是�?第一次访�?起始轨迹的子集。但如果两次访问在轨迹中相距较远，这种相关性会较弱，对估计结果的影响也有限�?
### 更高效地更新策略：何时更新？

现在让我们思考另一个问题：**我们必须在收集完所�?episode 之后才能更新策略吗？**

蒙特卡洛强化学习的另一个重要方面，�?**何时更新策略**"。目前有两种可用策略�?
**第一种策略：批量更新**

在策略评估步骤中，先收集所有从"同一状�?动作�?起始�?episode，再利用这些 episode 的回报平均值来近似该状�?动作对的动作值�?
蒙特卡洛基础算法（MC Basic）采用的就是这种策略。其缺点在于，智能体必须等到所�?episode 都收集完毕后，才能更新动作值的估计结果�?
> ⚠️ **问题**：这意味着我们需要等待很长时间才能看到算法的改进，这在实时学习中是不可接受的�?
**第二种策略：逐情节更�?*

可克服上述缺点，即利�?单段 episode 的回�?来近似对应状�?动作对的动作值。这样一来，每获取一�?episode，就能立即得到一个粗略的动作值估计，进而以"**逐情节（episode-by-episode�?*"的方式更新策略�?
> **疑问解答**：有人可能会疑问：单�?episode 的回报无法准确近似动作值，这种策略是否可行？事实上，该策略属于上一章介绍的"**广义策略迭代（generalized policy iteration�?*"范畴 —�?即便动作值的估计不够精确，我们仍然可以更新策略。关键在于：**不完美的估计 + 及时的更�?> 完美的估�?+ 延迟的更�?*�?
### MC Exploring Starts 算法：综合两种改�?
综合上述两种技术（高效样本利用和高效策略更新），我们可以得�?*蒙特卡洛探索起点算法（MC Exploring Starts�?*�?
> **改进总结**：MC Exploring Starts = MC Basic + 每次访问策略 + 逐情节更新。这样既提高了样本利用效率，又加快了策略更新速度�?
#### MC Exploring Starts (an efficient variant of MC Basic)

- **Initialization**: Initial policy `π₀(a|s)` and initial value `q(s, a)` for all `(s, a)`. `Returns(s, a) = 0` and `Num(s, a) = 0` for all `(s, a)`.
- **Goal**: Search for an optimal policy.
- **For each episode, do**:
  - **Episode generation**: Select a starting state-action pair `(s₀, a₀)` and ensure that all pairs can be possibly selected (this is the exploring-starts condition). Following the current policy, generate an episode of length `T`: `s₀, a₀, r�? ..., s_{T-1}, a_{T-1}, r_T`.
  - **Initialization for each episode**: `g �?0`
  - **For each step of the episode**, `t = T-1, T-2, ..., 0`, **do**:
    - `g �?γg + r_{t+1}`
    - `Returns(s_t, a_t) �?Returns(s_t, a_t) + g`
    - `Num(s_t, a_t) �?Num(s_t, a_t) + 1`
  - **Policy evaluation**:
    - `q(s_t, a_t) �?Returns(s_t, a_t) / Num(s_t, a_t)`
  - **Policy improvement**:
    - `π(a|s_t) = 1` if `a = arg max_a q(s_t, a)` and `π(a|s_t) = 0` otherwise

 

#### 探索起点条件的要求与限制

"**探索起点条件**"要求：从每一个状�?动作对起始，都要生成足够多的 episode。根据大数定律，只有当所有状�?动作对都被充分探索时，我们才能准确估计它们的动作值，进而成功找到最优策略�?
> 反之，若某一动作未被充分探索，其动作值的估计可能存在偏差 —�?即便该动作实际上是最优动作，也可能不被策略选中�?
蒙特卡洛基础算法（MC Basic）和蒙特卡洛探索起点算法（MC Exploring Starts）均需满足探索起点条件。然而，在许多实际应用中（尤其是涉及与环境物理交互的场景），这一条件很难满足�?
> **实际困难**：想象你在训练一个机器人，你无法随意"重置"到任意状�?动作对。你只能从环境的初始状态开始，这大大限制了探索的灵活性�?
**那么，有没有办法移除这个限制呢？**

答案是肯定的。接下来，我们对蒙特卡洛探索起点算法（MC Exploring Starts）进行扩展，移除�?探索起点条件"。该条件本质上要求所有状�?动作对都能被充分访问，而这一要求也可通过"**软策略（soft policies�?*"实现�?
> **核心思路**：如果我们能让策略在任意状态下都有概率选择任意动作，那么即使从固定起点开始，只要 episode 足够长，我们也能访问到所有状�?动作对�?
### ϵ-贪婪策略：实现软策略的经典方�?
**软策略的定义**：若某一策略�?任意状态下选择任意动作的概率均为正"，则该策略称为软策略�?
> **为什么叫"软策�?�?* 因为它不�?硬�?地只选一个动作（概率�?1），而是"软�?地给所有动作都分配一定的概率�?
考虑一种极端情况：我们仅有一�?episode。在软策略下，只要这�?episode 足够长，就能多次访问所有状�?动作对。因此，我们无需生成大量从不同状�?动作对起始的 episode，进而可移除"探索起点"的要求�?
**那么，如何设计一个软策略呢？**

ϵ-贪婪策略就是实现软策略的经典方法�?
**ϵ-贪婪策略的特�?*�?
ϵ-贪婪策略是一类常见的软策略。它是一种随机策略，其特点是：选择"贪婪动作"的概率更高，同时选择其他任意动作的概率均为非零且相等。其中，"**贪婪动作（greedy action�?*"指的�?动作值最大的动作"�?
**数学形式**�?
具体来说，设参数 `ε �?[0, 1]`，对应的 ϵ-贪婪策略概率形式如下�?
```
π(a|s) = {
  1 - (|A(s)| - 1)ε / |A(s)|,  �?a 为贪婪动�?  ε / |A(s)|,                   �?a 为其�?|A(s)| - 1 个动�?}
```

其中，`|A(s)|` 表示"状�?`s` 对应的动作数�?�?
**特殊情况**�?- �?`ε = 0` 时，ϵ-贪婪策略退化为贪婪策略（仅选择贪婪动作，概率为 1�?- �?`ε = 1` 时，选择任意动作的概率均等于 `1/|A(s)|`（即均匀随机策略�?
**概率性质**�?
对任�?`ε �?[0, 1]`，选择贪婪动作的概率始终大于选择其他任意动作的概率，推导如下�?
`1 - (|A(s)| - 1)ε / |A(s)| = 1 - ε + ε/|A(s)| �?ε/|A(s)|`

**实现方式**�?
尽管 ϵ-贪婪策略是随机策略，但我们可通过以下方式遵循该策略选择动作�?
1. 首先生成一个服�?`[0, 1]` 区间均匀分布的随机数 `x`
2. �?`x �?ε`，则选择贪婪动作
3. �?`x < ε`，则从状�?`s` 的动作集�?`A(s)` 中随机选择一个动作（选择概率均为 `1/|A(s)|`，可能再次选中贪婪动作�?
通过这种方式，选择贪婪动作的总概率为 `1 - ε + ε/|A(s)|`，选择其他任意单个动作的概率均�?`ε/|A(s)|`�?
### �?ϵ-贪婪策略融入蒙特卡洛学习

现在我们已经理解�?ϵ-贪婪策略的原理，接下来要做的就是将它融入蒙特卡洛学习�?
**核心思路**：要�?ϵ-贪婪策略融入蒙特卡洛学习，只需�?策略改进步骤"中的"贪婪策略"替换�?ϵ-贪婪策略"即可�?
> **关键改变**：之前我们总是选择"最好的动作"（贪婪策略），现在我们要�?最好的动作"�?其他动作"之间做平衡（ϵ-贪婪策略）�?
**原始策略改进**�?
蒙特卡洛基础算法（MC Basic）或蒙特卡洛探索起点算法（MC Exploring Starts）中的策略改进步骤，目标是求解以下问题：

`π_{k+1}(s) = arg max_{π �?Π} ∑_a π(a|s) q_{π_k}(s, a)` (5.4)

其中，`Π` 表示"所有可能策略的集合"。我们已知式�?.4）的解是一个贪婪策略：

```
π_{k+1}(a|s) = {
  1,  �?a = a_k^*
  0,  �?a �?a_k^*
}
```

这里，`a_k^* = arg max_a q_{π_k}(s, a)`（即状�?`s` 下的贪婪动作）�?
**修改后的策略改进**�?
如今，我们将策略改进步骤修改为求解以下问题：

`π_{k+1}(s) = arg max_{π �?Π_ε} ∑_a π(a|s) q_{π_k}(s, a)` (5.5)

其中，`Π_ε` 表示"给定参数 `ε` 下，所�?ϵ-贪婪策略的集�?。通过这种修改，我们强制策略为 ϵ-贪婪策略�?
式（5.5）的解为�?
```
π_{k+1}(a|s) = {
  1 - (|A(s)| - 1)ε / |A(s)|,  �?a = a_k^*
  ε / |A(s)|,                   �?a �?a_k^*
}
```

这里，`a_k^* = arg max_a q_{π_k}(s, a)`�?
经过上述修改，我们得到了一种新算法—�?*蒙特卡洛 ϵ-贪婪算法（MC ϵ-Greedy�?*�?
> **算法优势**：MC ϵ-Greedy 不需要探索起点条件，这意味着它可以在更实际的环境中应用。你只需要从环境的初始状态开始，让智能体按照 ϵ-贪婪策略行动，就能逐步学习到最优策略�?
### 算法描述

让我们看看完整的算法流程�?
#### MC ϵ-Greedy (a variant of MC Exploring Starts)

- **Initialization**: Initial policy `π₀(a|s)` and initial value `q(s, a)` for all `(s, a)`. `Returns(s, a) = 0` and `Num(s, a) = 0` for all `(s, a)`. `ε �?(0, 1]`
- **Goal**: Search for an optimal policy.
- **For each episode, do**:
  - **Episode generation**: Select a starting state-action pair `(s₀, a₀)` (the exploring starts condition is not required). Following the current policy, generate an episode of length `T`: `s₀, a₀, r�? ..., s_{T-1}, a_{T-1}, r_T`.
  - **Initialization for each episode**: `g �?0`
  - **For each step of the episode**, `t = T-1, T-2, ..., 0`, **do**:
    - `g �?γg + r_{t+1}`
    - `Returns(s_t, a_t) �?Returns(s_t, a_t) + g`
    - `Num(s_t, a_t) �?Num(s_t, a_t) + 1`
  - **Policy evaluation**:
    - `q(s_t, a_t) �?Returns(s_t, a_t) / Num(s_t, a_t)`
  - **Policy improvement**:
    - Let `a* = arg max_a q(s_t, a)` and
    - `π(a|s_t) = 1 - (|A(s_t)| - 1)ε / |A(s_t)|` if `a = a*`
    - `π(a|s_t) = ε / |A(s_t)|` if `a �?a*`

 

![The evolution process of the MC ϵ-Greedy algorithm based on single episodes](/img/rl/f5-4.png)

### Step 级更新：理解 MC ϵ-Greedy 的细粒度更新机制

在深入理解算法之前，让我们先澄清一个常见的误解�?
> **常见误解**：你以为"1 �?episode 对应 1 次策略更�?，但 MC ϵ-Greedy 的策略更新是"**�?step 反向更新**" —�?1 �?episode 包含 N �?step（示例中 N=100 万），这 N �?step 会逐个参与策略更新，相当于 1 �?episode 里有 N �?细粒度更�?，而不�?1 �?粗粒度更�?�?
**为什么这样设计？** 因为每个 step 都包含了从该 step �?episode 结束的所有信息，所以我们可以立即利用这些信息来更新策略，而不需要等到整�?episode 结束�?
#### 为什么是反向更新�?
这要结合 MC 算法的核心机�?—�?�?**回报（G�?*"估计 Q 值，而回报需要从"当前 step 之后的所有奖�?计算。为了高效计算，MC ϵ-Greedy 会在 1 �?episode 结束后，从最�?1 �?step（T-1 步）反向遍历到第 1 �?step�? 步），每遍历 1 �?step 就做一次更新�?
> **记忆技�?*：回报是�?当前时刻"�?未来"的所有奖励，所以从后往前算最方便 —�?先算最后一步的回报，再往前累加�?
#### 具体操作流程

我们用示例中"1 �?100 万步�?episode"为例，拆�?step 级更�?的具体操作（对应算法 5.3 的核心步骤）�?
**假设 1 �?episode �?step 序列�?*�?
`s₀, a₀, r�?�?s�? a�? r�?�?s�? a�? r�?�?... �?s₉₉₉₉₉₈, a₉₉₉₉₉₈, r₉₉₉₉₉₉ �?s₉₉₉₉₉₉, a₉₉₉₉₉₉, r₁₀₀₀₀₀₀`

（共 100 万步，记�?`t = 0` �?`t = 999999`，最后一步是 `t = 999999`�?
**策略更新的流程（反向遍历每个 step�?*�?
**初始�?*：每�?episode 开始前，重置折扣回�?`g = 0`（用于累计当�?step 之后的所有折扣奖励）�?
**从最�?1 步（`t = 999999`）反向算到第 1 步（`t = 0`�?*�?
- **�?`t = 999999`（最�?1 步）**�?  1. **算回�?*：`g = γ·g + r₁₀₀₀₀₀₀`（此�?`g = 0`，故 `g = r₁₀₀₀₀₀₀`，即最后一步的奖励�?  2. **累积累计回报**：`Returns(s₉₉₉₉₉₉, a₉₉₉₉₉₉) += g`
  3. **累加访问次数**：`Num(s₉₉₉₉₉₉, a₉₉₉₉₉₉) += 1`
  4. **更新 Q �?*：`q(s₉₉₉₉₉₉, a₉₉₉₉₉₉) = Returns / Num`（用平均回报估计 Q 值）
  5. **更新策略**：根据当�?Q 值选贪婪动�?`a*`，按 ε-贪婪公式调整 `π(a|s₉₉₉₉₉₉)` 的概�?
- **�?`t = 999998`（倒数第二步）**�?  1. **算回�?*：`g = γ·g + r₉₉₉₉₉₉`（此�?`g` 已包�?`t = 999999` 步的折扣奖励，现在加上当前步�?`r₉₉₉₉₉₉`，就�?`t = 999998` 步之后的总折扣回报）
  2. **重复上述 2-5 的操�?*，更�?`(s₉₉₉₉₉₈, a₉₉₉₉₉₈)` �?`Returns`、`Num`、Q 值和策略

- **... 以此类推**，直到遍历完 `t = 0`（第一步），完成整�?episode 的更新�?
#### 结论

> **核心要点**�? �?episode�?00 万步）里，每�?step 都会触发一�?Q 值更�?+ 策略更新"，相当于 1 �?episode 包含 100 万次细粒度的策略调整，而不�?1 次更新。这就是 step �?episode 细粒度更小的核心体现�?
> **为什么这样设计？** 因为每个 step 的回报都包含�?从该 step �?episode 结束"的所有信息，所以每处理一�?step，我们就能立即更新对应的 Q 值和策略，不需要等到整�?episode 结束。这样可以让算法更快地学习和适应�?
## Exploration and exploitation of ϵ -greedy policies

通过前面的学习，我们已经掌握�?MC ϵ-Greedy 算法的基本流程。但你可能已经注意到一个关键参数：**ε �?*。这个参数控制着探索与利用的平衡，是算法性能的关键�?
在深入分析之前，让我们先理解一个强化学习中的核心问题：**探索与利用的矛盾**�?
在强化学习中�?*探索（exploration�?* �?**利用（exploitation�?* 是一对永恒的矛盾�?
- **探索**：策略应尽可能尝试更多不同的动作，通过这种方式，所有动作都能被访问并得到充分评�?- **利用**：改进后的策略应选择"动作值最大的贪婪动作"

然而，由于当前时刻得到的动作值可能因探索不充分而存在偏差，因此我们在进行利用的同时必须保持探索，以避免错过潜在的最优动作�?
> **核心矛盾**：如果只探索不利用，我们永远学不到最优策略；如果只利用不探索，我们可能陷入局部最优�?
**那么，如何平衡探索和利用呢？**

这就�?ϵ-贪婪策略要解决的问题�?
**ϵ-贪婪策略（�?greedy policies�?* 为平衡探索与利用提供了一种解决方案：

- **一方面**，�?贪婪策略选择贪婪动作的概率更高，能够基于已估计的动作值进�?利用"
- **另一方面**，该策略也会以一定概率选择其他动作，从而维�?探索"能力

**利用与最优性的关系**�?
利用与最优性密切相�?—�?因为最优策略本质上是贪婪策略（始终选择当前动作值最大的动作）。�?贪婪策略的核心思想�?**通过牺牲部分最优�?利用效率来增强探索能�?*"�?
- 若想提升利用效率与策略最优性，需**减小 ε �?*
- 若想增强探索能力，则需**增大 ε �?*

> **直观理解**：�?值就像探索的"温度�?—�?ε 越大，探索越"�?（尝试更多动作）；�?越小，探索越"�?（更偏向利用已知的好动作）�?
接下来，我们通过几个具体示例来分析这种权衡关系，看看不同�?ε 值会带来什么影响�?
### ϵ-贪婪策略的最优性分析：ε 值如何影响性能�?
让我们通过具体例子来看�?ε 值如何影响策略的最优性�?
> **关键问题**：�?值越大，探索能力越强，但这是否意味着性能越好？答案可能出乎你的意料�?
![The state values of some ϵ-greedy policies](/img/rl/f5-5.png)

**图（a�?*展示�?贪婪最优策�?及其对应的最优状态值；**图（b�?（d�?*则展示了若干"一�?ε-贪婪策略"的状态值�?
> **一致策略的定义**：若两个 ε-贪婪策略�?概率最高的动作"相同，则称这两个策略�?一致的"�?
**观察结果**�?
从图中可观察到：随着 ε 值增大，ε-贪婪策略的状态值逐渐降低，这表明其最优性在不断变差。尤其当 ε 增大�?**** 时，目标状态的状态值达到最�?—�?这是因为 ε 值越大，智能体从目标区域出发�?进入周围禁止区域"的概率越高，从而更易获得负奖励�?
#### 最�?ε-贪婪策略 vs 贪婪最优策�?
![The optimal ϵ-greedy policies and their corresponding state values under different values of ϵ](/img/rl/f5-6.png)

上图展示�?**最�?ε-贪婪策略**"（即�?ε-贪婪策略集合 `Π_ε` 中最优的策略）及其对应的状态值：

- **�?`ε = 0` �?*：该策略即为贪婪策略，且在所有策略中均最�?- **�?ε 较小�?*（如 `ε = 0.1`）：最�?ε-贪婪策略�?贪婪最优策�?保持一�?- **但当 ε 增大到一定程�?*（如 `ε = 0.2`）：得到的最�?ε-贪婪策略�?贪婪最优策�?不再一�?
> 若想让最�?ε-贪婪策略�?贪婪最优策�?保持一致，ε 值需设置得足够小�?
**为什�?ε 值较大时策略会不一致？**

我们可从目标状态的策略选择来解释：

- **在贪婪策略下**：目标状态的最优动作是"停留不动"，以持续获得正奖�?- **但当 ε 值较大时**�?停留不动"仍有较高概率因探索而误入禁止区域、获得负奖励
- **结果**：此时，目标状态的最优动作会�?停留"变为"逃离"，导致策略一致性被打破

> **直观理解**：�?值太�?�?探索太强 �?即使在目标区域也会随机走 �?容易走到禁区 �?不如主动"逃离"目标区域更安�?�?策略改变

### ϵ-贪婪策略的探索能力分析：如何选择 ε 值？

通过上面的分析，我们已经看到�?ε 值对策略最优性的影响。现在让我们从另一个角度来分析�?*不同 ε 值下的探索能力如何？**

> **思�?*：探索能力越强，是否意味着算法性能越好？答案取决于你的目标——是快速探索整个状态空间，还是快速收敛到最优策略？

![Exploration abilities of ϵ-greedy policies with different values of ϵ](/img/rl/f5-7.png)

#### ε = 1：最强探索（均匀随机策略�?
考虑 `ε = 1` �?ε-贪婪策略（如上图（a）所示）：此时策略在任意状态下选择任意动作的概率均�?`0.2`（均匀随机策略），探索能力最强�?
**特点**�?- 由于探索能力极强，只�?episode 长度足够，单一�?episode 就能多次访问所有状�?动作对（见上图（c））
- 所有状�?动作对的访问次数几乎均匀分布

> **适用场景**：适合学习初期，需要快速探索整个状态空间�?
#### ε = 0.5：中等探�?
再考虑 `ε = 0.5` �?ε-贪婪策略（如上图（d）所示）：尽�?episode 足够长时仍能访问所有动作，但各动作的访问次数分布极不均衡�?
> **观察**：�?值减�?�?探索能力下降 �?更偏向利�?�?访问次数集中�?看起来更�?的动作上�?
#### 实践建议：动态调�?ε �?
在实际应用中，一种常用的策略�?**初始设置较大�?ε 值以增强探索，随后逐步减小 ε 值以保证最终策略的最优�?*"—�?这种动态调整方式能在学习初期充分探索状态空间，后期则聚焦于利用已学习到的最优动作�?
>  **记忆技�?*：�?�?= 探索�?温度"—�?开始时"高温"（高探索），逐渐"降温"（高利用），最�?冷却"（最优策略）�?
## Summary

通过这一章的学习，我们已经掌握了蒙特卡洛方法的核心思想和技术细节。让我们来总结一下关键要点�?
### 核心思想

总结一下，**Monte Carlo estimation** 是一类广义的技术方法，指通过"随机样本"来解决近似问题的技术总称�?
**无模型蒙特卡洛强化学习的核心思想**：将"有模型的策略迭代算法"转化�?无模型算�?。具体而言，策略迭代算法依赖系统模型计算值函数，而蒙特卡洛强化学习则将策略迭代中"基于模型的策略评估步�?，替换为"无模型的蒙特卡洛策略评估步骤"�?
> **核心转变**：从"知道环境如何工作"�?通过试错来学�?。这就是蒙特卡洛方法的革命性意义�?
### 三种 MC 算法对比

| 算法名称 | 核心特点 | 主要改进 |
| --- | --- | --- |
| **蒙特卡洛基础算法（MC Basic�?* | 最简单的基于蒙特卡洛的强化学习算�?| 将策略迭代算法中"基于模型的策略评估步�?，替换为"无模型的蒙特卡洛估计模块"。在样本足够多的前提下，可保证该算法收敛到最优策略和最优状态值�?|
| **蒙特卡洛探索起点算法（MC Exploring Starts�?* | MC Basic 的变�?| 通过�?MC Basic 中采�?首次访问策略（first-visit strategy�?�?每次访问策略（every-visit strategy�?，能更高效地利用样本�?|
| **蒙特卡洛 ε-贪婪算法（MC ε-Greedy�?* | MC Exploring Starts 的变�?| �?策略改进步骤"中，不再搜索"贪婪策略"，而是搜索"最�?ε-贪婪策略"。通过这种方式，策略的探索能力得到增强，从而可移除"探索起点"这一约束条件�?|

### 关键要点回顾

1. **无模�?vs 有模�?*：MC 方法不需要环境模型，只需�?episode 数据
2. **样本效率**：每次访问策略比初始访问策略更高�?3. **更新频率**：逐情节更新比批量更新更及�?4. **探索与利�?*：�?贪婪策略通过调整 ε 值来平衡探索和利�?5. **Step 级更�?*：MC ϵ-Greedy 采用反向遍历，每�?step 都会触发更新

> **记忆口诀**：MC = 无模�?+ 经验平均 + 探索利用平衡。从 MC Basic �?MC Exploring Starts �?MC ϵ-Greedy，每一步都在提升算法的实用性和效率�?
