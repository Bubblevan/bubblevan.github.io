---
id: rl-chapter-8
title: 第8章 价值函数方法
sidebar_label: 第8章
---



# Value Function Methods

## Value representation: From table to function

假设存在 $n$ 个状态 $\{s_i\}_{i=1}^n$，其状态值为 $\{v_{\pi}(s_i)\}_{i=1}^n$，其中 $\pi$ 是给定的策略。用 $\{\hat{v}(s_i)\}_{i=1}^n$ 表示真实状态值的估计值。若采用表格法，可将估计值存储在下表中。该表格可作为数组或向量存储在内存中，要检索或更新某个值，直接读取或改写表格中对应的条目即可。

| 状态（State） | $s_1$ | $s_2$ | $\cdots$ | $s_n$ |
|-------------|-------|-------|---------|-------|
| 估计值（Estimated value） | $\hat{v}(s_1)$ | $\hat{v}(s_2)$ | $\cdots$ | $\hat{v}(s_n)$ |

接下来我们将说明，上述表格中的值可通过一个函数来近似。具体而言，在下图

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-1.png)
</div>

中，$\{(s_i, \hat{v}(s_i))\}_{i=1}^n$ 被表示为 $n$ 个点，这些点可通过一条曲线进行拟合或近似。最简单的曲线是直线，其表达式可写为：

$$
\begin{aligned}
\hat{v}(s, w) = as + b = \underbrace{[s, 1]}_{\phi^T (s)} \underbrace{\begin{bmatrix} a \\ b \end{bmatrix}}_{w} = \phi^T (s)w \tag{8.1}
\end{aligned}
$$

其中，$\hat{v}(s, w)$ 是用于近似 $v_{\pi}(s)$ 的函数，由状态 $s$ 和参数向量 $w \in \mathbb{R}^2$ 共同决定，有时也记为 $\hat{v}_w(s)$；$\phi(s) \in \mathbb{R}^2$ 被称为状态 $s$ 的特征向量。

表格法与函数近似法的第一个显著区别，体现在值的检索方式和更新方式上。

- **值的检索方式**：当值以表格形式存储时，检索某个值只需直接读取表格中对应的条目；而当值以函数形式表示时，检索过程会稍复杂 —— 需将状态索引 $s$ 输入函数并计算函数值（见图 8.3）。以式（8.1）的示例为例，首先需计算特征向量 $\phi(s)$，再计算 $\phi^T(s)w$；若函数为人工神经网络，则需从输入到输出进行一次前向传播。

由于状态值的检索方式不同，函数近似法在存储效率上更具优势：表格法需存储 $n$ 个值，而函数近似法只需存储一个低维参数向量 $w$，因此存储效率可显著提升。但这种优势并非无代价 —— 函数可能无法准确表示所有状态值。例如，图 8.2 中的点无法用一条直线精确拟合，这也是该方法被称为 "近似法" 的原因。从本质上讲，用低维向量表示高维数据集时，必然会损失部分信息。因此，函数近似法是通过牺牲精度来换取存储效率的。

- **值的更新方式**：当值以表格形式存储时，更新某个值只需直接改写表格中对应的条目；而当值以函数形式表示时，更新方式完全不同 —— 必须通过更新参数 $w$ 来间接改变值。关于如何更新 $w$ 以得到最优状态值，后续会详细阐述。

得益于状态值的更新方式，函数近似法还具备另一项优势：泛化能力强于表格法。原因如下：采用表格法时，仅当某状态在一个回合中被访问时，其值才能被更新，未被访问状态的值无法更新；而采用函数近似法时，更新某状态值需先更新参数 $w$，且 $w$ 的更新会同时影响其他未被访问状态的值。因此，单个状态的经验样本可推广到其他状态，助力其值的估计。

上述分析可通过下图进一步说明（图中包含 3 个状态 $\{s_1, s_2, s_3\}$）：假设我们有状态 $s_3$ 的经验样本，并希望更新 $\hat{v}(s_3)$。

- **表格法**：如图 (a) 所示，仅能更新 $\hat{v}(s_3)$，$\hat{v}(s_1)$ 和 $\hat{v}(s_2)$ 保持不变；
- **函数近似法**：如图 (b) 所示，通过更新 $w$ 不仅能更新 $\hat{v}(s_3)$，还会改变 $\hat{v}(s_1)$ 和 $\hat{v}(s_2)$ 的值。

因此，状态 $s_3$ 的经验样本可助力其邻近状态值的更新。

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-2.png)
</div>

我们可使用比直线近似能力更强的复杂函数，例如二阶多项式：

$$
\begin{aligned}
\hat{v}(s, w) = as^2 + bs + c = \underbrace{[s^2, s, 1]}_{\phi^T (s)} \underbrace{\begin{bmatrix} a \\ b \\ c \end{bmatrix}}_{w} = \phi^T (s)w \tag{8.2}
\end{aligned}
$$

还可使用更高阶的多项式曲线拟合这些点：曲线阶数越高，近似精度可能越高，但参数向量的维度也会随之增加，进而需要更多的存储资源和计算资源。

需注意的是，无论是式（8.1）还是式（8.2）中的 $\hat{v}(s, w)$，其关于 $w$ 都是线性的（尽管关于 $s$ 可能是非线性的），这类方法被称为**线性函数近似法**，是最简单的函数近似法。

要实现线性函数近似，需选择合适的特征向量 $\phi(s)$—— 例如，需判断用一阶直线还是二阶曲线拟合这些点。选择合适的特征向量并非易事，它需要依赖对任务的先验知识：对任务的理解越深入，越能选择出更优的特征向量。例如，若已知图中的点大致分布在一条直线上，便可采用直线拟合；但在实际场景中，这类先验知识通常是未知的。若缺乏任何先验知识，一种常用的解决方案是采用**人工神经网络**作为非线性函数近似器。

另一重要问题是如何寻找最优参数向量：若已知 $\{v_{\pi}(s_i)\}_{i=1}^n$，则该问题可转化为最小二乘问题。通过优化以下目标函数，可得到最优参数：
> 最小二乘的核心思想特别简单：“让所有数据点的‘预测误差的平方和’最小”

$$
\begin{aligned}
J_1 &= \sum_{i=1}^n \left( \hat{v}(s_i, w) - v_{\pi}(s_i) \right)^2 = \sum_{i=1}^n \left( \phi^T(s_i)w - v_{\pi}(s_i) \right)^2 \\
&= \left\| \begin{bmatrix} \phi^T(s_1) \\ \vdots \\ \phi^T(s_n) \end{bmatrix} w - \begin{bmatrix} v_{\pi}(s_1) \\ \vdots \\ v_{\pi}(s_n) \end{bmatrix} \right\|_2^2 = \|\Phi w - v_{\pi}\|_2^2
\end{aligned}
$$

其中，$\Phi \triangleq \begin{bmatrix} \phi^T(s_1) \\ \vdots \\ \phi^T(s_n) \end{bmatrix} \in \mathbb{R}^{n \times 2}, \quad v_{\pi} \triangleq \begin{bmatrix} v_{\pi}(s_1) \\ \vdots \\ v_{\pi}(s_n) \end{bmatrix} \in \mathbb{R}^n$
> 右边的 2 范数平方（||・||₂²）：向量的 2 范数是 “各元素平方和的平方根”，所以 2 范数平方就是 “各元素平方和”—— 这里的向量是 “Φw - v_π”（预测值向量 - 真实值向量），其 2 范数平方就是所有状态的误差平方和，和左边完全等价。

可验证，该最小二乘问题的最优解为：$w^* = (\Phi^T \Phi)^{-1} \Phi v_{\pi}$

## TD learning of state values based on function approximation

### Objective function

设 $v_{\pi}(s)$ 和 $\hat{v}(s, w)$ 分别表示状态 $s \in S$ 的真实状态值和近似状态值。我们需要解决的问题是：找到最优参数 $w$，使得 $\hat{v}(s, w)$ 对每个状态 $s$ 都能最好地近似 $v_{\pi}(s)$。

具体而言，目标函数定义为：

$$
\begin{aligned}
J(w) = \mathbb{E}\left[(v_{\pi}(S) - \hat{v}(S, w))^2\right] \tag{8.3}
\end{aligned}
$$

其中，$w$ 是 "价值函数的参数向量"，比如对于线性近似场景，近似价值函数是 $\hat{v}(s,w) = w_1 \cdot x + w_2 \cdot y + w_3$（$x$、$y$ 是状态的行列索引），这里的 $w$ 就是向量 $w = [w_1, w_2, w_3]^T$（$w_1$、$w_2$ 是系数，$w_3$ 是偏置）。有了 $w$，输入一个状态的 $x=2$、$y=3$，就能算出 $\hat{v}(s,w) = 2w_1 + 3w_2 + w_3$；而对于神经网络场景，$w$ 就是神经网络里的 "所有权重和偏置"—— 比如输入层到隐藏层的权重矩阵、隐藏层到输出层的权重矩阵、各层的偏置向量，这些参数打包在一起就构成了向量 $w$。

期望 $\mathbb{E}[\cdot]$ 是基于随机变量 $S \in \mathcal{S}$ 计算的。尽管 $S$ 是随机变量，但其**概率分布是什么呢**？

定义 $S$ 的概率分布主要有以下两种方式：

**方式一：采用均匀分布**

即把所有状态视为同等重要，将每个状态的概率设为 $1/n$（$n$ 为状态总数）。此时，式（8.3）所示的目标函数可改写为：

$$
\begin{aligned}
J(w) = \frac{1}{n} \sum_{s \in \mathcal{S}} \left(v_{\pi}(s) - \hat{v}(s, w)\right)^2 \tag{8.4}
\end{aligned}
$$

该式表示所有状态近似误差的平均值。然而，这种方式**未考虑马尔可夫过程在给定策略下的真实动态** —— 由于某些状态可能被策略访问的频率很低，将所有状态视为同等重要可能并不合理。

**方式二：采用平稳分布**

平稳分布（stationary distribution）描述了马尔可夫决策过程的**长期行为**：具体而言，当智能体执行给定策略足够长的时间后，其处于任一状态的概率均可由该平稳分布描述。

设 $\{d_{\pi}(s)\}_{s \in S}$ 表示马尔可夫过程在策略 $\pi$ 下的平稳分布，即智能体长期执行策略后访问状态 $s$ 的概率为 $d_{\pi}(s)$。根据定义，所有状态的平稳分布概率之和满足 $\sum_{s \in S} d_{\pi}(s) = 1$。

此时，式（8.3）的目标函数可改写为：

$$
\begin{aligned}
J(w) = \sum_{s \in \mathcal{S}} d_{\pi}(s) \left(v_{\pi}(s) - \hat{v}(s, w)\right)^2 \tag{8.5}
\end{aligned}
$$

该式表示近似误差的**加权平均值**，其中访问概率更高的状态会被赋予更大的权重。

#### 马尔可夫决策过程（MDP）的平稳分布

要理解平稳分布，首先需明确其基础 —— **策略 $\pi$ 下的状态转移概率矩阵 $P_{\pi}$**：

- **定义**：$P_{\pi} \in \mathbb{R}^{n \times n}$（$n$ 为状态总数），矩阵元素 $[P_{\pi}]_{ij}$ 表示 "智能体在策略 $\pi$ 下，从状态 $s_i$ 一步转移到状态 $s_j$ 的概率"。
- **例**：若 $[P_{\pi}]_{12}=0.3$，则智能体从 $s_1$ 出发，一步到 $s_2$ 的概率是 30%。
- **关联背景**：文档提到其完整定义见 2.6 节，核心是 "策略 $\pi$ 决定了每个状态下的动作选择概率，进而结合环境转移规则（$p(r,s'|s,a)$）得到状态间的转移概率"。

**$P_{\pi}$ 的 $k$ 次幂（记为 $P_{\pi}^k$）描述 "多步转移概率"**，是理解长期行为的关键：

- **核心结论**：矩阵元素 $[P_{\pi}^k]_{ij}$ 表示 "智能体从 $s_i$ 出发，恰好经过 $k$ 步转移到 $s_j$ 的概率"，记为 $p_{ij}^{(k)} = \Pr(S_{t+k}=s_j | S_t=s_i)$。
- **直观解释**：
  - 当 $k=1$ 时，$P_{\pi}^1 = P_{\pi}$，$[P_{\pi}]_{ij}$ 就是一步转移概率（符合定义）；
  - 当 $k=2$ 时，$P_{\pi}^2 = P_{\pi} \times P_{\pi}$，其元素 $[P_{\pi}^2]_{ij} = \sum_{q=1}^n [P_{\pi}]_{iq} \cdot [P_{\pi}]_{qj}$。这是 "从 $s_i$ 先到中间状态 $s_q$（概率 $[P_{\pi}]_{iq}$），再从 $s_q$ 到 $s_j$（概率 $[P_{\pi}]_{qj}$）" 的联合概率，对所有中间状态 $s_q$ 求和后，就是 "两步从 $s_i$ 到 $s_j$ 的总概率"；
  - 同理，$k=3,4,\dots$ 时，$P_{\pi}^k$ 的元素对应 "恰好 $k$ 步转移概率"。
**平稳分布的本质是 "马尔可夫过程长期运行后，状态分布不再变化"**，文档通过 "初始分布→$k$ 步分布→极限分布→平稳条件" 逐步推导：

**1. 初始分布与 $k$ 步分布**

- **初始分布 $d_0$**：$d_0 \in \mathbb{R}^n$ 是初始时刻（$t=0$）的状态概率分布，例如 "从 $s_1$ 出发" 则 $d_0(s_1)=1$，其余元素为 0；
- **$k$ 步分布 $d_k$**：$d_k \in \mathbb{R}^n$ 是执行策略 $\pi$ $k$ 步后，智能体处于各状态的概率分布，满足：
  - **元素形式**：$d_k(s_i) = \sum_{j=1}^n d_0(s_j) \cdot [P_{\pi}^k]_{ji}$（从所有初始状态 $s_j$，经 $k$ 步转移到 $s_i$ 的概率之和）；
  - **矩阵形式**：$d_k^T = d_0^T \cdot P_{\pi}^k$（$d_k^T$ 表示行向量，便于矩阵乘法）。

**2. 极限分布：长期行为与初始状态无关**

当 $k$ 趋近于无穷大（即智能体执行策略足够久）时，若满足某些条件（后续会讲），则：

$$
\begin{aligned}
\lim_{k \to \infty} P_{\pi}^k = 1_n \cdot d_{\pi}^T
\end{aligned}
$$

其中：
- $1_n = [1,1,\dots,1]^T \in \mathbb{R}^n$（全 1 列向量）；
- $1_n \cdot d_{\pi}^T$ 是一个 $n \times n$ 矩阵，每行都等于 $d_{\pi}^T$（即所有行相同，均为平稳分布的行向量）。

将此代入 $k$ 步分布的矩阵形式，可得：

$$
\begin{aligned}
\lim_{k \to \infty} d_k^T = d_0^T \cdot \lim_{k \to \infty} P_{\pi}^k = d_0^T \cdot (1_n \cdot d_{\pi}^T) = d_{\pi}^T
\end{aligned}
$$

这里因 $d_0^T \cdot 1_n = 1$（初始分布概率和为 1），最终 $k$ 步分布收敛到 $d_{\pi}^T$。

这个收敛后的分布 $d_{\pi}$ 称为**极限分布**，其核心性质是：**与初始分布 $d_0$ 无关**—— 无论智能体从哪个状态出发，长期运行后，处于各状态的概率最终都会稳定到 $d_{\pi}$。

**3. 平稳条件：分布不再变化的数学表达**

若 $d_k$ 收敛到 $d_{\pi}$，则当 $k$ 足够大时，$d_k \approx d_{k-1} \approx d_{\pi}$。对 $k$ 步分布公式 $d_k^T = d_{k-1}^T \cdot P_{\pi}$ 两边取极限，可得：

$$
\begin{aligned}
d_{\pi}^T = d_{\pi}^T \cdot P_{\pi} \tag{8.10}
\end{aligned}
$$

这就是**平稳分布的核心条件**：

- **含义**：若当前状态分布是 $d_{\pi}$，执行一步策略 $\pi$ 后，分布仍保持为 $d_{\pi}$（"平稳" 即 "不变化"）；
- **线性代数视角**：$d_{\pi}$ 是转移矩阵 $P_{\pi}$ 对应特征值 1 的左特征向量（左特征向量满足 $\vec{v}^T \cdot M = \lambda \vec{v}^T$，此处 $\lambda=1$）；
- **约束**：平稳分布需满足 $\sum_{s \in S} d_{\pi}(s) = 1$（概率和为 1），且所有 $d_{\pi}(s) > 0$（后续解释原因）。
**并非所有马尔可夫过程都有唯一平稳分布**，"不可约（irreducible）" 和 "正则（regular）" 是关键条件，先明确基础定义：

**1. 基础概念：可达性与互通性**

- **可达性**：若存在有限步数 $k$，使得 $[P_{\pi}^k]_{ij} > 0$，则称 "状态 $s_j$ 可从 $s_i$ 到达"（智能体从 $s_i$ 出发，能在有限步内到 $s_j$）；
- **互通性**：若 $s_j$ 可从 $s_i$ 到达，且 $s_i$ 可从 $s_j$ 到达，则称 "$s_i$ 与 $s_j$ 互通"。

**2. 不可约马尔可夫过程**

- **定义**：所有状态两两互通（即从任意状态出发，都能在有限步内到达其他所有状态）；
- **例**：4 个格子状态（s1-s4），若从 s1 能到 s2、s3、s4，从 s2 也能到 s1、s3、s4，以此类推，则过程不可约。

**3. 正则马尔可夫过程**

- **定义**：存在某个正整数 $k$，使得 $P_{\pi}^k$ 的所有元素都大于 0（即任意两个状态之间，都能在 $k$ 步内到达，且概率为正）；
- **与不可约的关系**：
  - 正则过程一定是不可约的（所有元素正→所有状态互通）；
  - 不可约过程不一定是正则的（例如 "s1→s2→s3→s1" 的循环，$k=3$ 时 $P_{\pi}^3$ 对角元素为 1，其他为 0，不满足 "所有元素正"）；
- **例外**：若不可约过程中存在某个状态 $s_i$，满足 $[P_{\pi}]_{ii} > 0$（能自转移），则该过程是正则的（自转移可打破循环，确保有限步内到达所有状态）。

**4. 唯一性结论**

若马尔可夫过程是正则的（或满足更强条件），则其平稳分布唯一，且极限分布等于平稳分布（即 $\lim_{k \to \infty} d_k = d_{\pi}$）；正则过程的平稳分布满足 $d_{\pi}(s) > 0$（所有状态长期都有正概率被访问，无 "不可达" 或 "零概率" 状态）。
当策略 $\pi$ 确定后，MDP 就退化为一个马尔可夫过程，其长期行为由策略和环境模型共同决定。文档指出：

- **核心策略类型**：**探索性策略（如 $\epsilon$-贪婪策略）能产生正则马尔可夫过程**，进而有唯一平稳分布；
- **原因**：探索性策略在任意状态下，对所有动作都有正概率选择（例如 $\epsilon$-贪婪策略中，以 $\epsilon$ 概率随机选动作）。即使环境模型允许状态互通，这种 "全动作正概率" 也能确保：从任意状态出发，都能通过探索性动作到达其他所有状态，满足正则过程的 "所有元素正" 条件。
<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-3.png)
</div>
**平稳分布的两种计算方法：**

**1. 理论计算：解平稳条件 $d_{\pi}^T = d_{\pi}^T P_{\pi}$**

已知转移矩阵 $P_{\pi}^T$：

$$
\begin{bmatrix}
0.3 & 0.1 & 0.1 & 0 \\
0.1 & 0.3 & 0 & 0.1 \\
0.6 & 0 & 0.3 & 0.1 \\
0 & 0.6 & 0.6 & 0.8
\end{bmatrix}
$$

对应的单位右特征向量为 $[0.0463, 0.1455, 0.1785, 0.9720]^T$；对该特征向量进行 "归一化"（使其元素和为 1），得到理论平稳分布：

$$
\begin{aligned}
d_{\pi} = \begin{bmatrix} 0.0345 \\ 0.1084 \\ 0.1330 \\ 0.7241 \end{bmatrix}
\end{aligned}
$$

**含义**：长期运行后，智能体处于 s4（右下格）的概率约 72.41%，处于 s1（左上格）的概率仅 3.45%。

**2. 数值估计：通过长回合模拟**

- **方法**：从 s1 出发，执行策略 1000 步，统计每个状态的 "访问次数 / 总步数"，作为平稳分布的估计值；
- **结果**：模拟中各状态的访问比例，在数百步后逐渐收敛到理论值 $d_{\pi}$，验证了 "**极限分布 = 平稳分布**" 的结论。

**平稳分布的核心价值在于**：它描述了策略的 "**长期状态偏好**"，是后续定义 "加权误差目标函数" 的关键 —— 让访问频率高的状态在误差计算中占更大权重，更贴合实际交互场景。
 

### Optimization algorithm

为最小化式（8.3）中的目标函数 $J(w)$，可采用梯度下降算法：$w_{k+1} = w_k - \alpha_k \nabla_w J(w_k)$
> $\alpha_k$ 是 “步长”（学习率），控制每次更新的幅度

其中，梯度的计算过程如下：

$$
\begin{aligned}
\nabla_w J(w_k) &= \nabla_w \mathbb{E}\left[\left(v_{\pi}(S) - \hat{v}(S, w_k)\right)^2\right] \\
&= \mathbb{E}\left[\nabla_w \left(v_{\pi}(S) - \hat{v}(S, w_k)\right)^2\right] \\
&= 2\mathbb{E}\left[\left(v_{\pi}(S) - \hat{v}(S, w_k)\right)\left(-\nabla_w \hat{v}(S, w_k)\right)\right] \\
&= -2\mathbb{E}\left[\left(v_{\pi}(S) - \hat{v}(S, w_k)\right)\nabla_w \hat{v}(S, w_k)\right]
\end{aligned}
$$

因此，梯度下降算法可改写为：

$$
\begin{aligned}
w_{k+1} = w_k + 2\alpha_k \mathbb{E}\left[\left(v_{\pi}(S) - \hat{v}(S, w_k)\right)\nabla_w \hat{v}(S, w_k)\right] \tag{8.11}
\end{aligned}
$$

> 其中，$\alpha_k$ 前面的系数 2 可不失一般性地合并到 $\alpha_k$ 中。

式（8.11）所示的算法需要计算期望，也就是要遍历所有可能的状态 $S$，计算每个状态的 "$(v_{\pi}(S)-\hat{v}(S,w_k))\nabla_w \hat{v}(S,w_k)$"，再求平均。但实际场景中，状态空间可能非常大（比如有成千上万个状态），遍历所有状态计算期望不现实—— 这时候就需要按照**随机梯度下降**的思路，用 "单个随机样本" 的梯度，近似 "所有样本的平均梯度（期望）"。

比如在时刻 $t$，我们随机采样一个状态 $s_t$（比如智能体实际交互中遇到的状态），用 $s_t$ 的 "$(v_{\pi}(s_t)-\hat{v}(s_t,w_t))\nabla_w \hat{v}(s_t,w_t)$"，代替式（8.11）中 "所有状态的期望"。

此时式（8.11）变为：

$$
\begin{aligned}
w_{t+1} = w_t + \alpha_t \left(v_{\pi}(s_t) - \hat{v}(s_t, w_t)\right)\nabla_w \hat{v}(s_t, w_t) \tag{8.12}
\end{aligned}
$$

其中，$s_t$ 是时刻 $t$ 时状态 $S$ 的一个样本。
> 根据 “大数定律”，当采样的样本足够多时，“单个样本的梯度” 会逐渐逼近 “所有样本的平均梯度（期望）”—— 既降低了计算量，又能保证算法收敛（只要步长设置合理）。
需要注意的是，式（8.12）**不具备可实现性** —— 它需要用到真实状态值 $v_{\pi}$，而 $v_{\pi}$ 是未知的，必须通过估计得到。所以必须找一个 "近似值" 来代替 $v_{\pi}(s_t)$，本质都是 "用可观测的信息估计 $v_{\pi}(s_t)$"，使算法能够落地执行，具体可采用以下两种方法：

- **蒙特卡洛方法**：假设存在一个回合 $(s_0, r_1, s_1, r_2, \dots)$，令 $g_t$ 表示从状态 $s_t$ 出发的折扣回报，则可将 $g_t$ 作为 $v_{\pi}(s_t)$ 的近似值。此时，式（8.12）所示的算法变为：

$$
\begin{aligned}
w_{t+1} = w_t + \alpha_t \left(g_t - \hat{v}(s_t, w_t)\right)\nabla_w \hat{v}(s_t, w_t)
\end{aligned}
$$
蒙特卡洛的思路很直接：等一个回合结束，拿到从 $s_t$ 出发的 "完整累积奖励"，用它作为 $v_{\pi}(s_t)$ 的近似。
具体来说：假设一个回合的轨迹是 $(s_0, r_1, s_1, r_2, \dots, s_T)$（$s_T$ 是终止状态），从 $s_t$ 出发的 "折扣回报" $g_t$ 定义为：$g_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots + \gamma^{T-t} r_T$（$\gamma$ 是折扣因子，未来的奖励打折扣）。为什么能用 $g_t$ 近似 $v_{\pi}(s_t)$？因为 $v_{\pi}(s_t)$ 的定义就是 "从 $s_t$ 出发，遵循策略 $\pi$ 的长期折扣回报的期望"——$g_t$ 是 "一次实际回合中 $s_t$ 的真实累积回报"，是 $v_{\pi}(s_t)$ 的一个 "样本"，能近似它。
这就是**基于函数近似的蒙特卡洛学习算法**。

- **时序差分方法**：按照时序差分（TD）学习的思路，可将 $r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t)$ 作为 $v_{\pi}(s_t)$ 的近似值。此时，式（8.12）所示的算法变为：

$$
\begin{aligned}
w_{t+1} = w_t + \alpha_t \left[r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t)\right] \nabla_w \hat{v}(s_t, w_t) \tag{8.13}
\end{aligned}
$$
蒙特卡洛有个缺点：必须等 "整个回合结束" 才能计算 $g_t$，实时性差。时序差分（TD）的改进在于：不用等回合结束，每走一步就能用 **"当前看到的即时奖励 + 下一状态的近似值"** 估计 $v_{\pi}(s_t)$。
具体来说：用 "$r_{t+1} + \gamma \hat{v}(s_{t+1},w_t)$" 作为 $v_{\pi}(s_t)$ 的近似，这个值也叫 "TD 目标"。为什么能这样近似？根据贝尔曼方程，$v_{\pi}(s_t) = \mathbb{E}\left[r_{t+1} + \gamma v_{\pi}(s_{t+1})\right]$—— 真实状态值 $v_{\pi}(s_t)$ 等于 "即时奖励 + 下一状态真实值的折扣期望"。由于 $v_{\pi}(s_{t+1})$ 也未知，我们就用它的近似值 $\hat{v}(s_{t+1},w_t)$ 代替，得到 "$r_{t+1} + \gamma \hat{v}(s_{t+1},w_t)$"，作为 $v_{\pi}(s_t)$ 的近似。
这就是**基于函数近似的时序差分学习算法**。

#### TD learning of state values with function approximation

**Initialization**: A function $\hat{v}(s, w)$ that is differentiable in $w$. Initial parameter $w_{0}$.

**Goal**: Learn the true state values of a given policy $\pi$.

**For each episode** $\{(s_{t}, r_{t+1}, s_{t+1})\}_{t}$ generated by $\pi$, do:

**For each sample** $(s_{t}, r_{t+1}, s_{t+1})$, do:

- **In the general case**: $w_{t+1}=w_{t}+\alpha_{t}[r_{t+1}+\gamma \hat{v}(s_{t+1}, w_{t})-\hat{v}(s_{t}, w_{t})] \nabla_{w} \hat{v}(s_{t}, w_{t})$
- **In the linear case**: $w_{t+1}=w_{t}+\alpha_{t}[r_{t+1}+\gamma \phi^{T}(s_{t+1}) w_{t}-\phi^{T}(s_{t}) w_{t}] \phi(s_{t})$

 

### Selection of function approximators

要应用式（8.13）所示的时序差分算法，需选择合适的近似状态值函数 $\hat{v}(s, w)$，具体有以下两种方式：

**第一种方式是采用人工神经网络作为非线性函数近似器**：神经网络的输入为状态 $s$，输出为近似状态值 $\hat{v}(s, w)$，网络参数为 $w$。

**第二种方式是直接采用线性函数**：

$$
\begin{aligned}
\hat{v}(s, w) = \phi^T(s)w
\end{aligned}
$$

其中，$\phi(s) \in \mathbb{R}^m$ 是状态 $s$ 的特征向量，$\phi(s)$ 与 $w$ 的维度均为 $m$（通常远小于状态总数）。

在线性函数近似的情况下，近似函数关于 $w$ 的梯度为：$\nabla_w \hat{v}(s, w) = \phi(s)$

将其代入式（8.13），可得：

$$
\begin{aligned}
w_{t+1} = w_t + \alpha_t \left[r_{t+1} + \gamma \phi^T(s_{t+1})w_t - \phi^T(s_t)w_t\right] \phi(s_t) \tag{8.14}
\end{aligned}
$$

这就是**基于线性函数近似的时序差分学习算法**，简称 **TD-Linear 算法**。

在线性函数近似的情况下，算法的理论性质远比非线性函数近似更清晰易懂，但它的近似能力有限，且针对复杂任务选择合适的特征向量并非易事。相比之下，人工神经网络可作为 "**黑箱式通用非线性近似器**" 实现值的近似，使用起来更便捷。

对于赵老师这本书所涉及的简单网格世界任务，线性函数近似已足够解决问题；更重要的是，线性情况具有很强的包容性 —— **表格法可视为线性函数近似的一种特殊情况**，在接下来的专栏里面有讲解，这里直接跳过有缘再见。

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-4.png)
</div>

示例如上，给定策略在任意状态下选择任一动作的概率均为 0.2。我们的目标是估计该策略下的 **State Value**，总计需估计 25 个，真实 State Value 见上图右图，接下来我们将证明，**仅需使用少于 25 个参数即可近似这些状态值**。

**仿真设置如下**：
- 由给定策略生成 500 个回合，每个回合包含 500 步，且从服从均匀分布的随机状态-动作对开始；
- 在每次仿真实验中，参数向量 $w$ 随机初始化，其每个元素均从均值为 0、标准差为 1 的标准正态分布中采样；
- 设置奖励参数：禁止动作奖励 $r_{\text{forbidden}}$ = 边界动作奖励 $r_{\text{boundary}}$ = -1，目标动作奖励 $r_{\text{target}}$ = 1，折扣因子 $\gamma$ = 0.9。

要实现 **TD-Linear（基于线性函数近似的时序差分）算法**，首先需选择特征向量 $\phi(s)$，具体选择方式如下：
**一、基于多项式的特征向量**

在网格世界示例中，每个状态 $s$ 对应一个二维位置。设 $x$ 和 $y$ 分别表示状态 $s$ 的列索引和行索引。为避免数值问题，需对 $x$ 和 $y$ 进行归一化处理，使其取值范围落在 $[-1, +1]$ 区间内。为简化表述，归一化后的 $x$ 和 $y$ 仍用原符号表示。

**1. 简单线性特征向量（2 维 / 3 维）**

最简单的特征向量为：

$$
\begin{aligned}
\phi(s) = \begin{bmatrix} x \\ y \end{bmatrix} \in \mathbb{R}^2
\end{aligned}
$$

此时，近似状态值函数为：$\hat{v}(s, w) = \phi^T(s)w = [x, y] \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} = w_1x + w_2y$

当参数 $w$ 确定时，$\hat{v}(s, w) = w_1x + w_2y$ 表示一个过原点的二维平面。但真实状态值曲面可能不过原点，因此需为该二维平面引入偏置项以提升近似效果。

为此，我们采用如下 3 维特征向量：

$$
\begin{aligned}
\phi(s) = \begin{bmatrix} 1 \\ x \\ y \end{bmatrix} \in \mathbb{R}^3 \tag{8.15}
\end{aligned}
$$

此时，近似状态值函数为：$\hat{v}(s, w) = \phi^T(s)w = [1, x, y] \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix} = w_1 + w_2x + w_3y$

当参数 $w$ 确定时，该函数对应一个可能不过原点的平面。需注意，特征向量也可定义为 $\phi(s) = [x, y, 1]^T$，其元素顺序不影响结果。

采用式（8.15）所示特征向量的估计结果如下左图（a）所示。可见，估计得到的状态值构成一个二维平面：尽管随着回合数增加，估计误差会逐渐收敛，但由于二维平面的近似能力有限，误差无法降至零。

**2. 高维多项式特征向量（6 维 / 10 维）**

为提升近似能力，可增加特征向量的维度。例如，采用 6 维特征向量：

$$
\begin{aligned}
\phi(s) = [1, x, y, x^2, y^2, xy]^T \in \mathbb{R}^6 \tag{8.16}
\end{aligned}
$$

此时，近似状态值函数为：$\hat{v}(s, w) = \phi^T(s)w = w_1 + w_2x + w_3y + w_4x^2 + w_5y^2 + w_6xy$

该函数对应一个二次三维曲面。

我们还可进一步增加特征向量维度，例如采用 10 维特征向量：

$$
\begin{aligned}
\phi(s) = [1, x, y, x^2, y^2, xy, x^3, y^3, x^2y, xy^2]^T \in \mathbb{R}^{10} \tag{8.17}
\end{aligned}
$$

采用式（8.16）和式（8.17）所示特征向量的估计结果分别如下图（b）和下图（c）所示。可见，**特征向量的维度越高，对状态值的近似精度就越高**；但需注意，在上述三种情况下（3 维、6 维、10 维特征向量），由于这些线性近似器的近似能力仍存在局限，估计误差均无法收敛至零。

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-5.png)
</div>

**二、其他类型的特征向量**

除多项式特征向量外，还有**傅里叶基（Fourier basis）**、瓦片编码（tile coding）等多种特征类型。

以下以傅里叶基为例说明：首先，将每个状态的 $x$ 和 $y$ 值归一化至 $[0, 1]$ 区间，得到的傅里叶基特征向量为：

$$
\begin{aligned}
\phi(s) = \begin{bmatrix} \vdots \\ \cos\left(\pi(c_1x + c_2y)\right) \\ \vdots \end{bmatrix} \in \mathbb{R}^{(q+1)^2} \tag{8.18}
\end{aligned}
$$

其中：
- $\pi$ 表示圆周率（约为 3.1415…），而非策略符号；
- $c_1$ 和 $c_2$ 为用户指定整数 $q$ 范围内的任意整数（即 $c_1, c_2 \in \{0, 1, \dots, q\}$）；
- 由于 $(c_1, c_2)$ 共有 $(q+1)^2$ 种可能取值，因此特征向量 $\phi(s)$ 的维度为 $(q+1)^2$。

例如，当 $q = 1$ 时，特征向量为：

$$
\begin{aligned}
\phi(s) = \begin{bmatrix} \cos\left(\pi(0x + 0y)\right) \\ \cos\left(\pi(0x + 1y)\right) \\ \cos\left(\pi(1x + 0y)\right) \\ \cos\left(\pi(1x + 1y)\right) \end{bmatrix} = \begin{bmatrix} 1 \\ \cos(\pi y) \\ \cos(\pi x) \\ \cos\left(\pi(x + y)\right) \end{bmatrix} \in \mathbb{R}^4
\end{aligned}
$$

采用 $q = 1、2、3$ 的傅里叶基特征向量的估计结果如下图，三种情况下特征向量的维度分别为 4、9、16。可见，**特征向量的维度越高，对状态值的近似精度就越高**。

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-6.png)
</div>

### Theoretical Analysis

从 Objective Function 出发，为优化该目标函数，我们引入了式（8.12）所示的随机算法；随后，由于算法中的真实值函数未知，我们用近似值对其进行替换，最终得到了式（8.13）所示的 TD 算法。

接下来要做的就是数学部分，收敛性分析（Convergence analysis）、TD learning minimizes the projected Bellman error、Least-squares TD 这一块内容，咱第一遍过直接跳过，有缘再见。

## TD learning of action values based on function approximation
前一节介绍了状态值估计问题，本节则将介绍如何估计动作值。我们将表格型 Sarsa 算法与表格型 Q 学习算法扩展到价值函数近似的场景，这种扩展过程十分直观。
### Sarsa with function approximation

假设策略 $\pi$ 下的动作值 $q_{\pi}(s, a)$ 由近似函数 $\hat{q}(s, a, w)$ 表示，将式（8.13）中的 $\hat{v}(s, w)$ 替换为 $\hat{q}(s, a, w)$，可得：

$$
\begin{aligned}
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t) \tag{8.35}
\end{aligned}
$$

对式（8.35）的分析与式（8.13）类似，此处不再赘述。

当采用线性函数近似时，动作值近似函数可表示为：

$$
\begin{aligned}
\hat{q}(s, a, w) = \phi^T(s, a)w
\end{aligned}
$$

其中，$\phi(s, a)$ 是状态-动作对 $(s, a)$ 的特征向量。此时，近似函数关于参数 $w$ 的梯度为 $\nabla_w \hat{q}(s, a, w) = \phi(s, a)$。

式（8.35）中的值估计步骤可与策略改进步骤相结合，以学习最优策略，具体流程总结于算法 8.2 中。
#### Sarsa with function approximation

**Initialization**: Initial parameter $w_0$. Initial policy $\pi_0$. $\alpha_t = \alpha > 0$ for all $t$. $\epsilon \in (0, 1)$.

**Goal**: Learn an optimal policy that can lead the agent to the target state from an initial state $s_0$.

**For each episode**, do:

1. Generate $a_0$ at $s_0$ following $\pi_0(s_0)$
2. If $s_t$ ($t = 0, 1, 2, \dots$) is not the target state, do:
   - Collect the experience sample $(r_{t+1}, s_{t+1}, a_{t+1})$ given $(s_t, a_t)$: generate $r_{t+1}, s_{t+1}$ by interacting with the environment; generate $a_{t+1}$ following $\pi_t(s_{t+1})$.
   - **Update q-value**:
     $$
     w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t)
     $$
   - **Update policy**:
     $$
     \pi_{t+1}(a|s_t) = \begin{cases}
     1 - \frac{\epsilon}{|A(s_t)|}(|A(s_t)| - 1) & \text{if } a = \arg\max_{a \in A(s_t)} \hat{q}(s_t, a, w_{t+1}) \\
     \frac{\epsilon}{|A(s_t)|} & \text{otherwise}
     \end{cases}
     $$
   - $s_t \leftarrow s_{t+1}$, $a_t \leftarrow a_{t+1}$

 
需注意的是，要准确估计给定策略下的动作值，需多次执行式（8.35）；但在算法 8.2 中，每次切换到策略改进步骤前，仅执行一次式（8.35）的更新 —— 这一点与表格型 Sarsa 算法一致。此外，算法 8.2 的实现目标是 "从指定初始状态找到一条通往目标状态的较优路径"，因此无法为所有状态找到最优策略；但如果有充足的经验数据，可轻松调整该实现流程，使其能为所有状态找到最优策略。

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-7.png)
</div>

上图给出了一个示例，该示例的任务是 "找到一条从左上状态出发、能引导智能体到达目标状态的较优策略"。从图中可看出，每回合的总奖励与回合长度均会逐渐收敛到稳定值。在该示例中，线性特征向量选用 5 阶傅里叶函数，傅里叶特征向量的表达式可参考式（8.18）。

### Q-Learning with function approximation

表格型 Q 学习算法同样可扩展到函数近似场景，其参数更新规则为：

$$
\begin{aligned}
w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \max_{a \in A(s_{t+1})} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t) \tag{8.36}
\end{aligned}
$$

上述更新规则与式（8.35）类似，唯一区别在于：将式（8.35）中的 $\hat{q}(s_{t+1}, a_{t+1}, w_t)$ 替换为 "对下一状态 $s_{t+1}$ 所有可能动作 $a$ 取 $\hat{q}(s_{t+1}, a, w_t)$ 的最大值"（即 $\max_{a \in A(s_{t+1})} \hat{q}(s_{t+1}, a, w_t)$）。

与表格型 Q 学习类似，式（8.36）可通过 **on-policy 或 off-policy** 两种方式实现，其在线策略版本总结于算法 8.3 中。
#### Algorithm 8.3: Q-learning with function approximation (on-policy version)

**Initialization**: Initial parameter $w_0$. Initial policy $\pi_0$. $\alpha_t = \alpha > 0$ for all $t$. $\epsilon \in (0, 1)$.

**Goal**: Learn an optimal path that can lead the agent to the target state from an initial state $s_0$.

**For each episode**, do:

If $s_t$ ($t = 0, 1, 2, \dots$) is not the target state, do:

1. Collect the experience sample $(a_t, r_{t+1}, s_{t+1})$ given $s_t$: generate $a_t$ following $\pi_t(s_t)$; generate $r_{t+1}, s_{t+1}$ by interacting with the environment.
2. **Update q-value**:
   $$
   w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \max_{a \in A(s_{t+1})} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t)
   $$
3. **Update policy**:
   $$
   \pi_{t+1}(a|s_t) = \begin{cases}
   1 - \frac{\epsilon}{|A(s_t)|}(|A(s_t)| - 1) & \text{if } a = \arg\max_{a \in A(s_t)} \hat{q}(s_t, a, w_{t+1}) \\
   \frac{\epsilon}{|A(s_t)|} & \text{otherwise}
   \end{cases}
   $$

 

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-8.png)
</div>

上图给出了在线策略版本的示例，该示例的任务同样是 "找到从左上状态出发、引导智能体到达目标状态的较优策略"。基于线性函数近似的 Q 学习算法能成功学习到最优策略。该示例中选用的线性特征向量为 5 阶傅里叶基函数，其离线策略版本将在 8.4 节介绍深度 Q 学习时进一步演示。

读者可能会注意到，在算法 8.2 和算法 8.3 中，尽管 "值" 以函数形式表示，但策略 $\pi(a|s)$ 仍以表格形式存储 —— 因此，这两种算法仍假设状态和动作的数量是有限的。在第 9 章中我们将看到，策略也可以表示为函数形式，从而能够处理连续的状态空间和动作空间。

## Deep Q-learning (DQN)
深度 Q 学习是最早且最成功的深度强化学习算法之一。值得注意的是，这里的神经网络未必需要很深：对于像本书中网格世界这样的简单任务，带有 1~2 个隐藏层的浅层网络可能就足够了。

### Algorithm description

从数学角度来看，深度 Q 学习的目标是最小化以下目标函数：

$$
\begin{aligned}
J = \mathbb{E}\left[ \left( R + \gamma \max_{a \in A(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w) \right)^2 \right] \tag{8.37}
\end{aligned}
$$

其中，随机变量 $(S, A, R, S')$ 分别表示状态、动作、即时奖励和下一状态。

该目标函数可视为**贝尔曼最优误差的平方**，原因如下：对所有状态 $s$ 和动作 $a$，贝尔曼最优方程为：

$$
\begin{aligned}
q(s, a) = \mathbb{E}\left[ R_{t+1} + \gamma \max_{a \in A(S_{t+1})} q(S_{t+1}, a) \mid S_t = s, A_t = a \right]
\end{aligned}
$$

因此，当 $\hat{q}(S, A, w)$ 能准确近似最优动作值时，在期望意义下，$R + \gamma \max_{a \in A(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w)$ 应等于 0。

为最小化式（8.37）中的目标函数，可采用梯度下降算法。为此，需要计算目标函数 $J$ 对参数 $w$ 的梯度。需注意的是，参数 $w$ 不仅出现在 $\hat{q}(S, A, w)$ 中，还出现在 $y \triangleq R + \gamma \max_{a \in A(S')} \hat{q}(S', a, w)$ 中，因此梯度计算并不简单。

为简化计算，我们假设 $y$ 中的 $w$ 在短时间内固定不变，这样梯度计算会大幅简化。具体而言，我们引入两个网络：

- **主网络（main network）**：表示动作值近似函数 $\hat{q}(s, a, w)$，参数为 $w$；
- **目标网络（target network）**：表示动作值近似函数 $\hat{q}(s, a, w_T)$，参数为 $w_T$。

此时，目标函数可改写为：

$$
\begin{aligned}
J = \mathbb{E}\left[ \left( R + \gamma \max_{a \in A(S')} \hat{q}(S', a, w_T) - \hat{q}(S, A, w) \right)^2 \right]
\end{aligned}
$$

当 $w_T$ 固定时，$J$ 对 $w$ 的梯度为：

$$
\begin{aligned}
\nabla_w J = -\mathbb{E}\left[ \left( R + \gamma \max_{a \in A(S')} \hat{q}(S', a, w_T) - \hat{q}(S, A, w) \right) \nabla_w \hat{q}(S, A, w) \right] \tag{8.38}
\end{aligned}
$$

其中，为不失一般性，省略了部分常数系数。

#### 深度 Q-Learning 的两个关键问题与改进
假设我们用 "深度网络直接替换线性近似" 做 Q-Learning，会发生什么？

打砖块游戏的状态是 "游戏画面"（高维，比如 $210 \times 160$ 像素），动作是 "左移、右移、发射"。我们用一个 3 层神经网络（输入：画面像素，隐藏层：200 个神经元，输出：3 个动作的 Q 值）代替线性特征，直接按 Q-Learning 的规则更新：

**更新公式（无经验回放 + 无双网络）：**

$$w_{t+1} = w_t + \alpha \cdot \left[ r_{t+1} + \gamma \max_{a} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \cdot \nabla_w \hat{q}(s_t, a_t, w_t)$$

此时会遇到两个致命问题，而这两个问题在 "线性近似 Q-Learning" 中几乎不存在。

**问题 1：样本序列相关性太强，深度网络学 "偏" 了（为什么需要经验回放）**

**线性 Q-Learning 为什么没问题？**

比如之前的网格世界，状态是 "$(x,y)$ 坐标"（低维），每次探索的状态序列是 "$(1,1) \to (1,2) \to (2,2) \to \dots$"，状态之间的关联性弱（比如 $(1,1)$ 和 $(2,2)$ 的特征差异大），即使在线更新（拿到样本就更参数），模型也不会学偏。

**深度网络 + 打砖块游戏：没有经验回放会怎样？**

游戏中，连续 10 帧画面的状态几乎一样（比如球在左边慢慢移动，连续 10 帧都是 "球在左半屏"），此时我们拿到的样本序列是：

$$(s_1,a_1,r_2,s_2) \to (s_2,a_2,r_3,s_3) \to \dots \to (s_{10},a_{10},r_{11},s_{11})$$

其中 $s_1$ 到 $s_{10}$ 的画面高度相似（都是球在左半屏），对应的 "最优动作" 都是 "左移"（让挡板接住球）。如果直接用这些 "连续相似样本" 在线更新：每次更新都会让网络认为 "左移动作的 Q 值应该更大"，参数 $w$ 会一直往 "强化左移" 的方向调整；即使后来球到了右半屏，网络也会因为之前的 "过度训练左移"，依然优先选左移，导致挡板接不到球 —— 这就是 "样本序列相关性" 导致的模型学 "偏"，训练永远无法收敛到最优策略。

**经验回放怎么解决这个问题？**

经验回放就像一个 "样本仓库"：我们把每次游戏的经验 $(s,a,r,s')$ 都存进仓库（比如存 10000 个样本）；每次更新网络时，不直接用 "刚拿到的样本"，而是从仓库里随机抽 100 个样本（mini-batch）来更新；这 100 个样本可能包含 "球在左半屏""球在右半屏""球快落地" 等各种场景，样本之间的关联性被打破，网络能学到 "不同场景下的最优动作"，不会再学偏。比如抽中的样本里，既有 "左移接左球"，也有 "右移接右球"，网络会均衡调整参数，最终学会 "根据球的位置选动作"—— 这就是经验回放的核心作用：打破样本序列相关性，让样本分布更接近目标函数要求的 "均匀分布"。

**问题 2：目标值跟着参数 "跑"，深度网络训练 "震荡"（为什么需要双网络）**

Q-Learning 的更新核心是 "用目标值（$r+\gamma\max_a \hat{q}(s',a,w)$）减去当前 Q 值，得到误差，再调整参数"。关键在于：目标值的计算依赖参数 $w$。

**线性 Q-Learning 为什么没问题？**

线性模型的参数少（比如傅里叶基 6 维），每次更新 $w$ 的幅度很小，目标值（$r+\gamma\max_a \phi^T(s',a)w$）的变化也很小，误差不会剧烈波动，训练能稳定收敛。

**深度网络 + 打砖块游戏：没有双网络会怎样？**

深度网络的参数多（比如 200 个神经元的隐藏层，仅输入到隐藏层的权重就有 "$210 \times 160 \times 200$" 个），每次更新 $w$ 的幅度可能很大，导致 "目标值跟着 $w$ 一起剧烈变化"—— 我们称之为 "目标值震荡"。

举个具体数值例子（假设 $\gamma=0.9$）：

- **t 时刻**：参数 $w_t$，计算 $s'$ 的 max Q 值是 $\hat{q}(s',a_{max},w_t)=4$，目标值 = $r + \gamma \times 4 = 1 + 0.9 \times 4 = 4.6$；当前 Q 值 $\hat{q}(s_t,a_t,w_t)=3$，误差 = $4.6-3=1.6$，用这个误差更新 $w_t \to w_{t+1}$；
- **t+1 时刻**：参数 $w_{t+1}$ 已经变了，再算 $s'$ 的 max Q 值，可能变成 $\hat{q}(s',a_{max},w_{t+1})=5$，目标值 = $1 + 0.9 \times 5=5.5$；当前 Q 值 $\hat{q}(s_{t+1},a_{t+1},w_{t+1})=3.5$，误差 = $5.5-3.5=2$，又用这个误差更新 $w_{t+1} \to w_{t+2}$；
- **循环往复**：每次更新后，目标值都跟着 $w$ 变高，误差永远无法缩小，网络参数在 "追着波动的目标值" 震荡，永远收敛不了 —— 就像你要打一个移动的靶子，但靶子的位置会因为你开枪的动作而改变，永远打不准。

**双网络怎么解决这个问题？**

双网络相当于 "把靶子固定一段时间"：

- **主网络（参数 $w$）**：负责 "实时学习"—— 用经验回放的样本更新，每次迭代都调整 $w$，输出当前的 Q 值 $\hat{q}(s,a,w)$；
- **目标网络（参数 $w_T$）**：负责 "提供稳定的目标值"—— 初始时 $w_T = w$，之后每 1000 次迭代才同步一次 $w_T = w$（固定一段时间），目标值用 $w_T$ 计算：$y_T = r + \gamma \max_a \hat{q}(s',a,w_T)$；

再看刚才的例子：

- **t 时刻**：$w_T =$ 1000 次前的 $w$，计算目标值 = $1 + 0.9 \times 4=4.6$（$w_T$ 固定，目标值不变）；用误差 $1.6$ 更新主网络 $w \to w_{t+1}$，但 $w_T$ 还是老参数，下次目标值还是 $4.6$；
- **t+1 到 t+999 时刻**：每次更新主网络 $w$，目标值都用固定的 $w_T$ 计算（一直是 $4.6$），误差会逐渐缩小（比如 t+100 次时，当前 Q 值变成 $4.5$，误差 = $0.1$）；
- **t+1000 时刻**：同步 $w_T =$ 当前的 $w$，目标值更新为新的稳定值（比如 $5.0$），之后继续用这个固定目标值训练；

这样一来，目标值不再 "实时波动"，网络能朝着一个稳定的方向学习，误差会逐渐收敛到零 —— 这就是双网络的核心作用：固定目标值的计算参数，避免目标值震荡，稳定训练。
 

要利用式（8.38）的梯度最小化目标函数，需重点关注以下两项技术：

**技术 1：双网络架构（主网络 + 目标网络）**

如计算式（8.38）梯度时所提及，深度 Q 学习需同时维护主网络和目标网络，具体实现细节如下：

- 设主网络参数为 $w$，目标网络参数为 $w_T$，初始时将两者设置为相同值；
- 每次迭代中，从回放缓冲区（replay buffer，后续将详细说明）中抽取一小批量（mini-batch）样本 $\{(s, a, r, s')\}$；
- 主网络的输入为状态 $s$ 和动作 $a$，输出 $y = \hat{q}(s, a, w)$ 为估计的 Q 值；
- 输出的目标值定义为 $y_T \triangleq r + \gamma \max_{a \in A(s')} \hat{q}(s', a, w_T)$（即基于目标网络计算的目标 Q 值）；
- 更新主网络的参数 $w$，以最小化所有样本 $\{(s, a, y_T)\}$ 的 TD 误差（也称为损失函数）$\sum (y - y_T)^2$。

需要注意的是，主网络参数 $w$ 的更新并非显式使用式（8.38）的梯度，而是依赖现有的神经网络训练工具。因此，与式（8.38）中基于单个样本更新主网络不同，训练网络需要一小批量样本 —— 这是**深度强化学习算法与非深度强化学习算法的一个显著区别**。

主网络在每次迭代中都会更新，而目标网络则会每间隔一定迭代次数就与主网络同步一次参数（即令 $w_T = w$），以此满足计算式（8.38）梯度时 "$w_T$ 固定" 的假设。

**技术 2：经验回放（Experience Replay）**

经验回放技术的核心是：收集到经验样本后，不按样本的采集顺序使用，而是将其存储在一个名为 "回放缓冲区"（replay buffer）的数据集里。

具体而言，设 $(s, a, r, s')$ 为一个经验样本，$B \triangleq \{(s, a, r, s')\}$ 为回放缓冲区；每次更新主网络时，从回放缓冲区中抽取一小批量样本，且抽取过程需遵循均匀分布（这种抽取过程也称为 "经验回放"）。

**为什么深度 Q 学习需要经验回放？为什么回放必须遵循均匀分布？**

答案藏在式（8.37）的目标函数中：要合理定义该目标函数，必须明确随机变量 $S, A, R, S'$ 的概率分布。其中，一旦给定 $(S, A)$，$R$ 和 $S'$ 的分布由系统模型决定；而描述状态-动作对 $(S, A)$ 分布的最简单方式，就是假设其服从均匀分布。

但在实际场景中，状态-动作样本是根据行为策略（behavior policy）按序列生成的，样本序列存在相关性，无法保证服从均匀分布。为满足 "均匀分布" 假设，需打破样本间的序列相关性 —— 通过从回放缓冲区中均匀抽取样本，恰好能实现这一点。这就是**经验回放必不可少、且必须遵循均匀分布的数学原因**。

随机抽样的另一个好处是，每个经验样本可被多次使用，能提高数据利用效率，这在数据量有限时尤为重要。

#### Algorithm 8.3: Deep Q-learning (off-policy version)

**Initialization**: A main network and a target network with the same initial parameter.

**Goal**: Learn an optimal target network to approximate the optimal action values from the experience samples generated by a given behavior policy $\pi_b$.

1. Store the experience samples generated by $\pi_b$ in a replay buffer $B = \{(s, a, r, s')\}$
2. **For each iteration**, do:
   - Uniformly draw a mini-batch of samples from $B$
   - For each sample $(s, a, r, s')$, calculate the target value as $y_T = r + \gamma \max_{a \in A(s')} \hat{q}(s', a, w_T)$, where $w_T$ is the parameter of the target network
   - Update the main network to minimize $(y_T - \hat{q}(s, a, w))^2$ using the mini-batch of samples
   - Set $w_T = w$ every $C$ iterations

 
<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-9.png)
</div>

上图示例的目标是学习所有状态-动作对的最优动作值 —— 一旦得到最优动作值，就能立即得到最优贪婪策略。

- **（a）所示的行为策略生成一个回合**，该行为策略具有探索性：即在任意状态下选择任一动作的概率均相同。
- 如图（b）所示，该回合仅包含 1000 步；尽管步数较少，但由于行为策略的探索能力较强，该回合中几乎访问了所有状态-动作对。
- **回放缓冲区 replay buffer** 中存储了这 1000 个经验样本，**mini-batch** 大小设为 100，即每次获取训练样本时，从回放缓冲区中均匀抽取 100 个样本。

**主网络与目标网络具有相同的结构**：均为包含 1 个隐藏层（100 个神经元）的神经网络（层数和神经元数量可根据任务调整）。该神经网络包含 3 个输入和 1 个输出：前两个输入是状态的归一化行索引和归一化列索引，第三个输入是归一化的动作索引（此处 "归一化" 指将数值转换到 $[0,1]$ 区间）；网络的输出为动作值的估计值。

我们将输入设计为状态的行、列索引而非状态编号，原因在于网格中的每个状态对应一个二维位置 —— **在网络设计中利用的状态信息越多，网络性能通常越好**。此外，神经网络还可采用其他设计方式，例如：将其设计为 2 个输入和 5 个输出，其中 2 个输入为状态的归一化行、列索引，5 个输出为该输入状态对应的 5 个动作的估计值。

- （d）所示，损失函数（定义为每个小批量样本的平均平方 TD 误差）最终收敛到零，这表明网络能很好地拟合训练样本；
- （e）所示，状态值估计误差也收敛到零，说明最优动作值的估计已足够准确，此时对应的贪婪策略即为最优策略。

该示例充分体现了**深度 Q 学习的高效性**：仅需一个 1000 步的短回合，就能学到最优策略；相比之下，如图所示，表格型 Q 学习需要一个包含 100000 步的回合才能实现这一目标。深度 Q 学习高效的原因有两点：一是**函数近似法具有较强的泛化能力**，二是**经验样本可被重复利用**。

接下来，我们通过 "经验样本较少" 的场景，刻意对深度 Q 学习算法提出挑战。下图给出了一个仅包含 100 步的回合示例，在该场景中：尽管网络仍能 "良好训练"（表现为损失函数收敛到零），但状态值估计误差无法收敛到零。这表明网络虽能拟合给定的经验样本，但由于样本数量过少，无法准确估计所有状态-动作对的最优动作值。

<div style={{textAlign: 'center'}}>
  ![](/img/rl/f8-10.png)
</div>
## Summary

本章继续介绍时序差分（TD）学习算法，但核心是将价值表示方式从表格法切换到函数近似法。理解函数近似法的关键在于：它本质是一个优化问题 —— 最简单的目标函数是 "真实状态值与估计值的平方误差"，此外还有**贝尔曼误差**、**投影贝尔曼误差**等其他目标函数。我们已证明，**TD-Linear（线性函数近似 TD 算法）实际上最小化的是投影贝尔曼误差**，并介绍了多种基于价值近似的优化算法（如基于函数近似的 Sarsa、Q 学习）。

价值函数近似法之所以重要，核心原因之一是它使得人工神经网络能够与强化学习结合 —— 例如，**深度 Q 学习就是最早且最成功的深度强化学习算法之一**。尽管神经网络已被广泛用作非线性函数近似器，但本章仍全面介绍了线性函数近似的情况：**充分理解线性情况，是更好地掌握非线性情况的基础**。
