---
id: rl-chapter-1
title: 第1章 Basic Concepts
sidebar_label: 第1章
---

## 1、Basic Concepts

![Grid World Example](/img/rl/f1-2.png)

以这个网格迷宫为例。

### 1.1 State and Action

状态是描述智能体相对于环境所处状态的核心概念，在网格世界示例中，状态直接对应智能体（机器人）的位置。由于网格包含 9 个可区分的单元格，因此存在 9 个状态，分别记为 $s_1, s_2, ..., s_9$。

$S=\{s_1,...,s_9\}$表示所有状态的集合，即**状态空间**。

针对每个状态，智能体可执行 5 种可能的动作：向上移动（$a_1$）、向右移动（$a_2$）、向下移动（$a_3$）、向左移动（$a_4$）和原地不动（$a_5$）（如图 1.3 (b) 所示），所有动作的集合称为动作空间，用符号 $A = \{a_1, ..., a_5\}$ 表示。

（这里简化了，比如 $s_1$ 的不能做出 $a_1$、$a_4$ 的动作，但是我们权当所有状态都有全局动作空间一致）

### 1.2 State Transittion

当智能体在某一状态下执行特定动作时，会从当前状态转移到另一状态，这一过程称为状态转移。以网格世界为例，若智能体处于状态 $s_1$ 并选择动作 $a_2$（向右移动），则会转移到状态 $s_2$，该过程可表示为 $s_1 \stackrel{a_2}{\to} s_2$。

- 两类特殊情况：
    - 撞上forbidden cell，反弹回原状态
    - 超出网格边界，反弹回原状态

状态转移过程可通过表格直观描述：

![State Transition Table](/img/rl/table1.png)

但也可通过条件概率描述：例如在 $s_1$ 执行 $a_2$ 时，转移到 $s_2$ 的概率为 1，转移到其他状态的概率为 0，即 $p(s_2 | s_1, a_2) = 1$，$p(s_i | s_1, a_2) = 0$（$i \neq 2$）。这种转移概率为 1 或 0 的情况称为**确定性转移**，而现实中可能存在随机转移（如网格中有阵风时，执行 $a_2$ 可能被吹到 $s_5$ 而非 $s_2$，此时 $p(s_5 | s_1, a_2) > 0$），但这本书举的网格迷宫示例中均简化为确定性转移。

### 1.3 Policy

策略是指导智能体在每个状态下选择动作的规则。

从数学角度，策略用条件概率 $\pi(a | s)$ 描述，其含义是 "在状态s下选择动作a的概率"，且对任意状态s，所有动作的概率和满足 $\sum_{a \in A(s)} \pi(a | s) = 1$。策略分为两种类型：确定性策略和随机性策略。

### 1.4 Reward

智能体在某一状态下执行动作后，会从环境中获得一个奖励值r，该奖励是状态s和动作a的函数，因此也记为 $r(s, a)$。奖励值可为正、负或零，其核心作用是引导智能体的行为：正奖励鼓励智能体重复对应动作，负奖励则抑制该动作。

从数学角度，奖励过程可通过条件概率 $p(r | s, a)$ 描述，例如在 $s_1$ 执行 $a_1$ 时，获得 $r = -1$ 的概率为 1，即 $p(r = -1 | s_1, a_1) = 1$，$p(r \neq -1 | s_1, a_1) = 0$，这属于确定性奖励过程；现实中也存在随机奖励（如学生努力学习后，考试成绩为正奖励，但具体分数不确定），此时 $p(r | s, a)$ 会包含多个正概率的奖励值。

### 1.5 Trajectories, Returns, and Episodes

![Trajectory Example](/img/rl/f1-6.png)

（1）**轨迹**：智能体遵循策略与环境交互时，生成的状态 - 动作 - 奖励链式序列：

$$s_1 \xrightarrow[a_2, r=0]{} s_2 \xrightarrow[a_3, r=0]{} s_5 \xrightarrow[a_3, r=0]{} s_8 \xrightarrow[a_2, r=1]{} s_9$$

（2）**回报（又称总奖励、累积奖励）**：轨迹中所有奖励的总和，用于定量评估策略的优劣。

```
return_a = 0+0+0+1 = 1
return_b = 0+(-1)+0+1 = 0
```

对比两个策略的回报（1 > 0），可直接判断左策略更优。

上述回报计算适用于有限长度轨迹，而当轨迹为无限长，直接求和会导致回报发散，为此需引入折扣回报的概念，将未来奖励按折扣率 $\gamma$（$\gamma \in (0,1)$）进行衰减：

$$\text{discounted return} = r_0 + \gamma r_1 + \gamma^2 r_2 + \gamma^3 r_3 + ...$$

（等比数列求和）

折扣率 $\gamma$ 的核心作用有两点：一是解决无限轨迹的回报发散问题；二是调整对近 / 远期奖励的重视程度 —— $\gamma$ 接近 0 时，智能体更关注即时奖励（短视）；$\gamma$ 接近 1 时，智能体更重视远期奖励（远视，愿意承担短期负奖励以获取长期收益）。

（3）**情节**：指包含终止状态的有限轨迹，对应的任务称为 **episodic 任务**（如网格迷宫中到达 $s_9$ 即视为一次情节结束）。

若任务无终止状态，轨迹持续无限长，则称为 **continuing 任务**（如持续运行的机器人导航）。

两类任务可通过对终止状态的处理实现统一：一种方式是将终止状态设为吸收状态（如 $s_9$ 的动作空间仅为 $a_5$，或所有动作均转移回 $s_9$），智能体进入后不再离开；另一种方式（本书采用）是将终止状态视为普通状态，其动作空间与其他状态一致，智能体可离开但会因 "返回目标可获正奖励" 而逐渐学会停留。需注意，若 continuing 任务中存在持续正奖励，必须通过折扣回报避免求和发散。

### 1.6 Markov Decision Processes, MDPs
前面通过网格世界示例引入的状态、动作、转移、策略、奖励等概念，可通过马尔可夫决策过程（MDP）进行形式化统一：
（1）**核心集合**
- 状态空间：所有可能状态的集合，记为S；
- 动作空间：每个状态 $s \in S$ 对应的可用动作集合，记为 $A(s)$（本书中 $A(s) = A$）；
- 奖励集：每个状态 - 动作对\((s, a)\)对应的可能奖励集合，记为\(R(s, a)\)。

（2）**环境模型（Dynamics）**
- 状态转移概率：在状态s执行动作a，转移到状态 $s'$ 的概率，记为 $p(s' | s, a)$，且对任意 $(s, a)$ 满足 $\sum_{s' \in S} p(s' | s, a) = 1$（所有可能转移的概率和为 1）；
- 奖励概率：在状态s执行动作a，获得奖励r的概率，记为 $p(r | s, a)$，且对任意 $(s, a)$ 满足 $\sum_{r \in R(s, a)} p(r | s, a) = 1$。

（3）**策略**
$\pi(a | s)$ 的定义与 1.4 节一致，即状态s下选择动作a的概率，且满足 $\sum_{a \in A(s)} \pi(a | s) = 1$

（4）**马尔可夫性（Markov Property）**

未来状态和奖励仅依赖当前状态与动作，与历史状态 / 动作无关：

$$p(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = p(s_{t+1} | s_t, a_t)$$

$$p(r_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = p(r_{t+1} | s_t, a_t)$$
![马尔科夫过程](/img/rl/f1-7.png)

区分 MDP 与马尔可夫过程（Markov Process, MP）：当 MDP 中的策略 $\pi$ 固定后，状态转移概率可表示为 $p(s' | s) = \sum_{a \in A(s)} \pi(a | s) p(s' | s, a)$，此时 MDP 退化为仅包含状态转移的 MP；若 MP 为离散时间且状态数有限 / 可数，则称为马尔可夫链（Markov Chain）。

### 1.7 Summary

强化学习的本质是 "智能体 - 环境交互闭环"：智能体（如网格中的机器人）感知当前状态、依据策略选择动作、通过执行器作用于环境；环境则根据动作产生新状态和奖励，智能体通过 "解释器" 接收新状态与奖励，进而更新策略，形成循环。

