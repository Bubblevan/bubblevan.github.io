# 导航算法的全景解析

导航算法可以按照从底层感知到高层决策的层次结构进行划分。

## 第一层：感知与定位（Perception & Localization）

**核心问题**："我在哪？"以及"我周围的世界长什么样？"

### SLAM (同步定位与地图构建)

这是导航的基石。

**主流SLAM方案**：

- **ORB-SLAM3**：目前最先进的视觉**SLAM**（同步定位与建图）方案，支持单目、双目、**RGB-D**和惯性传感器，能在复杂环境下保持高精度
- **Cartographer**：由**Google**开源的激光**SLAM**框架，擅长构建大规模的**2D/3D**地图，在室内移动机器人中应用极广
- **VINS-Mono**：一种单目视觉惯性系统，通过融合摄像头和**IMU（惯性测量单元）**的数据，在剧烈运动或弱光环境下依然表现鲁棒
- **语义SLAM (Semantic SLAM)**：在定位的同时对物体进行识别（如：那是桌子，那是人），为机器人提供"带标签"的地图
- **多传感器融合 (EKF/UKF)**：利用**扩展卡尔曼滤波**或**无迹卡尔曼滤波**，将轮式里程计、**IMU**和**GPS**数据融合，修正单一传感器的漂移误差

**例子**：就像你在黑夜里拿着手电筒进入一个陌生房间，你一边观察家具（特征点），一边在脑海中画出房间的草稿。

**工具**：利用激光雷达或视觉信息构建地图。

### 观测数据 ($O_t$)

包含以下信息：

- **彩色图像 ($V_t$)**：机器人的"眼睛"看到的画面
- **深度图 ($D_t$)**：提供距离信息
- **机器人当前的位姿 ($P_t$)**：记录机器人在三维空间中的坐标和朝向

**深度图的作用**：它告诉机器人前方是一堵墙（障碍物）还是一段平坦的路（可行驶区域）。

## 第二层：全局规划（Global Planning）

**核心任务**：解决"从A到B的最优路径是什么？"。它通常在已知地图上寻找一条不考虑动态障碍物的粗略轨迹。

**主流算法**：

- **$A^*$ 算法**：最经典的启发式搜索算法，通过估算从起点到终点的成本，快速找到最短路径
- **Dijkstra**：**$A^*$**的基石，确保找到绝对最短路径，虽然计算量在大型地图中较大，但逻辑最严密
- **RRT (Rapidly-exploring Random Tree)**：采样搜索算法，特别适合高维空间和复杂障碍物环境，能够不断优化路径直至收敛到最优
- **PRM (Probabilistic Roadmap)**：首先在地图上随机采样点并建立连接，形成"路线网"，之后在网中快速查询路径
- **Theta\***：**$A^*$**的改进版，它允许路径不局限于网格的网格线，能生成更加平滑、更符合物理实际的对角线路径

## 第三层：局部规划与避障（Local Planning & Obstacle Avoidance）

**核心任务**：解决"如何安全通过？"。它根据实时的传感器数据，调整速度和转向，躲避行人或突然出现的障碍物。

这是做**社交导航（Social Navigation）**最核心的关注点。全局路径只是个参考，局部规划负责实时修正。

**主流算法**：

- **DWA (Dynamic Window Approach)**：在速度空间内进行采样，模拟机器人未来一小段时间的轨迹，选择一条得分最高且不碰撞的路径
- **TEB (Timed Elastic Band)**：将路径看作一条橡皮筋，障碍物产生斥力，目标点产生引力，动态优化轨迹的时间戳，使运动极其平滑
- **APF (人工势场法)**：将目标设为引力源，障碍物设为斥力源。虽然容易陷入局部最优，但在简单避障任务中非常高效
- **MPC (模型预测控制)**：考虑机器人的动力学约束（如惯性、最大转弯半径），通过求解优化问题预报未来轨迹
- **RL-based Avoidance (基于强化学习)**：利用**PPO**等算法在虚拟环境训练，使机器人在拥挤人群中学会"侧身"或"等待"等复杂的拟人化避障行为

## 第四层：指令对齐与决策（Embodied AI Layer）

**核心任务**：解决"我要做什么？"。这是最高层，负责将人类的自然语言指令转化为导航目标，并进行逻辑推理。

这是**InstructNav**论文最核心的贡献点，它将人类语言转化为机器人能理解的数学概率。

**主流方法**：

- **VLN (Vision-Language Navigation)**：典型例子如**R2R**数据集任务。机器人需要理解"走过厨房，在那个蓝色花瓶左转"，将视觉特征与语言指令对齐
- **SayCan**：由**Google**提出，利用**LLM（大模型）**拆解复杂任务。例如"我把可乐洒了"，**LLM**会拆解为：寻找抹布 → 导航到洗手间 → 回到洒水处
- **ObjectNav (目标驱动导航)**：机器人不再去往特定坐标，而是去寻找"特定的物体"。例如"去帮我拿个苹果"，机器人需具备识别苹果并在未探索区域搜索的能力
- **VLA Models (Vision-Language-Action)**：如**RT-2**。它直接将视觉和语言输入映射为机器人的动作序列，模糊了感知与规划的边界
- **Social Navigation (社交导航)**：机器人需要理解人类的社交规则（如：不从两个交谈的人中间穿过），这需要对人类意图进行高阶建模

### 多源价值地图 (Multi-sourced Value Maps)

**InstructNav**的核心创新，将多种信息源融合成一张地图：

- **语义价值 ($m_s$)**："那里看起来像个沙发"
- **动作价值 ($m_a$)**："指令说要左转"
- **轨迹价值 ($m_t$)**："刚才走过那了，别回去了"
- **直觉价值 ($m_i$)**："**GPT-4V** 觉得那边更可能有出口"

## 总结

这四个层次构成了完整的导航系统：

1. **感知层**：回答"我在哪"和"周围什么样"
2. **全局规划层**：回答"大方向往哪走"
3. **局部规划层**：回答"下一步怎么走"
4. **决策层**：回答"如何理解指令并做出决策"

每一层都依赖前一层的信息，共同实现从自然语言指令到物理动作执行的完整闭环。




