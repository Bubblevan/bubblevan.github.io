---
title: "VLA"
weight: 1
---

# 视觉语言动作模型 (Vision-Language-Action Models)

**VLA (Vision-Language-Action)** 是具身智能领域的核心研究方向，旨在让机器人能够理解视觉和语言指令，并执行相应的动作。

## 核心概念

VLA 模型将**视觉感知**、**语言理解**和**动作执行**统一在一个框架下，使机器人能够：
- 理解自然语言指令（如"把杯子放到桌子上"）
- 通过视觉观察环境状态
- 生成并执行相应的动作序列

## 学习内容

### [ACT: Action Chunking with Transformers](./vla/act)

**ACT** 是 VLA 领域的经典工作，提出了基于 Transformer 的动作分块算法，解决了模仿学习中的误差累积问题。

