---
title: "Locomotion 文献精读（七）Learning Agile and Dynamic Motor Skills for Legged Robots"
---

> https://arxiv.org/abs/1901.08652
> Science Robotics 2019

ETH 2019 的经典工作，发表在 Science Robotics，婆罗门中的婆罗门。

属于是“强化学习做足式 sim2real”的标杆之一，不靠花里胡哨的感知（那个时候外感知确实没有2025年的好），而是把 **仿真保真度** 和 **执行器建模** 这两块先补齐，再让策略去学“敏捷动作”。

## 研究背景

2019 年前后，足式机器人的主流路线大致有三类困境（这里按“工程可落地”的角度来理解）：
- **模块化控制**：把问题拆成“落脚点规划 → 足端轨迹 → 关节跟踪（PID/PD）”，能跑，但要靠工程师 **数月手工调参**，而且很难把速度、灵活性拉上去
- **轨迹优化**：理论很优雅，但足式的 **间歇接触** 会把问题搞得非常硬（接触时序/接触点未知），要么近似得厉害，要么算不动
- **强化学习**：需要海量试错，真实机上代价高且风险大，于是很多工作停在仿真（甚至大规模并行RL的Isaac还没出来）；但仿真和现实之间的差距（动力学、执行器、接触）又会让策略一上机就崩
### 主流方法 1：模块化控制器设计
原理：把控制拆成“foothold 规划 → 足端轨迹 → 关节跟踪（PID/PD）”等解耦子模块，每个模块基于模板动力学（例如“质点 + 无质量肢体”）或经验规则生成参考，再通过大量手工调参把整条 pipeline 拼起来。

成果：在不少场景里确实能做出很强的效果（比如崎岖地形移动、一些动态步态）。

局限：
- **性能天花板**：为了可解，模型一定会简化；简化越多，可用状态域越小，最后表现往往变成“能走但不敢快、不敢猛”
- **开发不经济**：每换一个机器人/动作，就得重新设计和调参，工程周期很长，而且经验不可复用

所以说，这篇不是在“再做一个更强的 RL”，而是先回答一个更现实的问题——**怎么让 RL 在真实足式上稳定工作**。它的答案是把迁移最容易翻车的两块补上：**高保真的接触刚体仿真** + **用 Actuator Net 学出 SEA 执行器的真实动态**，然后再用 RL（TRPO）去学策略，最后做到“基本零调参直接上机”。
### 主流方法 2：轨迹优化方法
原理：上层用刚体动力学 + 数值优化做规划（算一条“最优”轨迹/力/接触时序），下层再去跟踪这条规划结果。
核心局限：
- **接触建模太硬**：足式的接触点/接触时序天然是离散事件，问题很容易变成“你不先假设，就很难算”
- **实时性压力大**：要在线优化就需要很强算力；降精度又会直接牺牲性能
- **规划-跟踪耦合弱**：规划假设跟踪能跟上，但实际遇到欠驱动/意外接触时，跟踪误差会反过来破坏规划

## 方法论

论文的方法本质是：**先把仿真“尽量做得像”，再在仿真里把策略“尽量练得强”，最后直接部署到波士顿动力的 ANYmal**。流程可以按 4 步理解：

![](/paper/anymal-agile-overview.png)
*整体训练流程。先做物理参数辨识与不确定性建模，再训练执行器网络拟合执行器/软件动态，最后在前两步的模型上训练策略并直接上机。*

![](/paper/anymal-agile-sim.png)
*仿真中训练控制器的闭环。策略网络输出关节位置目标，执行器网络基于关节历史状态把“位置目标 + 历史”映射为关节扭矩，扭矩驱动刚体仿真得到下一状态。*

### 1. 机器人状态（Robot State）：输入信息的来源
框架的所有决策都基于 “机器人当前及历史状态”，分为整体状态和关节级状态两类：
- **整体状态**：用广义坐标 $q$ 和广义速度 $u$ 描述，对应机器人的 “宏观运动状态”，包括：
  机身姿态（body pose，如翻滚、俯仰角）、机身运动（body twist，如线速度、角速度）；
  全局位置、环境接触状态（如脚与地面是否接触）。
- **关节级状态**：针对 ANYmal 的 12 个关节（每条腿 3 个：髋外展 / 内收 HAA、髋屈 / 伸 HFE、膝屈 / 伸 KFE），核心是 2 个关键指标：
  关节速度 $\dot{\phi}$：关节当前的转动速度；
  关节位置误差 $\phi-\phi^*$：关节 “当前实际位置 $\phi$” 与 “策略网络输出的目标位置 $\phi^*$” 的差值（反映关节是否偏离控制目标）。

### 2. 关节状态历史（Joint State History）：动态特性的 “记忆载体”
由于执行器（如 SEA）存在通信延迟（硬件 / 软件层） 和机械响应时间（如弹簧形变延迟），仅用 “当前关节状态” 无法准确建模其动态。因此框架引入 “关节状态历史”：
- **缓存内容**：最近 3 个时间步的关节速度 $\dot{\phi}$ 和关节位置误差 $\phi-\phi^*$（对应时间点：当前 $t$、前 0.01s $t-0.01$、前 0.02s $t-0.02$）；
- **核心作用**：为执行器网络提供 “时间维度的动态信息”，使其能学习到执行器的延迟特性（如 “当前扭矩输出依赖前 0.02s 的位置误差”），避免仿真中忽略延迟导致的策略失效。

### 3. 策略网络（Policy Net）：“决策大脑”—— 从状态到控制目标
策略网络是 “学习控制逻辑的核心”，本质是一个多层感知机（MLP），功能是 “根据机器人状态生成关节的目标位置”，具体细节如下：
- **输入**：两部分信息的融合
  - 当前整体状态（机身姿态、运动指令 command，如 “向前 0.5m/s”）；
  - 关节状态历史（最近 3 步的速度和位置误差）；
  *注：加入关节历史是为了让策略网络 “预见关节动态”，比如 “当前位置误差过大时，提前调整目标位置以避免后续震荡”。*
- **网络结构**：2 个隐藏层，分别含 256 个和 128 个节点，激活函数用 tanh（选择原因：tanh 是有界函数，输出范围 [-1,1]，可避免生成过大的关节目标位置，防止仿真中关节超限导致机器人 “摔倒”）；
- **输出**：12 个关节的位置目标 $\phi^*$（而非直接输出扭矩）：
  论文选择 “位置目标” 而非 “扭矩” 的关键原因：位置控制更稳定（初始训练时机器人不易因扭矩突变摔倒），且通过后续执行器网络可间接转换为符合真实执行器特性的扭矩。

### 4. 执行器网络（Actuator Net）：“动态转换器”—— 从目标到真实扭矩
执行器网络是解决 “现实差距” 的核心创新，功能是 “模拟真实执行器的动态特性，将关节位置目标转换为实际可用的关节扭矩”，细节如下：
- **设计背景**：ANYmal 的 SEA 执行器结构复杂（含电机、减速器、弹簧、内部 PID 控制器），传统解析模型（如 Gehring et al. 2016 的模型）需近百个参数且无法准确建模延迟和非线性，因此论文用 “数据驱动的网络” 替代解析模型；
- **输入**：关节状态历史（与策略网络共享，确保动态一致性） + 策略网络输出的位置目标 $\phi^*$；
- **网络结构**：3 个隐藏层，每个含 32 个节点，激活函数用 softsign（选择原因：计算效率比 tanh 高，且同样能提供平滑映射 —— 测试显示，12 个关节的网络推理仅需 12.2μs，远快于 tanh 的 31.6μs，且验证误差相当）；
- **输出**：12 个关节的控制扭矩 $\tau$（直接输入刚体仿真，驱动机器人运动）。

### 5. 刚体仿真（Rigid-body Simulation）：“环境交互与状态更新”
刚体仿真模块是 “策略训练的虚拟环境”，功能是 “基于执行器网络输出的扭矩，计算机器人下一个时刻的状态”，核心特性如下：
- 采用论文团队此前提出的硬接触求解器（Hwangbo et al. 2018）：能准确模拟 “脚与地面碰撞”“关节内部接触” 等不连续接触 dynamics（如 Coulomb 摩擦锥约束），避免传统仿真中 “接触平滑化” 导致的动态失真；
- 计算效率极高：普通桌面机上每秒可处理 90 万个时间步，支撑 “训练需数亿样本” 的强化学习需求（论文中训练一次策略仅需 4-11 小时，单 CPU+GPU 即可完成）。

### 框架数据流闭环（按时间步迭代）
1. **状态感知**：刚体仿真输出机器人当前的 “整体状态（$q,u$）” 和 “关节级状态（$\dot{\phi},\phi$）”；
2. **历史缓存**：将当前关节速度 $\dot{\phi}$ 和 “当前位置 $\phi$ - 上一步目标位置 $\phi^*$” 存入 “关节状态历史”，保留最近 3 个时间步的数据；
3. **策略决策**：策略网络输入 “当前整体状态 + 关节状态历史”，输出 12 个关节的新位置目标 $\phi^*$；
4. **扭矩转换**：执行器网络输入 “关节状态历史 + 新位置目标 $\phi^*$”，输出符合真实执行器动态的关节扭矩 $\tau$；
5. **状态更新**：刚体仿真接收扭矩 $\tau$，计算下一个时刻的机器人状态（如机身移动、关节转动、接触状态变化），回到步骤 1，开始下一个时间步的迭代。

### ANYmal 四足机器人
硬件特点：中型犬尺寸（32kg，腿长 55cm），12 个串联弹性执行器（SEA）。SEA 的好处是抗冲击、可测扭矩，但坏处也很直接：**动力学更复杂、延迟/摩擦更明显、解析建模非常痛苦**，这也是本文把 Actuator Net 放在核心位置的原因之一。

对比一些常被拿来对照的平台（这里更像是论文的“时代背景”）：
- 波士顿动力液压平台：能量密度高，但体积/噪音大，更偏户外
- MIT Cheetah：高速高效，但当时在户外适应性/续航上公开信息有限
- SpotMini：电驱、室内外通用，但细节公开较少
> 注意这个机器狗，它会贯穿ETH这个工作后续四篇顶会顶刊里

### 刚体动力学建模
这里我觉得作者抓住了足式 sim2real 的一个关键：**接触不能太假**，但**仿真又不能太慢**（否则 RL 根本练不起来）。

- **硬接触求解器**：采用作者之前的工作 [41]，基于库仑摩擦锥约束做硬接触（具体解释有点麻烦，简单说就是不把接触“抹平”成光滑力），能更像真实地呈现防滑、冲击响应
- **仿真效率**：普通台式机上每秒可生成约 90 万时间步，比实时快约 1000 倍，足够支撑数亿级样本训练
- **鲁棒性增强（随机化）**：CAD 给出的惯性参数会偏（线缆/电子元件没建模），论文用 30 种 ANYmal 变体训练策略：质心位置（±2cm）、连杆质量（±15%）、关节位置（±2cm）加均匀噪声，避免策略过拟合到单一“理想模型”
### 执行器网络（Actuator Net）
如果执行器动力学没建对，策略很容易在仿真里学到一套“仿真专用技巧”，一上机就炸。

ANYmal 的 12 个 SEA 做的事就是**把“关节指令 → 扭矩响应”这段最难写解析模型的动态，直接用监督学习拟合出来**。

1. 技术方案（怎么学）
- **输入/输出**：输入关节位置误差 + 速度历史，输出关节扭矩（把控制延迟、电机阻尼、摩擦等都“包”进去）
- **数据采集**：用正弦轨迹驱动 ANYmal，让足部反复接触/离地；再手动扰动机器人，采 12 个关节的误差/速度/扭矩数据（400Hz，约 4 分钟拿到 100 万+样本）
- **网络结构**：3 层 MLP（每层 32 单元），激活用 softsign（推理更快：12 关节约 12.2μs）

2. 核心效果（学得像不像）
- 验证集扭矩预测误差约 0.74Nm（接近扭矩传感器分辨率 0.2Nm），明显优于“理想执行器模型”
- 即便在真实运动（如 1.6m/s 高速奔跑）里，误差仍约 0.966Nm，说明它确实抓住了 SEA 的关键动态，从而缩小 **仿真-现实差距**

![](/paper/anymal-agile-valid.png)
*执行器网络的验证结果。预测扭矩与实测扭矩几乎重合；“理想模型”（零延迟、零机械响应时间）反而偏差明显。*
### TRPO（Trust Region Policy Optimization）

TRPO 对超参数不敏感（默认设置就能跑），而且在这种 **高维连续控制**（12 关节 + 多状态维度）上优化更稳。再加上前面快仿真，论文里提到 4 小时能处理 2.5 亿个状态转换，最长训练也在 11 小时量级。

我觉得这篇在策略接口上也做了两个很关键的工程选择：

#### 观测（Ot）
尽量只用真实机上“可靠能测”的量，同时把任务相关信息塞进去：

\[ o_k = \langle \phi_g,\ r_z,\ v,\ \omega,\ \phi,\ \dot{\phi},\ \Theta,\ a_{k-1},\ C \rangle \]  

| 观测组件       | 含义                                  | 作用（为什么需要）                                                                 |
|----------------|---------------------------------------------------|-----------------------------------------------------------------------------------|
| \(\phi_g\)     | IMU坐标系中的重力单位向量，IMU只能直接测量“2个自由度的姿态”（如前后倾斜、左右倾斜），无法直接测“第三个自由度（如绕竖直轴的旋转）”，导致完整姿态无法直接获取，所以利用“重力向量”间接表示姿态——IMU能测量重力在自身坐标系中的方向（如“站立时重力沿IMUz轴向下，跌倒时重力沿IMUz轴向前”），这个方向可表示为一个**单位向量\(\phi_g\)**                          | 替代直接姿态测量，反映机身是否倾斜（如“重力向量向前=机身前倾”）                   |
| \(r_z\)        | 基座高度（腿部运动学+卡尔曼滤波估计）真实机器人无法直接测量“机身离地面的高度”，所以是假设地形平坦（多数训练场景满足），通过**腿部运动学模型**推算——比如“已知每条腿的关节角度，可计算脚掌到髋关节的距离，再结合地面接触状态，就能估算机身高度”；同时用**1D卡尔曼滤波**平滑估算结果              | 控制抬脚高度，避免机身磕地或抬脚过高浪费能量                                       |
| \(v\) / \(\omega\) | 机身线速度/角速度                                  | 判断是否符合指令（如“指令向前0.5m/s，实际是否达到”），调整运动强度                 |
| \(\phi\) / \(\dot{\phi}\) | 关节角度/关节速度                                  | 控制关节运动，避免关节超限（如“膝关节弯曲角度不能超过2.45rad”）                   |
| \(\Theta\)     | 关节状态历史（\(t_{k-0.01s}\)和\(t_{k-0.02s}\)的\(\phi/\dot{\phi}\)） | 检测接触（如“脚落地时关节速度突然下降”），弥补无测力传感器的缺陷                   |
| \(a_{k-1}\)    | 上一步的动作（上一步输出的关节位置目标）            | 让策略有“记忆”，避免动作突变（如“上一步让髋关节前摆，下一步不能突然后摆”）         |
| \(C\)          | 任务指令（如“向前1m/s、侧向0m/s、转身0rad/s”）     | 让策略根据指令调整行为（如“指令加速时，增大关节运动幅度”）                         |

#### 动作（at）
策略输出 **关节位置目标**（而不是直接输出扭矩），再用固定 PD 增益转成扭矩（`k_p=50Nm/rad`，`k_d=0.1Nm·s/rad`）：

\[
\tau = K_p(\phi^{*}-\phi) + K_d(\dot{\phi}^{*}-\dot{\phi})
\]

这样做在训练初期会更稳（不至于像纯扭矩控制那样一开始就频繁摔），而且策略可以利用“位置误差”来间接塑形接触力，不会被传统轨迹跟踪式 PD 的框架绑死。

另一个细节是“课程式调权”：用课程因子 `k_c`（从 0.3 逐步增到 1）去调节扭矩/关节速度惩罚的权重——先让策略学会动起来，再去抠动作自然性与能耗，避免一开始就陷入“最省电就是不动”的局部最优。

论文将机器人的控制问题转化为**离散时间的强化学习问题**，明确了“智能体（机器人）、环境（仿真）、奖励（任务目标）”的交互逻辑，确保训练可量化、可优化：

#### 离散时间交互模型（每一步的流程）
在每个时间步\(t\)，机器人与环境的交互遵循固定循环：  
1. 环境输出观测\(o_t\)（即前文的\(o_k\)，包含姿态、关节状态、指令等）；  
2. 机器人根据策略\(\pi(a_t | O_t)\)选择动作\(a_t\)（这里的动作是“关节位置目标\(\phi^*\)”）；  
3. 环境根据动作反馈奖励\(r_t\)（奖励是“任务目标的量化”，如“跟随速度指令得正奖，跌倒得负奖”）；  
4. 进入下一个时间步\(t+1\)，重复上述流程。  

其中，\(O_t = \langle o_t, o_{t-1}, ..., o_{t-h} \rangle\)是“最近h步的观测历史”——让策略有“短期记忆”，能应对动态变化（如“前一步被推了，下一步需要调整平衡”）。


#### 目标函数：最大化贴现奖励总和
强化学习的目标是找到最优策略\(\pi^*\)，让“无限时间内的贴现奖励总和”最大：  
\[ \pi^{*}=\underset{\pi}{argmax} \mathbb{E}_{\tau(\pi)}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right] \]  
关键参数解释：  
- \(\gamma \in (0,1)\)：表示“未来奖励的重要性”：  
   - 速度控制/高速奔跑：`γ=0.9988`（半衰期 5.77s），更偏长视野，站姿更自然
   - 跌倒恢复：`γ=0.993`（半衰期 4.93s），更偏短视野，优先快速翻身
- \(\tau(\pi)\)：策略\(\pi\)生成的“轨迹分布”——即“用这个策略，机器人可能走出的所有运动轨迹”，期望\(\mathbb{E}\)表示“在所有可能轨迹中，奖励总和的平均值最大”。


## 实验
### 任务 1：指令条件下的运动
该任务是足式机器人的基础需求 —— 根据高层指令（前向 / 侧向速度、偏航率）调整步态，需验证策略的鲁棒性、精度和效率。
#### 1. 实验设计
- 定性测试：用操纵杆发送随机指令，同时对机器人主体施加外部推扰，观察是否稳定跟随；
- 定量测试：每 2 秒发送 1 个随机指令（持续 30 秒，共 15 次转换），记录速度误差、关节扭矩、机械功率；
- 对比对象：传统最优方法（Bellicoso et al. [12]），采用其唯一能达 1.0m/s 的 “飞行 trot” 步态，及最高效的 “动态侧向行走” 步态；
- 消融实验：用 “理想执行器模型”（假设无延迟、无误差）和 “解析执行器模型”（手工调参的传统模型）训练策略，验证 Actuator Net 的必要性。
#### 2. 关键结果
不仅“跟得准”，还“跑得省”，并且步态是自己长出来的（不是人手工写死的）。

| 评估维度 | 本文（RL + Actuator Net） | 传统方法（Bellicoso [12]） | 核心差异 |
| --- | --- | --- | --- |
| 速度跟踪精度 | 平均线速度误差 0.143m/s；偏航率误差 0.174rad/s | 线速度误差 0.231m/s；偏航率误差 0.278rad/s | 误差显著降低，尤其在低速与高速端都更稳 |
| 能耗效率 | 平均扭矩 8.23Nm；机械功率 78.1W | 扭矩 11.7Nm；功率 97.3W | 同样任务下更省力、更省电 |
| 步态自适应 | 低速自动变“步行 trot”；高速变“飞行 trot” | 需要手工指定步态 | **无需预定义步态**，能自然切换 |
| 消融（执行器模型） | 稳定行走，无明显抖动 | 理想/解析模型：容易每步跌倒、肢体抖动 | Actuator Net 是 sim2real 的关键补丁 |
#### 3. 核心结论
本文方法在 “精度 + 效率” 上全面超越传统方法：
- 精度优势源于策略能 “端到端优化”（从传感器到控制信号），避免传统模块化控制器的 “近似误差累积”；
- 效率优势源于策略学到 “10-15 度更直的膝盖标称姿态”，而传统方法因怕跌倒不敢调整此姿态；
A- ctuator Net 是迁移成功的核心 —— 传统执行器模型无法精准建模 SEA 的延迟和非线性，导致策略在真实机器人上失效。

![](/paper/anymal-agile-quan.png)
*指令条件下的定量评估与步态可视化。包含步态相位、速度跟踪误差，以及功率/扭矩等指标与传统控制器的对比。*

### 任务 2：高速运动
该任务验证方法能否 “利用硬件全部性能”——ANYmal 的设计目标是鲁棒性而非速度，传统方法难以突破 1.2m/s 的记录，需测试策略能否达硬件极限（扭矩 40Nm，关节速度 12rad/s）。
#### 1. 实验设计
- 发送 1.6m/s 的阶跃速度指令，让机器人跑 10 米后减速至 0，记录实际速度、关节扭矩 / 速度、步态模式；
- 对比基准：ANYmal 此前的速度记录（1.2m/s，Bellicoso [12] 的飞行 trot）。
#### 2. 关键结果
- 速度突破：真实机器人最高达 1.5m/s（比此前记录提升 25%），仿真中达 1.58m/s；
- 硬件利用：策略完全用到硬件极限 —— 关节扭矩达 40Nm（最大值），关节速度达 12rad/s（最大值）；
- 独特步态：学到 “长腾空 + 不对称腾空时间” 的飞行 trot（非自然界常见步态，但接近最优解），如图 3D 所示；
- 传统方法局限：传统规划模块无法 “预知硬件约束”，输出的指令常超出执行器能力，导致无法达高速。

![](/paper/anymal-agile-eva-high-speed.png)
*高速运动评估。包含前向速度、关节速度/扭矩以及对应的步态相位变化。*
3. 核心结论
本文方法能 “主动探索硬件极限”，生成传统手工设计无法实现的最优步态 —— 传统控制器因依赖 “预定义步态 + 解析模型”，无法突破性能瓶颈，而 RL 策略通过试错能充分利用硬件潜力。

### 任务 3：跌倒恢复
该任务是足式机器人的 “鲁棒性关键”—— 传统方法需手工设计恢复轨迹（耗时且仅适用于简单姿态），而动物的恢复依赖 “动态多接触点协调”，需验证策略能否学到此类复杂行为。
####1. 实验设计
- 初始姿态：将 ANYmal 置于 9 种随机姿态，包括 “倒扣”（几乎完全颠倒）、“腿部自接触”（靠自身腿部支撑）等极端场景；
- 评估指标：是否成功翻转直立、恢复时间；
- 传统方法对比：手工调参的轨迹回放（商用机器人常用）、简单三连杆机器人的恢复（Morimoto et al. [50]），均无法扩展到复杂四足系统。
#### 2. 关键结果
- 100% 恢复成功率：所有 9 种姿态均成功翻转直立，恢复时间 < 3 秒；
- 动态行为特征：策略能协调所有肢体，利用动量实现 “快速翻转”（如倒扣时先蹬地产生力矩，再调整接触点），如图 4 所示；
- 传统方法无法实现：手工设计轨迹需考虑 41 个碰撞体的接触关系（ANYmal 的碰撞模型含 41 个箱体 / 圆柱体 / 球体），规划难度极高；简单系统的恢复方法无法 scaling 到真实四足机器人。

![](/paper/anymal-real-world.png)
*实机上的跌倒恢复示例。策略能在随机初始构型下约 3 秒内翻身站稳。*

本文方法首次在 “同等复杂度的四足机器人” 上实现动态跌倒恢复 —— 通过 “仿真中随机化碰撞体姿态 + RL 试错”，策略能自主学习多接触点的协调逻辑，解决传统规划方法 “维度爆炸” 的难题。


## 小结与局限
它真正“卷”的不是策略网络结构，而是把 sim2real 最容易翻车的两块（**接触仿真**、**执行器动态**）先补齐，于是强化学习才能在真实四足上稳定释放性能。

1. **优势 1：开发效率质变** —— 从“数月手工调参”到“数天仿真设计”
   - 传统痛点：控制器往往要“手工写解析模型 + 逐层调参”（动力学/规划/跟踪一层层叠），每换一种动作（跑/跳/攀爬）几乎都得重来；为了算得动还得引入各种模型抽象（平滑接触、预设接触点），最后只能靠调低层控制器去“补误差”
   - 本文突破：把工作量压到“**任务描述 + 仿真随机化 + 策略训练**”，新任务主要改三件事：成本函数、初始状态分布、随机化范围；论文给的开发周期是“运动策略约 2 天、跌倒恢复约 1 周”（后者因为要嵌入安全约束更费工）
   - 关键原因：用**执行器网络**替代“百余参数的解析执行器模型”，数据采集 + 训练成本低，同时把延迟/摩擦等非线性动态一并学进去

2. **优势 2：硬件鲁棒性更像“产品”** —— 对磨损、配置变化不敏感
   - 实验证据：策略在真实 ANYmal 上持续测试约 3 个月，期间经历关节摩擦变化、重量 ±2kg、弹簧刚度变为原来的 3 倍，以及高频切换多种控制器
   - 结果：策略无需修改仍能稳定运行；相比之下，传统方法通常会被迫重新调参甚至改模型
   - 核心逻辑：训练时用多种随机惯性参数模型（论文里提到 30 种变体）提前“见过”硬件差异，让策略学到的是**抗扰动能力**而不是“记住某一台机器的参数”

3. **优势 3：算力分配倒挂** —— 训练期集中算力，运行期极致高效
   - 传统困境：在线轨迹优化/动力学解算会把机载算力吃爆，导致不得不简化模型，性能天花板更低
   - 本文分配：训练期用普通桌面机（1 CPU + 1 GPU）跑数小时（最长约 11 小时）吃掉数亿样本；运行期机载推理仅 **<25μs/步**（单 CPU 线程），资源占用约 0.1%
   - 额外收益：策略直接输出关节指令，避开了一些基于数值求解的“奇异构型分支逻辑”，工程上更干净

现存局限（这篇也讲得很直白，我觉得是它严谨的一面）：
1. **仍需人工设计任务参数**：成本函数、初始状态分布等还是依赖 RL 专家经验；例如跌倒恢复里“如何把避免碰撞脆弱部件写成代价项”并不直观，会带来反复调试
2. **单网络单任务**：一个网络基本对应一个任务（速度控制 vs 跌倒恢复），缺少跨任务的通用性；论文也指向未来要用**分层策略**做任务选择与执行解耦
3. **执行器建模的可迁移性**：当前执行器网络更适合“每个执行器相对独立”的系统（如 ANYmal 的 SEA）；如果是强耦合执行器（例如液压共享蓄能器），需要重新设计网络结构来建模耦合动力学

`Discussion`部分其实在强调一种研究范式——用**学习建模（执行器网络）**替代“硬写解析模型”，用**仿真随机化**替代“在实体上试错”，从而把复杂足式控制从“少数专家的手工艺术”推向更可复用、更工程化的流程。
