---
title: "Locomotion 文献精读（七）Learning Agile and Dynamic Motor Skills for Legged Robots"
---

> https://arxiv.org/abs/1901.08652
> Science Robotics 2019

ETH 2019 的经典工作，发表在 Science Robotics，婆罗门中的婆罗门。我一直把它当成“强化学习做足式 sim2real”的标杆之一：不靠花里胡哨的感知，而是把 **仿真保真度** 和 **执行器建模** 这两块先补齐，再让策略去学“敏捷动作”。

## 研究背景

2019 年前后，足式机器人的主流路线大致有三类困境（我这里按“工程可落地”的角度来理解）：
- **模块化控制**：把问题拆成“落脚点规划 → 足端轨迹 → 关节跟踪（PID/PD）”，能跑，但要靠工程师 **数月手工调参**，而且很难把速度、灵活性拉上去
- **轨迹优化**：理论很优雅，但足式的 **间歇接触** 会把问题搞得非常硬（接触时序/接触点未知），要么近似得厉害，要么算不动
- **强化学习**：需要海量试错，真实机上代价高且风险大，于是很多工作停在仿真；但仿真和现实之间的差距（动力学、执行器、接触）又会让策略一上机就崩
### 主流方法 1：模块化控制器设计
原理：把控制拆成“foothold 规划 → 足端轨迹 → 关节跟踪（PID/PD）”等解耦子模块，每个模块基于模板动力学（例如“质点 + 无质量肢体”）或经验规则生成参考，再通过大量手工调参把整条 pipeline 拼起来。
成果：在不少场景里确实能做出很强的效果（比如崎岖地形移动、一些动态步态）。
核心局限（我比较在意两点）：
- **性能天花板**：为了可解，模型一定会简化；简化越多，可用状态域越小，最后表现往往变成“能走但不敢快、不敢猛”
- **开发不经济**：每换一个机器人/动作，就得重新设计和调参，工程周期很长，而且经验不可复用

我的理解是：这篇不是在“再做一个更强的 RL”，而是先回答一个更现实的问题——**怎么让 RL 在真实足式上稳定工作**。它的答案是把迁移最容易翻车的两块补上：**高保真的接触刚体仿真** + **用 Actuator Net 学出 SEA 执行器的真实动态**，然后再用 RL（TRPO）去学策略，最后做到“基本零调参直接上机”。
### 主流方法 2：轨迹优化方法
原理：上层用刚体动力学 + 数值优化做规划（算一条“最优”轨迹/力/接触时序），下层再去跟踪这条规划结果。
核心局限：
- **接触建模太硬**：足式的接触点/接触时序天然是离散事件，问题很容易变成“你不先假设，就很难算”
- **实时性压力大**：要在线优化就需要很强算力；降精度又会直接牺牲性能
- **规划-跟踪耦合弱**：规划假设跟踪能跟上，但实际遇到欠驱动/意外接触时，跟踪误差会反过来破坏规划

## 方法论

论文的方法本质是：**先把仿真“尽量做得像”，再在仿真里把策略“尽量练得强”，最后直接部署到真实 ANYmal**。流程可以按 4 步理解：

![](/paper/anymal-agile-overview.png)
*图：整体训练流程。先做物理参数辨识与不确定性建模，再训练执行器网络拟合执行器/软件动态，最后在前两步的模型上训练策略并直接上机。*

![](/paper/anymal-agile-sim.png)
*图：仿真中训练控制器的闭环。策略网络输出关节位置目标，执行器网络基于关节历史状态把“位置目标 + 历史”映射为关节扭矩，扭矩驱动刚体仿真得到下一状态。*

我把它拆成 4 个模块来记：
- **刚体模拟器**：输入关节扭矩 + 当前状态，输出下一状态
- **状态缓存**：保存一个短时间窗的关节速度、位置误差等历史（用来“隐式感知接触/延迟”）
- **策略网络**：输入当前观测 + 历史信息，输出 12 维关节位置目标
- **执行器网络（Actuator Net）**：输入历史信息 + 位置目标，输出关节扭矩（把真实 SEA 的复杂动态学进来）
### ANYmal 四足机器人
硬件特点：中型犬尺寸（32kg，腿长 55cm），12 个串联弹性执行器（SEA）。SEA 的好处是抗冲击、可测扭矩，但坏处也很直接：**动力学更复杂、延迟/摩擦更明显、解析建模非常痛苦**，这也是本文把 Actuator Net 放在核心位置的原因之一。

对比一些常被拿来对照的平台（这里更像是论文的“时代背景”）：
- 波士顿动力液压平台：能量密度高，但体积/噪音大，更偏户外
- MIT Cheetah：高速高效，但当时在户外适应性/续航上公开信息有限
- SpotMini：电驱、室内外通用，但细节公开较少

### 刚体动力学建模
这里我觉得作者抓住了足式 sim2real 的一个关键：**接触不能太假**，但**仿真又不能太慢**（否则 RL 根本练不起来）。

- **硬接触求解器**：采用作者之前的工作 [41]，基于库仑摩擦锥约束做硬接触（而不是把接触“抹平”成光滑力），能更像真实地呈现防滑、冲击响应
- **仿真效率**：普通台式机上每秒可生成约 90 万时间步，比实时快约 1000 倍，足够支撑数亿级样本训练
- **鲁棒性增强（随机化）**：CAD 给出的惯性参数会偏（线缆/电子元件没建模），论文用 30 种 ANYmal 变体训练策略：质心位置（±2cm）、连杆质量（±15%）、关节位置（±2cm）加均匀噪声，避免策略过拟合到单一“理想模型”
### 执行器网络（Actuator Net）
ANYmal 的 12 个 SEA 是本文的“真硬菜”：如果执行器动力学没建对，策略很容易在仿真里学到一套“仿真专用技巧”，一上机就炸。

我的理解是：Actuator Net 做的事很具体——**把“关节指令 → 扭矩响应”这段最难写解析模型的动态，直接用监督学习拟合出来**。

1. 技术方案（怎么学）
- **输入/输出**：输入关节位置误差 + 速度历史，输出关节扭矩（把控制延迟、电机阻尼、摩擦等都“包”进去）
- **数据采集**：用正弦轨迹驱动 ANYmal，让足部反复接触/离地；再手动扰动机器人，采 12 个关节的误差/速度/扭矩数据（400Hz，约 4 分钟拿到 100 万+样本）
- **网络结构**：3 层 MLP（每层 32 单元），激活用 softsign（推理更快：12 关节约 12.2μs）

2. 核心效果（学得像不像）
- 验证集扭矩预测误差约 0.74Nm（接近扭矩传感器分辨率 0.2Nm），明显优于“理想执行器模型”
- 即便在真实运动（如 1.6m/s 高速奔跑）里，误差仍约 0.966Nm，说明它确实抓住了 SEA 的关键动态，从而缩小 **仿真-现实差距**

![](/paper/anymal-agile-valid.png)
*图：执行器网络的验证结果。预测扭矩与实测扭矩几乎重合；“理想模型”（零延迟、零机械响应时间）反而偏差明显。*
### TRPO
TRPO（Trust Region Policy Optimization）

选择 TRPO 的理由比较“朴素但实用”：对超参数不敏感（默认设置就能跑），而且在这种 **高维连续控制**（12 关节 + 多状态维度）上优化更稳。再加上前面快仿真，论文里提到 4 小时能处理 2.5 亿个状态转换，最长训练也在 11 小时量级。

我觉得这篇在策略接口上也做了两个很关键的工程选择：

观测（Ot）：尽量只用真实机上“可靠能测”的量，同时把任务相关信息塞进去：
- **机身状态**：IMU 的重力方向（反映姿态）、基座高度（Kalman 估计）、线速度/角速度
- **关节状态**：关节位置/速度 + 极短窗口的历史（例如 0.01s、0.02s 之前）
- **任务条件**：上一动作 + 高层指令 `C`（速度/偏航率等）
- **一个小技巧**：关节历史能做“接触的隐式指示”，一定程度替代力传感器

动作（at）：策略输出 **关节位置目标**（而不是直接输出扭矩），再用固定 PD 增益转成扭矩（`k_p=50Nm/rad`，`k_d=0.1Nm·s/rad`）：

\[
\tau = K_p(\phi^{*}-\phi) + K_d(\dot{\phi}^{*}-\dot{\phi})
\]

我的理解是：这样做在训练初期会更稳（不至于像纯扭矩控制那样一开始就频繁摔），而且策略可以利用“位置误差”来间接塑形接触力，不会被传统轨迹跟踪式 PD 的框架绑死。

另一个细节是“课程式调权”：用课程因子 `k_c`（从 0.3 逐步增到 1）去调节扭矩/关节速度惩罚的权重——先让策略学会动起来，再去抠动作自然性与能耗，避免一开始就陷入“最省电就是不动”的局部最优。

按任务的折扣因子也做了区分：
- 速度控制/高速奔跑：`γ=0.9988`（半衰期 5.77s），更偏长视野，站姿更自然
- 跌倒恢复：`γ=0.993`（半衰期 4.93s），更偏短视野，优先快速翻身

## 实验
### 任务 1：指令条件下的运动
该任务是足式机器人的基础需求 —— 根据高层指令（前向 / 侧向速度、偏航率）调整步态，需验证策略的鲁棒性、精度和效率。
1. 实验设计
定性测试：用操纵杆发送随机指令，同时对机器人主体施加外部推扰，观察是否稳定跟随；
定量测试：每 2 秒发送 1 个随机指令（持续 30 秒，共 15 次转换），记录速度误差、关节扭矩、机械功率；
对比对象：传统最优方法（Bellicoso et al. [12]），采用其唯一能达 1.0m/s 的 “飞行 trot” 步态，及最高效的 “动态侧向行走” 步态；
消融实验：用 “理想执行器模型”（假设无延迟、无误差）和 “解析执行器模型”（手工调参的传统模型）训练策略，验证 Actuator Net 的必要性。
2. 关键结果
我觉得这里最有意思的是：它不仅“跟得准”，还“跑得省”，并且步态是自己长出来的（不是人手工写死的）。

| 评估维度 | 本文方法（RL + Actuator Net） | 传统方法（Bellicoso [12]） | 我理解的核心差异 |
| --- | --- | --- | --- |
| 速度跟踪精度 | 平均线速度误差 0.143m/s；偏航率误差 0.174rad/s | 线速度误差 0.231m/s；偏航率误差 0.278rad/s | 误差显著降低，尤其在低速与高速端都更稳 |
| 能耗效率 | 平均扭矩 8.23Nm；机械功率 78.1W | 扭矩 11.7Nm；功率 97.3W | 同样任务下更省力、更省电 |
| 步态自适应 | 低速自动变“步行 trot”；高速变“飞行 trot” | 需要手工指定步态 | **无需预定义步态**，能自然切换 |
| 消融（执行器模型） | 稳定行走，无明显抖动 | 理想/解析模型：容易每步跌倒、肢体抖动 | Actuator Net 是 sim2real 的关键补丁 |
3. 核心结论
本文方法在 “精度 + 效率” 上全面超越传统方法：
精度优势源于策略能 “端到端优化”（从传感器到控制信号），避免传统模块化控制器的 “近似误差累积”；
效率优势源于策略学到 “10-15 度更直的膝盖标称姿态”，而传统方法因怕跌倒不敢调整此姿态；
Actuator Net 是迁移成功的核心 —— 传统执行器模型无法精准建模 SEA 的延迟和非线性，导致策略在真实机器人上失效。

![](/paper/anymal-agile-quan.png)
*图：指令条件下的定量评估与步态可视化。包含步态相位、速度跟踪误差，以及功率/扭矩等指标与传统控制器的对比。*

### 任务 2：高速运动
该任务验证方法能否 “利用硬件全部性能”——ANYmal 的设计目标是鲁棒性而非速度，传统方法难以突破 1.2m/s 的记录，需测试策略能否达硬件极限（扭矩 40Nm，关节速度 12rad/s）。
1. 实验设计
发送 1.6m/s 的阶跃速度指令，让机器人跑 10 米后减速至 0，记录实际速度、关节扭矩 / 速度、步态模式；
对比基准：ANYmal 此前的速度记录（1.2m/s，Bellicoso [12] 的飞行 trot）。
2. 关键结果
速度突破：真实机器人最高达 1.5m/s（比此前记录提升 25%），仿真中达 1.58m/s；
硬件利用：策略完全用到硬件极限 —— 关节扭矩达 40Nm（最大值），关节速度达 12rad/s（最大值）；
独特步态：学到 “长腾空 + 不对称腾空时间” 的飞行 trot（非自然界常见步态，但接近最优解），如图 3D 所示；
传统方法局限：传统规划模块无法 “预知硬件约束”，输出的指令常超出执行器能力，导致无法达高速。

![](/paper/anymal-agile-eva-high-speed.png)
*图：高速运动评估。包含前向速度、关节速度/扭矩以及对应的步态相位变化。*
3. 核心结论
本文方法能 “主动探索硬件极限”，生成传统手工设计无法实现的最优步态 —— 传统控制器因依赖 “预定义步态 + 解析模型”，无法突破性能瓶颈，而 RL 策略通过试错能充分利用硬件潜力。

### 任务 3：跌倒恢复
该任务是足式机器人的 “鲁棒性关键”—— 传统方法需手工设计恢复轨迹（耗时且仅适用于简单姿态），而动物的恢复依赖 “动态多接触点协调”，需验证策略能否学到此类复杂行为。
1. 实验设计
初始姿态：将 ANYmal 置于 9 种随机姿态，包括 “倒扣”（几乎完全颠倒）、“腿部自接触”（靠自身腿部支撑）等极端场景；
评估指标：是否成功翻转直立、恢复时间；
传统方法对比：手工调参的轨迹回放（商用机器人常用）、简单三连杆机器人的恢复（Morimoto et al. [50]），均无法扩展到复杂四足系统。
2. 关键结果
100% 恢复成功率：所有 9 种姿态均成功翻转直立，恢复时间 < 3 秒；
动态行为特征：策略能协调所有肢体，利用动量实现 “快速翻转”（如倒扣时先蹬地产生力矩，再调整接触点），如图 4 所示；
传统方法无法实现：手工设计轨迹需考虑 41 个碰撞体的接触关系（ANYmal 的碰撞模型含 41 个箱体 / 圆柱体 / 球体），规划难度极高；简单系统的恢复方法无法 scaling 到真实四足机器人。

![](/paper/anymal-real-world.png)
*图：实机上的跌倒恢复示例。策略能在随机初始构型下约 3 秒内翻身站稳。*
3. 核心结论
本文方法首次在 “同等复杂度的四足机器人” 上实现动态跌倒恢复 —— 通过 “仿真中随机化碰撞体姿态 + RL 试错”，策略能自主学习多接触点的协调逻辑，解决传统规划方法 “维度爆炸” 的难题。


## 小结与局限
我对这篇的总结是：它真正“卷”的不是策略网络结构，而是把 sim2real 最容易翻车的两块（**接触仿真**、**执行器动态**）先补齐，于是强化学习才能在真实四足上稳定释放性能。核心优势我更愿意按 3 点来记：

1. **优势 1：开发效率质变** —— 从“数月手工调参”到“数天仿真设计”
   - 传统痛点：控制器往往要“手工写解析模型 + 逐层调参”（动力学/规划/跟踪一层层叠），每换一种动作（跑/跳/攀爬）几乎都得重来；为了算得动还得引入各种模型抽象（平滑接触、预设接触点），最后只能靠调低层控制器去“补误差”
   - 本文突破：把工作量压到“**任务描述 + 仿真随机化 + 策略训练**”，新任务主要改三件事：成本函数、初始状态分布、随机化范围；论文给的开发周期是“运动策略约 2 天、跌倒恢复约 1 周”（后者因为要嵌入安全约束更费工）
   - 关键原因：用**执行器网络**替代“百余参数的解析执行器模型”，数据采集 + 训练成本低，同时把延迟/摩擦等非线性动态一并学进去

2. **优势 2：硬件鲁棒性更像“产品”** —— 对磨损、配置变化不敏感
   - 实验证据：策略在真实 ANYmal 上持续测试约 3 个月，期间经历关节摩擦变化、重量 ±2kg、弹簧刚度变为原来的 3 倍，以及高频切换多种控制器
   - 结果：策略无需修改仍能稳定运行；相比之下，传统方法通常会被迫重新调参甚至改模型
   - 核心逻辑：训练时用多种随机惯性参数模型（论文里提到 30 种变体）提前“见过”硬件差异，让策略学到的是**抗扰动能力**而不是“记住某一台机器的参数”

3. **优势 3：算力分配倒挂** —— 训练期集中算力，运行期极致高效
   - 传统困境：在线轨迹优化/动力学解算会把机载算力吃爆，导致不得不简化模型，性能天花板更低
   - 本文分配：训练期用普通桌面机（1 CPU + 1 GPU）跑数小时（最长约 11 小时）吃掉数亿样本；运行期机载推理仅 **<25μs/步**（单 CPU 线程），资源占用约 0.1%
   - 额外收益：策略直接输出关节指令，避开了一些基于数值求解的“奇异构型分支逻辑”，工程上更干净

现存局限（这篇也讲得很直白，我觉得是它严谨的一面）：
1. **仍需人工设计任务参数**：成本函数、初始状态分布等还是依赖 RL 专家经验；例如跌倒恢复里“如何把避免碰撞脆弱部件写成代价项”并不直观，会带来反复调试
2. **单网络单任务**：一个网络基本对应一个任务（速度控制 vs 跌倒恢复），缺少跨任务的通用性；论文也指向未来要用**分层策略**做任务选择与执行解耦
3. **执行器建模的可迁移性**：当前执行器网络更适合“每个执行器相对独立”的系统（如 ANYmal 的 SEA）；如果是强耦合执行器（例如液压共享蓄能器），需要重新设计网络结构来建模耦合动力学

最后我对“讨论部分”的理解是：这篇其实在强调一种研究范式——用**学习建模（执行器网络）**替代“硬写解析模型”，用**仿真随机化**替代“在实体上试错”，从而把复杂足式控制从“少数专家的手工艺术”推向更可复用、更工程化的流程。
