---
title: "Locomotion 文献精读（九）Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning"
---

> https://arxiv.org/abs/2109.11978
> CoRL 2021

ETH Zurich 和 NVIDIA，主要是后者作为金主大力推广了开源，不然ETH死都不愿意弄。现在Isaac有这样的影响力也都多亏了这篇工作。

## 研究背景

随着机器人任务从 “简单结构化场景”（如平坦地面行走）转向 “复杂非结构化场景”（如楼梯、障碍地形），训练一个有效策略所需的数据量呈指数级增长。这带来两个直接问题：

1.  **“仿真优先” 成为必然选择**：真实机器人的训练数据获取成本极高（如硬件损耗、环境搭建、时间成本），因此主流方案均采用 “先在仿真环境中训练策略，再迁移到真实机器人（Sim-to-Real）” 的流程。
2.  **传统仿真训练耗时极长，效率低下**：
    -   OpenAI 的 “积木重定向任务”：训练时间长达 14 天；
    -   OpenAI 的 “魔方求解策略”：训练时间耗时数月。
    -   这意味着即使在仿真中，复杂任务的训练也需 “数天至数周”，严重制约研发效率。

DRL 的性能高度依赖 “超参数”（如 batch size、学习率、折扣因子等），而要找到最优超参数，需反复执行完整的训练流程。例如：若一次训练需 7 天，调优 10 次超参数则需 70 天 —— 这进一步放大了 “训练耗时” 的问题，成为 DRL 落地机器人领域的关键瓶颈。

为解决训练耗时问题，此前研究已尝试 “并行化” 和 “仿真优化”，但存在明显缺陷：

*   **并行方案：只增 batch size，不优化总时间**。早期大规模并行依赖 “数千 CPU 集群”，每个 CPU 运行独立仿真实例，通过 “梯度平均” 实现并行训练。但这种方式仅能增大每次策略更新的 batch size（提升学习稳定性），并未减少整体训练时间（因 CPU 并行度低，数据传输和计算效率有限）。
*   **仿真器：CPU 架构制约并行规模**。主流机器人仿真器（如 Mujoco、Bullet、Raisim）虽支持多体动力学计算，但均基于 CPU 开发，并行能力极弱（无法同时模拟数千机器人），且存在 “PCIe 数据传输瓶颈”（数据从 CPU 传到 GPU 的速度比 GPU 单独计算慢 50 倍）。
*   **腿式机器人专项痛点：复杂地形运动训练效率低**。针对 “四足机器人在非结构化地形行走” 这一核心任务，此前基于学习的方案耗时仍居高不下：
    -   盲走（无地形感知）策略：需 12 小时训练；
    -   感知型行走（结合地形感知）策略：需 82-120 小时训练；

**本篇工作的突破**：通过和 NVIDIA Isaac 的合作，针对四足机器人的 “感知型复杂地形行走” 任务，单 GPU 训练时间可压缩至 **20 分钟内**（对比此前 120 小时，速度提升 360 倍），且能稳定迁移到真实机器人（ANYmal C）。

## 方法论

**大规模并行深度强化学习（Massively Parallel Deep Reinforcement Learning）** 是针对传统强化学习（RL）在机器人等复杂任务中 “训练耗时过长、并行效率低” 的痛点提出的优化方案。

核心是通过 **全流程 GPU 化、数千机器人并行仿真、算法与超参数适配**，将原本 “数天至数月” 的训练压缩至 “分钟级”。本质是：在单工作站 GPU 上，同时仿真数千个智能体（如机器人）执行任务，同步收集训练数据并完成策略更新，最终实现 **“以‘并行规模’换‘训练时间’”**。

| 瓶颈类型 | 具体问题 | 数据佐证 |
| :--- | :--- | :--- |
| **1. 数据传输与计算脱节** | 传统 RL 流程中，“仿真、观测 / 奖励计算” 在 CPU 执行，“策略推理、更新” 在 GPU 执行；CPU 与 GPU 间的 PCIe 数据传输速度，比 GPU 单独计算慢 50 倍，形成 “通信瓶颈”。 | 每次策略更新需将大量 CPU 生成的数据传入 GPU，整体流程被拖拽变慢。 |
| **2. 仿真并行度极低** | 主流机器人仿真器（Mujoco、Bullet 等）基于 CPU 开发，最多支持 “数十个智能体并行”，且受 CPU 核心数、内存限制，无法满足 “数千机器人同时仿真” 的需求。 | 此前四足机器人复杂地形行走训练需 12-120 小时，核心原因是仿真吞吐量不足。 |
| **3. 算法与并行场景不兼容** | 传统 RL 算法（如 PPO）的超参数（如 batch size、重置逻辑）是为 “小规模单机训练” 设计的，直接用于 “数千智能体并行” 时会出现 “策略不收敛、Critic 性能差” 等问题。 | 例如：传统 PPO 假设 “episode 无超时重置”，但并行训练中机器人常因 “超时” 重置，破坏 Critic 对未来奖励的预测。 |

区别于传统 CPU 仿真器，**Isaac Gym** 的核心优势是：
1.  **全 GPU 执行**：仿真（基于 PhysX 引擎）、观测计算、奖励计算、策略推理均在 GPU 内完成，完全规避 “PCIe 数据传输瓶颈”；
2.  **支持数千机器人并行**：现代 GPU 可处理数万并行指令，Isaac Gym 适配该特性，单仿真实例可同时模拟 128-16384 个机器人（论文中最优配置为 2048-4096 个）；
3.  **向量计算优化**：观测、奖励、策略推理等操作均通过 GPU 向量化计算实现，随机器人数量增加，耗时近乎线性增长（而非指数级），保证高吞吐量。

> 现在已经没有 Isaac Gym 了，六根拆掉分给 Isaac Sim 和 Isaac Lab 了。

### 地形平铺（Terrain Tiling）

传统仿真中，“更换地形类型 / 难度” 需每次重置环境，耗时极高。论文提出 “地形平铺” 方案：
-   将 5 类训练地形（平坦、25° 斜坡、0.1m 粗糙地形、±0.2m 障碍、0.3m 宽楼梯，见图 2）拼成一个 “单一网格”；
-   机器人切换地形 / 难度时，无需重置环境，仅通过 “物理移动机器人在网格中的位置” 实现（如从平坦区移至楼梯区）；
-   **优势**：规避 “频繁环境重置” 的耗时，提升仿真吞吐量约 2 倍。

### PPO 算法改进

传统 PPO（近端策略优化）无法直接支持 “数千机器人并行”，论文对其做了超参数调整、重置逻辑优化两大关键修改：

#### 1. 超参数修改：适配 “大并行规模”

核心逻辑是 **“以‘并行机器人数量（n_robots）’补‘单机器人步数（n_steps）’”**：

| 超参数 | 传统 PPO 设置 | 大规模并行 PPO 修改 | 修改原因 |
| :--- | :--- | :--- | :--- |
| **batch size (B)** | 数百 - 数千样本 | **B = n_robots × n_steps**<br>（如 4096 × 24 = 98304） | 并行场景中 n_robots 提升 1-2 个数量级，需减小 n_steps 以控制 B 在 “10 万 - 20 万样本”（避免样本重复或梯度噪声）。 |
| **n_steps**<br>(单机器人每轮步数) | 128-2048 步 | **≥25 步**<br>（对应 0.5 秒模拟时间） | 依赖 “广义优势估计（GAE）” 计算奖励，GAE 需要 “多步时间连贯性信息”；n_steps < 25 时，策略无法收敛。 |
| **mini-batch size** | batch size 的 1/4-1/8 | **数万个样本**<br>（如 24576） | 大 mini-batch 可稳定 GPU 反向传播过程，且不增加总训练时间（GPU 擅长批量计算）。 |

#### 2. 重置处理：解决 “超时重置” 导致的 Critic 性能问题

传统 PPO 假设 “episode 仅因‘失败’或‘达成目标’重置”，但并行训练中存在 “超时重置”（20 秒强制重置），这会破坏 Critic 网络对 “无限 horizon 未来奖励” 的预测。

-   **步骤 1**：修改 Gym 接口，在仿真环境中增加标记：“失败重置”（如摔倒）、“超时重置”（时间到）。
-   **步骤 2**：对 “超时重置” 的 episode，不直接截断奖励计算，而是用 Critic 网络自身预测的 “未来折扣奖励和” 作为 “引导值”，补充到当前奖励目标中（Bootstrapping）。
-   **效果**：Critic 网络的损失降低 10%-20%，总奖励提升。

![](/paper/anymal-parallel-drl-terrain-types.png)
*图：仿真训练和测试中使用的地形类型。(a) 0.1m 随机粗糙地形。(b) 25° 斜坡。(c) 0.3m 宽、0.2m 高的楼梯。(d) 高达 ±0.2m 的离散障碍物。*

### 任务目标

让四足机器人（ANYmal C 等）在 5 类真实场景代表性地形中，同时满足两个要求：
1.  **地形适应性**：能稳定穿越所有地形（斜坡 / 楼梯采用 “金字塔式布局”，支持机器人全方向通行）。
2.  **指令跟随性**：遵循 “基座航向指令” 和 “线速度指令”。

### 游戏启发式课程学习（Game-Inspired Curriculum）

传统 RL 训练 “直接让机器人学高难度地形” 会导致策略崩溃，因此论文设计了 **“从易到难、自适应难度”** 的课程学习方案 —— 灵感来自游戏 “关卡升级机制”。

| 环节 | 具体规则 | 目的 |
| :--- | :--- | :--- |
| **1. 初始难度分配** | 所有并行机器人被随机分配 “地形类型 + 难度等级”：<br>- 楼梯：5cm → 20cm<br>- 斜坡：0° → 25° | 让机器人从 “能完成的简单任务” 开始，建立基础行走能力。 |
| **2. 难度升级条件** | 机器人成功 “走出当前地形的边界” → 下一轮分配更高难度。 | 基于机器人实际能力动态提升挑战，避免 “难度过高导致挫败”。 |
| **3. 难度降级条件** | 单轮 episode 移动距离 < “目标距离的 50%” → 下一轮分配更低难度。 | 防止机器人因 “难度超出能力” 陷入 “持续失败”，保护策略稳定性。 |
| **4. 最高难度循环** | 达到 “最高难度” 的机器人，下一轮随机分配低难度地形。 | 1. 避免 “灾难性遗忘”（防止忘了简单地形怎么走）；<br>2. 增加训练样本多样性。 |

![](/paper/anymal-parallel-drl-4k.png)
*图：4000 个机器人在自动课程下的训练进度。上图为 500 次更新后，下图为 1000 次更新后。机器人从第一排（离相机最近）开始，逐渐向更难的地形推进。*

### 观测、动作与奖励设计
涉及机器人的 “感知 - 决策 - 激励” 系统，定义 “机器人能感知什么（观测）、能做什么（动作）、做得好如何奖励（奖励）”，且设计原则是 “简洁通用，适配所有地形”（无需为不同地形单独设计策略）

#### 观测空间（Observation）：本体 + 地形双感知
-   **本体感知（Proprioceptive）**：基座状态（线速度、角速度、重力向量）、关节状态（位置、速度）、历史动作（上一轮输出的关节目标位置，保持动作连贯性）。
-   **地形感知（Terrain）**：在机器人周围采样 108 个点的 “地形高度”（即 “地形表面到基座的距离”），覆盖前方、侧方区域。

#### 动作空间（Action）：无步态依赖
-   策略直接输出 “关节目标位置”，由 PD 控制器转换为电机扭矩。
-   **核心特点**：无任何步态依赖元素（不预设 “四足交替迈步”“对角步态” 等规则），所有步态（如平路小步、楼梯大步）由策略自主学习。

#### 奖励函数（Reward）：多维度引导

| 奖励类型 | 具体项 | 作用 |
| :--- | :--- | :--- |
| **主项**（正向引导指令跟随） | 1. 跟踪线速度指令，速度越接近目标，奖励越高；<br>2. 抑制非目标方向速度，如目标朝前走，横向速度越大，惩罚越高。 | 确保机器人 “按指令行走”，不偏离方向。 |
| **正则项**（反向约束风险动作） | 1. 惩罚关节扭矩 / 加速度过大，防止电机过载、机械损伤；<br>2. 惩罚关节目标突变；<br>3. 惩罚碰撞（膝/小腿触地）；<br>4. 碰撞重置（基座触地）。 | 保护硬件，提升行走稳定性。 |
| **额外项** | 鼓励长步幅，步幅越大，奖励越高。 | 让机器人动作更接近真实动物，避免 “小碎步”。 |

### Sim-to-Real 迁移优化

1.  **地面摩擦随机化**：摩擦系数在 [0.5, 1.25] 间随机，适应不同材质如水泥地 vs 草地。
2.  **观测数据加噪声**：添加基于真实 ANYmal C 机器人实测的噪声（传感器误差、测量延迟）。
3.  **随机推力扰动**：每 10 秒施加 ±1m/s 的随机推力（x/y 方向，模拟真实环境中的外力干扰，如风吹、轻微碰撞）。
4.  **执行器动态适配**：ANYmal 使用系列弹性执行器（SEA），用 **LSTM** 网络替代传统 MLP，让网络自主学习执行器的复杂动态规律。

## 实验

### 大规模并行的影响

这一节回答了：**并行机器人的数量（n_robots）如何影响策略性能和训练耗时？**

![](/paper/anymal-parallel-drl-deviation.png)
*图：(a) 1500 次更新后的平均奖励（5 次运行）。红色为理想基线（20000 机器人）。(b) 总训练时间。(c) 奖励与训练时间的关系。左上角区域（绿色）为最佳配置。*

**结论**：
1.  **n_robots 过多（> 4096）**：性能骤降。原因是 `n_steps` 被迫减小（为了保持 batch size），导致时间域过短，GAE 无法有效计算奖励。
2.  **n_robots 过少（< 2048）**：性能缓慢下降。原因是 `n_steps` 很大但样本同质化严重（Sample Correlation），梯度噪声大。
3.  **最佳平衡点**：**n_robots = 4096**，**Batch Size ≈ 100k**。此时既能保证策略性能接近基线，又能将训练时间压缩至 **20 分钟内**。

### 仿真性能验证

在最佳配置下，机器人对地形的适应能力：

| 地形类型 | 成功率表现 |
| :--- | :--- |
| **楼梯（0.2m 高）** | 成功率 ≈ 100%，接近机械结构极限。 |
| **离散障碍（±0.2m）** | 成功率随高度增加稳步下降。 |
| **斜坡（≤25°）** | 可稳定攀爬；>25° 时能 “受控滑下” 而不摔倒。 |

**跨机器人泛化性**：
-   **ANYmal C（带机械臂）**：增重 20%，无需修改直接适配。
-   **Unitree A1**：尺寸小、结构不同，仅需移除执行器模型并调整 PD 增益即可适配。
-   **Agility Cassie（双足）**：增加 “单脚站立奖励” 后也能在相同地形行走。

### Sim-to-Real 迁移

真实部署非常“极简”，无额外滤波。
-   **挑战**：真实 Lidar 构建的高程图有误差（遮挡、噪声）。
-   **适配**：将最大线速度指令从仿真的 0.75m/s 降至 0.6m/s 以换取鲁棒性。
-   **结果**：真实机器人能稳定 “上下楼梯”、“动态避障”。

![](/paper/anymal-parallel-drl-real.png)
*图：20 分钟内训练出的策略部署在真实机器人上。*

## 小结与局限

通过 **在线深度强化学习（On-Policy DRL）** 结合 **端到端 GPU 流水线**，复杂的机器人任务终于可以在 “分钟级” 内完成训练。

这篇论文的核心贡献不在于设计了多么复杂的网络结构，而在于证明了 **“大规模并行” + “课程学习”** 是解决 RL 落地效率问题的终极答案。它让 RL 变得像编译代码一样快，极大地加速了迭代周期。这也是后来 NVIDIA Isaac Gym / Isaac Sim 能够统治机器人仿真领域的奠基之作。
