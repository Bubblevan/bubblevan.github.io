---
title: "Locomotion 文献精读（八）Learning Quadrupedal Locomotion over Challenging Terrain"
---

> https://arxiv.org/abs/2010.11251
> Science Robotics 2020

ETH牛逼！依然是在 ETH 自家的 ANYmal 波士顿动力机器狗上对 2019 年的工作继续改进。

## 研究背景

老生常谈，足式机器人在**【多样复杂自然环境下的动态运动】**依然是个老大难问题：
- **环境本身就很恶劣**：地形不规则（碎石堆、陡坡）、地面可变形（泥泞、积雪），甚至还有很多干扰（茂密植被、流水）；
- **感知太难**：外感受传感器（相机/雷达）不仅会被植被遮挡，还经常滞后（比如脚踩松了地面塌陷，传感器是看不出来的），更别提测出摩擦系数这种“物理属性”了；
- **多目标冲突**：机器人既要不摔倒、不磕碰自己，又要抵抗风吹/打滑，还得按指令方向走。

传统方法一般是写一个精细的状态机，预先定义大量 “运动基元”（如抬腿高度、步频）和 “反射逻辑”（如打滑时调整脚的受力），通过状态切换协调动作。

但这玩意儿太依赖**显式的状态估计**了：你得先判断“是否接地”、“是否打滑”，为触发状态切换或反射，这些判断通常依赖一堆人工调参的阈值，换个环境可能就失效了。四足动物能本能解决这些问题，但机器人领域至今仍是 “未解决的挑战“。

所以这篇论文的核心思路就是：**与其费劲去搞不可靠的外感受，不如把本体感受（Proprioception）做到极致**。它通过三个核心创新，搞定了一个“盲控”但极度鲁棒的控制器：

| 创新点 | 核心作用 | 解决的问题 |
| :--- | :--- | :--- |
| **时序卷积网络（TCN）** | 用 2 秒的本体感受历史（而不是单帧快照）来做决策 | MLP 没法记住历史（比如脚卡住之前的运动趋势），TCN 能从历史数据里“隐式”推断出地形特征 |
| **特权学习（两阶段）** | 先让“老师”在仿真里看着真值学，再让“学生”只看本体感受去模仿 | 直接用本体感受训练很难收敛（信号太稀疏），老师带着学能解决这个问题 |
| **自适应地形课程** | 用粒子滤波动态调整训练地形的难度 | 避免“太简单学不到东西”或“太难直接训崩”，始终让机器人在“有挑战但能通过”的边缘疯狂试探 |

> 传统 MLP 是在 cue 他们 2019 年的那篇 Science Robotics 工作。

## 方法论

### 控制器核心目标

任务很简单：**按指令在崎岖地形移动**。
这里有个细节：指令只给“方向”`ψ_T`和“转向”`ω_z`，不给“目标速度”。这是因为在复杂地形里，能走多快完全取决于路况（下坡快、上坡慢），强制规定速度反而容易让机器人失衡。

指令向量定义得比较数学：
指令向量定义：⟨(B_IBŵ_T)xy, (ω̂_T)z⟩
- 水平分量：$\langle \cos\psi_T, \sin\psi_T \rangle$
- 转向分量：$\hat{\omega}_{T,z} \in \{-1, 0, 1\}$
- 停止：$\langle 0.0, 0.0 \rangle$

![](/paper/anymal-tcn-overview.png)
*图：方法总览。(A) 两阶段训练流程：先在仿真中用特权信息训练 Teacher，再让仅有本体感受的 Student 模仿 Teacher。(B) 自适应地形课程：用粒子滤波动态调整地形参数，确保难度适中。(C) 控制器架构：策略网络调制基础运动原语，最后输出给关节 PD 控制器。*

> 控制架构基础采用的 PMTG 来自原参考文献的[34]和[12]，不过不愧是Science的文章，附录里实验做的太扎实了
### 运动合成（Motion Synthesis）

整个架构采用了 **PMTG（策略调制轨迹生成器）** 的思路，分为两步：

1.  **FTG（足部轨迹生成器）提供基础运动原语（如周期性踏步）**：
    -   给每条腿定义一个相位 $\phi_i$，在 [0, 2π) 之间循环。
    -   前半段是接触相（支撑），后半段是摆动相（迈步）。
    -   这就相当于给了一个“默认步态”（这里用的是 Trot，对角腿同步）。

2.  **神经网络策略进行调制**：
    -   策略输出两个东西：**频率偏移** $f_i$（调整步频，比如遇到障碍慢点走）和 **足位残差** $\Delta r$（修正落脚点，比如抬高腿跨台阶）。
    -   最终的目标足位 = 基础轨迹 + 策略修正。

为了让训练更稳，目标足位是定义在**足部水平坐标系 ($H_i$)** 下的。这个坐标系跟随腿的髋关节，但永远保持水平（不随基座倾斜）。这样即使机身晃得厉害，策略输出的足位指令也是相对稳定的，不会被带偏。

最后，通过 IK（逆运动学）算出关节角度，扔给 PD 控制器去执行。这里 PD 的参数也是学习的一部分（或者说在仿真里建模了），确保 Sim2Real 的时候动力学一致。

### 教师策略（Teacher Policy）

教师策略拥有“上帝视角”（Privileged Information），主要负责在仿真里先把路走通。
它的输入除了本体感受 $o_t$，还包括：
-   **地形信息**：脚周围的高度扫描图。
-   **物理真值**：接触力、摩擦系数、外部干扰力。

网络结构用了个 Encoder-Decoder 的变体：先用 MLP 把特权信息压缩成一个 latent 向量 $l_t$，再和本体感受一起输给 Actor 输出动作。
训练算法用的 **TRPO**，比较稳。

### 学生策略（Student Policy）

学生策略才是真正部署到真机上的，它**没有**特权信息，只能靠**本体感受历史**。

**核心假设**：虽然我看不到地形，但我踩在上面的感觉（关节受力、姿态变化）和地形是强相关的。只要我记得足够久（比如过去 2 秒的历史），我就能从这些本体感受里“猜”出地形特征。

这也是为什么要用 **TCN（时序卷积网络）** 而不是 MLP：
-   **TCN** 能处理长序列（这里用了 2 秒的历史，覆盖 2 个步态周期）。
-   它通过因果卷积（Causal Convolution）保证不利用未来信息。
-   相比 RNN/LSTM，TCN 梯度更稳，且能并行计算。

**训练方法**：**DAgger（数据集聚合）**。
不仅要模仿老师的动作 $\bar{a}_t$，还要模仿老师的 latent 向量 $\bar{l}_t$。这就强迫学生的 TCN 必须从本体感受历史里提取出和“地形特权信息”等价的特征。

### 自适应地形课程（Adaptive Terrain Curriculum）

为了让机器人学会走各种路，训练地形不能一成不变，也不能随机生成（太简单没用，太难学不会）。
论文设计了一个**粒子滤波**机制来自动生成地形：

1.  **定义难度**：用“通过率”（Traversability）来衡量。目标是把通过率维持在 **0.5 到 0.9** 之间（既有挑战又能走过去）。
2.  **动态调整**：
    -   如果当前地形参数（比如台阶高度 10cm）通过率太高，就增加难度（比如变成 12cm）。
    -   如果太低，就降低难度。
    -   这就形成了一个自动的 Curriculum，随着策略变强，地形也越来越难。

### 消融方法验证

![](/paper/anymal-tcn-ablation.png)
*图：消融实验分析。(B-D) TCN 记忆长度的重要性：记忆越长，抗干扰越强。(E-G) 特权学习的重要性：直接训练（红色）完全失败，必须有特权引导（蓝色）。(H-J) 自适应课程的重要性：有课程（蓝色）比随机地形（红色）效果更好。*

| 消融实验 | 对比组 | 结论 |
| :--- | :--- | :--- |
| **TCN 记忆长度** | TCN-1 (0.02s) vs TCN-100 (2s) | 记忆越长，台阶能过得越高，抗干扰能力（推力）越强（偏差降低 35.5%）。 |
| **特权训练** | 直接训练 vs 两阶段训练 | 直接用本体感受训练 TCN 基本不收敛（无法上坡/过台阶），特权学习是关键。 |
| **自适应课程** | 随机地形 vs 自适应地形 | 随机地形效率低，自适应课程能让策略在“舒适区”边缘不断进步，最终泛化性更强。 |

### 涌现行为分析（Emergent Behavior）

这一段非常有意思，作者试图证明 TCN 到底学到了什么。
他们做了一个解码器，试图从学生网络的中间层还原出地形信息。结果发现（对应原论文图 6）：
-   **地形重构**：学生网络竟然真的能从本体感受里“重构”出脚下的台阶高度（虽然它根本没视觉！）。
-   **注意力分析**：当脚卡在台阶上时，网络会特别关注那一刻的关节状态，并在随后的几步里调整抬腿高度。

这说明 TCN 不是在死记硬背，而是真的**理解**了环境。

## 实验

实验部分非常扎实，覆盖了仿真、实验室受控环境，以及 DARPA 地下挑战赛的实战。

### 场景 1：自然环境测试 —— 零样本泛化

最震撼的是这个“零样本泛化”：训练时只用了刚性的台阶和山丘，测试时直接扔到泥泞、积雪、茂密植被里。

**结果对比**（Ours vs 传统 MPC 基线）：

| 环境 | 指标 | 论文方法 (Ours) | 基线 (Baseline) | 核心差异 |
| :--- | :--- | :--- | :--- | :--- |
| **苔藓** | 速度 (m/s) | **0.452** | 0.199 | 速度提升 2.27 倍 |
| | COT (能效) | **0.423** | 0.625 | 能效提升 32.3% |
| **泥泞** | 速度 (m/s) | **0.338** | 0.197 | 速度提升 1.71 倍 |
| | COT (能效) | **0.692** | 0.931 | 能效提升 25.7% |
| **植被** | 速度 (m/s) | **0.248** | — | 基线完全无法稳定运动 |

**结论**：在植被和泥泞这种外感受容易失效、物理特性复杂的环境里，纯本体感受的 TCN 策略表现出了惊人的鲁棒性。基线控制器因为依赖显式的接触检测，在植被里经常误判，导致频繁腿部打架或死机。

![](/paper/anymal-tcn-challenges.png)
*图：在多种挑战性自然环境中的部署测试。策略展现了极强的零样本泛化能力。*

### 场景 2：DARPA 地下挑战赛

这是真刀真枪的实战。
-   **任务**：60 分钟内自主导航地下环境（含 45° 陡坡、18cm 台阶）。
-   **结果**：4 次任务全部**零故障**完成。
-   **意义**：这是首次证明 RL 训练的控制器能胜任这种高强度的实战竞赛，不再是实验室里的玩具。

### 场景 3：室内受控实验

为了量化分析，作者在室内设计了几个专门搞破坏的场景。

![](/paper/anymal-tcn-indoor-eva.png)
*图：室内受控环境下的定量评估。(A) 不稳定碎石模拟（松动木板）。(B) 足部陷阱反射（卡脚）。(C) 小腿碰撞障碍物。(D) 额外负载实验。(E-G) 各项指标的统计数据。*

1.  **不稳定支撑面**（图 A）：脚下的木板会动。基线控制器直接翻车，Ours 稳如老狗。
2.  **足部陷阱反射**（图 B）：专门设计了个 16.8cm 的台阶卡脚。Ours 学会了“绊倒后抬腿更高”的反射动作，成功越障。
3.  **模型失配**（图 D）：直接加了 10kg 负载（没训练过）。Ours 依然能过 13.4cm 的台阶，基线直接过不去。

| 条件 | 论文方法 (Ours) | 基线 (Baseline) |
| :--- | :--- | :--- |
| **最大可过台阶高度** | **13.4cm** | 0cm |
| **故障率** | **0%** | 100% |

这证明了基于本体感受历史的策略，对动力学模型的不准确性有极强的容忍度。

4.  **抗足部打滑**（湿白板，补充视频 S5）：
    -   **场景**：地面是湿润的白板，摩擦系数极低，模拟冰面。
    -   **结果**：基线控制器 10 秒内就滑倒摔傻了；Ours 通过降低步频、增加接触面积，稳稳地走了过去。

## 小结与局限

这篇论文不仅仅是“做出了一个能跑的机器人”，更重要的是它在**方法论**上打破了之前的认知偏见：

-   **打破仿真迷信**：以前大家觉得仿真必须做得极其逼真（模拟泥泞、软土、流体）才能 Sim2Real。但这篇论文证明：**根本不需要！** 只要用简单的刚性地形（台阶、山丘）配合**特权学习**和**本体感受历史**，机器人就能自己学会应对泥泞和植被。它学到的是“物理交互的底层逻辑”（比如受力突变=有障碍），而不是死记硬背某种地形。
-   **盲控的极限**：证明了纯本体感受（Blind Locomotion）在极端环境下的鲁棒性远超预期，甚至比很多带视觉的方法更稳。

当然，局限性也很明显：
1.  **步态单一**：只会 Trot（小跑）。真狗是会根据地形切换步态的（比如爬坡用 Walk，平地用 Gallop）。
2.  **盲控的硬伤**：虽然稳，但它是“反应式”的。遇到悬崖它看不见，只能踩空了再救（往往来不及）。而且因为看不见，走得比较保守（步频慢、抬腿高），效率不如有视觉的机器人。

**未来方向**也很明确：
1.  **加眼睛**：把视觉（远距离规划）和本体感受（近距离反射）结合起来。看得见的时候规划最优路径，看不见的时候靠本体感受保命。
2.  **多步态**：通过奖励函数鼓励机器人自动切换步态。


