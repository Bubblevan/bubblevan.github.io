---
title: "Locomotion 文献精读（十）Learning robust perceptive locomotion for quadrupedal robots in the wild"
---

> https://arxiv.org/abs/2201.08117
> Science Robotics 2022

我妈逮着一只 Anymal 使劲薅啊？！又发 Science 了？

## 研究背景
老生常谈，复制粘贴。不过不一样的是这里额外批判了一下外感受感知的 “不可靠性”。

现有腿式机器人依赖的深度传感器（如立体相机、ToF 相机），受环境影响极大：
- **立体相机**：通过比对左右镜头图像的 “纹理差异” 计算深度，但在 “低纹理表面”（如光滑墙面、雪地）或 “曝光异常”（过亮 / 过暗）时，无法完成纹理匹配，深度计算失效；
- **ToF 相机（飞行时间相机）**：通过光的飞行时间推断距离，但对 “深色表面”（吸收光线，信号弱）感知差，且在 “强光（如 sunlight）” 下会产生大量噪声，深度数据不可靠；
- **共性问题**：所有依赖 “光线推断距离” 的传感器，在 “高反射表面”（如金属、水面）会产生 “伪影” —— 因为传感器默认光线沿直线传播，但反射光会打乱这一假设，导致深度计算错误。

深度传感器只能感知 “地形的几何形状”（比如 “这里有个凸起”），但无法区分表面的 “软硬 / 稳定性”：例如，植被（柔软，踩上去会塌陷）和岩石（坚硬，可支撑）在深度数据中都是 “凸起”，机器人无法提前判断是否能踩踏，只能靠物理接触后调整，进一步限制速度。

现有方法常用 “高程图（Elevation Map）” 整合深度传感器数据，以表示地形的高度分布，但这种表征存在 3 个关键误差来源：
1. **依赖 “机器人位姿估计”**：高程图需要先确定机器人自身的位置 / 姿态，才能将传感器数据对应到正确的地形位置；若位姿估计有误差（比如机器人打滑导致定位漂移），整个高程图会 “偏移”，变得不可靠；
2. **遮挡问题**：传感器被障碍物（如树木、岩石）遮挡时，遮挡区域的地形数据会缺失；
3. **动态物体干扰**：若环境中有移动的物体（如风吹动的植被、滚落的石块），会导致不同时间采集的地形数据不一致，高程图出现 “时间冲突”。

综上，现有依赖外感受的方法，在真实野外环境中极易因上述问题失效。

面对外感受的不可靠性，传统腿式机器人控制方法要么 “假设感知完美”，要么 “放弃外感受”，均无法解决根本问题，具体分为两类：
1. **离线方法（Offline Methods）**
    - **核心逻辑**：依赖预先扫描的 “完美地形图”，通过手工设计的 “成本函数”（如 “避开高度差超过 10cm 的区域”）优化运动轨迹，再让机器人 “复现” 该轨迹；
    - **致命缺陷**：假设 “地形已知且不变”“机器人状态完全可控”，但真实野外环境无法提前扫描（如灾区、未知星球），且复杂地形的轨迹规划耗时极长，无法实时响应变化。
2. **在线方法（Online Methods）**
    - **核心改进**：不用预扫描地图，而是通过机器人 “实时车载传感器” 构建高程图，并持续更新轨迹；
    - **本质局限**：仍默认 “实时构建的高程图足够准确”，没有解决 “传感器噪声、遮挡、位姿漂移” 等根本问题 —— 一旦外感受失效（如雪地反光导致深度数据缺失），轨迹规划会立即出错，机器人无法应对。

正是因为 “外感受不可靠”“传统方法回避感知不确定性”“仅依赖本体感受导致速度慢”，才需要新的技术突破：通过端到端训练的注意力循环编码器，无缝融合本体感受与外感受，既利用外感受的 “预判能力” 提升速度，又在感受失效时 fallback 到本体感受保证稳健性。

## 方法论
方法的核心逻辑是：先在 “感知完美的仿真环境” 中训练一个 “教师策略”（掌握最优运动模式），再让 “学生策略” 在 “带噪声的仿真感知” 下模仿教师，同时学习融合多模态感知以应对真实环境的不确定性，最后将学生策略零样本迁移（无需微调）到物理机器人。

三个阶段的流程如下图所示：

![](/paper/anymal-transformer-pipeline.png)
*训练方法与部署流程概览。首先利用强化学习（RL）训练一个能获取仿真中特权数据的 “教师策略”。随后，该教师策略被蒸馏给 “学生策略”，学生策略不仅要模仿教师的动作，还要从带噪声的观测中重建真实环境状态。最终，我们将学生策略零样本部署到真实硬件上，利用以机器人为中心的高程图中的高度采样作为输入。*

### 问题定义
**部分可观测马尔可夫决策过程（POMDP）**
- **MDP（完全可观测）**：若机器人能获取所有环境状态（如无噪声地形、外力、摩擦力），则状态 $s_t$ = 观测 $o_t$，可直接用强化学习（RL）优化策略；
- **POMDP（部分可观测）**：真实环境中，机器人无法观测到 “完整地形”“地面摩擦力”“外部扰动” 等信息，且外感受（深度 / 视觉）存在噪声 / 遮挡，导致 $o_t \neq s_t$。

解决 POMDP 的关键是构建 “信念状态（Belief State）$b_t$”：通过 “观测历史（$o_0 \sim o_t$）” 压缩过去的感知信息，间接捕捉不可观测的环境状态。论文采用 “循环神经网络（GRU）” 实现信念状态编码，而非简单堆叠观测序列，以高效压缩时序信息。

### 训练环境设计
- 采用 RaiSim 模拟器，并行模拟多台 ANYmal-C 四足机器人；
- 集成 “高精度执行器模型”，还原真实机器人的关节动力学（如扭矩限制、摩擦），避免仿真中 “超现实运动” 导致迁移失败。
- 地形建模为 “高度图”，包含斜坡、台阶等基础地形；
- **关键设计**：用立方体构建 4 种楼梯（标准、开放、凸缘、随机），而非直接用高度图生成楼梯 —— 因为高度图模拟的楼梯立板 “非完全垂直”，策略会利用这一仿真缺陷（如踩立板边缘），导致迁移到真实机器人时失败。

为了让策略对 “机器人参数波动” 和 “环境干扰” 鲁棒，每个训练回合（Episode）域随机化（Domain Randomization）以下参数：
- **机器人参数**：身体 / 腿部质量、初始关节位置 / 速度、初始身体姿态 / 速度；
- **环境干扰**：对机器人身体施加随机外力 / 扭矩、随机降低足部摩擦系数（模拟打滑）。

当机器人进入 “危险状态” 时终止回合，强制策略学习 “安全运动”：
机身与地面碰撞、机身大幅倾斜（超出平衡阈值）、关节扭矩超过执行器限制。

### 阶段 1：教师策略训练（Teacher Policy Training）
**1. 核心算法与策略建模**
- 采用 PPO（近端策略优化） 算法（RL 常用算法，稳定且样本效率高）；
- 策略建模为 “高斯策略”：动作 $a_t \sim N(\pi_\theta(o_t=s_t), \sigma I)$，其中 $\pi_\theta$ 是用多层感知器（MLP）实现的策略网络，$\sigma$ 是动作方差（控制探索程度）。

**2. 观测与动作空间**
教师策略能获取 “特权信息”，观测空间包含三类信息（确保感知完整）：

| 观测类型 | 具体内容 |
| :--- | :--- |
| **本体感受 ($o^p_t$)** | 身体速度 / 姿态、关节位置 / 速度历史、动作历史、每条腿的运动相位（Phase） |
| **外感受 ($o^e_t$)** | 每个足部周围 “5 个不同半径” 的高度样本（提前感知地形起伏） |
| **特权信息 ($s^p_t$)** | 接触状态（脚是否着地）、接触力 / 法线、地面摩擦系数、外力 / 扭矩、摆动相时长 |

动作空间受中枢模式发生器（CPGs） 启发（模拟动物周期性步态）：
- 每条腿有一个 “相位变量 $\phi_l$”，基于 $\phi_l$ 生成 “名义关节轨迹”（通过逆运动学计算）；
- 策略输出的动作是 “相位差 $\Delta \phi_l$”（调整步态节奏）和 “残余关节位置 $\Delta q_i$”（微调关节目标位置，适应地形细节）。

**3. 奖励函数设计**
通过 “正奖励引导目标行为，负奖励约束危险行为”，确保策略既高效又安全：
- **正奖励**：遵循指令速度（如水平速度 $v$ 与期望速度 $v_{des}$ 匹配时，奖励最高 1.0；偏差越大奖励越低，用指数函数衰减）、身体姿态稳定；
- **负奖励**：与期望速度正交的速度分量（避免偏移方向）、关节扭矩 / 速度 / 加速度过大（避免剧烈运动）、足部打滑、小腿 / 膝盖碰撞。

**4. 课程学习（Curriculum Learning）**
为了避免策略 “一开始在复杂地形上失败”，逐步增加训练难度：
- **地形课程**：用粒子滤波器动态调整地形参数（如斜坡角度、台阶高度），确保地形 “有挑战性但可通过”；
- **干扰课程**：用逻辑函数 $c_{k+1}=(c_k)^d$ ($0<d<1$) 逐步增加 “域随机化幅度” 和 “约束奖励权重”，最终趋于 1（完全模拟真实干扰）。

### 阶段 2：学生策略训练（Student Policy Training）
仅使用 “真实机器人可获取的感知”（带噪声的外感受 + 本体感受），模仿教师策略，同时学习 “应对感知不确定性”，核心是循环信念状态编码器的设计。

**1. 核心改进：感知噪声模拟**
学生的外感受输入添加参数化噪声模型 $n(o^e_t)$，模拟真实环境中的感知失效（如遮挡、位姿漂移），观测空间变为 $o^{student}_t = (o^p_t, n(o^e_t))$，噪声类型分三类：

| 噪声类型 | 模拟场景 | 应用比例 |
| :--- | :--- | :--- |
| **名义噪声（Nominal）** | 常规环境下的轻微噪声（地图质量良好） | 60% |
| **大偏移噪声（Large Offset）** | 位姿估计漂移或地形变形导致的地图偏移（如高程图整体偏移） | 30% |
| **大噪声（Large Noise）** | 遮挡或传感器故障导致的地形信息完全缺失（如扫描点全为随机值） | 10% |

噪声施加方式：横向移动扫描点、扰动高度值，且噪声在 “每个扫描点、每个足部、每个回合” 三个维度独立采样（覆盖不同尺度的感知误差）。

**2. 学生策略架构：循环信念状态编码器**
核心创新点，用于 “融合多模态感知、捕捉不可观测信息”，如图所示（组件 C）：
- **输入**：带噪声的外感受特征 ($l^e_t = g_e(n(o^e_t))$，$g_e$ 是外感受编码器)、本体感受 $o^p_t$、上一时刻的隐藏状态 $h_t$；
- **核心组件**：
    - **GRU 循环单元**：处理时序信息，将多模态感知压缩为 “中间信念状态 $b'_t$”，并更新隐藏状态 $h_{t+1}$；
    - **注意力门控（Attentional Gate）**：计算注意力向量 $\alpha$（sigmoid 函数输出，0~1），动态控制外感受信息的权重 —— 外感受可靠时 $\alpha \approx 1$（多利用地形预判），失效时 $\alpha \approx 0$（无缝切换到本体感受）；
    - **最终信念状态 $b_t$**：$b_t = g_b(b'_t) + l^e_t \cdot \alpha$（$g_b$ 是全连接网络，融合本体感受与加权外感受）。

**3. 训练损失：双损失约束**
通过 “行为克隆损失 + 重构损失”，确保学生策略既 “模仿教师动作”，又 “理解环境信息”：
- **行为克隆损失（Behavior Cloning Loss）**：最小化 “学生动作” 与 “同一状态下教师动作” 的平方距离，确保模仿精度；
- **重构损失（Reconstruction Loss）**：通过 “信念解码器”（组件 D）从 $b_t$ 中还原 “无噪声外感受 $o^e_t$” 和 “特权信息 $s^p_t$”，最小化还原值与真实值的平方距离 —— 强制 $b_t$ 捕捉环境的真实状态，避免仅模仿动作而不理解地形。

**4. 初始化优化**
学生策略的 MLP 结构与教师完全一致，复用教师策略的已训练权重初始化学生网络 —— 加速训练收敛，避免从零开始学习。

### 阶段 3：部署（Deployment）
**1. 传感器配置与高程图构建**
- **传感器**：两种配置（兼容不同场景）——2 个 Robosense Bpearl 穹顶激光雷达 / 4 个 Intel RealSense D435 深度相机；
- **高程图（2.5D）**：
    - **生成频率 20Hz**：通过卡尔曼滤波更新地图（融合多帧点云，降低噪声），并进行 “漂移补偿” 和 “射线投射”（确保地图一致性）；
    - **GPU 加速**：并行处理点云，满足实时性（避免地图更新滞后于机器人运动）。

**2. 策略运行流程**
- **策略运行频率 50Hz**（高于高程图更新频率，确保步态响应迅速）；
- **外感受输入**：从最新高程图中采样 “足部周围高度样本”，若某位置无地图信息（如遮挡），填充随机值（策略已通过训练适应此类情况）；
- **输出**：关节目标位置，直接控制机器人执行器。

**3. 零样本迁移关键**
无需任何现场微调，原因是：
- 仿真中的 “域随机化” 覆盖了机器人参数波动和环境干扰；
- 学生策略已通过 “高度样本随机化” 学习应对感知噪声 / 失效；
- 高程图作为 “传感器抽象层”，兼容激光雷达 / 深度相机（无需重新训练）。

## 实验
通过实地测试、定量对比、鲁棒性验证三个维度，全面证明了所提控制器的核心优势 ——“野外环境下的高速性与鲁棒性统一”，同时验证了 “外感受与本体感受融合” 的有效性。

### 核心结果 1：野外环境中的快速稳健运动（Fast and Robust Locomotion in the Wild）
**1. 测试场景与核心表现**
控制器在 4 类典型野外环境中完成测试，全程零摔倒，关键亮点如下：

| 环境类型 | 具体挑战 | 控制器表现 |
| :--- | :--- | :--- |
| **自然环境**<br>（高山 / 森林 / 雪地） | 陡坡（最大 38%）、湿滑表面、草地、积雪（高反射 + 腿部下陷） | 提前预判地形（如积雪下的地面高度），调整步态避免打滑 / 下陷，无感知失效导致的故障 |
| **地下环境** | 松散砾石、沙尘、积水、低光照（传感器噪声增加） | 兼容激光雷达 / 深度相机，即使低光照下也能通过高程图稳定感知，无卡滞 |
| **城市环境** | 楼梯、反光地面（如金属地板） | 无需专门楼梯模式（对比波士顿动力 Spot），可侧向 / 斜向 / 转身爬楼梯，反光地面无感知伪影 |

**2. 关键对比：与波士顿动力 Spot 的差异**
- **Spot 的局限**：爬楼梯需启用 “专门模式”，且必须与楼梯严格对齐（方向固定）；
- **本文控制器优势**：通过外感受实时感知楼梯结构，支持 “任意方向 / 姿态” 爬楼梯（如侧向走、中途转身），更适应真实场景中的非理想楼梯条件（如积雪覆盖的楼梯，图 1R）。

**3. 复合挑战应对**
针对 “积雪 + 楼梯” 这类 “多重感知干扰” 场景（积雪导致：①深度传感器反射失效；②高程图误判地面高度；③足部打滑导致位姿漂移），控制器通过 “信念状态” 动态调整：
- 初始外感受不可靠时，快速切换到本体感受；
- 足部接触积雪后，通过本体感受修正地形高度估计，避免腿部下陷导致的失衡，最终零故障通过。

![](paper/anymal-transformer-e1.png)
*野外环境中的鲁棒运动。该运动控制器在多种复杂环境中经历了跨季节的广泛测试，克服了一系列现实挑战，且往往是多种挑战并发。包括自然环境中的湿滑表面、陡坡、复杂地形和植被。在搜救场景中，控制器应对了陡峭楼梯、未知负载和影响感知的浓雾。在地下洞穴系统中，遭遇了高反光表面、松散地面、低光照和积水。冬季还面临积雪堆积。在所有这些环境中，控制器均实现了零故障通行。*

### 核心结果 2：阿尔卑斯山徒步实验（A Hike in the Alps）
> 这太酷了，比爬玉泉老何山还酷

这是最具代表性的实地验证，模拟真实 “长时间、高难度” 野外任务，直接证明控制器的实用价值。

**1. 实验参数（严苛性设计）**
- **路线**：瑞士埃策尔山（Etzel mountain）徒步环路，全长 2.2km，海拔提升 120m，最大坡度 38%；
- **地形复杂度**：包含岩石地、湿滑陡坡、树根障碍、植被遮挡（干扰高程图）；
- **难度评级**：第三方徒步工具评为 “困难”（基于体能需求、技术复杂度）。

**2. 核心结果：速度与稳健性双达标**

| 指标 | 本文控制器 | 人类参考（官方标牌 / 徒步工具） | 结论 |
| :--- | :--- | :--- | :--- |
| **登顶时间** | 31 分钟 | 35 分钟（官方标牌建议） | 比人类快 11.4%，体现外感受带来的速度优势 |
| **全程时间** | 78 分钟 | 76 分钟（徒步工具建议） | 与人类接近，无因故障导致的额外耗时 |
| **中途停顿原因** | 仅 2 次<br>（修复脱落的 “机器人足部” + 换电池） | 无（人类可自主应对简单故障） | 非控制器性能问题，纯机械维护需求 |
| **故障次数** | 0 次 | - | 验证长时间复杂环境下的鲁棒性 |

**3. 实验中的关键挑战与应对**
- **植被遮挡**：机器人上方的树枝导致高程图出现 “伪影”（误判为障碍物），控制器通过 “信念状态” 识别外感受不可靠，切换到本体感受，绕开伪影区域；
- **湿滑陡坡（38% 坡度）**：外感受提前感知坡度，调整腿部支撑角度（增大接地面积），避免打滑；本体感受实时监测身体姿态，修正微小失衡。

![](/paper/anymal-transformer-e2.png)
*ANYmal 搭载该运动控制器完成瑞士埃策尔山徒步。这条 2.2 公里的路线包含 120 米的海拔爬升和高达 38% 的坡度，涵盖了多种挑战性地形。ANYmal 登顶速度快于官方标牌指示的人类时间，且全程耗时与徒步指南给出的时间几乎一致。*

### 核心结果 3：外感受挑战应对（Exteroceptive Challenges）
这部分聚焦 “外感受失效的典型场景”，解析控制器如何通过 “信念状态” 容错，证明其鲁棒性的根源。

![](/paper/anymal-transformer-e3.png)
*我们的运动控制器通过高程图中的高度采样（红点）感知环境 (A)。控制器对野外常见的多种感知挑战具有鲁棒性：因传感失效导致的地图信息缺失 (B, C, G)，以及因非刚性地形 (D, E) 和位姿估计漂移 (F) 导致的误导性地图信息。*

**1. 6 类典型感知失效场景与应对机制**

| 感知失效场景 | 高程图问题 | 控制器应对策略（信念状态作用） |
| :--- | :--- | :--- |
| **反光表面**<br>（如金属地板） | 深度传感器出现 “异常值”，高程图显 “沟槽” | 信念状态识别外感受噪声，降低外感受权重，依赖本体感受保持直线行走，无偏离路径 |
| **深雪** | 传感器无深度数据，高程图 “空白” | 接触雪面后，本体感受感知腿部下陷，修正信念状态中的地形高度，调整步长避免继续下陷 |
| **悬垂物体**<br>（如树枝） | 2.5D 高程图误判为 “高大障碍物” | 信念状态结合 “历史感知”（树枝不会阻碍地面行走），忽略伪影，正常前进 |
| **非刚性地形**<br>（如软植被） | 高程图无法区分 “植被 / 地面”，误判高度 | 足部接触后，本体感受感知地面硬度，修正信念状态，避免踩空 / 倾倒 |
| **足部打滑**<br>（光滑平台） | 位姿漂移导致高程图 “偏移” | 实时监测打滑（本体感受的关节速度异常），切换到本体感受控制，同时暂停依赖高程图，待漂移修正后恢复 |
| **遮挡**<br>（如上坡时的前方地形） | 传感器视角受限，高程图 “缺失前方数据” | 信念状态利用 “历史地形趋势”（上坡坡度）预判前方高度，提前抬升腿部，避免接触时卡滞 |

**2. 核心结论**
控制器的鲁棒性并非依赖 “完美外感受”，而是通过 “信念状态” 动态评估外感受可靠性：
- **外感受可靠时**：利用其 “预判能力” 提升速度；
- **外感受失效时**：无缝切换到本体感受（已验证为稳健模式），避免灾难性故障。

### 核心结果 4：外感受的定量贡献（Evaluating the Contribution of Exteroception）
通过对照实验（与 “纯本体感受基线” 对比），定量证明外感受对 “速度、障碍通过性” 的提升，是结果的核心数据支撑。

**1. 实验设计**
- **基线组**：仅用本体感受的控制器（文献方法，无外感受）；
- **实验组**：本文控制器（外感受 + 本体感受融合）；
- **测试维度**：台阶跨越成功率、障碍赛道完成时间、最大运动速度。

**2. 关键定量结果**

**(1) 台阶跨越成功率**

| 台阶高度 | 基线组（纯本体感受）成功率 | 实验组（本文方法）成功率 | 差异原因 |
| :--- | :--- | :--- | :--- |
| **≤18cm** | 100% | 100% | 低台阶下，本体感受可通过物理接触调整步态 |
| **20cm** | 降至 60%（前腿卡滞） | 100% | 实验组提前预判台阶高度，抬升腿部；基线需接触后调整，易卡滞 |
| **30.5cm** | 0%（无法跨越） | 100% | 实验组通过外感受感知台阶极限高度，调整身体前倾辅助后腿跨越；基线超出物理适应能力 |
| **>32cm** | - | 0%（主动犹豫） | 控制器学习到 “超出物理极限”，主动停止前进，避免摔倒（体现安全性） |

**(2) 障碍赛道完成时间**
- **赛道设计**：包含倾斜平台（20cm 高）、凸起平台（20cm 高）、楼梯（17cm 高）、积木堆（20cm 高）；
- **结果**：
    - **实验组**：33 秒，无任何辅助，全程平稳（外感受提前调整步态）；
    - **基线组**：75 秒，需人工辅助 3 次（在所有障碍物处卡滞，需抬起机器人）。

**(3) 最大运动速度**

| 运动场景 | 指标 | 基线组（纯本体感受） | 实验组（本文方法） | 提升幅度 | 差异原因 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **平坦地面** | 前进 / 横向速度 | 0.6 m/s | 1.2 m/s | 2 倍 | 实验组无需 “物理试探” 地形，可维持高速；基线需缓慢接触地面确认安全性 |
| **平坦地面** | 转向速度 | 0.6 rad/s | 3 rad/s | 5 倍 | 实验组通过外感受感知周围无障碍，可快速转向；基线需谨慎调整姿态避免失衡 |
| **跨越 20cm 台阶** | 速度保持率 | 降至 0.2 m/s（卡顿） | 1.0 m/s（无减速） | - | 实验组提前抬升腿部；基线需碰撞台阶后调整，导致卡顿 |

**1. 核心结论**
外感受的核心价值是 **“预判性”**：避免机器人通过 “物理碰撞” 获取地形信息，从而大幅提升速度（最高 2 倍）和障碍通过性（30.5cm 台阶 vs 20cm 台阶），同时减少卡滞风险。

![](/paper/anymal-transformer-e4.png)
*感知失效期间的内部信念状态检查（通过学习到的信念解码器）。红点表示输入策略的高度采样，蓝点表示控制器内部对地形轮廓的估计。(A) 踩踏无法支撑落脚点的软障碍物后，策略正确地向下修正了地形轮廓估计。(B) 透明障碍物在接触后被正确纳入地形轮廓。(C) 传感器正常工作时，机器人迅速优雅地爬楼梯，无误触。(D) 遮挡传感器致盲时，策略虽无法预判地形但仍保持鲁棒，成功穿越楼梯。(E) 踏上湿滑平台时，策略识别出低摩擦并补偿了引发的位姿估计漂移。图表显示了解码出的摩擦系数。*

### 核心结果 5：信念状态可视化验证鲁棒性（Evaluating Robustness with Belief State Visualization）
通过 “信念状态解码”，直观展示控制器 “如何融合多模态感知”，揭示鲁棒性的微观机制（非黑箱结果）。

**1. 实验设计**
选取 4 类 “感知模糊 / 失效” 场景，通过 “红色（外感受输入）” 与 “蓝色（信念状态估计）” 的对比，观察信念状态的修正过程：

**(1) 软泡沫障碍实验**
- **场景**：泡沫外观为 “固体”，但无法支撑机器人重量（外感受误导）；
- **过程**：
    - **初始阶段**：信念状态（蓝）完全信任外感受（红），准备按 “固体障碍” 抬升腿部；
    - **接触后**：本体感受感知泡沫变形（腿部下陷），信念状态快速向下修正地形高度估计；
    - **离开后**：信念状态保留 “泡沫不可支撑” 的历史信息（循环结构作用），后续步伐避开该区域。

**(2) 透明亚克力障碍实验**
- **场景**：透明材料无法被传感器感知（外感受缺失）；
- **过程**：
    - **接触前**：信念状态（蓝）按 “平地” 估计地形，机器人正常行走；
    - **接触后**：本体感受感知障碍存在，信念状态向上修正地形高度，调整步态跨越障碍。

**(3) 传感器完全遮挡实验**
- **场景**：物理覆盖所有传感器（外感受完全失效，输入为随机噪声），测试爬楼梯；
- **过程**：
    - **遮挡前**：信念状态（蓝）准确估计楼梯高度，平稳爬梯；
    - **遮挡后**：外感受无有效信息，信念状态切换到 “本体感受主导”；
    - **爬梯时**：前腿碰撞台阶竖板（无法预判），但本体感受快速调整步态，成功爬上；下梯时前足重落地，但通过身体姿态修正保持平衡（无摔倒）。

**(4) 光滑平台打滑实验**
- **场景**：平台摩擦系数低，足部打滑导致位姿漂移（高程图失效）；
- **过程**：
    - **踏上平台**：本体感受感知打滑（关节速度异常），信念状态降低外感受权重；
    - **漂移期间**：完全依赖本体感受，调整步频（加快步伐）保持平衡；
    - **离开平台**：位姿漂移修正，高程图恢复稳定，信念状态重新融入外感受。

**2. 核心结论**
信念状态的循环结构（GRU）和注意力门控，实现了 **“历史信息记忆 + 动态感知权重调整”**：
- 可修正外感受的误导 / 缺失；
- 可保留关键地形信息（如泡沫不可支撑）；
- 实现外感受 / 本体感受的无缝切换，是鲁棒性的核心机制。

## 小结与局限
控制器的核心突破是解决了 “外感受不可靠” 与 “本体感受速度慢” 的矛盾，实现两者的 “无缝切换”：
- **外感受的价值**：通过 “预判地形”（如提前感知台阶高度、斜坡角度），让机器人在接触地形前调整步态，避免 “物理试探”（如基线控制器碰撞台阶后调整），从而实现 “快速且优雅” 的运动（前文结果：平坦地面速度 1.2m/s，是基线的 2 倍）；
- **本体感受的价值**：作为 “鲁棒 fallback”，当外感受完全失效（如传感器被遮挡、积雪反光导致深度数据缺失）时，控制器能平稳切换到本体感受主导，即使机器人处于 “近乎失明” 的状态（如传感器全覆盖爬楼梯），仍能保持平衡不摔倒（前文信念状态可视化实验验证）；
- **融合方式的创新**：通过端到端学习实现融合，无需手工编写规则（如 “台阶高度 > 20cm 时抬腿 30cm”“反光表面忽略深度数据”）—— 模型通过信念状态编码器自主学习 “何时信任外感受、何时依赖本体感受”，避免了传统方法 “启发式规则难泛化” 的问题。

**技术定位**：首个 “速度与鲁棒性统一” 的崎岖地形控制器
此前技术要么 “依赖外感受求速度”（但易因感知失效故障），要么 “依赖本体感受求稳健”（但速度慢）；而本控制器是首个同时具备 “视觉驱动的速度” 与 “本体感受的稳健” 的方案，填补了野外复杂环境应用的空白（前文阿尔卑斯徒步、障碍赛道结果均验证这一点）。

通过两个 “高难度真实场景” 的应用案例，证明控制器的实用价值（非仅实验室性能），进一步强化成果的说服力：
1. **案例 1：阿尔卑斯山 “困难级” 徒步**
    - **任务难度**：2.2km 路线、海拔提升 120m、最大坡度 38%，第三方评级 “困难”（需人类具备一定体能与地形判断能力）；
    - **核心表现**：
        - **自主性**：全程无人类协助（仅 2 次机械维护：修复脱落的 “机器人足部”、更换电池）；
        - **效率**：全程耗时 78 分钟，与人类徒步指南推荐时间（76 分钟）几乎一致，登顶速度（31 分钟）甚至快于官方标牌的人类预期时间（35 分钟）；
        - **意义**：证明控制器能支撑 “高长时间、非结构化” 的野外探索任务（如灾区搜救、偏远地区勘探）。
2. **案例 2：DARPA 地下挑战赛（冠军任务）**
    - **任务背景**：DARPA（美国国防高级研究计划局）地下挑战赛是机器人领域的 “顶级实战测试”，要求机器人在地下隧道、城市废墟、洞穴三类复杂环境中自主探索，面临 “低光照、灰尘 / 雾气 / 烟雾干扰感知、崎岖地形（碎石 / 积水）” 等多重挑战；
    - **控制器角色**：作为 Cerberus 团队的 “默认控制器”，驱动 4 台 ANYmal 机器人完成探索；
    - **核心成果**：
        - **探索范围**：三类赛道累计探索超 1700 米；
        - **稳健性**：全程零摔倒，即使在 “感知严重退化”（如灰尘遮挡传感器、雾气导致深度数据噪声）的场景中，仍能持续运动；
        - **意义**：证明控制器能支撑 “高可靠性要求的军事 / 救援任务”，是从 “技术原型” 到 “实用工具” 的关键验证。

**局限与改进方向**

1. **方向 1：显式利用信念状态的 “不确定性信息”**
    - **当前局限**：目前信念状态仅 “隐式” 利用不确定性（如外感受噪声大时降低其权重），但未 “显式” 量化不确定性 —— 例如，在 “悬崖边缘”“垫脚石间隙” 等场景，因遮挡导致高程图信息缺失，控制器会 “默认地形连续”，可能导致机器人踏空坠落；
    - **改进思路**：让模型显式估计 “地形信息的不确定性” —— 若某区域不确定性高（如悬崖遮挡区），控制器可主动采取 “谨慎策略”（如用足部轻探地面确认安全性），而非默认前进，进一步提升极端场景的稳健性。
2. **方向 2：直接处理 “原始传感器数据”（替代高程图）**
    - **当前局限**：目前控制器依赖 “高程图” 作为感知中间层（将激光雷达 / 深度相机数据转化为高度分布），虽实现 “传感器无关性”（兼容激光雷达 / 相机，无需重训），但丢失了原始数据中的细节信息（如地形材质：草地 / 岩石 / 冰面的纹理差异、表面硬度相关特征）；
    - **改进思路**：让模型直接输入原始点云 / 图像数据，而非经过高程图抽象 —— 这样模型可学习 “从纹理判断表面摩擦系数”“从点云密度判断材质软硬”，进一步提升地形适应能力（如区分冰面与岩石，提前调整步态防打滑）。
3. **方向 3：联合训练 “位姿估计模块” 与控制器**
    - **当前局限**：目前高程图的构建依赖 “独立的经典位姿估计模块”（未与控制器的信念状态联合训练）—— 当机器人打滑、碰撞时，位姿估计易漂移，导致高程图失真，进而影响外感受可靠性；
    - **改进思路**：将 “位姿估计” 融入控制器的端到端训练，让信念状态同时学习 “地形估计” 与 “位姿修正” —— 例如，当检测到足部打滑（本体感受信号）时，信念状态可主动修正位姿估计误差，避免高程图漂移，提升感知 - 控制闭环的一致性。
4. **方向 4：学习 “遮挡模型” 与应对 “特殊动作任务”**
    - **当前局限 1（遮挡）**：控制器无法识别 “遮挡区域的存在”（如悬崖后方被遮挡，模型不知道 “后面是空的”），仅能被动应对遮挡导致的信息缺失；
    - **改进**：学习 “遮挡模型”，让模型从 “传感器视角” 推断 “哪些区域是遮挡区”，主动避开（如悬崖边缘），而非被动假设地形连续；
    - **当前局限 2（动作范围）**：控制器仅能完成 “正常行走相关动作”（如爬楼梯、过斜坡），无法应对 “非典型动作”（如腿卡在狭窄洞穴中恢复、爬上高于自身腿长的岩架）；
    - **改进**：扩展训练场景，加入 “故障恢复”“高平台攀爬” 等任务，让模型学习更灵活的运动模式，进一步扩大应用场景（如复杂废墟搜救）。

![](/paper/anymal-transformer-components.png)
*鲁棒地形感知组件详情。(A) 学生训练期间，向高度采样添加随机噪声。噪声采样自高斯分布，其中每个分量控制每条腿的不同噪声成分。(B) 使用多种噪声配置 z 模拟不同工况。“零噪声” 用于教师训练，“名义噪声” 代表学生训练时的正常建图条件。“大偏移” 噪声模拟因位姿漂移或地形变形导致的大幅地图偏移。“大噪声” 模拟因遮挡或传感器故障导致的地形信息完全缺失。(C) 学生策略信念编码器包含一个循环核心和一个注意力门控，用于融合本体感受和外感受模态。门控显式控制通过的外感受数据。(D) 信念解码器有一个用于重建外感受数据的门控，仅用于训练和信念状态内省。*
