---
title: "Locomotion 文献精读（六）Obstacle-Aware Quadrupedal Locomotion With Resilient Multi-Modal Reinforcement Learning"
---

> https://arxiv.org/abs/2409.19709
> T-RO 2024

## DreamWaQ vs DreamWaQ++
所谓的DreamWaQ++，改进的方法很喜感。[DreamWaQ](./dreamwaq)原先不是把本体感知还有外感受器都批判一遍了吗，然后后者就用本体感知，前者就学术排列组合加入了外感受器。

唉正经一点的话，先用一句话粗暴区分这两篇工作的定位：

- **DreamWaQ**：坚持“只看本体感知也要走得很稳”的**盲走控制器**，靠从 IMU、关节、触地信息里压缩出一个地形 latent \(z_t\)，来弥补没有外部传感器的先天劣势。
- **DreamWaQ++**：承认“完全不用外感受有点固执”，于是把**本体 + 外感受**一起端到端学进一个多模态 latent 里，希望做到**既能提前感知障碍，又能在传感器不可靠时靠本体兜底**。

### 感知模态
从“撞上去再补救”到“提前看一眼”，DreamWaQ++ 的主要变化就是把外感受纳入闭环。

| 维度 | DreamWaQ（基线） | DreamWaQ++（改进） | 改进价值 |
| --- | --- | --- | --- |
| 感知输入 | 仅本体感知（50Hz）：IMU、关节位置 / 速度、接触力 | 本体感知（50Hz）+ 外部感知（10Hz）：3D 点云（LiDAR / 深度相机） | **突破“必须接触才能感知”的被动性**，能提前预判台阶/坑/陡坡 |
| 外部感知处理 | 无 | **分层外部记忆**：缓存最近 \(K\) 帧点云并做 SE(3) 对齐，再拼成 \(o_t^{e,K}\) 以匹配 50Hz 控制；**置信度滤波器（公式 1）**：按特征方差抑制不可靠点 | 解决外感受**频率低、噪声大**的现实问题，让外感受“既跟得上控制频率，又更干净” |
| 感知融合逻辑 | 无（仅本体感知隐式编码地形） | **多模态混合器**：融合 \(z_t^p\) 与 \(z_t^e\) 得到 \(z_t^{pe}\)，并用约束重参数化稳定训练 | 实现“本体兜底 + 外感受预判”的**双向增益** |

### 核心架构
我把这套设计概括成一句：**把“会看路”的能力塞进 latent**，并且让它在训练里被明确地约束出来。

| 模块 | DreamWaQ | DreamWaQ++ | 改进细节（我的理解） |
| --- | --- | --- | --- |
| 本体编码器 | CENet（全连接为主）：从本体观测序列输出速度估计 \(v_t\) 与隐变量 \(z_t\) | 改进版 CENet（MLP-Mixer + \(\beta\)-VAE）：输出带随机性的 \(z_t^p\) | MLP-Mixer 更擅长抓短时序相关；随机 latent 更像“去噪 + 探索”的内置机制 |
| 外部编码器 | 无 | PointNet + 置信度滤波器 | PointNet 适配点数变化；置信度过滤在 pooling 前抑制离群点特征，提升外感受鲁棒性 |
| 记忆机制 | 短窗口堆叠（\(H=5\)，约 100ms） | 本体短窗口 + 外部分层记忆（\(K\) 帧点云） | 外部长记忆拉长了“预判窗口”，减少对突发障碍的滞后反应 |

训练目标上，DreamWaQ 主要靠速度估计损失 \(L_{\text{est}}\) 来把编码器训稳；DreamWaQ++ 额外加了一个**高度重建任务**（从 \(z_t^{pe}\) 重建高度图 \(h_t\) 的 VAE 损失），逼着 latent 里真的压进“地形几何”信息。

## 研究背景
这篇工作的出发点其实可以拆成三条“抱怨”：

- 只用本体感知（proprioception）的话，机器人**必须先撞上去 / 踩上去**才能知道前面有台阶或坑，天生做不到“无碰撞通过”；
- 只用外部感知（exteroception）又往往要求构建比较精细的环境表示（比如 dense 高程图），这在真实机器人上很难做到既实时又可靠；
- 传统的“加记忆 + 加表征学习”方案（LSTM、CNN 等）一方面训练不稳定（BPTT 的梯度问题），另一方面对非结构化时序 / 点云数据的归纳偏置也不太合适。

再加上很多视觉 locomotion 工作很少显式考虑“技能探索”，容易在固定地形上学得很好，但一换场景就暴露缺乏通用性的问题，DreamWaQ++ 就试图在：

- 感知模态上，走“本体 + 外感受”的折中路线；
- 表征学习上，靠 VAE + 对比损失把“可供性 + 地形几何”压进 latent；
- 策略层面，用互信息鼓励策略在不同地形上主动探索更多步态模式。

## 方法论
![](/paper/dreamwaq++-overview.png)
*图：DreamWaQ++ 总体架构。(A) 编码器由低阶原始测量编码器与时空混合器组成；(B) 外感受编码器采用类 PointNet 结构并加入置信度过滤层，在聚合为外部上下文 \(z_{t}^{e}\) 前过滤不可靠点云特征。*
DreamWaQ++ 是一种端到端多模态强化学习控制器，核心是融合本体感知与外部感知，通过结构化表征学习实现鲁棒控制。
输入：原始本体感知（IMU、关节位置 / 速度，50Hz）+ 外部感知（3D 点云，10Hz）；
核心模块：多模态上下文编码器 → 状态估计器 → 高度重建器 → 策略网络；
输出：目标关节位置（50Hz），由低阶 PD 控制器（200Hz）转换为扭矩指令；
训练算法：基于 PPO（Proximal Policy Optimization）的非对称 Actor-Critic（Actor 用部分观测，Critic 用特权状态）。
DreamWaQ++ 的核心网络（上下文编码器、状态估计器、高度重建器、策略网络）并非独立训练，而是通过集成目标函数实现 “协同学习”—— 即一个网络的优化会为其他网络提供有效信号，最终诱导出 “信息丰富的潜变量（latent）”。
设计动机：避免传统 “分阶段训练”（如先训编码器再训策略）导致的 “模块间适配性差” 问题。例如，高度重建器的误差会反馈给上下文编码器，使其更关注地形几何特征；策略网络的奖励信号会引导编码器生成对 “ locomotion 有用” 的 latent，而非单纯追求重建精度。
训练范式类比：类似 “少样本元强化学习（meta-RL）”，上下文编码器生成的 latent 作为 “条件向量”，让策略能快速适配不同地形（如台阶、陡坡），无需针对每种地形重训。
### 多模态上下文编码器
我更喜欢把它理解成 **3 个“把信息喂进去的管道” + 1 个“把两种信息混起来的阀门”**，分别解决四个现实问题：**外感受时间稀疏、点云噪声、本体短时序相关、多模态融合不稳定**。

#### 分层外部感知记忆（Hierarchical Exteroceptive Memory）：
外部传感器通常只有 10Hz，但控制需要 50Hz 输出动作（每 20ms 一次）。如果直接用 10Hz 点云去驱动 50Hz 控制，策略会天然“反应慢半拍”。

DreamWaQ++ 的做法可以概括成一句：**缓存 + 对齐 + 拼接，用时间换密度**。

- **点云缓存**：保存最近 \(K\) 帧（例如 \(K=5\)）的原始点云；
- **SE(3) 对齐**：利用网络估计的身体线速度 + IMU 姿态，把历史点云变换到当前机器人坐标系；
- **稠密输出**：将对齐后的 \(K\) 帧点云拼接成 \(o_t^{e,K}\)，让后续编码器按 50Hz 看到一个“更稠密”的外部观测。

这里的直觉是：你不需要每一帧都重新扫描世界，只要把过去几帧“搬到现在”，策略就能在高频控制里持续看到一个还算靠谱的局部地形形状。
#### 外感受编码器（Exteroceptive Encoder）：
外感受编码器的关键词是：**点数不固定、噪声很要命**。作者把它拆成两个层次来处理：

- **骨干：PointNet-like**
  - 选择 PointNet 的理由很现实：真实点云点数会变、点顺序也没意义，PointNet 天生对这些不敏感；
  - 你可以把它当成“把一堆点压成一个向量”的稳定做法。

- **关键：置信度过滤器（Confidence Filter）**
  - 这层的作用是：在 pooling 之前先把“不太可信的点特征”压下去，避免一堆地面反射/离群点把全局特征带偏。

对应论文公式（1），更清爽的写法是：

\[
C(o_t^{e,K})=\psi_e(o_t^{e,K})\odot\left(1-\tanh\left(\sigma\!\left(\psi_e(o_t^{e,K})\right)\right)\right)
\]

我自己的理解：

- \(\psi_e(o_t^{e,K})\)：PointNet 提取的逐点特征（每个点一个 embedding）；
- \(\sigma(\cdot)\)：对逐点特征做标准差统计，标准差越大意味着“这批点特征分布越飘”；
- \(1-\tanh(\sigma)\)：把“飘”的程度变成 \([0,1]\) 的软权重，飘得越厉害权重越接近 0；
- \(\odot\)：逐点/逐维相乘，相当于给特征做一个自适应的“静音器”。

最后再做 max-pooling，就更容易得到一个噪声鲁棒的外部上下文 \(z_t^e\)。
#### 本体编码器（Proprioceptive Encoder）：
改进自 CENet，采核心解决 “如何从本体感知中提取‘时序关联 + 去噪’的 latent”，优化点集中在 “架构升级” 和 “随机 latent 设计”。
（1）架构升级：MLP-Mixer 替代全连接层
DreamWaQ 的 CENet 使用 “全连接层” 处理本体感知序列，难以捕捉 “不同时间帧、不同模态（如 IMU 角速度与关节位置）” 的交互关系；DreamWaQ++ 改用MLP-Mixer 架构：通过 “跨时间帧注意力” 和 “跨模态混合”，增强对 “短期时序信息” 的建模能力（如前 5 帧关节运动趋势），让 latent 更能反映 “机器人自身状态与地形的交互”（如脚踩台阶时的关节速度变化）。
（2）时序记忆：短窗口观测堆叠
输入是一个短窗口堆叠的本体观测：

\[
o_t^{p,H} = [o_t^p, o_{t-1}^p, \ldots, o_{t-H}^p]^{\mathsf T}
\]

论文里 \(H=5\)（50Hz 下约 100ms）。这个长度我觉得挺务实：够用来判断打滑/触地模式的短期变化，又不会把序列拉得太长导致训练变慢、还更难稳定。
（3）随机 latent：变分推断（Variational Inference）
编码器输出的 “本体感知上下文（z_t^p）” 并非固定值，而是一个高斯分布（通过变分推断建模）：
作用 1：去噪 —— 随机 latent 可视为 “对观测噪声的鲁棒表示”，避免策略过度依赖带噪声的原始观测；
作用 2：探索 —— 训练时 latent 的随机性会诱导策略尝试不同步态（如轻微调整抬脚高度），提升对未知地形的适配性；
额外功能：同时输出 “身体速度估计（\tilde{v}_t）”，通过与仿真真值（v_t）的误差优化编码器，确保 latent 包含 “机器人运动状态” 信息
#### 多模态混合器（Multi-Modal Mixer）：
将 “本体感知上下文（z_t^p）” 与 “外部感知上下文（z_t^e）” 融合为 “跨模态 latent（z_t^pe）”，是实现 “多感知协同” 的关键，核心解决 “融合时数值不稳定” 的问题。
（1）端到端融合逻辑
混合器与所有网络（编码器、策略、重建器）联合训练，而非独立设计 —— 策略网络的奖励会反馈给混合器，使其学习 “对控制有用的融合权重”（如平坦地形时侧重本体感知，复杂地形时侧重外部感知）。
（2）稳定训练：约束重参数化技巧
多模态融合时，因 “z_t^p的随机性” 和 “z_t^e的噪声过滤波动”，易导致训练数值震荡；DreamWaQ++ 提出 “约束重参数化”：通过对融合后的 latent 施加 “L2 正则化” 和 “分布约束”，确保其数值范围稳定（如限制在 [-1,1]），避免策略输出的关节位置突变。

### 训练目标（多损失协同）
DreamWaQ++ 通过三类核心损失（估计损失、VAE 损失、对比损失）优化上下文编码器，同时作为 “辅助损失” 融入策略训练，确保 latent 既 “准确” 又 “对控制有用”。
#### 估计损失 L_est
​目标：训练本体感知编码器准确估计机器人身体速度（\tilde{v}_t）；
计算方式：均方误差（MSE），即 L_est = MSE(\tilde{v}_t, v_t)，其中v_t是仿真中的真实身体速度；
额外优化：自适应引导（AdaBoot）—— 通过 “累积奖励的变异系数（CV (R)）” 动态调整 “使用估计速度\tilde{v}_t的概率（p_boot）”：

\[
p_{\text{boot}} = 1 - \tanh(\mathrm{CV}(R))
\]

逻辑：训练初期奖励波动大（CV (R) 高）→ p_boot低，减少对不准确估计的依赖；训练后期奖励稳定（CV (R) 低）→ p_boot高，强化编码器学习。
#### VAE 损失
分为 “本体 VAE 损失（L_{VAE}^p）” 和 “外部 VAE 损失（L_{VAE}^e）”，分别优化本体感知和外部感知的 latent 质量。
（1）本体 VAE 损失（L_{VAE}^p）：预测未来观测
目标：让z_t^p包含 “预测未来状态” 的信息（如根据当前关节运动预测下一帧观测）；
公式（4）：

\[
L_{\text{VAE}}^{p}
=\mathrm{MSE}(\tilde{o}_{t+1}, o_{t+1})
\,+\,\beta\,D_{\mathrm{KL}}\!\left(q(z_t^{p}\mid o_t^{p,H})\,\|\,p(z_t^{p})\right)
\]

第一项（重构损失）：最小化 “预测的未来观测（\tilde{o}_{t+1}）” 与 “真实未来观测（o_{t+1}）” 的误差；
第二项（KL 散度）：约束z_t^p的分布接近先验高斯分布（p(z_t^p)），避免过拟合；
β=5.0：通过固定 β 值促进 latent “解耦”（即不同维度对应不同语义，如一个维度对应抬脚高度，一个对应身体倾斜）。
（2）外部 VAE 损失（L_{VAE}^e）：重建高度图
目标：让z_t^pe（多模态融合 latent）包含 “地形几何” 信息；
公式（5）：

\[
L_{\text{VAE}}^{e}
=\mathrm{MSE}(\tilde{h}_{t}, h_{t})
\,+\,\beta\,D_{\mathrm{KL}}\!\left(q(z_t^{pe}\mid o_t^{pe})\,\|\,p(z_t^{pe})\right)
\]

h_t：仿真中 “以机器人为中心的真实高度图”；\tilde{h}_t：通过解码器从z_t^pe重建的高度图；
关键创新：自适应 β 调度（解决固定 β“重构精度和解耦性不可兼得” 的问题）：
先计算缩放因子k（公式 6）：

\[
k=\exp\left(\delta\cdot(\tau-L_{\text{recon}})\right)
\]

L_{recon}：当前高度图重建损失；τ：允许的重建误差阈值；δ：k的学习率；
逻辑：若L_{recon} > τ（重建差）→ k减小→ β 缩小，优先保证重构精度；若L_{recon} < τ（重建好）→ k增大→ β 放大，优先促进 latent 解耦。
再更新 β（公式 7）：

\[
\beta\leftarrow
\begin{cases}
\beta_{\min} & \text{if } k\beta \le \beta_{\min}\\
k\beta & \text{if } \beta_{\min} < k\beta < \beta_{\max}\\
\beta_{\max} & \text{if } k\beta \ge \beta_{\max}
\end{cases}
\]

通过β_min（如 0.1）和β_max（如 10）限制 β 范围，避免极端值导致训练崩溃。
#### 对比损失 
传统方法让策略通过 “回归” 预测不可观测的环境属性（如地形高度），易因 “观测不足” 导致 “可实现性缺口”（即仿真中能学，真实中用不了）；DreamWaQ++ 改用对比学习，直接缩小 “Actor 的观测 latent（z_t^pe）” 与 “Critic 的特权状态 latent（g_{\theta_h}(h_t)）” 的分布差距：
公式（8）：

\[
L_{\text{contrastive}}
=\lambda\left\lVert z_t^{pe}-g_{\theta_h}(h_t)\right\rVert_2^2
(1-\lambda)\max\left(0,\,m-\left\lVert z_t^{pe}-z_t^{\text{random}}\right\rVert_2^2\right)
\]

各部分含义：
正样本对：z_t^pe（Actor 的多模态 latent）与g_{\theta_h}(h_t)（Critic 的真实高度图编码）—— 迫使z_t^pe接近 “真实地形信息”；
负样本对：z_t^pe与z_t^{random}（均匀随机采样的 latent）—— 通过 “margin m” 确保z_t^pe远离无意义的随机表示；
λ∈[0,1]：平衡正 / 负样本的权重（实验中设为 0.5）。
作用：无需让策略 “显式预测地形”，只需让其 latent 与 “真实地形信息” 对齐，既避免可实现性缺口，又保留策略的灵活性

![](/paper/dreamwaq++-terrain-aware.png)
*图：外部感知辅助地形感知。(A) 不同环境下的多模态上下文嵌入可视化；(B) 不规则地形中多模态嵌入激活的分布差异；(C) 调制少数关键嵌入后步态在抬脚高度与频率上的实时变化；(D) 跨模态相关性热力图，刻画不同地形下多模态测量的不确定性。*
![](/paper/dreamwaq++-ood.png)
*图：分布外场景中的快速适应。(A) 移除支撑平台造成外界扰动；(B) 感知突变触发关节快速调整；(C) 通过扩大支撑多边形确保安全着陆；(D) PaCMAP 嵌入展示多模态上下文如何随时间跟踪环境变化；(E) 相机意外脱落时仍能保持鲁棒步态；(F) 在 35° 坡上行走时 DreamWaQ 与 DreamWaQ++ 的扭矩对比，后者显著降低后腿扭矩。*

### 策略学习与技能发现
这一部分明确了 “如何将 latent 转化为步态控制指令”，并通过 “通用性增益” 诱导策略学习多样化技能，核心是 “POMDP 问题建模” 和 “互信息最大化”。
四足机器人的真实环境是 “部分可观测” 的（如传感器噪声、视线遮挡），因此将控制问题建模为 POMDP，在[DreamWaQ](./dreamwaq)亦有记载，就不再赘述。
DreamWaQ 的策略易 “过拟合”（仅能适应训练过的地形），DreamWaQ++ 通过 “互信息（MI）最大化” 引入 “无监督技能探索目标”，即 “通用性增益”：
（1）定义：互信息最大化
通用性增益是 “观测（o_t^{pe}）与多模态 latent（z_t^pe）” 的互信息，公式（10）：

\[
G_{\text{versatility}}
=I(o_t^{pe}; z_t^{pe})
=H(z_t^{pe})-H(z_t^{pe}\mid o_t^{pe})
\]

互信息I(·;·)：衡量两者的 “关联程度”——I越大，说明z_t^pe越能 “代表o_t^{pe}的关键信息”；
分解为两项：
H(z_t^pe)（z_t^pe的香农熵）：最大化该项→ 增加z_t^pe的多样性→ 诱导策略尝试不同步态（如爬台阶、探测地形）；
H(z_t^pe | o_t^{pe})（条件熵）：最小化该项→ 确保相同观测对应相似z_t^pe→ 过滤观测噪声，保证策略稳定性。
（2）融入策略训练
将G_{versatility}作为 “正则化项” 加入 PPO 损失，最终优化目标为：

\[
J = G_{\text{versatility}} - \lambda_e L_{\text{encoder}}
\]

L_{encoder}：编码器的 KL 散度损失（最小化z_t^pe与o_t^{pe}的分布差距，确保z_t^pe不脱离观测语义）；
λ_e=0.1：平衡 “探索（通用性增益）” 与 “利用（KL 散度）”—— 既让策略探索新技能，又不偏离稳定 locomotion 目标。
涌现技能：实验中，该目标诱导出 “主动探测地形”（用脚触碰未知高台）、“弹性越障”（50cm 高台跳跃）等 DreamWaQ 不具备的技能。
3. 低层控制：PD 控制器
策略网络输出 “目标关节位置（θ_{des}）”，由 “200Hz 的低阶 PD 控制器” 转换为电机扭矩指令：

\[
\tau = K_p(\theta_{\text{des}}-\theta_{\text{current}})
+K_d(\dot{\theta}_{\text{des}}-\dot{\theta}_{\text{current}})
\]

K_p=28、K_d=0.7（实验调优值）：确保关节位置跟踪的响应速度和稳定性，避免电机震荡。
## 实验
![](/paper/dreamwaq++-cluttered.png)
*图：拥挤复杂地形上的敏捷行走。单一 DreamWaQ++ 控制器即可在楼梯、跨缝隙、跳跃、探测下陷、软地形、移动平台和 35° 坡等多种场景中切换合适步态，无需为每种场景单独训练。*

整体实验可以分成三块：**训练与硬件设置、弹性爬楼梯、不确定场景下的应对**。

### 硬件与训练设置（概览）

- **机器人与传感器**：
  - 机器人：Unitree Go1（13kg）、A1（12kg）、ANYmal-C、KAIST HOUND；
  - 外感受：Intel RealSense D435f（45° 下倾）、Ouster OS-01 LiDAR、Livox Mid-360 LiDAR。
- **仿真与并行训练**：
  - 使用 NVIDIA Isaac Gym + Legged Gym，约 3500 个智能体并行；
  - A5000 级别 GPU 上训练 11 小时左右即可得到可部署策略。
- **训练细节**：
  - 领域随机化：负载（-1~2kg）、摩擦系数（0.2~1.25）、系统延迟（0~15ms）等，确保策略不要死记某一个“理想机器人”；
  - 奖励课程：类似指数退火，把“扭矩、速度等惩罚”在训练早期放得更松一点，给策略更多探索空间；
  - 对抗观测：训练阶段就注入本体 / 外感受噪声（关节位置 ±0.01rad、点云噪声 0~0.3m），提前习惯“脏数据”。
- **可扩展性**：
  - ANYmal-C、HOUND 上都能跑；HOUND 因为关节活动范围更大，能跨 42cm 高台（训练最大障碍仅 27cm）；
  - 实机上 Go1 携带 2.5kg 额外负载，也能爬上 41cm 高的变形沙发（训练中没出现过的场景）。
### 弹性爬楼梯（Resilient Stair-Climbing）
![](/paper/dreamwaq++-stairs.png)
*图：多种楼梯场景下的弹性爬楼。包括与基线控制器的头对头比赛、比赛环境三维地图、左右高度不同楼梯上的可供性感知步态、通过加大摆脚幅度实现“跨两级台阶”的涌现行为，以及在不同楼梯高度与踏步长度上的仿真定量对比和 1000 次仿真下的成功率统计。*

这里的核心是：**在一个很“扎心”的楼梯场景里，让 DreamWaQ++ 和几个典型基线正面对抗**。

- **对比对象（都是真实机器人）**：
  - R1：DreamWaQ++ 控制器 + Unitree Go1；
  - R2：DreamWaQ 控制器 + Unitree A1（结构/电机性能接近，尽量保证硬件公平）；
  - R3：Unitree 内置视觉控制器 + Go1（走“建图 + 规划”路线）。
- **实验设置**：
  - 50 级连续台阶（复刻真实楼梯），所有机器人从统一起点出发（R3 因为第一阶跨不过，被迫从前一阶起步）；
  - 控制方式是人工触发指令，方便人在机器人要摔时减速保护硬件；
  - 指标包括：完成时间、水平距离、爬升高度、是否摔倒/中途退出。

**结果非常“拉开档次”**：

- R1（DreamWaQ++）：35 秒走完 50 级台阶，水平位移 30.03m，爬升 7.38m；
  - 显性行为：会根据台阶高度微调抬脚高度、主动把脚落在“更安全的台阶区域”，看上去就像在“选好踩点再迈脚”；
- R2（DreamWaQ）：同样 35 秒，只走了 20.05m，爬升 5.44m，没上完楼；
  - 问题出在：经常在台阶边缘绊一下，靠本体反馈调整虽然能救回来，但速度跟踪误差拉得很大——本质仍是“看不见，只能撞了再修正”。
- R3（内置视觉控制器）：走了 6.38m、爬升 2.44m 之后就绊倒结束；
  - Root cause：实时建图 + 路径规划占了太多预算，策略不得不放慢移动速度，面对连续台阶时显得非常“犹豫”。

总结一句：**DreamWaQ++ 在楼梯这种场景里，既摆脱了纯本体“被动撞”的迟钝，又避免了重建图 + 规划那种“为安全牺牲速度”的保守**。

### 不确定性处理（Handling Uncertainties）
![](/paper/dreamwaq++-uncertain.png)
*图：不确定地形下的主动探测与极端场景适应。(A) 机器人在高风险未知地形前主动停下并用脚探测；(B) 速度指令与估计显示控制器会“顶住”原始指令，为探测腾出时间；(C) 小腿关节角的突然变化揭示了显著的屈伸动作；(D) 在含 50cm 高台的极端场景中微调策略后，会涌现安全跳下高台的行为。*

作者在这一节想证明两件事：**策略会在风险高时主动“减速 + 探测”，并且能在极端分布外场景里快速换一种稳定姿态**。

- **涌现探测技能（Emergent Probing）**
  - 面对高度未知的高台，机器人不会一条道走到黑，而是先停下，脚尖去点几下前方地面（图 3A）；
  - 对应的速度曲线里可以看到，控制器会“顶住”原本的前进指令，给探测动作让路（图 3B）；
  - 小腿关节角出现的明显屈伸（图 3C）说明这不是噪声，而是一套比较稳定的“摆脚试探”模式。
- **极端场景适应（Out-of-Distribution）**
  - 平台被突然移除时，机器人会快速调整前髋关节，让支撑多边形面积扩大约 20.12%（图 4C），类似人踩空时会本能张大步伐；
  - 外部感知彻底失效（相机掉落）时，策略会自然退回一个更保守的“膝盖 + 脚着地”步态（图 4E），本体 latent 接管决策；
  - 在 35° 陡坡上，DreamWaQ 倾向维持水平身体姿态，后腿扭矩很大；DreamWaQ++ 会让身体贴坡降低高度，后腿扭矩下降 1.5 倍，而且统计上 p<0.001。
- **外部感知辅助地形感知**
  - 多模态上下文 \(z_t^{pe}\) 会按地形难度聚类成“平坦 / 不规则 / 易台阶 / 难台阶”几类，同时仍保留本体的步态周期特征（图 5A）；
  - 调节外感受相关的几个 latent 维度（41/42/55/64），可以连续地改变步态高度和频率（类似在“平地步态”和“爬楼梯步态”之间滑动）（图 5B/5C）；
  - 跨模态相关性在不规则地形上显著降低，意味着模型学会在不确定时更多依赖外感受，而在平坦地形时两种感知更一致（图 5D）。

### 核心实验 2：可供性感知运动
这组实验我更愿意理解成：作者在回答一个很朴素的问题——**多模态 latent 里到底有没有“地形可行动性（affordance）”这种信息？**

- 从图 5A 的嵌入可视化能看到，多模态上下文 \(z_t^{pe}\) 会按地形难度自动聚类：平坦、不规则、易台阶、难台阶各成一团。换句话说，编码器学到的不是“点云长什么样”，而更像“眼前这块地大概率需要什么步态”。
- 图 5B / 5C 则更像对 latent 的“黑箱探针”：手动放大 / 缩小少数关键维度（41/42/55/64），步态会出现**可解释的联动**（抬脚更高、步频更低、动作更像爬楼梯）。这说明 latent 的部分维度已经对某些步态调节起到了“旋钮”作用。
- 图 5D 的跨模态相关性热图也挺直观：平坦地形时，本体与外感受信息更一致（相关性更高）；地形越不规则，跨模态相关性越低，策略会更依赖外感受来形成“谨慎”决策。

核心结论就是：DreamWaQ++ 不是把外部感知当成一个“更大的观测向量”硬塞给策略，而是把它压成了**能直接影响步态选择的语义 latent**。

### 核心实验 3：足部摆动适应
对应图 3 的不确定地形实验，本质上是在验证：**前方风险高的时候，策略会不会“自作主张慢下来、先探测再通过”？**

- 当面对不确定下陷 / 高台边缘等高风险区域时，机器人会抵消“继续向前走”的速度指令，先停下并用脚探测（图 3A / 3B）。从控制角度看，这等价于策略内部有了一个 **“安全优先” 的模式切换**。
- 探测动作本身并不是随机抖腿：小腿关节会出现明显的屈伸（图 3C），更像一种涌现出的“摆脚技能”，用最小代价去获取更多地形信息。
- 在 50cm 高台这类极端场景里，继续训练同一个策略会涌现 **“跳下 + 折叠后腿”** 的组合动作（图 3D）。这也侧面说明策略不是简单放大既有动作，而是能在新任务里重新组织动作序列。

### 核心实验 4：定量性能对比
这一部分主要是给前面的“现象”补上更硬的数值结论，我觉得最有信息量的点是这三条：

- **楼梯成功率与速度**：在不同台阶高度与踏步长度组合里，DreamWaQ++ 的成功率更稳定，且能在较高指令速度下仍保持通过率，体现了“提前预判 + 及时本体反馈”的协同价值。
- **35° 坡上扭矩更低**：图 4F 的箱线图里，DreamWaQ++ 的后腿扭矩显著更低（文中给出统计显著性），直观理解就是它不只“能上坡”，还更省力、对关节更友好。
- **跨机器人泛化**：在 ANYmal-C、HOUND 这类不同形态机器人上能较顺利复用，同一策略还出现了跨越训练上限障碍的行为，说明 latent 学到的更像“可供性”而不是某一台机器人的特定动作模板。

## 小结与局限
### 优势

- **sim-to-real 友好**：基本不需要微调就能上实机，依赖的是原始感知 + 相对轻量的网络计算。
- **传感器无关性**：理论上支持 3D LiDAR / 深度相机，关键是把外参标定好并让点云落在正确的机器人坐标系里。
- **可解释性更好**：多模态上下文可以当作一种“信息先验”，更容易和上层规划或 MPC 做结合。

### 局限性

- **外参标定依赖强**：外部传感器外参一旦不准，多模态融合就会把错误几何当真，收益会快速变成负收益。
- **latent 语义不稳定**：无监督得到的 latent 维度含义会随训练种子漂移，同一维度不一定总对应同一类技能/语义。

### 未来方向

- **主动视角控制**：加一个可控相机俯仰机构，让“看哪里”也变成策略的一部分，去提升可观测性。
- **扩展到更激进的动作库**：通过奖励权重（例如降低速度跟踪、提升通用性增益）诱导更多跳跃、跨越等行为，而不是只在稳态行走上做文章。
