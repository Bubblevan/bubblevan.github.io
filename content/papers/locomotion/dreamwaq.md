---
title: "Locomotion 文献精读（五）DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning"
---

> https://arxiv.org/abs/2301.10602
> ICRA 2023

## 摘要
四足机器人能走复杂地形，但“怎么写控制器”一直是个硬问题：传统 **model-based** 要堆一条很长的 pipeline（状态估计 → 轨迹 → 步态 → 控制），学习派要想走得稳又往往得依赖相机、LiDAR 这类外感受，碰上恶劣天气/光照就容易翻车。

DreamWaQ 的核心思路很直接：**不指望外部视觉，尽量用 IMU + 关节编码器这点本体感受，把地形“想象”出来**，然后用深度强化学习直接学出一个足够鲁棒的步态策略。

我觉得这篇的贡献点可以概括成三句：

- 用**非对称 Actor-Critic** 绕开教师-学生两阶段训练，把“特权信息”留给 critic 做训练引导
| 角色 | 输入内容 | 核心作用 |
|------|----------|----------|
| Actor（策略） | 1. 带噪声的部分观测（本体感知：IMU、关节数据；BTW DreamWaQ++还用到了外部感知编码后的点云 latent）<br>2. 多模态上下文 latent（z_t^pe） | 部署时仅依赖真实机载传感器，输出目标关节位置，控制机器人移动 |
| Critic（价值） | 特权状态（sim 中可获取，如真实地形高度图、外力扭矩、足部真实位置） | 训练时提供更精准的价值估计，引导 Actor 学习（避免 Actor 直接依赖不可获取的特权信息） |

- 设计 **CENet**，把“身体速度估计 + 环境上下文编码”合在一个网络里端到端学出来
- 仿真训练后能直接 **sim2real**，并且给了比较扎实的实机评估

## 引言：为什么“盲走”很难
如果把四足走地形当成一个工程系统，最常见的解法其实是“能量守恒式堆模块”：先估状态，再规划，再调步态，最后下发到电机。它能跑，但代价是 pipeline 又长又脆，尤其是地形这块，你很难在真实世界里永远有一个**靠谱的显式地形模型**。

学习方法的直觉是：既然地形这么难建模，那就让网络自己学。但多数高性能方案会依赖外感受（相机、雷达），而这些传感器在现实里并不总是可用；反过来，完全只用本体感受虽然更“抗环境”，但又常常在长距离复杂地形里遇到瓶颈：**你得先走起来才能理解地形，但不理解地形又很难走起来**。

DreamWaQ 就是在这个矛盾里，给了一个我认为很实用的折中：**把地形信息压缩成一个可学习的隐变量 `z_t`**，让策略在本体感受里“猜”出来。

## 方法论
### 基础模型定义
![DreamWaQ 总体框架](/paper/dreamwaq-overview.png)
*图：DreamWaQ 总体框架。训练时 critic 可见特权信息，部署时只保留 Actor + CENet。*


环境被建模为一个无限时域的 **POMDP**，可以写成 `M=(S, O, A, d0, p, r, γ)`：

- `S`：全状态（含地形属性）
- `O`：可观测量，这里主要就是本体感受 `o_t`
- `A`：动作（12 个关节的目标角度偏移）
- 时间堆叠观测：`o_t^H = [o_t, o_{t-1}, ..., o_{t-H}]^T`，论文里 `H=5`
- `z_t`：隐式上下文（我更愿意把它理解为“地形/接触条件的压缩表示”）
目标：最大化“折扣未来奖励期望”，也就是去找一条最优策略 \(\pi^*\)：

\[
\pi^*=\arg\max_{\pi}\ \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t r_t\right]
\]
### 非对称 Actor-Critic
这篇我比较认可的一点是：它没有走“老师先用特权信息学到很好 → 学生再模仿老师”的两阶段路，而是用**非对称 Actor-Critic** 一步到位：

| 网络 | 输入 | 输出 | 优化 |
| --- | --- | --- | --- |
| **Actor**（策略） | `o_t^H + v_t + z_t` | `a_t`（关节角度偏移） | **PPO** |
| **Critic**（价值） | `s_t = [o_t, v_t, d_t, h_t]^T` | `V(s_t)` | **MSE** |

- `d_t`：对机器人施加的随机外力（训练时可得）
- `h_t`：高度图/地形信息（训练时可得）

这里的味道是：**critic 更聪明没关系，但 actor 必须“盲”**，这样部署时不需要补外感受也能跑。

### 动作与低层控制
- 动作是 12 维：`a_t`
- 目标关节角度：`θ_des = θ_stand + a_t`
- 最后由 **PD 控制器**去跟踪 `θ_des`

### 奖励函数：除了走得像，还得“电机别局部过热”
常规奖励基本就是“速度跟踪 + 稳定性惩罚”，但 DreamWaQ 额外加了一个我挺喜欢的项：**功率分布奖励**，大意是惩罚“某几个电机一直过载”，避免局部过热导致的实机问题。

| 奖励类型 | 核心作用 | 权重示例 |
| --- | --- | --- |
| 线速度 / 角速度跟踪 | 匹配指令速度 | 1.0 / 0.5 |
| 稳定性惩罚 | 抑制垂直速度、关节加速度 | -2.0 / -2.5e-7 |
| 功率分布 | 平衡电机负载 | -1e-5 |

如果要对齐论文里的完整表格：

| 奖励项 | 公式 $\left(r_{i}\right)$ | 权重 $\left(w_{i}\right)$ |
| --- | --- | --- |
| 线速度跟踪 | $\exp \left\{-4\left(v_{x y}^{cmd}-v_{x y}\right)^{2}\right\}$ | 1.0 |
| 偏航角速度跟踪 | $\exp \left\{-4\left(\omega_{yaw}^{cmd}-\omega_{yaw}\right)^{2}\right\}$ | 0.5 |
| 垂直速度惩罚 | $v_{z}^{2}$ | -2.0 |
| 横滚/俯仰角速度惩罚 | $\omega_{x y}^{2}$ | -0.05 |
| 姿态惩罚 | $\lVert g\rVert^{2}$ | -0.2 |
| 关节加速度惩罚 | $\ddot{\theta}^{2}$ | -2.5×10^{-7} |
| 关节功率惩罚 | $\lVert \tau\rVert^{2}$ | -2×10^{-5} |
| 机身高度惩罚 | $\left(h^{des}-h\right)^{2}$ | -1.0 |
| 抬脚高度奖励/惩罚 | $\left(p_{f,z,k}^{des}-p_{f,z,k}\right)^{2} \cdot v_{f,xy,k}$ | -0.01 |
| 动作变化率惩罚 | $\left(a_{t}-a_{t-1}\right)^{2}$ | -0.01 |
| 平滑性惩罚 | $\left(a_{t}-2a_{t-1}+a_{t-2}\right)^{2}$ | -0.01 |
| 功率分布惩罚 | $var(\tau \cdot \dot{\theta})^{2}$ | -1e-5 |

### 课程学习
训练里用了两类课程：

- 游戏启发式课程：地形从平滑 → 粗糙 → 离散 → 楼梯，坡度从 0° 拉到 22°
- 网格自适应课程：重点照顾低速转弯这类容易绊脚的情况

## CENet：把“估速度”和“编码地形上下文”绑在一起
![CENet 架构](/paper/dreamwaq-cenet.png)
*图：CENet 采用单编码器 + 多解码器结构。编码器同时输出 `v_t` 与 `z_t`。*

我对 CENet 的理解是：它不是单纯做一个“速度估计器”，而是借着速度估计这个监督信号，让网络顺手学出一个 `z_t` 去承载“地形/接触条件”这类难以显式监督的东西。

- 编码器：输入 `o_t^H`，输出 `v_t` 与 `z_t`
- 解码器 1：监督 `v_t`（MSE，对齐模拟器真值）
- 解码器 2：预测/重构 `o_{t+1}`（β-VAE：重构 + KL）

损失写成一行就是：

`L_CE = L_est + L_VAE`

自适应引导（**AdaBoot**）是个很“工程化但有效”的细节：用 episodic 奖励的变异系数（CV）来调引导概率，直觉上就是“训练稳定就多引导，不稳定就少引导”：

`p_boot = 1 - tanh(CV(R))`


## 实验

### 对比方法
- **Baseline**：无自适应机制
- **AdaptationNet**：教师-学生框架，隐式环境编码
- **EstimatorNet**：只估身体状态，不估环境上下文
- **DreamWaQ w/o AdaBoot**：去掉自适应引导
- **DreamWaQ w/ AdaBoot**：完整方法

### 仿真（Isaac Gym）
- 训练：并行 **4096** 个环境，训练 **1000** 轮（负载 1–2 kg、摩擦系数 0.2–1.25 等域随机化）
- PPO：剪辑范围 **0.2**，折扣因子 **0.99**

![训练曲线](/paper/dreamwaq-learning-curve.png)
*图：训练曲线。DreamWaQ（w/ AdaBoot）的回报接近“有高度图的 Oracle 策略”。*

鲁棒性测试（随机推力）里，DreamWaQ w/ AdaBoot 的最大可承受推力达到 **1.121 m/s**，生存率 **95.23%**；对比 Baseline 为 **0.511 m/s** 与 **20.51%**。

![指令跟踪误差箱线图](/paper/dreamwaq-command-tracking.png)
*图：指令跟踪误差的箱线图。$v_x^{\epsilon}$、$v_y^{\epsilon}$ 分别表示前向/横向速度跟踪误差（单位 $m/s$），$\omega_z^{\epsilon}$ 表示偏航角速度误差（单位 $rad/s$）。图中“星号标注”表示差异显著（$p<10^{-4}$）。*

![速度估计误差对比](/paper/dreamwaq-estimation-error.png)
*图：CENet 与 EstimatorNet 的速度估计误差对比。机器人在楼梯处发生绊脚时，CENet 的优势更明显。*

### 实机（Unitree A1）
- 硬件：Intel NUC（控制）+ 额外 PC（负载 **500 g**）
- 频率：策略 / CENet **50 Hz**，PD 控制器 **200 Hz**（`K_p=28`，`K_d=0.7`）

![实机：绊脚与打滑恢复](/paper/dreamwaq-real-world1.png)
*图：实机上的“脚反射”行为。即使在（a）绊脚或（b）非结构化地形打滑时，也能快速调整步态恢复稳定。*

![实机：室外长距离路线](/paper/dreamwaq-real-world2.png)
*图：室外长距离评测路线（RTK-GPS 记录）。课程 A 是庭院中的多种自然非结构化地形，课程 B 是登山步道；色条表示相对起点的高度变化（单位 m）。*

结果总结成三点更直观：

- **命令跟踪**：ATE 显著低于其他方法（例如线速度误差 **< 0.1 m/s**，$p<10^{-4}$）
- **显式估计**：楼梯环境中绊脚时，**CENet** 仍能较稳定地估计速度；**EstimatorNet** 会出现误差骤增
- **长距离行走**
  - 课程 A（**430 m** 庭院）：坡度、植被、干湿地形（雨后湿滑楼梯、泥泞）均可通过
  - 课程 B（**465 m** 小山，海拔 **22 m**）：沥青 + 碎石混合地形，解决电机过热后 **10 分钟**完成登顶

## 小结与局限
DreamWaQ 给我的最大启发是：在“盲走”场景下，关键不是强行显式重建地形，而是让网络学到一个足够有用的隐表示 `z_t`，并通过非对称 AC 在训练期把特权信息用到极致、部署期又能彻底丢掉。

局限也很明确：**机器人必须先用腿“碰到/踩到”障碍，才有机会自适应地形**；对于需要提前预判的高速越障场景，这条路线天然会吃亏。
