---
title: "Locomotion 文献精读（十二）Learning Agile Robotic Locomotion Skills by Imitating Animals"
---

> https://arxiv.org/abs/2004.00784
> RSS 2020

2020年，那时候仿生还名盛一时。Google Research 和 UC Berkeley 的大佬们（Xue Bin Peng 等）搞出来的这个工作

## 研究背景
让机器人复现动物的多样、敏捷运动技能，是机器人学领域的长期挑战。

### 1. 动物运动的核心优势
动物（如狗）能在复杂环境中展现出极高的敏捷性：不仅能灵活穿越障碍，还拥有丰富的 **“技能库”**（如快走、慢跑、跳跃、转弯等），且运动过程流畅、自然，这是机器人难以企及的。

### 2. 现有控制器方案的局限
为实现腿式机器人的运动控制，领域内主要有两类传统方案，但均存在明显短板：

**(1) 手动设计控制器（Manual Controllers）**
- **优势**：能实现部分复杂行为（如特定步态行走）；
- **致命问题**：
    - **开发过程耗时且困难**：需要开发者同时掌握 “机器人系统动力学” 和 “目标技能细节” 的深厚专业知识（比如设计 “trot 步态” 需懂四足动物运动力学，还要调机器人关节参数）；
    - **效果有限**：即便成功，机器人的运动也远不如动物自然、灵活，难以覆盖多样敏捷技能。

**(2) 强化学习（Reinforcement Learning, RL）**
- **潜力**：作为 “自动化控制器开发” 的替代方案，RL 能自主学习复杂技能（无需手动编码每个动作），在仿真中已展现出处理多样任务的能力（如机器人跳跃、避障）；
- **关键痛点**：
    - **“仿真 - 现实鸿沟”**：仿真中训练的策略，部署到现实机器人时，常出现不自然甚至危险的行为（比如关节动作幅度过大导致摔倒）；
    - **奖励函数设计难**：要让 RL agent 学会 “目标行为”，需设计精准的 “奖励函数”（如 “走得稳 + 速度快” 的权重），但这类设计依赖大量 “技能特定经验”（比如设计 “转弯奖励” 和 “跳跃奖励” 的逻辑完全不同），本质仍是 “手动调参” 的负担。

### 3. 本文切入点
| 研究方向 | 现有进展 | 关键局限 | 本文的改进切入点 |
| :--- | :--- | :--- | :--- |
| **传统腿式机器人控制** | 用手动工程、轨迹优化、模型预测控制（MPC）实现运动，覆盖部分步态 | 依赖开发者经验，需简化机器人动力学模型（如忽略部分关节惯性），技能泛化性差（换技能需重调） | 完全放弃 “手动设计”，用模仿学习自动化控制器开发，无需简化模型 |
| **运动模仿（Motion Imitation）** | 能让机器人模仿参考运动，避免手动编码动作；仿真中可实现高动态技能 | 1. 多聚焦 “上半身运动”（如下半身仅保持静态平衡）；<br>2. 仿真效果好，但难迁移到现实 | 1. 专注 “全身动态运动”（如下半身步态 + 躯干协调）；<br>2. 结合域适应技术，实现仿真到现实的有效迁移 |
| **仿真 - 现实迁移（Sim-to-Real）** | 提出 3 类方案：<br>1. 高保真仿真（精准建模现实）；<br>2. 域随机化（仿真中随机动力学参数）；<br>3. 潜空间方法（用编码适配新域） | 1. 高保真仿真建模难度极大；<br>2. 域随机化易导致策略 “过度鲁棒”（失去灵活性）；<br>3. 现有潜空间方法未结合模仿学习 | 结合 “域随机化” 和 “带信息瓶颈的潜空间编码”，平衡 “鲁棒性” 与 “适应性”；且首次与动物模仿结合 |
| **RL 在腿式机器人的应用** | 仿真 / 现实中实现行走、简单跳跃；部分工作用 RL 优化手动控制器 | 1. 现实部署需精准建模机器人动力学；<br>2. 依赖技能特定奖励函数；<br>3. 现有模仿 + RL 方案仅覆盖简单行为 | 1. 无需精准动力学建模（靠域适应适配现实）；<br>2. 用动物运动数据替代奖励函数设计；<br>3. 实现更多样的敏捷行为（如空中转弯） |

针对上述局限，论文提出 **“基于动物运动模仿的学习框架”**，核心思路是 “直接向动物‘取经’”，用动物运动数据解决 “奖励函数设计难” 和 “仿真 - 现实迁移” 问题，具体设计逻辑如下：

1.  **核心创新：以 “动物运动数据” 为核心**
    *   **参考运动（Reference Motion）**：将真实动物的运动数据（如狗的动作捕捉数据）作为 “学习范本”，为机器人提供 “可行的控制先验”—— 即告诉机器人 “正确的运动应该是什么样的”；
    *   **规避奖励函数痛点**：无需为每个技能设计专属奖励函数（比如 “trot 步态” 和 “跳跃” 共用同一框架），只需让机器人 “模仿参考运动” 即可，大幅降低开发成本。

2.  **解决 RL 的 “样本效率” 与 “迁移” 问题**
    *   **仿真预训练**：RL 需要大量样本（百万级以上），直接在现实机器人上训练成本高、易损坏，因此先在仿真环境中训练模仿策略；
    *   **样本高效的域适应（Sample-Efficient Domain Adaptation）**：仿真与现实的 “动力学差异”（如机器人重量、关节摩擦力不同）会导致策略失效，本文通过 “潜空间编码”“域随机化” 等技术，让仿真训练的策略仅需少量现实尝试（约 50 次）就能适配真实机器人。

## 方法论

![](/paper/motion-imitation-pipeline.png)
*框架包含三个阶段：运动重定向、运动模仿和域适应。它接收从动物录制的运动数据作为输入，并输出一个使真实机器人能够复现该运动的控制策略。*

### 阶段 1：运动重定向（Motion Retargeting）
动物（如狗）的身体结构（腿长、关节数量）与机器人（Laikago）完全不同，直接用动物的运动数据控制机器人会导致动作失效。此阶段的目标是将动物的运动数据 “适配” 到机器人的形态上，核心工具是 **逆运动学（Inverse Kinematics, IK）**。

![](/paper/motion-imitation-ik.png)
*逆运动学（IK）用于将真实狗（左）的动作捕捉片段重定向到 Laikago 机器人（右）。在狗和机器人的身体上指定了对应的关键点对（红色），然后使用 IK 计算机器人的姿态以跟踪这些关键点。*

**(1) 定义 “对应关键点”**
在动物和机器人身上标记功能相同的关键点（如图 3 所示），确保运动映射的合理性：
- **动物侧**：狗的髋关节、膝关节、脚踝（脚掌）；
- **机器人侧**：Laikago 的髋部关节、大腿关节、小腿关节（足部末端）；
- **作用**：这些关键点的 3D 位置是 “运动适配” 的核心参考，比如 “狗的脚掌位置” 对应 “机器人的足部位置”。

**(2) IK 优化：求解机器人的姿态序列**
已知动物关键点在每个时刻的 3D 位置 $\hat{x}_i(t)$（$i$ 为关键点索引，$t$ 为时间步），需计算机器人的姿态序列 $q_{0:T}$（$q_t$ 为 $t$ 时刻机器人的关节旋转角度，即 “姿态”），使得机器人关键点的实际位置 $x_i(q_t)$ 尽可能接近动物的 $\hat{x}_i(t)$。

优化目标用公式（1）表示，本质是 **“最小化位置误差 + 避免姿态偏离默认值”**：

$$
\arg \min_{q_{0:T}} \sum_t \left[ \sum_i \| \hat{x}_i(t) - x_i(q_t) \|^2 + (q_{default} - q_t)^T W (q_{default} - q_t) \right]
$$

*   **第一部分（位置误差项）** $\sum_i \| \hat{x}_i(t) - x_i(q_t) \|^2$：让机器人每个关键点的位置尽可能贴合动物关键点，是优化的核心目标。
*   **第二部分（正则项）** $(q_{default} - q_t)^T W (q_{default} - q_t)$：避免机器人姿态过于极端（如关节旋转角度过大导致机械损坏）：
    *   $q_{default}$：机器人的 “默认姿态”（如站立时各关节的中性角度）；
    *   $W$：对角矩阵，为每个关节设置 “正则化系数”（关节越关键，系数越大，比如髋关节的正则权重高于踝关节，防止核心姿态偏移）。

经过 IK 优化后，得到机器人的参考姿态序列 $\hat{q}_{0:T}$ —— 这个序列保留了动物运动的 “动态特征”（如狗的 trot 步态节奏），但适配了 Laikago 的关节结构，可直接作为后续模仿学习的 “目标动作”。

### 阶段 2：运动模仿（Motion Imitation）
此阶段的目标是让仿真环境中的机器人，通过强化学习（RL）复现重定向后的参考姿态序列 $\hat{q}_{0:T}$。核心是将 “模仿任务” 转化为 RL 的 “状态 - 动作 - 奖励” 框架，确保机器人不仅能跟踪动作，还能保持平衡与流畅性。

**1. RL 框架设计：状态、动作与目标**

**(1) 状态（State）：包含历史信息，确保运动平滑**
状态 $s_t$ 需反映机器人的 “近期运动趋势”，避免因单步信息导致动作抖动：
$$
s_t = (q_{t-2:t}, a_{t-3:t-1})
$$
- $q_{t-2:t}$：过去 3 个时刻（$t-2, t-1, t$）的机器人姿态（包含 IMU 测量的躯干朝向 + 各关节旋转角度）；
- $a_{t-3:t-1}$：过去 3 个时刻的动作（关节控制指令）；
- **设计逻辑**：机器人运动有惯性（如关节加速需要时间），历史信息能帮助策略预测 “下一步该如何调整动作”。

**(2) 目标（Goal）：让机器人 “有预见性”**
目标 $g_t$ 指定参考运动中未来的关键姿态，避免机器人仅关注 “当前步” 而导致整体运动脱节：
$$
g_t = (\hat{q}_{t+1}, \hat{q}_{t+2}, \hat{q}_{t+10}, \hat{q}_{t+30})
$$
- 包含未来 4 个时刻的参考姿态（覆盖约 1 秒，对应 30Hz 采样的 30 个时间步）；
- **作用**：让机器人提前 “规划” 动作，比如 “当前步调整关节角度，是为了 10 步后达到目标姿态”，确保运动的连贯性。

**(3) 动作（Action）：关节 PD 控制器的目标指令**
策略输出的动作 $a_t$ 是各关节的 “目标旋转角度”，而非直接的电机扭矩：
- 后续通过 “PD 控制器” 将目标旋转角度转化为电机驱动力（确保关节运动稳定）；
- 动作输出后会经过 “低通滤波”（去除高频噪声），避免关节突然抖动。

**2. 奖励函数：多维度鼓励 “精准模仿”**
奖励函数是 RL 的 “指挥棒”，需同时鼓励 “姿态跟踪”“速度匹配”“平衡保持”，公式（4）为总奖励：
$$
r_t = w_p r_t^p + w_v r_t^v + w_e r_t^e + w_{rp} r_t^{rp} + w_{rv} r_t^{rv}
$$
各权重与分项的作用如下（权重反映优先级，姿态跟踪最重要）：

| 奖励分项 | 符号 | 权重 | 计算方式 | 核心目标 |
| :--- | :--- | :--- | :--- | :--- |
| **关节姿态奖励** | $r_t^p$ | 0.5 | $\exp[-5 \sum_j \| \hat{q}^j_t - q^j_t \|^2]$ | 最小化机器人关节旋转与参考姿态的误差（指数衰减：误差越小，奖励越接近 1） |
| **关节速度奖励** | $r_t^v$ | 0.05 | $\exp[-0.1 \sum_j \| \dot{\hat{q}}^j_t - \dot{q}^j_t \|^2]$ | 匹配关节角速度（避免关节 “急加速 / 急减速”） |
| **末端执行器奖励** | $r_t^e$ | 0.2 | $\exp[-40 \sum_e \| \hat{x}^e_t - x^e_t \|^2]$ | 让机器人足部（末端执行器）位置贴合参考位置（权重高，确保 “脚落地准确”） |
| **躯干姿态奖励** | $r_t^{rp}$ | 0.15 | $\exp[-20 \| \hat{x}^{root}_t - x^{root}_t \|^2 - 10 \| \hat{q}^{root}_t - q^{root}_t \|^2]$ | 保持躯干（根节点）的位置与朝向稳定（避免机器人前倾 / 后仰） |
| **躯干速度奖励** | $r_t^{rv}$ | 0.1 | $\exp[-2 \| \dot{\hat{x}}^{root}_t - \dot{x}^{root}_t \|^2 - 0.2 \| \dot{\hat{q}}^{root}_t - \dot{q}^{root}_t \|^2]$ | 匹配躯干的平移 / 旋转速度（确保整体运动节奏与动物一致） |

**设计逻辑**：所有奖励均用指数函数（而非线性函数），原因是 “对小误差敏感，对大误差惩罚显著”—— 比如关节角度误差稍大时，奖励会快速下降，迫使策略精准跟踪。

**3. 训练结果**
在 PyBullet 仿真环境中，用 PPO 算法训练约 2 亿样本后，仿真机器人能流畅复现参考动作（如狗的 trot 步态、跳跃），且运动过程稳定（无跌倒）。

### 阶段 3：域适应（Domain Adaptation）
仿真环境的 “动力学参数”（如机器人质量、关节摩擦力、电机扭矩）与现实存在差异 —— 比如仿真中设置 “关节摩擦系数 0.01”，但现实中可能是 0.03。直接将仿真策略部署到现实，会导致动作失效（如关节动力不足、步态偏移）。此阶段的目标是用 “样本高效” 的方法，让策略适配现实环境。

**1. 关键技术 1：域随机化（Domain Randomization）—— 提升策略鲁棒性**
在仿真训练时，随机调整动力学参数（而非固定值），让策略学会 “适应不同动力学环境”，而非仅依赖单一仿真参数。
参数随机范围如表 1 所示（训练范围窄，测试范围宽，验证泛化性）：

| 动力学参数 | 训练随机范围 | 测试随机范围 | 作用 |
| :--- | :--- | :--- | :--- |
| **机器人质量** | [0.8, 1.2] × 默认值 | [0.5, 2.0] × 默认值 | 适应现实中 “机器人载重 / 部件磨损导致的质量变化” |
| **惯性矩** | [0.5, 1.5] × 默认值 | [0.4, 1.6] × 默认值 | 适应关节旋转惯性的差异 |
| **电机强度** | [0.8, 1.2] × 默认值 | [0.7, 1.3] × 默认值 | 适应电机老化导致的动力衰减 |
| **电机摩擦** | [0, 0.05] N·ms/rad | [0, 0.075] N·ms/rad | 适应现实中关节摩擦的不确定性 |
| **控制延迟** | [0, 0.04] s | [0, 0.05] s | 适应现实中信号传输的延迟 |
| **地面侧向摩擦** | [0.05, 1.25] N·s/m | [0.04, 1.35] N·s/m | 适应不同地面（如瓷砖 / 地毯）的摩擦差异 |

**设计逻辑**：通过 “随机化” 迫使策略学习 “通用的运动规律”（如 “无论电机强 / 弱，都需调整关节角度保持平衡”），而非 “依赖特定仿真参数”。

**2. 关键技术 2：带信息瓶颈的潜空间编码 —— 平衡 “鲁棒性” 与 “适应性”**
仅靠域随机化仍有局限：现实中可能存在 “仿真未覆盖的动力学参数”（如未建模的关节松动），导致策略失效。此技术通过 “潜变量 $z$” 编码动力学信息，同时用 “信息瓶颈” 避免策略过度依赖精确参数。

**(1) 潜变量 $z$ 的作用**
- **编码器 $E(z|\mu)$**：将当前动力学参数 $\mu$（如质量、摩擦）映射为潜变量 $z$（高斯分布，维度远低于 $\mu$）；
- **策略输入**：策略 $\pi(a|s,z)$ 同时接收状态 $s$ 和潜变量 $z$，即 “根据当前状态和动力学环境，输出动作”；
- **作用**：$z$ 是 “动力学环境的压缩表示”，让策略能根据不同环境调整动作（如 $z$ 反映 “电机弱” 时，策略会增加关节驱动力）。

**(2) 信息瓶颈：避免策略 “过拟合” 到仿真参数**
若 $z$ 与 $\mu$ 的关联过强（即 $z$ 完全代表 $\mu$），策略会 “过度依赖精确的动力学参数”—— 而现实中 $\mu$ 无法精确建模，导致策略失效。因此引入信息瓶颈：
- **约束**：限制 $\mu$ 与 $z$ 的互信息 $I(M, Z) \le I_c$（$I_c$ 为上限），即 “$z$ 只能包含 $\mu$ 的核心信息，而非全部细节”；
- **近似计算**：互信息难以直接计算，用 KL 散度近似：
$$
I(M, Z) \le \mathbb{E}_\mu [D_{KL}[E(\cdot|\mu) \| \rho(\cdot)]]
$$
其中 $\rho(z) = \mathcal{N}(0, I)$ 是 $z$ 的先验分布；
- **优化目标**：在 “最大化奖励” 与 “最小化 KL 散度” 之间权衡：
$$
\arg \max_{\pi, E} \mathbb{E}_{\mu, z, \tau} \left[ \sum_{t=0}^{T-1} \gamma^t r_t \right] - \beta \mathbb{E}_\mu [D_{KL}[E(\cdot|\mu) \| \rho(\cdot)]]
$$
- **$\beta$（权衡系数，实验中取 $10^{-4}$）**：
    - $\beta$ 大：KL 散度权重高，$z$ 与 $\mu$ 关联弱，策略更鲁棒（但适应性差）；
    - $\beta$ 小：KL 散度权重低，$z$ 与 $\mu$ 关联强，策略适应性好（但易过拟合）。

**3. 关键技术 3：现实迁移 —— 用 AWR 算法高效搜索最优 $z$**
最终需在现实机器人上找到 “最优潜变量 $z^*$”，让策略在现实中表现最佳。论文采用 **优势加权回归（Advantage-Weighted Regression, AWR）**，仅需约 50 次现实尝试（样本高效），步骤如下：

**(1) 核心流程**
1.  **初始化**：潜变量分布 $\omega_0(z) = \mathcal{N}(0, I)$（先验为标准高斯）；
2.  **迭代搜索**：
    *   **采样**：从当前分布 $\omega_k(z)$ 中采样 $z_k$；
    *   **执行**：用 $z_k$ 控制现实机器人，记录 episode 回报 $R_k$（回报越高，$z_k$ 越优）；
    *   **更新**：根据历史回报 $R_1, ..., R_k$，调整 $\omega_{k+1}(z)$ —— 让 “高回报 $z_i$” 的采样概率更高，权重为：
    $$
    \exp\left( \frac{1}{\alpha} (R_i - \bar{v}) \right)
    $$
    （$\bar{v}$ 为平均回报，$\alpha$ 为温度参数）；
3.  **输出**：迭代 $k_{max}$ 次后，用最终分布 $\omega_{k_{max}}(z)$ 的均值作为 $z^*$，部署策略。

**(2) 优势：样本高效**
传统 RL 在现实中训练需数千次尝试（易损坏机器人），而 AWR 通过 “加权更新分布” 快速聚焦 “优质 $z$”，仅需 50 次左右即可找到 $z^*$，大幅降低现实部署成本。

## 实验

### 1. 核心平台与工具

| 项目 | 具体配置 / 说明 |
| :--- | :--- |
| **机器人平台** | **Laikago 四足机器人**（18 自由度 DoF）<br>• **腿部（12 DoF）**：每腿 3 个主动驱动关节（髋部 2 个 + 膝关节 1 个），直接控制运动<br>• **躯干（6 DoF）**：欠驱动（平移 3 个 + 旋转 3 个），无法直接控制，需通过腿部动作间接平衡 |
| **仿真工具** | **PyBullet**：开源物理引擎，用于模拟机器人刚体动力学与接触力 |
| **训练算法** | **近端策略优化（PPO）**：每个策略训练约 **2 亿** 仿真样本，确保策略充分收敛 |
| **域适应算法** | **优势加权回归（AWR）**：每个策略仅需约 **50 次** 现实尝试，样本效率极高，避免因过度试错损坏机器人 |

### 2. 数据来源：两类参考运动

实验使用两类截然不同的参考数据，旨在验证框架的通用性：

- **动物动捕数据（Real Dog）**：
  - 来源：公开的狗类动作捕捉数据集。
  - 内容：慢步（Pace）、快步（Trot）等自然步态。
  - 特点：符合生物运动规律，物理真实。

- **艺术家动画数据（Artist Animated）**：
  - 来源：人工制作的关键帧动画。
  - 内容：空中转弯（Hop-Turn）、Running Man 舞步等动态动作。
  - 特点：**可能违反物理规律**（如空中转体角动量不守恒），挑战框架的物理修正能力。

### 3. 性能指标：归一化回报（Normalized Return）

为了统一衡量不同动作的模仿效果，定义归一化回报 $R \in [0, 1]$：
- **0**：完全失败（如持续跌倒、动作严重脱节）；
- **1**：完美复现（关节角度、足部位置、躯干姿态与参考运动完全重合）。

> **注意**：由于参考运动可能超出机器人物理极限（如狗的腿比 Laikago 长、动画动作不守恒），实际实验中几乎无法达到 "1"。重点在于对比不同算法（如 Adaptive vs Robust）的**相对差异**。

### 4. 网络结构：端到端学习架构

框架核心模型由三部分组成（对应图 6），实现从 "环境感知" 到 "动作执行" 的完整流：

1.  **编码器 $E(z|\mu)$**：
    -   **结构**：全连接网络（256 $\to$ 128 ReLU）。
    -   **功能**：将高维动力学参数 $\mu$（如质量、摩擦）压缩映射为 **潜变量 $z$** 的高斯分布（输出均值 $m_E$ 和标准差 $\Sigma_E$）。
2.  **策略网络 $\pi(a|s,g,z)$**：
    -   **结构**：全连接网络（512 $\to$ 256 ReLU）。
    -   **输入**：当前状态 $s$、目标运动 $g$、潜变量 $z$。
    -   **输出**：动作 $a$ 的高斯分布均值（标准差固定），即关节位置指令。
3.  **价值函数 $V(s,g,\mu)$**：
    -   **结构**：全连接网络（512 $\to$ 256 单元）。
    -   **功能**：估计当前状态的期望回报，辅助 PPO 算法训练策略。

---

## 结果分析：学到的敏捷技能

实验最直接的成果，是让 Laikago 掌握了 **6 类** 多样化技能，覆盖稳健步态与高动态动作。

### 1. 技能分类与表现

![](/paper/motion-imitation-skills.png)
*Laikago 机器人展示通过模仿参考动作学到的技能。上图：参考动作；中图：仿真机器人；下图：实体机器人。*

| 技能类别 | 具体技能 | 核心特征 | 性能亮点 |
| :--- | :--- | :--- | :--- |
| **步态类** | **狗慢步 (Dog Pace)** | 同侧腿同步运动（如左前+左后），适合低速 | 运动极其稳定，无跌倒记录 |
| | **狗快步 (Dog Trot)** | 对角腿同步运动（如左前+右后），适合中高速 | 速度达 **1.08 m/s**，超越厂商自带控制器的 0.84 m/s |
| | **后退快步 (Backwards Trot)** | 反向 Trot，对角腿同步后退 | 速度达 **1.20 m/s**，所有步态中最快 |
| **动态类** | **侧步 (Side-Steps)** | 身体朝向不变，向左/右平移 | 横向移动平稳，足部无打滑 |
| | **转弯 (Turn)** | 原地或行进中转向（如 90° 转弯） | 转向过程重心稳定，无失衡 |
| | **空中转弯 (Hop-Turn)** | 跳跃中完成 90° 转向（"空中转体"） | **高难度动作**：成功复现人工动画，现实落地稳定 |
| **失败案例** | **Running Man** | 后退时模仿 "向前走" 的腿部动作（太空步） | **仅能拖地后退**，无法抬脚。原因：动作严重违反机器人物理极限（后腿关节活动范围不足） |

> **核心洞察**：
> - **多样性**：仅需更换参考运动数据，无需调整奖励函数或网络结构，即可生成全新技能。
> - **优越性**：学习到的步态速度超越了精心设计的手动控制器。
> - **兼容性**：成功跨越 "真实物理" 与 "非物理动画" 的界限。

### 2. 域适应的有效性验证

为了证明 "自适应（Adaptation）" 的必要性，实验对比了三种策略：

- **No Rand (基准 1)**：无随机化。仿真中仅用固定动力学参数训练。
- **Robust (基准 2)**：仅鲁棒化。仿真中应用域随机化，但部署时无适应过程（$z$ 设为先验均值 0）。
- **Adaptive (本文方法)**：自适应。仿真域随机化 + 现实 AWR 适应（搜索最优 $z^*$）。

#### (1) 跟踪精度对比（归一化回报）

![](/paper/motion-imitation-normalized.png)
*现实世界中模仿各种技能的性能统计。性能记录为 [0, 1] 之间的平均归一化回报。针对每种技能和方法的组合，训练了三个以不同随机种子初始化的策略。每个策略在 5 个回合中进行评估，每种方法总共进行 15 次试验。自适应策略在大多数技能上优于非自适应策略。*

- **简单技能**（侧步、原地踏步）：Robust 与 Adaptive 差距不大（回报 $\approx$ 0.6-0.7），说明简单动作对动力学差异不敏感。
- **动态技能**（快步、转弯）：**Adaptive 显著优于 Robust**（$\approx$ 0.65 vs 0.45）。No Rand 几乎完全失效（< 0.3）。
- **结论**：动作越剧烈，对摩擦、质量等动力学参数越敏感，越需要在线适应。

#### (2) 平衡能力对比（跌倒时间）

![](/paper/motion-imitation-comparison.png)

- **Adaptive**：绝大多数测试能坚持到 episode 结束（5-10s），无跌倒。
- **Robust**：在动态技能中平均坚持时间 < 3s（如 Dog Spin 仅 2s 就摔倒）。
- **No Rand**：几乎启动后 1s 内即摔倒。

#### (3) 泛化性测试：分布外（OOD）环境

为验证极限适应能力，在测试集使用了比训练集**更宽**的参数范围（例如质量变化范围从 $\pm 20\%$ 扩大到 $\pm 100\%$）。

![](/paper/motion-imitation-ood.png)
*策略在 100 个具有不同动力学的仿真环境中的性能。纵轴代表归一化回报，横轴记录了策略达到特定回报值的环境比例。与其他策略相比，自适应策略在更多样化的动力学下实现了更高的回报。*

- **结果**：以 Dog Pace 为例，Adaptive 策略在 50% 的极端环境中仍保持 > 0.6 的回报，而 Robust 仅占 38%。
- **原因**：Adaptive 策略通过调整 $z^*$ 主动 "适配" 新环境，而 Robust 策略只能被动 "忍受"，一旦超出训练时的扰动范围即失效。

#### (4) 适应效率：学习曲线

![](/paper/motion-imitation-curves.png)
*使用学习到的潜在空间使策略适应不同仿真环境的学习曲线。策略能够在相对较少的回合数内适应新环境。*

- **极速收敛**：大多数技能在 **5-10 次** 迭代（现实时间约 50-100 秒）后性能即趋于稳定。
- **意义**：证明了 AWR 的高样本效率，使得在实体机器人上进行在线微调成为可能。

### 3. 参数优化：信息瓶颈（Information Bottleneck）的影响

信息瓶颈系数 $\beta$ 控制了潜变量 $z$ 与环境参数 $\mu$ 的互信息量，决定了策略是倾向于 "死记硬背环境参数" 还是 "学习通用特征"。

![](/paper/motion-imitation-performance.png)
*使用不同信息惩罚系数 $\beta$ 训练的自适应策略的性能。“No IB” 对应于没有信息瓶颈训练的策略。虚线表示适应前的性能，实线表示适应后的性能。*

- **$\beta$ 过大**：互信息太少，$z$ 携带信息不足，适应能力差。
- **$\beta$ 过小**：互信息太多，策略过度依赖精确的仿真参数，导致 Sim-to-Real 时因仿真误差而表现不佳（过拟合仿真）。
- **最优 $\beta = 10^{-4}$**：
    - **适应前**：性能优于小 $\beta$（鲁棒性好）。
    - **适应后**：提升幅度优于大 $\beta$（适应性强）。
    - **对比无 IB**：有信息瓶颈的策略最终回报显著更高（$\approx$ 0.75 vs 0.65）。

---

## 总结与局限

### 1. 现有局限性

尽管取得了显著成果，该框架仍面临以下挑战：

1.  **无法复现 "高动态" 行为**：
    -   目前仅实现了慢步、快步、小跳等中低动态动作。
    -   **原因**：硬件限制（电机扭矩不足、带宽受限）以及算法对瞬时冲击（如大跳落地）的建模不足。
2.  **长期稳定性不及手动控制器**：
    -   虽然速度更快，但在面对复杂地形或持续干扰时，偶尔会失去平衡。
    -   **原因**：手动控制器针对特定步态进行了精细的工程调优，而学习策略主要依赖数据泛化，对 "长尾" 极端情况覆盖不足。
3.  **数据来源受限**：
    -   高度依赖昂贵的动捕数据或专家制作的动画。
    -   **痛点**：缺乏大规模、多样化的野外动作数据（如爬山、涉水），限制了技能库的扩展。

### 2. 未来展望

针对上述问题，未来的研究方向包括：

1.  **探索高动态行为**：
    -   结合更强的硬件（高扭矩电机）。
    -   改进仿真建模（更精准的冲击动力学）和奖励函数（爆发力奖励），挑战后空翻、疾跑等极限动作。
2.  **提升策略鲁棒性**：
    -   扩大域随机化的覆盖范围（如加入地面高度变化、外部推力）。
    -   引入 **在线持续适应**（Continuous Adaptation），利用 IMU 等传感器实时微调策略，而非仅在初始阶段适应。
3.  **利用视频数据学习**：
    -   **目标**：摆脱动捕设备限制，直接从 YouTube 视频（如动物纪录片）中学习。
    -   **技术**：结合计算机视觉（3D 姿态估计），从单目视频中提取运动轨迹，极大地丰富机器人的技能库（如学习猎豹奔跑、山羊攀岩）。
