---
title: "Locomotion 论文精读（十一）RMA: Rapid Motor Adaptation for Legged Robots"
---

> https://arxiv.org/abs/2107.04034
> RSS 2021

## 研究背景

过去四十年，足式机器人（如四足、双足）的发展主要依赖物理动力学建模与控制理论工具，典型案例包括：
- **MIT Cheetah 3 机器人**：通过 “正则化模型预测控制（MPC）+ 简化动力学模型” 实现高速运动与越障；
- **ANYmal 机器人**：基于 “参数化控制器优化 + 倒立摆模型” 实现复杂地形行走。

这类传统方法的核心逻辑是 **“先精准建模，再设计控制规则”**，但存在致命局限：需要大量人类专家知识 —— 不仅要精确刻画机器人的物理参数（如质量、关节刚度）和环境动力学（如地面摩擦、地形硬度），还需手动调试步态参数（如步频、步长）。一旦环境变化（如从水泥地到沙地），原有模型和参数就会失效，适配成本极高。

为降低对专家知识的依赖，近年研究转向强化学习（RL）和模仿学习，其核心范式是：“在仿真环境中训练控制策略（RL 控制器）→ 通过 ‘仿真到现实（Sim-to-Real）’ 技术迁移到实体机器人”。

这种方案的优势是 **“无需手动建模”** —— 策略可通过仿真中的试错自动学习（如 “摔倒则惩罚、前进则奖励”），但落地时面临 **“仿真到现实差距（Sim-to-Real Gap）”** 这一核心挑战，具体源于三个关键差异：

| 差异类型 | 具体表现 |
| :--- | :--- |
| **（a）机器人模型差异** | 仿真中机器人的 “数字孪生” 无法完全匹配实体机器人的物理特性（如电机迟滞、关节磨损）； |
| **（b）地形模型差异** | 仿真中的地形多是 “规则化模型”（如平整地面、固定坡度），而现实地形复杂多变（如沙地、泥泞、草丛，见图 1）； |
| **（c）物理引擎精度限制** | 仿真难以准确模拟 “接触力、可变形表面” 等复杂物理现象（比 “刚体自由运动” 建模难度高一个量级）； |

> 例如：仿真中 “硬地面行走” 的策略，迁移到现实 “软泡沫地面” 时，会因地面支撑力预测错误导致机器人摔倒。

文中以 **“人类行走”** 为类比，指出四足机器人的关键需求 —— **实时适应未知场景**：人类能在 “沙地 / uphill / 负重” 等未经历的场景中，亚秒级（几百毫秒）调整步态（如沙地迈小步、负重时抬腿更用力）；而现有机器人要么 “依赖预定义场景参数”，要么 “需要现实中收集数据再调参”（如收集 3-5 分钟行走数据优化策略），不仅适应慢，还可能因试错摔倒损坏机器人（如在岩石地无经验行走）。

因此，本文提出 **“快速运动适应（Rapid Motor Adaptation, RMA）”** 的核心目标：让机器人在无现实数据微调、无专家预定义参数的前提下，仅通过仿真训练，就能在实体机器人上实现 **“亚秒级在线适应”**（如从水泥地踩入沙地瞬间调整步态），且适配低成本机器人（如文中实验用的 Unitree A1，算力有限）。

为实现上述目标，文中提出 **“基础策略（$\pi$）+ 适应模块（$\phi$）” 双组件架构**（对应图 2 的训练与部署流程），核心思路是 “仿真中用 ‘特权信息’ 训基础能力，现实中用 ‘历史数据’ 估环境状态”，具体拆解如下：

### 1. 两个核心组件的功能定位

| 组件 | 核心作用 | 依赖信息 | 训练方式 |
| :--- | :--- | :--- | :--- |
| **基础策略（$\pi$）** | 生成机器人关节位置指令（如 “前腿抬 10°、后腿伸 5°”），实现稳定行走 | 当前状态（$x_t$，如关节角度、机身姿态）、前一动作（$a_{t-1}$）、环境外在因子（$z_t$） | 强化学习（RL） |
| **环境因子编码器（$\mu$）** | 将高维环境参数（$e_t$，如摩擦系数、负载重量、地形高度，共 17 维）编码为低维向量 $z_t$（8 维），提炼环境关键特征 | 仿真中的 “特权信息” $e_t$（现实中无法直接测量） | 与 $\pi$ 联合 RL 训练 |
| **适应模块（$\phi$）** | 现实中无 $e_t$ 时，从 “近期状态 - 动作历史”（如过去 50 步的关节数据、机身摇晃情况）反向估计 $z_t$（记为 $\hat{z}_t$） | 状态 - 动作历史（$x_{t-50} \sim x_{t-1}, a_{t-50} \sim a_{t-1}$） | 监督学习（MSE 损失） |

### 2. 两阶段训练：全仿真完成，无需现实数据

**阶段 1：训 “基础策略 + 环境编码器”（RL 训练）**
- **场景生成**：用 “分形地形生成器” 创建多样地形（如高低起伏的虚拟山地），同时随机化环境参数（如摩擦系数 0.1~1.0、负载 0~10kg），模拟现实中可能遇到的复杂场景；
- **奖励设计**：用 “生物能量启发的奖励函数”（如 “前进速度越快奖励越高、关节抖动越剧烈惩罚越重”），让策略学习 “高效、平稳” 的步态（而非 “为了前进不顾摔倒风险”）；
- **目标**：让基础策略 $\pi$ 学会 “根据 $z_t$ 调整动作” —— 比如 $z_t$ 显示 “低摩擦” 时，$\pi$ 自动减小步幅避免打滑；$z_t$ 显示 “高负载” 时，$\pi$ 增加关节扭矩支撑重量。

**阶段 2：训 “适应模块”（监督学习）**
- **数据来源**：用阶段 1 的仿真数据（包含 “状态 - 动作历史” 和对应的真实 $z_t$）作为训练数据；
- **目标**：让 $\phi$ 学会 “从历史数据反推 $z_t$” —— 比如看到 “机身突然摇晃、关节扭矩变大”，$\phi$ 能估计出 “当前地面变软（$z_t$ 中对应维度变化）”，替代现实中无法测量的 $e_t$。

### 3. 现实部署：异步运行，适配低成本机器人

Unitree A1 这类低成本机器人的机载算力有限（无法同时运行高频率的策略和适应模块），因此采用 **“异步设计”**：
- **基础策略 $\pi$**：**100Hz 高频运行**，读取 $\phi$ 最新估计的 $\hat{z}_t$，生成关节位置指令，再通过 PD 控制器转换为电机扭矩（保证步态流畅）；
- **适应模块 $\phi$**：**10Hz 低频运行**，持续分析近期状态 - 动作历史，更新 $\hat{z}_t$（无需与 $\pi$ 严格同步，降低算力消耗）。

**RMA 区别于过往学习方法的关键优势：**
1. **无需现实数据调参**：全流程在仿真中训练，部署到实体机器人时无需任何微调（过往方法如 [41] 需在现实中收集 4~8 分钟数据优化策略）；
2. **在线适应速度快**：$\phi$ 仅需 “过去几十步历史数据” 就能估计 $z_t$，适应时间 < 1 秒（亚秒级），避免机器人因适应慢而摔倒；
3. **不依赖预定义知识**：无需手动设计 “足部轨迹生成器”“步态模板” 等（过往方法如 [32] 依赖这类人工规则），策略完全自动学习。

> **总结**：这段内容的核心逻辑
> 传统方法（控制理论）→ 依赖专家建模，泛化差；
> 现有 RL 方法 → 无需建模，但受限于仿真到现实差距；
> **本文 RMA** → 用 “双组件架构 + 全仿真训练” 解决差距：基础策略学 “根据环境调动作”，适应模块学 “现实中估环境”，最终实现低成本机器人的亚秒级在线适应。

## 方法论

![](/paper/rma-overview.png)
*RMA 由两个子系统组成 —— 基础策略 $\pi$ 和适应模块 $\phi$。上图：RMA 分两个阶段训练。第一阶段，基础策略 $\pi$ 接收当前状态 $x_t$、上一动作 $a_{t-1}$ 以及特权环境因子 $e_t$（通过环境因子编码器 $\mu$ 编码为潜在外在向量 $z_t$）作为输入。基础策略在仿真中使用无模型 RL 进行训练。第二阶段，适应模块 $\phi$ 通过监督学习（使用同策略数据），从状态和动作历史中预测外在向量 $\hat{z}_t$。下图：部署时，适应模块 $\phi$ 以 10Hz 频率生成外在向量 $\hat{z}_t$，基础策略以 100Hz 频率生成期望关节位置（通过 A1 的 PD 控制器转换为扭矩）。由于适应模块运行频率较低，基础策略使用适应模块预测的最新外在向量 $\hat{z}_t$ 来预测 $a_t$。这种异步设计对于在算力有限的低成本机器人（如 A1）上实现无缝部署至关重要。*

### 基础策略（Base Policy, $\pi$）：学习 “稳定且泛化的步态”

基础策略是 RMA 的 “运动执行核心”，负责根据环境状态和外在因子生成机器人关节指令，核心目标是在仿真中学习 “无需现实微调即可迁移” 的稳定步态。

**1. 核心设计：输入、输出与联合训练**

**输入**：3 类关键信息，覆盖 “当前状态 + 历史动作 + 环境特征”
- **当前状态 $x_t \in \mathbb{R}^{30}$**：包含机器人关节角度、机身姿态（滚转 / 俯仰）、线性 / 角速度、足部接触状态等；
- **前一动作 $a_{t-1} \in \mathbb{R}^{12}$**：12 个关节（四足机器人每条腿 3 个关节）的上一时刻目标位置；
- **外在因子 $z_t \in \mathbb{R}^8$**：由环境因子编码器 $\mu$ 将 17 维环境参数 $e_t$（摩擦、负载、地形高度等）编码得到的低维特征（仅保留影响步态的关键信息）。

**输出**：
- **$a_t \in \mathbb{R}^{12}$**：12 个关节的当前时刻目标位置，后续通过 PD 控制器转换为电机扭矩。

**训练方式**：与环境因子编码器 $\mu$ 端到端联合训练，采用模型无关强化学习（RL）（结合前文可知为 PPO 算法），目标是最大化累积折扣奖励：
$$J(\pi) = \mathbb{E}_{\tau \sim p(\tau|\pi)} \left[ \sum_{t=0}^{T-1} \gamma^t r_t \right]$$
（$\gamma$ 为折扣因子，$r_t$ 为时刻 $t$ 的奖励）

传统 Sim-to-Real 方法需通过 “人工添加仿真噪声” 或 “现实数据校准” 提升迁移性，而 RMA 通过两大自然约束让策略自发学习鲁棒步态：

**生物能量启发的奖励函数**：从 “人类 / 动物高效运动” 规律出发，避免 “为前进牺牲稳定性”，10 项奖励 / 惩罚项精准引导步态（权重已优化，直接决定策略行为）：

| 奖励项 | 计算逻辑 | 核心作用 | 权重 |
| :--- | :--- | :--- | :--- |
| **1. 前进奖励** | $\min(v_{t,x}, 0.35)$：速度不超过 0.35 m/s | 鼓励向前运动，避免超速失控 | 20 |
| **2. 横向 / 旋转惩罚** | $-|v_{t,y}|^2 - |\omega_{t,yaw}|^2$ | 抑制横向偏移和航向旋转，保证沿直线行走 | 21 |
| **3. 功惩罚** | $-\tau^T \cdot (q_t - q_{t-1})$ | 惩罚电机输出的机械功，最小化能耗 | 0.002 |
| **4. 地面冲击惩罚** | $-|f_t - f_{t-1}|^2$ | 惩罚足部地面反作用力突变，减少落地冲击 | 0.02 |
| **5. 扭矩平滑惩罚** | $-|\tau_t - \tau_{t-1}|^2$ | 惩罚电机扭矩突变，避免关节 “卡顿” | 0.001 |
| **6. 动作幅度惩罚** | $-|a_t|^2$ | 限制关节运动范围，避免超量程损伤 | 0.07 |
| **7. 关节速度惩罚** | $-|\dot{q}_t|^2$ | 避免关节超速，降低电机负载 | 0.002 |
| **8. 姿态惩罚** | $-|\theta_{t,roll/pitch}|^2$ | 维持机身平稳，避免侧翻或前倾 | 1.5 |
| **9. Z 轴加速度惩罚** | $-|v_{t,z}|^2$ | 减少上下颠簸，提升运动平稳性 | 2.0 |
| **10. 足部滑动惩罚** | $-|diag(g_t) \cdot v_{f,t}|^2$ | 惩罚接触时的足部速度，避免打滑 | 0.8 |

**不平坦地形训练**：直接在 “分形生成的随机不平坦地形” 上训练（而非先训平坦地面再迁移），替代传统方法中 “手动添加足部 clearance 奖励” 或 “外部推力鲁棒性奖励”，让策略自然适应地形高度变化。

**3. 训练课程（Curriculum）：避免 “策略坍缩”**
若直接用上述奖励训练，策略可能因 “惩罚项过重” 而 “不敢动”（停在原地以最小化惩罚），因此设计渐进式课程：
- **初始阶段**：惩罚项系数极小（如功惩罚权重从 0.0001 开始），让策略先学会 “前进”；
- **迭代过程**：逐步增大惩罚项系数，同时线性提升环境扰动难度（如摩擦系数、负载、电机强度的随机范围）；
- **地形无课程**：从训练开始就随机采样 “全难度地形”，避免策略依赖 “简单地形记忆”，提升泛化性。

### 适应模块（Adaptation Module, $\phi$）：在线估计 “环境外在因子”

现实中无法直接测量 17 维环境参数 $e_t$（如地面摩擦、实时负载），适应模块的核心目标是从 “近期状态 - 动作历史” 中反向估计外在因子 $z_t$（记为 $\hat{z}_t$），替代仿真中的 “特权信息”。

**1. 核心设计：输入、输出与训练逻辑**

**输入**：$k=50$ 步的状态 - 动作历史（对应 0.5 秒，因仿真 / 现实步长为 0.01 秒），即 $(x_{t-50:t-1}, a_{t-50:t-1})$ —— 包含足够的 “环境交互信息”（如足部打滑时的关节速度变化、负载增加时的扭矩变化）。

**输出**：$\hat{z}_t \in \mathbb{R}^8$：对真实 $z_t$ 的估计，直接输入基础策略 $\pi$。

**架构选择**：**1D CNN（一维卷积神经网络）** —— 专门捕捉 “时间序列相关性”（如 50 步内足部反作用力的渐变趋势，对应地面硬度变化），比纯 MLP 更能提取历史中的动态特征。

**On-Policy 数据生成**：
传统监督学习若仅用 “基础策略的完美轨迹”（机器人平稳行走的数据）训练 $\phi$，会导致 $\phi$ 无法应对现实中的 “步态偏差”（如突然打滑、轻微磕碰）。RMA 通过 On-Policy 数据生成解决此问题：
1. 初始化 $\phi$ 为随机参数，用它预测的 $\hat{z}_t$ 驱动基础策略 $\pi$ 生成 “非完美轨迹”（因 $\hat{z}_t$ 不准，机器人可能轻微摇晃或打滑）；
2. 收集这些 “非完美轨迹” 的状态 - 动作历史，与对应的真实 $z_t$（由 $\mu$ 生成）组成训练对；
3. 用 MSE 损失（$\|\hat{z}_t - z_t\|^2$）更新 $\phi$，迭代至收敛。

这种方式让 $\phi$ 见过 “探索性轨迹”（类似现实中的突发情况），部署时面对步态偏差仍能准确估计 $z_t$。

### 异步部署（Asynchronous Deployment）

Unitree A1 等低成本机器人的机载算力有限（无法同时运行高频策略和复杂历史分析），RMA 通过异步高低频设计实现无缝部署，核心逻辑是 **“$z_t$ 变化慢，$x_t$ 变化快”**。

**1. 双模块运行频率与协作**
- **基础策略 $\pi$**：**100 Hz 高频运行** —— 关节控制需高频响应（0.01 秒 / 步），才能保证步态流畅，避免机身晃动；运行时读取 $\phi$ 最新输出的 $\hat{z}_t$，无需等待 $\phi$ 的下一次更新。
- **适应模块 $\phi$**：**10 Hz 低频运行** —— 处理 50 步历史数据需一定计算时间，且 $z_t$ 对应环境特征（如地面摩擦、负载）不会瞬间剧变（如从水泥地到沙地需 0.1 秒以上才能体现步态变化），10 Hz（0.1 秒 / 次更新）足够。

**2. 异步设计的优势（对比 “耦合设计”）**
若将 $\pi$ 和 $\phi$ 耦合为单模块（让 $\pi$ 直接输入历史数据），会出现三大问题：
1. **步态不自然**：历史数据维度高，单模块难以平衡 “历史分析” 与 “实时动作生成”，导致步态卡顿；
2. **频率受限**：单模块最多运行 10 Hz，无法满足关节控制的高频需求；
3. **需同步校准**：模块间需严格时钟同步，增加部署复杂度。

而异步设计完全规避这些问题，无需任何现实校准，直接部署即可。

为验证 RMA 的 “超范围适应能力”，仿真中训练范围小于测试范围，确保策略能应对 “未见过的极端环境”，关键参数如下：

| 参数 | 训练范围（仿真中随机采样） | 测试范围（仿真 / 现实验证） | 核心意义 |
| :--- | :--- | :--- | :--- |
| **摩擦系数** | [0.05, 4.5] | [0.04, 6.0] | 覆盖从 “冰面（低摩擦）” 到 “橡胶地（高摩擦）” |
| **PD 控制器参数** | Kp[50,60], Kd[0.4,0.8] | Kp[45,65], Kd[0.3,0.9] | 适应不同电机刚度，避免现实中 PD 参数偏差 |
| **负载（kg）** | [0, 6] | [0, 7] | 应对超训练负载（如携带 7kg 物品） |
| **质心偏移（cm）** | [-0.15, 0.15] | [-0.18, 0.18] | 适应机器人重心变化（如负载位置偏移） |
| **电机强度** | [0.90, 1.10] | [0.88, 1.22] | 应对电机老化或电压波动导致的强度变化 |
| **地形重采样概率** | 0.004 | 0.01 | 测试时地形变化更频繁，验证鲁棒性 |

## 实验

### 1. 核心平台与工具：Unitree A1

A1 是 RMA 的唯一实体测试平台，其硬件特性直接决定了算法的 “控制目标” 和 “传感器输入”。

| 硬件指标 | 具体规格 | 核心作用 |
| :--- | :--- | :--- |
| **自由度（DoF）** | 共 18 个 DoF，其中 **12 个为驱动 DoF**<br>（每条腿 3 个：髋关节前 / 后摆、膝关节伸 / 屈） | 对应动作空间维度，策略直接输出 12 个关节目标位置 |
| **体重** | 约 **12 kg** | 负载实验中 “超体重负载”（如 8 kg）的基准 |
| **状态传感器** | • **电机编码器**：关节位置 / 速度<br>• **IMU**：机身滚转 / 俯仰角<br>• **足部传感器**：二进制接触标识（1=接触，0=悬空） | 构成 **30 维状态空间** 的核心输入数据 |
| **控制方式** | **关节位置控制** | 基础策略输出 “目标位置”，通过 PD 控制器转为扭矩，避免直接力控的不稳定性 |
| **PD 参数** | 固定增益 $K_p=55$（比例），$K_d=0.8$（微分） | 统一仿真与现实的关节控制逻辑，减少 “Sim-to-Real 差异” |

- **模拟器**：**RaiSim** —— 擅长 “刚体动力学” 与 “接触力模拟”（如摩擦、可变形表面），适配四足 locomotion。
- **地形生成**：内置分形地形生成器（Fractal Terrain），参数固定（八度=2，间隙度=2.0，增益=0.25，z轴缩放=0.27），生成随机不平坦地形。
- **Episode 终止条件**：
  - 单轮最大 1000 步（10 秒）。
  - **摔倒判定**：机身高度 < 0.28m。
  - **姿态失稳**：滚转角 > 0.4 rad (23°)，俯仰角 > 0.2 rad (11.5°)。

### 2. 状态与动作空间定义

#### (1) 状态空间（30 维， $x_t \in \mathbb{R}^{30}$）

| 状态分量 | 维度 | 物理意义 |
| :--- | :--- | :--- |
| **关节位置** | 12 | 12 个驱动关节的当前角度 |
| **关节速度** | 12 | 12 个驱动关节的当前角速度 |
| **机身姿态** | 2 | 滚转 (Roll) / 俯仰 (Pitch) 角，反映机身平稳度（忽略偏航角 Yaw） |
| **足部接触** | 4 | 4 条腿的二进制接触状态（1=接触，0=悬空） |

#### (2) 动作空间（12 维， $a_t \in \mathbb{R}^{12}$）

- **输出**：12 个驱动关节的 “目标位置” $\hat{q}$。
- **执行**：通过 PD 控制器计算电机扭矩 $\tau$：
  $$ \tau = K_p (\hat{q} - q) + K_d (\dot{\hat{q}} - \dot{q}) $$
  其中目标速度设为 0 ($\dot{\hat{q}}=0$) 以增加阻尼稳定性。

#### (3) 环境参数（17 维， $e_t$）

这是 **环境因子编码器 $\mu$** 的输入（现实中不可见），涵盖了影响运动的关键物理量。

| $e_t$ 分量 | 维度 | 物理意义 |
| :--- | :--- | :--- |
| **质量与质心** | 3 | 机器人总质量 + 质心在 x/y 轴的偏移 |
| **电机强度** | 12 | 12 个电机的输出强度系数（模拟老化/电压波动） |
| **地面摩擦** | 1 | 地面摩擦系数（范围 0.05 ~ 4.5） |
| **局部地形** | 1 | 足下地形高度的最大值（离散化处理） |

> **关键设计**：局部地形高度仅取 “最大值 + 离散化”，让策略**不依赖高精度地形扫描**，而是通过历史步态（本体感受）间接感知地形。

### 3. 网络架构详细设计

#### (1) 基础策略 $\pi$ 与环境编码器 $\mu$

| 模块 | 网络类型 | 输入 $\to$ 隐藏层 $\to$ 输出 | 核心作用 |
| :--- | :--- | :--- | :--- |
| **基础策略 $\pi$** | MLP | $(x_t, a_{t-1}, z_t)$ [50维] $\to$ [128, 128] $\to$ $a_t$ [12维] | 根据状态和环境特征输出动作 |
| **环境编码器 $\mu$** | MLP | $e_t$ [17维] $\to$ [256, 128] $\to$ $z_t$ [8维] | 将物理参数压缩为低维潜变量 $z_t$ |

#### (2) 适应模块 $\phi$：1D CNN + MLP

适应模块需从 **50 步历史数据**（0.5秒）中提取环境特征，设计了专门的时序处理架构：

1.  **嵌入层 (Embedding)**：MLP 将每步的 $(x_t, a_{t-1})$ [42维] 映射为 32 维特征。
2.  **时序卷积 (TCN)**：3 层 1D CNN 提取时序特征。
    -   Layer 1: Kernel=8, Stride=4 (快速压缩序列)
    -   Layer 2: Kernel=5, Stride=1 (捕捉局部动态)
    -   Layer 3: Kernel=5, Stride=1 (强化长期依赖)
3.  **输出层**：Flatten 后经线性层输出 $\hat{z}_t$ [8维]。

### 4. 训练参数

- **基础策略 (PPO)**：
  - 迭代：15,000 次 (共 12 亿步)。
  - 算力：单 GPU 训练约 **24 小时**。
- **适应模块 (监督学习)**：
  - 损失函数：MSE Loss $\|\hat{z}_t - z_t\|^2$。
  - 数据：使用 **On-Policy** 数据（包含非完美策略产生的轨迹），提升鲁棒性。
  - 算力：单 GPU 训练约 **3 小时**。

---

## 结果分析

### 1. 多场景现实对比实验

实验对比了 RMA、RMA (无适应模块) 和 A1 原厂控制器在多种挑战性场景下的表现。

![](/paper/rma-eva.png)
*RMA 在现实世界中多种分布外场景的评估。我们比较了 RMA、A1 自带控制器以及不带适应模块的 RMA。结果发现：RMA 能以 80% 的成功率走下 15cm 高的台阶，并以 100% 的成功率通过未见过的可变形表面（如记忆棉床垫和轻微不平的泡沫）。它还能成功攀爬斜坡和台阶。A1 控制器无法在不平泡沫上行走。底部图表分析了三种方法的负载能力：A1 控制器在 8kg 负载时性能开始下降；无适应模块的 RMA 在超过 8kg 负载时无法移动；而 RMA（本方法）在 12kg 负载下仍能移动。总体而言，RMA 始终优于基线方法。*

**关键结论**：

| 测试场景 | RMA 表现 | RMA (无适应) | A1 控制器 | 结论 |
| :--- | :--- | :--- | :--- | :--- |
| **不平泡沫** | **100% 成功** | 20% | 100% | 简单不平地形，A1 控制器尚可应对 |
| **记忆棉床垫** | **100% 成功** | 0% | 100% | 软表面需适应刚度变化 |
| **15cm 下台阶** | **80% 成功** | 0% | 60% | RMA 对高度差的鲁棒性更强 |
| **8cm 上台阶** | **60% 成功** | 0% | 20% | **RMA 是唯一能应对 8cm 台阶的方案** |
| **12kg 负载** | **可移动** | 无法移动 | 性能退化 | RMA 支持 **100% 自重负载** (12kg) |

> **核心洞察**：
> - **无适应模块 (w/o Adapt)**：在几乎所有挑战场景中失效，证明了 "适应模块" 对于 Sim-to-Real 的决定性作用。
> - **A1 控制器**：仅适配已知简单场景，面对高台阶或重负载时鲁棒性不足。
> - **RMA**：通过实时估计 $z_t$，实现了对 "未知地形" 和 "动态负载" 的全场景适应。

### 2. 打滑场景适应分析

实验设计了一个极具挑战性的场景：**脚底贴胶带 + 地面铺油板**（极低摩擦），观察 RMA 的实时反应。

![](/paper/rma-analyze.png)
*RMA 在铺有油性塑料板的地面上行走的分析（脚底额外贴有塑料膜）。上图：膝关节扭矩；中图：四足接触状态（步态模式）；下图：适应模块预测的外在向量 $\hat{z}$ 的第 1 和第 5 分量（经过中值滤波）。当机器人进入打滑区域时，$\hat{z}$ 的两个分量发生突变，表明适应模块检测到了打滑事件。注意适应后，恢复的步态周期与原始步态相似，扭矩幅度增加，且 $\hat{z}$ 持续反映地面湿滑的状态。RMA 在 90% 的测试中成功通过油性区域。*

- **打滑检测 (t ≈ 2s)**：
    - 适应模块输出的 $\hat{z}_t$（第 1、5 维）**瞬间突变**，表明 $\phi$ 成功识别出 "摩擦力急剧下降"。
- **步态恢复**：
    - 基础策略立即响应，**膝关节扭矩翻倍**（10 Nm $\to$ 20 Nm），通过增加下压力来提升抓地力。
    - 步态从短暂的 "紊乱" 迅速恢复为稳定的 Trot 步态。
- **持续适应**：
    - 即使步态恢复正常，$\hat{z}_t$ 仍维持在 "低摩擦" 的数值区间，确保策略持续保持高扭矩模式，防止二次打滑。

---

## 总结与局限

### 1. RMA 的核心突破：无依赖、强适配

RMA 在四足机器人 Locomotion 领域确立了新的范式：

1.  **无需外部辅助**：
    -   无需人类演示数据（如模仿学习）。
    -   无需预定义运动模板（如传统控制）。
    -   策略完全由 **"RL + 仿真"** 自主演化而来。
2.  **仅靠本体感受（Proprioception Only）**：
    -   即便没有视觉 / 激光雷达，仅凭 **关节编码器 + IMU**，RMA 就能通过 "身体的感觉"（如脚底反作用力、关节阻力）来推断环境特性。
    -   这证明了本体感受中蕴含着丰富的环境信息，足以支持复杂的盲行走（Blind Locomotion）。

### 2. 局限性：盲机器人的天花板

原文形象地将 RMA 称为 "**Blind Robot**"（盲机器人），这也直接导致了其局限性：**只能 "应对"，无法 "预判"**。

-   **场景 1：下楼踩空**
    -   **问题**：只有当脚真的 "踩空"（机身下坠）时，本体感受才会报警。此时再调整姿态，往往为时已晚（物理惯性已导致失衡）。
    -   **解法**：需要视觉提前看到台阶，提前减速或调整落点。
-   **场景 2：多腿卡死**
    -   **问题**：在乱石堆中，如果两条前腿同时卡入石缝，本体感受只能感知 "推不动"，但无法规划 "绕行" 路径，导致机器人卡死或倾倒。
    -   **解法**：需要视觉识别障碍物分布，规划避障路径。

### 3. 未来展望：融合视觉

作者明确指出，未来的方向是 **"本体感受 + 视觉感知"** 的融合：

-   **视觉 (Vision)**：负责 **"远距离预判"**（1-5米），提前规划路径，规避显性风险（如大坑、高墙）。
-   **本体感受 (Proprioception)**：负责 **"近距离交互"**（0米），处理视觉看不清的隐性特性（如地面松软、摩擦力变化）。

二者互补，才能打造出真正 "高可靠" 的野外足式机器人。这也是后续工作（如 RMA Phase 2 或结合 VIO 的导航）的重点方向。
