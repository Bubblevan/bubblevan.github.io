---
title: "CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos"
---

https://arxiv.org/pdf/2411.17820
> CVPR2025
> 从网络级视频中学习具身城市导航，有一说一CVPR最近就喜欢这种视频相关的玩意。
> 多任务强化学习，还有MoE的事？
---

在**动态城市环境**中导航对**具身智能体**提出了重大挑战，这需要先进的**空间推理能力**和对**常识规范**的遵守。尽管取得了一些进展，但现有的**视觉导航方法**在无地图或非街道环境中仍存在困难，这限制了像**最后一公里配送机器人**等自主智能体的部署。为了克服这些障碍，我们提出了一种可扩展的、数据驱动的**类人城市导航方法**，通过在**数千小时来自网络的野外城市步行和驾驶视频**上训练智能体来实现。我们引入了一个简单且可扩展的**数据处理管道**，从这些视频中提取**动作监督信息**，从而实现大规模的**模仿学习**，无需昂贵的标注成本。我们的模型学习了复杂的**导航策略**，以应对各种挑战和关键场景。实验结果表明，在**大规模、多样化的数据集**上进行训练能显著提升导航性能，超越了现有方法。这项工作展示了利用丰富的**在线视频数据**为动态城市环境中的具身智能体开发稳健导航策略的潜力。

### 研究背景

**具身城市导航**（如**配送机器人**、**自动驾驶**）需应对**动态障碍物**、**交通规则**、**社交规范**等复杂场景，但现有方法存在局限：

- **强化学习（RL）**：适用于静态/受控环境，**sim-to-real 迁移**效果差
- **模仿学习**：依赖稀缺**专家数据**，泛化能力弱

**核心问题**：如何利用**低成本、大规模数据**，训练出**鲁棒性强**、**可泛化**的城市导航模型？

> Despite their success in simulators or moderately complex environments, existing methods fall short in addressing the complexity of embodied urban navigation. Public urban spaces are inherently dynamic and unpredictable, characterized by "multifarious terrains, diverse obstacles, and dense pedestrians" that require real-time adaptability [15].
> 
> 把我的 WM 预测性方法给否了（哭）但是我还是要做实验看效果

## Related work

### Navigation in Simulation

- **点目标导航（PointNav）**：目标是明确空间点（如 **GPS 坐标**），侧重安全高效**轨迹规划**
- **图像目标导航**：以参考图像为目标，侧重找目标
- **物体目标导航**：以特定物体为目标，侧重**目标定位**
- **视觉-语言导航（VLN）**：结合**语言指令** + 视觉，侧重**指令理解**

**PointNav** 在模拟器中实现近乎完美性能，被认为"已解决"，但真实城市空间的**航点导航**仍未解决（动态、复杂、需遵守**社会规范**）

### Real-World Navigation

**核心问题**：现有方法难以适配**动态复杂城市环境**

**发展脉络**：

- 早期依赖**测距传感器** + 模块化 **SLAM** [1-3]
- 后续尝试弥合 **sim-to-real 差距** [38-41]，或从**专家轨迹**中学习**模仿策略** [42-45]

**现有局限**：仅能在郊区、停车场等简单静态环境运行，无法泛化到动态、复杂、需遵守**交通/社交规范**的城市场景（**sim-to-real 迁移**难、**专家数据**覆盖不足）

**文档切入点**：**CityWalker** 延续"**模仿学习**"思路，但聚焦城市环境必需的**规则/规范学习**，适配**配送机器人**、**自动驾驶**等实际需求

### In-the-Wild Video Learning

**核心思路**：用**网络规模视频**解决导航的"**数据稀缺** + **标注昂贵**"痛点

**网络级数据**在语言、视觉任务中成效显著 [19-21]，但**野外视频**缺乏**动作标签**，难以直接用于**模仿学习**

**现有方案痛点**：

- 要么用 **VLM** 生成动作标签 [23,24]（成本高、难规模化）
- 要么用 **BEV 模型**伪标注 [50]（**SelfD** 观点）

**CityWalker 解决方案**：用现成**视觉里程计（VO）** [25] 生成"**带噪声伪标签**"，低成本、可并行处理，适配大规模视频（**2000+ 小时**），且跨领域（行走/驾驶视频均可）

**与相关工作差异**：**LeLaN** [24] 依赖 **VLM 提示**，而 **CityWalker** 仅用 **VO 伪标签**，更高效可扩展

## Methodology

### Problem Definition

**Embodied Urban Navigation**

在每个时间步 $t$，智能体接收：
- **RGB 观测** $o_t$
- 当前 **GPS 位置** $p_t$
- **子目标航点** $w_t$

智能体旨在学习一个策略：

$$\pi(a_t | o_{(t-k):t}, p_{(t-k):t}, w_t)$$

该策略将过去的观测和位置信息映射到动作空间 $A$ 中的动作 $a_t$，表示为欧几里得空间中的一系列动作航点。通常，我们取 $k = 5$，并预测未来 5 个时间步的动作。


### Evaluation Metrics

![](/paper/citywalker-evaluation-metrics.png)

**平均方向误差**：我们将每个预测动作与真实动作之间的误差作为离线遥操作数据的主要评估指标。虽然欧几里得空间中的 **L2 距离**是一个直观的指标，但我们发现它不能充分反映该问题。如图右侧所示，与绿色轨迹相比，红色轨迹的 **L2 误差**更小，但运动方向却偏离了目标。**导航中方向错比距离偏更致命**（如转弯时方向错会闯红灯，而距离偏一点可能仍能到达目标）

#### AOE（Average Orientation Error）

衡量"预测动作与真实动作的角度偏差"，公式本质是计算两者在空间中的夹角（以正 y 轴为基准），再对所有样本取平均。

**公式**：

$$AOE(k) = \frac{1}{n} \sum_{i}^{n} \theta_{i_k} = \frac{1}{n} \sum_{i}^{n} \arccos \frac{\langle \hat{a}_{i_k}, a_{i_k} \rangle}{\|\hat{a}_{i_k}\| \|a_{i_k}\|}$$

其中，$k$ 是预测动作的索引，$n$ 是数据数量。

**示例**：真实动作是"向东北走"（45°），预测动作是"向东南走"（-45°），则 **AOE** 为 90°，反映方向完全错误。

#### MAOE（Maximum AOE）

解决"AOE 取平均掩盖关键错误"的问题——先计算每个样本在"未来 5 步动作"中的最大 **AOE**（如某样本第 3 步 **AOE** 最大），再对所有样本的最大 **AOE** 取平均。

**公式**：

$$MAOE = \frac{1}{n} \sum_{i}^{n} \max_k \theta_{i_k}$$

**示例**：某样本前 4 步 **AOE** 仅 5°，但第 5 步（路口转弯）**AOE** 达 30°，**MAOE** 会捕捉到这个"致命错误"，而普通 **AOE** 可能因前 4 步的小误差拉低整体值，无法反映风险。

**补充指标**：由于当动作距离较小时，方向误差可能存在噪声，我们还将 **L2 距离**用作补充评估指标。

#### 关键场景

并非轨迹中的所有时间步都同等重要。例如，与在开阔区域向前移动时出现的错误相比，在**路口**或**转弯处**出现的错误更有可能导致导航失败。

我们确定了导航过程中几个最关键的场景。针对每个场景，我们分别计算了 **AOE** 和 **MAOE**，并且为了更全面地进行评估，我们使用了**场景均值**而非样本均值。

### Learning from In-the-Wild Videos

**City Walking Videos（城市行走视频）**是 **CityWalker** 模型用于大规模**模仿学习**的核心数据来源，本质是从互联网收集的、以**第一人称视角（FPV）**记录人类在城市中行走的视频。

为了给**模仿学习**提供"可学习的动作标签"，**CityWalker** 没有用昂贵的人工标注或 **VLM** 生成标签，而是通过**视觉里程计（VO，Visual Odometry）**工具从视频中提取"轨迹姿态"作为动作标签。

**VO** 是一种从连续图像帧中计算"相机（或观察者）运动轨迹"的技术——它通过对比相邻帧的视觉特征（如建筑边缘、路面纹理），估算出帧间的"相对位置变化"（如向前走 0.8 米、左转 15°），最终输出一条"伪轨迹"。**CityWalker** 采用当前最优的 **VO** 工具 **DPVO**，也兼容其他 **VO** 方法。

![](/paper/citywalker-vo.png)

**VO 提取的轨迹存在天然缺陷**，需针对性处理才能作为有效标签：

| 问题 | 具体表现 | 解决方案 |
| --- | --- | --- |
| **全局轨迹累积误差** | **VO** 通过"帧间递推"计算轨迹，误差会逐步叠加，导致全局轨迹（如从 A 地到 B 地的总路径）不准确 | 模型仅依赖**短时间窗口**内的相对姿态（如"过去 5 帧→未来 5 帧"的局部运动），局部范围内 **VO** 的误差可忽略，无需全局轨迹精度 |
| **尺度模糊** | 不同视频的"轨迹长度"无统一标准（如驾驶视频实际移动 10 米，**VO** 输出可能与行走视频移动 2 米的输出长度一致） | 用轨迹内的**平均步长归一化**动作：先计算单段视频中人类行走的平均步长（如每步 0.7 米），再将所有动作按该步长缩放，消除"行走/驾驶""不同人步频"的尺度差异，让模型在统一动作空间学习 |

相比依赖 **VLM**（如 **GPT-4V**）生成动作标签的方案（成本高、难并行），**VO-based** 方案可批量处理 **2000+ 小时视频**，耗时短、无额外成本，是实现"**大规模模仿学习**"的关键前提。
### Pipeline

![](/paper/citywalker-overview.png)

整体是「**输入编码** → **Transformer 时序融合** → **双头解码**」的三段式结构。

#### 输入

**第一类：k 个图像特征 Token**

- 对连续的 $k$ 帧历史 **RGB 图像**，用 **DINOv2** 模型提取每帧的视觉特征，得到 $k$ 个图像特征向量（即 $k$ 个 **Token**）
- 训练时完全冻结 **DINOv2** 的主干网络（**backbone**）——复用 **DINOv2** 在海量图像上预训练的通用视觉特征，既避免模型过拟合，又大幅降低训练成本，还能保证视觉特征的通用性

**第二类：1 个坐标嵌入 Token**

- 专门编码「空间位置信息」，由两部分拼接堆叠而成：$k$ 个智能体历史 **GPS 位置** + $1$ 个导航目标 **GPS 位置**
- **作用**：让模型同时感知「自己走过的轨迹」和「要去的终点」，实现"视觉 + 位置"的联合决策，这是 **PointNav** 点导航任务的核心要求

**最终送入 Transformer 的输入序列**：$[k \text{ 个图像特征 Token} + 1 \text{ 个坐标嵌入 Token}]$，是一个固定长度的时序特征序列。

#### 输出

**Transformer 的输出规则**：输入是多长的序列，输出就是多长的特征序列（完美继承输入的时序 + 空间特征信息）。

**解码层（核心输出）**：在 **Transformer** 输出特征之上，接两个独立的 **MLP（多层感知机）**头，完成最终任务预测，也是模型的最终输出：

- **Action Head（动作预测头）**：解码输出「智能体未来的导航动作」（对应 **PointNav** 任务，输出下一步/未来多步的移动方向、步长）
- **Arrival Prediction Head（到达预测头）**：解码输出「二分类结果」——判断智能体是否已经抵达当前子目标（**waypoint**）

#### Feature Hallucination（特征幻觉）

引导 **Transformer** 学会「预判未来的环境特征」，让模型不仅能利用「历史视觉信息」，还能主动预测「未来即将遇到的场景特征」，进而让动作预测更精准、更贴合真实城市场景的动态变化。

**计算方式**：计算「**Transformer** 输出的预测未来图像 **Token**」和「从真实未来帧中直接用 **DINOv2** 提取的图像 **Token**」之间的 **MSE 损失**（均方误差）。

**作用机制**：让它知道真实的未来场景特征长什么样，逼着模型自己预测的未来特征和真实特征尽可能一致，这个过程就叫「**特征幻觉（脑补未来特征）**」。

**效果**：通过**特征幻觉**的辅助约束，让 **Transformer** 输出的「未来特征 **Token**」更具信息价值（更贴近真实），反过来帮助上述两个 **MLP** 头（动作头、到达头）做出更准确的最终预测——最终实现"预判未来→优化当下动作"的闭环，尤其适配城市动态场景（行人、路口、转弯）的导航需求。

#### 损失函数

**核心损失 1：方向损失 $L_{ori}$**

原文专门为导航任务设计了方向损失，用于监督「预测动作」和「真实动作」的方向一致性。

**公式**：

$$L_{ori} = -\frac{1}{k}\sum_{i=1}^k \frac{\langle \hat{a}_i, a_i \rangle}{\|\hat{a}_i\| \|a_i\|}$$

**符号含义**：
- $\hat{a}_i$ = 模型预测的第 $i$ 步动作向量
- $a_i$ = 真实标注的第 $i$ 步动作向量
- $k$ = 动作预测的步数
- $\langle \cdot, \cdot \rangle$ = 向量内积
- $\|\cdot\|$ = 向量的模长

**公式本质**：计算「预测动作向量」和「真实动作向量」的**余弦相似度**，再取负数、求平均。

**训练目标**：余弦相似度的取值范围是 $[-1,1]$，两个向量方向越一致，余弦相似度越接近 1 → $L_{ori}$ 越接近 $-1$；训练时让 $L_{ori}$ 尽可能小，就代表模型的动作方向越准确。

**最终优化目标**：模型的最终优化目标是四个独立损失的加权和，原文明确：每个权重的选择原则是「让所有损失项的数值处于同一量级」，保证训练时各损失均衡发力。

$$L = \omega_{l1} L_{l1} + \omega_{ori} L_{ori} + \omega_{arr} L_{arr} + \omega_{feat} L_{feat}$$

**损失函数详解**：

| 损失函数 | 监督对象 | 作用 | 权重 |
| --- | --- | --- | --- |
| **$L_{l1}$（L1 损失）** | **Action Head** 输出的「预测动作」 | 约束预测动作与真实动作的绝对位置偏差（步长、平移距离），是回归任务的经典损失，补充方向损失的"距离维度"监督 | $\omega_{l1}$ |
| **$L_{ori}$（方向损失）** | **Action Head** 输出的「预测动作」 | 核心损失，约束预测动作与真实动作的方向一致性 | $\omega_{ori} = 5.0$（最大） |
| **$L_{arr}$（BCE 损失，二分类交叉熵）** | **Arrival Prediction Head** 输出的「到达状态」 | 监督模型"是否抵达子目标"的二分类预测（抵达 = 1，未抵达 = 0） | $\omega_{arr} = 1.0$ |
| **$L_{feat}$（MSE 损失）** | **Feature Hallucination** 模块的「预测未来特征」 | 约束预测未来特征与真实未来特征的偏差，是辅助损失 | $\omega_{feat} = 0.1$（最小） |


## Experiment

### Setup

### Baselines

实验选取了与自身设置高度相关的**户外导航模型**作为性能对比基准，同时说明部分模型的适配调整与未纳入对比的原因：

- **核心对比模型**：包括 **GNM** [14]、**ViNT** [44]、**NoMaD** [45]，三者均为成熟的户外导航模型
- **适配调整**：这些模型原本是为"**图像目标导航（image-goal navigation）**"设计的（即通过参考图像定位目标），实验中改用自身采集数据中的"目标图像"来适配测试
- **额外评估**：**ViNT** 作为视觉导航领域的基础模型，支持**微调（fine-tuning）**，因此实验同时评估了其**零-shot**（无微调）和微调后的性能
- **未纳入对比的模型**：**CoNVOI** [58] 是近期与本研究设置相似的工作，但因缺乏开源代码和测试所需的提示词（**prompt**），无法开展对比
- **补充实验**：还测试了与 **CoNVOI** 类似的、基于 **VLM（视觉-语言模型）**的方法，就只有一个 **GPT-4o**：
  - **导航动作生成**：表现极差，**MAOE** 高达 **85.03°**（**CityWalker** 仅 **11.53°**），无法生成合理的导航动作
  - **到达状态预测**：相对尚可，到达率 **70.04%**（**CityWalker** 为 **87.84%**），仅因"判断是否抵达目标"的逻辑更简单，无需复杂动作决策

#### 实验数据（Data）

实验所用的微调与离线测试数据为**自建专家遥操作数据集**，采集与处理细节如下：

- **采集方式**：通过"**遥操作**"控制机器人移动，手动采集真实城市场景数据
- **采集设备**：使用 **Unitree Go1** 四足机器人，搭载 **Livox Mid-360** 激光雷达（用于定位）和摄像头（用于 **RGB 视觉观测**），同时用智能手机获取位置数据
- **地面真值（ground truth）获取**：采用 **LiDAR-SLAM** 方法 [59] 计算机器人的姿态，作为动作的真实标签
- **降噪处理**：为减少四足机器人运动时的振动对位置数据的干扰，采集时采用"手持手机"的方式 [38]
- **数据规模与分配**：共采集 **15 小时数据**（覆盖纽约市多个区域），其中 **6 小时**用于模型微调，**9 小时**用于离线测试

#### Critical Scenarios

为精准评估模型在城市导航"**高风险、高复杂度**"场景中的性能，基于**地面真值轨迹**和**目标检测** [60] 技术，定义了 **5 类核心场景**（场景不互斥，单个数据样本可同时属于多个场景）：

- **转弯（Turn，占比 8%）**：地面真值动作的方向发生显著变化，判定条件为动作角度 $\varphi_{action} > 20°$
- **路口（Crossing，占比 12%）**：智能体处于道路交叉口，判定条件为检测到交通灯且置信度 $> 0.5$
- **绕行（Detour，占比 12%）**：动作方向与目标方向偏差较大，判定条件为 $|\varphi_{action} - \varphi_{target}| > 45°$
- **近距离行人（Proximity，占比 6%）**：有行人靠近智能体，判定条件为检测到的行人边界框最大占比超过图像面积的 **25%**
- **人群（Crowd，占比 7%）**：智能体被人群包围，判定条件为检测到的行人数 $\geq 5$

![](/paper/citywalker-qualitative-results.png)

这些场景虽仅占总数据的不到一半，但却是决定城市导航能否成功的关键因素，实验会针对每个场景单独评估模型性能。

| Method | Metric | Mean | Turn 8% | Crossing 12% | Detour 12% | Proximity 6% | Crowd 7% | Other 55% | All 100% |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GNM [ 14 ] (fine-tuned) | ￬ L2 (m) | 1.22 | 2.36 | 1.36 | 1.42 | 0.88 | 0.76 | 0.55 | 0.74 |
| ￬ MAOE ( ◦ ) | 16.2 | 31.1 | 14.8 | 12.5 | 14.7 | 12.8 | 11.0 | 12.1 |
| ￪ Arrival (%) | 68.6 | 66.4 | 69.7 | 66.4 | 69.0 | 69.2 | 70.7 | 70.0 |
| ViNT [ 44 ] (fine-tuned) | ￬ L2 (m) | 1.30 | 1.91 | 1.13 | 1.14 | 0.77 | 0.66 | 0.57 | 0.70 |
| ￬ MAOE ( ◦ ) | 16.5 | 31.1 | 15.4 | 12.9 | 14.8 | 13.3 | 11.6 | 12.6 |
| ￪ Arrival (%) | 70.5 | 71.2 | 68.7 | 70.7 | 73.0 | 68.6 | 71.0 | 70.7 |
| NoMaD [ 45 ] (fine-tuned) | ￬ L2 (m) | 1.39 | 2.49 | 1.56 | 1.55 | 1.06 | 0.95 | 0.76 | 0.74 |
| ￬ MAOE ( ◦ ) | 19.1 | 35.1 | 18.5 | 15.6 | 18.1 | 14.3 | 12.8 | 12.1 |
| ￪ Arrival (%) | 68.6 | 66.4 | 69.7 | 66.4 | 69.0 | 69.3 | 70.7 | 70.0 |
| Ours (zero-shot) | ￬ L2 (m) | 1.34 | 1.30 | 1.09 | 1.33 | 1.44 | 1.48 | 1.39 | 1.38 |
| ￬ MAOE ( ◦ ) | 16.5 | 26.8 | 15.5 | 16.3 | 16.3 | 13.1 | 11.3 | 12.7 |
| ￪ Arrival (%) | 79.1 | 69.3 | 71.4 | 78.8 | 84.1 | 84.5 | 86.4 | 84.1 |
| Ours (fine-tuned) | ￬ L2 (m) | 1.11 | 1.27 | 1.00 | 1.15 | 1.06 | 1.12 | 1.06 | 1.07 |
| ￬ MAOE ( ◦ ) | 15.2 | 26.6 | 14.1 | 13.9 | 14.3 | 12.0 | 10.4 | 11.5 |
| ￪ Arrival (%) | 81.8 | 68.9 | 75.3 | 78.5 | 90.6 | 87.5 | 90.2 | 87.8 |

#### 整体基准测试

上表展示了不同关键场景下的基准测试结果。总体而言，我们微调后的模型在所有场景的所有指标上都表现更优，除了"**绕路**"场景。我们认为在绕路场景中表现欠佳是由于我们的视频训练数据中此类数据占比较小。微调在该场景中带来的性能提升最为显著，这一点可以印证上述观点。值得注意的是，我们的**零样本模型**与经过微调的基线模型相比，性能相当甚至更优。这体现了**大规模预训练**的优势，尤其是考虑到与人类行走视频之间存在的**领域和模态差异**。

#### 真实世界测试

| Method | All | Forward | Left turn | Right turn |
| --- | --- | --- | --- | --- |
| **ViNT** * [44] | 37.7 | 62.5 | 0.0 | 50.0 |
| **ViNT** † [44] | 57.1 | 100.0 | 25.0 | 25.0 |
| **NoMaD** * [45] | 42.9 | 75.0 | 16.7 | 28.6 |
| **Ours** † | **77.3** | **100.0** | **62.5** | **66.7** |

上表展示了在所有真实世界测试案例中的成功率。我们的模型在所有案例中都取得了最高的成功率，与表现第二好的微调 **ViNT** 相比有显著差距。由于基线模型主要是在郊区或越野环境的导航数据上训练的 [14,61]，它们在处理复杂操作方面存在局限，因此不适合城市导航任务。我们的模型在直行和转弯时均表现稳健，这凸显了其有效应对动态多变的城市场景的能力，使其能更可靠地应用于需要频繁且精确转向的真实世界环境中。这一显著改进不仅提高了整体导航成功率，还将我们模型的适用性扩展到了更具挑战性和更真实的城市环境中，使其成为动态环境中具身智能体的卓越解决方案。

### 数据规模与性能分析

**数据规模与性能强相关，大规模预训练碾压有限专家微调**

![](/paper/citywalker-data-scale.png)

如上图显示，随着训练视频时长增加，模型**零样本性能**（以 **MAOE** 衡量，值越低越好）持续提升——数据量越大，方向误差越小，导航越精准。

**关键发现**：

- 训练数据超 **1000 小时**时，**CityWalker** 的零样本模型（未用任何专家数据微调）性能，超过了用有限专家数据微调后的 **ViNT** 模型（行业主流基线）
- **网络级海量、多样化视频数据**的价值，可超越"小数据 + 微调"的传统模式，为具身导航提供更强泛化能力

### 特征幻觉损失分析

**零样本推理中的问题**：未使用"**特征幻觉损失**"的模型，性能优于使用该损失的模型。

**原因分析**：存在「**领域 + 具身差距**」——训练数据是"人类行走视频"，而测试对象是"四足机器人"：

- **特征幻觉损失**的目标是让模型"模仿人类运动的未来视觉观测"，但人类行走的视角、运动模式（如步频、转向幅度）与四足机器人差异极大
- 这种"贴合人类运动的未来预测"会给机器人导航带来误导，导致模型学到的表征不适用，最终降低零样本性能

**上述"拖后腿"问题仅存在于零样本场景，微调后完全消失**。

![](/paper/citywalker-feature-hallucination.png)

**微调后的效果**：上图显示，加入**特征幻觉损失**后：

- **2000 小时训练**的总损失更低，且两种数据规模（**1000/2000 小时**）下的方向损失均更小
- 损失曲线下降更陡峭，说明模型学习效率更高，随着训练步数增加，误差还能进一步降低

**结论**：**特征幻觉损失**是有效的辅助训练手段，只是零样本阶段会受"领域/具身差距"影响，微调后可充分发挥作用。

### 跨领域数据训练分析

**数据处理管线支持"驾驶视频"训练**（无需额外适配），实验验证两种数据使用方式的效果：

- **仅用驾驶视频训练**：模型性能与零样本基线（如 **ViNT** 零样本）相当，说明驾驶和步行是"不同领域、不同具身"的导航任务，但核心导航逻辑可迁移
- **混合驾驶 + 步行视频训练**：性能大幅提升——仅 **250 小时混合数据**，性能就接近 **1000 小时纯步行视频**训练的模型

**核心价值**：**跨领域、跨具身的数据**能显著提升模型的泛化性和鲁棒性，用更少数据实现更好效果。

