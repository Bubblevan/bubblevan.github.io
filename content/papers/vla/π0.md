# π 系列论文概述

π 系列是由具身智能初创公司 Physical Intelligence (Pi) 发布的机器人基础模型（VLA）系列论文。这个系列从 2024 年底开始走红，目前的版本迭代包括 $\pi_0$、$\pi_{0.5}$ 和 $\pi_{0.6}$。

## 版本演进

| 版本 | 时间 | 论文标题 | 定位 | 核心创新/特点 |
|------|------|---------|------|--------------|
| $\pi_0$ | 2024年10月 | $\pi_0$: A Vision-Language-Action Flow Model for General Robot Control | 奠基之作 | • 首个能通过单一模型控制多种不同形态机器人（单臂、双臂、移动底座等）的基础模型<br>• 引入**流匹配（Flow Matching）**技术，打破传统 VLA 模型推理慢的瓶颈<br>• 可平滑生成机器人连续动作指令（50Hz） |
| $\pi_0$-FAST | 2025年1月 | FAST: Efficient Robot Action Tokenization | 效率优化版 | • 解决自回归模型在动作生成上的效率问题<br>• 通过新的动作分词器（Tokenizer）提升训练速度 5 倍<br>• 推理延迟更低 |
| $\pi_{0.5}$ | 2025年4月 | $\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization | 开放世界通用版 | • 强调"泛化能力"，不再局限于实验室环境<br>• 可在完全陌生的家庭、卧室、厨房中直接执行任务（如收衣服、铺床）<br>• 引入层次化架构，将任务拆解为"高层语义理解"和"底层动作执行" |
| $\pi^*_{0.6}$ | 2025年11月 | $\pi^*_{0.6}$: a VLA That Learns From Experience | 经验进化版 | • 目前该系列的最新前沿<br>• 核心进步：**"从经验中学习（RECAP 方法）"**<br>• 从单纯模仿人类演示，转变为可通过现实中的"试错"和"反馈"提升成功率<br>• 在洗衣服、组装盒子、做浓缩咖啡等高难度任务中，成功率翻倍 |

## $\pi_0$

![](/paper/π0-overview.png)

其核心思想在于一个**通才模型**，通过这样的**预训练方法**能够解决**泛化性与鲁棒性**的问题。

| 维度 | RT-1 / RT-2 | ACT | $\pi_0$ (Pi-zero) |
|------|------------|-----|------------------|
| **基础骨干** | RT-1 (CNN/Trans) / RT-2 (PaLM-E) | Transformer (通常从头训练) | **预训练 VLM (PaliGemma)** |
| **动作预测** | 离散化 Token（精度受限） | 动作分块 (Action Chunking) | **流匹配 (Flow Matching) 连续控制** |
| **控制频率** | 较低 (通常 1-5Hz) | 较高 (通常用于特定任务) | **极高 (最高 50Hz)** |
| **泛化能力** | 语义泛化强，物理灵巧度弱 | 灵巧度强，语义泛化弱 | **语义与灵巧度平衡** |
| **数据规模** | 主要是 relocation 任务 | 少量、特定任务数据 | **10,000+ 小时，跨机器人数据** |

和前代 VLA 工作不同的是，其 **Backbone 为一个 VLM**（这个据他说是 the first，准确的说是 **VLM + Flow Matching 是 the first**），通过集成其一般知识、语义推理和问题解决能力，加以整合动作使之成为一个 VLA 模型。

训练方法是**跨实体训练**，即将多种机器人类型的数据整合到同一个模型中。

> 但是应该也得有一个标准格式来兼容吧，比如单臂和双臂，移动操作之类的？

以往的模型（如 **RT-2**）通常将动作离散化为类似文字的"Token"，这限制了动作的精度和频率。$\pi_0$ 引入了**流匹配（Flow Matching）**（一种扩散模型的变体），它可以直接在连续空间中建模动作分布。

- **动作分块 (Action Chunking)**: 一次性预测未来 **50 步**的动作（$H=50$），支持高达 **50Hz** 的实时控制。
- **数学表达**: 模型通过最小化条件流匹配损失函数来训练：

$$L(\theta) = \mathbb{E}_{p(A, o)} [\|v_{\theta}(A^{\tau}, o) - u(A^{\tau}|A)\|^2]$$

这对我来说确实是新的东西，因为以往都是类似那种 Transformer Decoder 的标准交叉熵损失，这里的扩散挺麻烦的。

### THE $\pi_0$ MODEL

![](/paper/π0-framework.png)

$\pi_0$ 并没有从头开始设计一个模型，而是基于 **PaliGemma** 这一成熟的 **30 亿参数（3B）** 视觉语言模型进行构建。

为了平衡语义理解与动作执行，$\pi_0$ 在原始 VLM 的基础上增加了一个名为 **"动作专家 (Action Expert)"** 的模块，也就是用到了 **MoE 策略**：

- **VLM 主干**：负责处理图像和文本输入，继承了互联网规模的语义常识和推理能力。
- **动作专家**：拥有约 **3 亿个参数**，专门用于处理机器人**本体感觉状态（Proprioceptive state）**和生成动作（Actions）。

这种设计允许模型在保留强大的泛化知识的同时，针对机器人特定的连续动作输出进行优化，且不会导致预训练权重的剧烈分布偏移。

#### 观察输入与编码

- **观察输入 ($o_t$)**：包含多张 RGB 图像、一段语言指令以及机器人的本体感觉状态 $q_t$（通常是关节角度）。
- **图像编码**：遵循标准的**"晚期融合 (Late Fusion)"**方案，图像被编码并投影到与语言 Token 相同的嵌入空间中。
- **动作分块 (Action Chunking)**：模型不只是预测单步动作，而是预测一个**"动作块"** ($A_t$)，包含未来 **50 步**的连续动作。

#### 训练目标

在训练过程中，模型被要求学习一个**向量场** $v_\theta$，用于将随机噪声平滑地转化为实际动作。其损失函数公式为：

$$L(\theta) = \mathbb{E}_{p(A|o)} [\|v_{\theta}(A^{\tau}, o) - u(A^{\tau}|A)\|^2]$$

其中 $A^{\tau}$ 是不同时间步下的带噪声动作，$u$ 是去噪向量场。

#### 推理过程

- **欧拉积分 (Euler Integration)**：在推理阶段，模型从纯噪声出发，通过 **10 个积分步骤**（Step size $\delta=0.1$）逐步生成动作块。
- **效率优化**：由于图像和指令 Token 在这 10 步内是不变的，模型采用了**缓存机制 (Caching)**，只重复计算动作 Token 部分，大大加快了推理速度。

#### 对比模型：$\pi_0$-small

为了验证 **VLM 预训练的价值**，团队还设计了一个对比模型 $\pi_0$-small：

- **参数量**：约 **4.7 亿（470M）参数**
- **差异点**：它不使用 PaliGemma 初始化，而是从头开始训练，并使用 **DistilBERT** 处理语言 Token
- **作用**：用于对比实验，以证明引入互联网量级的视觉语言知识对提升机器人灵巧度和指令跟随能力的重要性


### DATA COLLECTION AND TRAINING RECIPE

摒弃了传统的"针对特定任务训练"的思路，转而采用了类似 **GPT 的双阶段训练范式**：

#### 预训练（Pre-training）：广度优先

目标是让模型接触尽可能多的任务和行为，建立起对物理世界的**"常识"**。在这个阶段，数据不需要绝对完美，甚至低质量的数据也有助于模型学习如何从错误中恢复。

#### 后训练（Post-training）：深度优先

利用高质量、经过筛选的数据，将预训练获得的通用能力"对齐"到具体的下游任务上（如精准地折叠衣服），使其动作更加流畅、稳健。

#### 跨机器人训练的技术挑战

要在同一个模型里训练 **7 种甚至更多形态迥异的机器人**，最大的难题是**动作空间的不统一**：

- **维度归一化**：团队采取了一种简单但高效的工程手段——**零填充（Zero-padding）**。他们定义了一个最大为 **18 维**的向量（涵盖了双臂、手爪、移动底盘等所有可能的关节），不足该维度的机器人（如 7 维的单臂）直接补零。
- **数据加权平衡**：为了防止某些简单任务或特定机器人（如过剩的折衣服数据）主导模型，他们设计了一个加权公式，按任务样本数 $n$ 的 $n^{0.43}$ 次方进行重采样，确保模型不会在训练中偏科。

#### 数据来源

| 数据来源 | 规模与特性 | 贡献点 |
|---------|-----------|--------|
| **自有数据集** | **9.03 亿个时间步（约 10,000 小时）** | 涵盖 **68 种**极其复杂的任务，如双臂协作、复杂语义识别等 |
| **开源数据集 (OXE, etc.)** | 包含 **22 种机器人** | 提供多样化的环境和物体，增强模型的泛化底色 |
| **语言标签** | 任务名 + **2 秒级的细粒度标注** | 建立了语言指令与物理动作之间的紧密语义链接 |

对于那些持续时间长达**数十分钟**的任务（如"清理这张桌子"），光有低层的动作控制是不够的。

- **语义解构**：团队引入了类似 **SayCan** 的机制。他们利用一个**高层 VLM** 来进行推理，将"收拾桌子"分解成一系列子指令，如"捡起餐巾纸" $\rightarrow$ "把它扔进垃圾桶"。
- **自主决策**：由于 $\pi_0$ 本身就是基于 VLM 构建的，它对这类中间指令的理解力极强，从而实现了**端到端控制与逻辑推理的无缝结合**。

### EXPERIMENTAL EVALUATION

#### 1. 零样本性能对比

研究团队首先测试了预训练后的基座模型，在**不进行任何额外微调**的情况下，直接通过指令执行任务。

- **降维打击对比**：在折 T 恤、收餐具（Bussing）、装杂货袋等任务中，$\pi_0$ 的表现远超 **RT-2** 的继任者 **OpenVLA** 和 **Octo**。
- **计算效率公平性**：为了公平起见，团队训练了一个"计算等价"版本的 $\pi_0$（仅训练 **16 万步**，与基准模型一致），结果这个"未满血"版本依然在所有任务中胜出。
- **架构优势的体现**：OpenVLA 的失败在于其自动回归机制无法处理高频的动作块，而 $\pi_0$ 的**流匹配（Flow Matching）**在处理这种高频灵巧动作时显得游刃有余。

#### 2. VLM 预训练的价值

这一部分实验解答了一个关键疑问：**互联网规模的 VLM 预训练对机器人到底有多大帮助？**

- **VLM 的价值**：对比从零训练且没有 VLM 背景的 $\pi_0$-small，拥有 **PaliGemma 基因**的 $\pi_0$ 在理解复杂语言指令方面展现了显著的跨越。
- **长程任务的衔接**：通过引入**高层（High-level）VLM 策略**来分解指令，$\pi_0$ 能够自主完成一系列连续动作（如：捡起餐巾纸 $\rightarrow$ 扔进垃圾桶），其指令跟随的准确率显著提升。

#### 3. 少样本学习能力

研究者总是希望模型能用最少的数据学会最多的技能。$\pi_0$ 在学习"微波炉里放餐盒"或"更换卷纸"等全新任务时表现惊人。

- **少样本微调**：实验证明，即便只有 **1 小时**的微调数据，$\pi_0$ 在新任务上的成功率往往能达到那些"从零训练"模型在拥有 **10 小时数据**时的水平。
- **正向迁移**：由于预训练中已经见过了类似的"抓取"和"放置"动作，模型在面对新物体时表现出了**极强的迁移能力**，而不是死记硬背。

#### 4. 复杂任务与鲁棒性

$\pi_0$ 处理那些"**人类都觉得麻烦**"的任务的能力。

- **端到端挑战**：任务包括：从烘干机取出衣物并折叠、组装纸质包装盒、甚至是从碗里抓取光滑的鸡蛋并整齐地放进蛋托。
- **鲁棒性的质变**：与以往那些"一碰就碎"的演示模型不同，$\pi_0$ 展现出了**惊人的容错率**。如果折衣服时手滑了，它能利用预训练中积累的"恢复行为"常识，自主尝试重新抓取，而不是陷入死循环。
- **时长记录**：团队成功实现了长达 **20 分钟**的连续灵巧操作，这在端到端机器人学习领域是**前所未有的**。

[Doubao对照](https://www.doubao.com/chat/34185824676435714)