# InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment

**论文链接**：https://sites.google.com/view/instructnav

## 核心思想

首先启发性的一点是：**different types of instructions emphasize different navigation strategies**。

比如说：
- **Object Goal Navigation** approaches primarily concentrate on performing efficient exploration to find the target object in unseen environments，即在未知环境中定位目标物体
- **Visual Language Navigation** methods focus on following step-by-step instruction
- 而**Demand-driven Navigation** works are geared towards conducting demand-based commonsense reasoning（感觉和前面VLN的方法在通过LLM Planner进行趋同）

这篇文章想把这些侧重不同任务的策略合在一起，结果就是**动态导航链（DCoN）**。

## 问题建模

### 1. 输入 (Input)

输入主要由**静态指令**和**动态感知**两部分组成：

- **自然语言指令 (I)**：这是机器人的"任务说明书"。例如："直走穿过客厅，在蓝色的沙发边左转，最后进入厨房停在冰箱前。"
- **当前多模态观测 (O_t)**：由于机器人身处陌生环境（未探索），它每一时刻只能看到自己面前的东西。
  - **RGB 图像 (V_t)**：机器人的"眼睛"看到的彩色画面，用于识别物体（沙发、门、地毯）
  - **深度图像 (D_t)**：提供距离信息。深度图让机器人知道前方障碍物有多远，哪些地方是可以通过的空地
  - **相机位姿 (P_t)**：机器人的"小脑"。它记录了当前机器人在三维空间中的坐标 $(x, y, z)$ 以及它的旋转角度（朝向）。这有助于它在没有地图的情况下，知道自己走过的相对路径

### 2. 输出 (Output)

输出是机器人根据输入信息做出的"决策"。

**底层动作 (a_t)**：在每一个时间步 $t$（即每一秒或每一帧），算法需要计算并输出一个具体的控制指令。常见的动作集合包括：

- **前进一步** (Move Forward)
- **左转/右转** (Turn Left/Right)
- **视角上仰/下俯** (Look Up/Down)
- **停止** (Stop)：当机器人认为自己已经抵达目标点时，必须发出停止信号才算任务完成

> **约束**：不允许提供预构建地图，不然就是SLAM了。

![InstructNav工作流程](/paper/instructnav-workflow.png)

## 动态导航链（Dynamic Chain-of-Navigation, DCoN）

其 **3.2 动态导航链（Dynamic Chain-of-Navigation, DCoN）**主要解决的是**语义模糊性**和**环境未知性**之间的对齐问题，比如说：

- **语义标签对齐**：指令里说"拱形木门"，但语义分割模型可能只识别为"门"。DCoN 利用 **LLM** 的推理能力将两者挂钩
- **常识性探索**：当目标不可见时，**LLM** 会根据常识（如：沙发通常在电视对面）规划中间临时目标（**Landmarks**），而不是盲目乱撞
- **处理抽象需求**：面对"我渴了"这种非导航指令，**LLM** 需要将其转化为"寻找水瓶"的导航动作

但其实还是一个比较常规的 Idea，就是离散环境 Navigation 里，简单地每到一个点调用一次 **LLM**。

![InstructNav框架](/paper/instructnav-framework.png)

## 多源价值地图（Multi-source Value Map）

**3.3 多源价值地图**才是真正的创新点，通过四种价值地图的融合，将 **LLM** 生成的自然语言（**Linguistic Planning**）转化为连续环境中的可执行轨迹。

简单来说，系统把机器人的周围空间看作一张网格图，并为每个格子计算一个分数。分数越高的格子，机器人就越倾向于往那里走。

$$m = m_i + m_a + m_t + m_s$$

在每一时刻，机器人会找到图中分值最高点 $(x, y, z)$ 作为临时目标点（**Waypoint**），然后利用 **A\*** 算法规划出一条避开障碍物的平滑路径。

### 语义价值地图 ($m_s$) —— "目标在那儿"

这是最直观的一张图。

- **原理**：系统利用语义分割模型（如 **GLEE**）识别彩色图像中的物体，并结合深度图将其投影到 3D 空间
- **计算**：如果 **DCoN** 规划的当前目标是"沙发"，那么地图上被识别为"沙发"的区域分值最高
- **公式逻辑**：计算导航区域内每个点到目标物体点云的最小距离，距离越近，分值 $C_{sem}$ 越接近 1

### 动作价值地图 ($m_a$) —— "执行动作指令"

当指令包含方向（如"左转"、"前进"）时，这张图起作用。

- **前进/左转/右转**：会在机器人当前位置对应的方向扇区（如左侧 90° 区域）填充高分（值为 1）
- **探索 (Explore)**：如果目标还没找到，这张图会给环境的"边界（**Frontiers**）"打高分，引导机器人往没去过的地方走

### 轨迹价值地图 ($m_t$) —— "别在原地打转"

为了防止机器人陷入死循环或在同一个房间反复徘徊。

- **原理**：系统会记录机器人走过的所有位置点云 $PCD_{traj}$
- **计算**：离历史轨迹越远的地方，分值 $C_{traj}$ 越高。这会产生一种"排斥力"，强迫机器人去探索新区域

### 直觉价值地图 ($m_i$) —— "视觉常识"

这是利用了**多模态大模型（MLM，如 GPT-4V）**的视觉直觉。

- **操作**：机器人原地拍摄一组环视照片（全景图），发给 **GPT-4V**
- **决策**：**LLM** 结合当前指令和照片，判断"哪张照片的方向看起来更有可能通往目标"
- **结果**：被选中的方向区域会被赋予高分。这能弥补语义地图无法处理"走在...之间"或"穿过..."这类复杂空间逻辑的缺陷

## 价值地图协同工作示例

- $m_t$ 告诉它："你刚才在卧室转过了，别回去了。"
- $m_a$ 告诉它："指令说直走，前面的分给你打高点。"
- $m_i$ 告诉它："凭照片看，左前方那个门洞看起来很像厨房入口。"
- $m_s$ 告诉它："如果你已经看到了冰箱，那冰箱所在的格子就是满分。"
## 实验结果

**InstructNav** 的仿真实验主要在 **Habitat** 和 **AI2-THOR** 两个主流仿真器中完成。它最核心的卖点在于：它是第一个在多种任务上实现**Zero-shot（零样本/无需导航训练）**且性能超过许多训练过（**Task-trained**）模型的方法。

### 任务类型与数据集

| 任务类型 | 数据集 | 核心挑战 |
|---------|--------|---------|
| ObjectNav (目标物导航) | HM3D | 在从未见过的超大场景中寻找特定物体（如：床、马桶） |
| VLN (视觉语言导航) | R2R-CE | 必须严格遵循长串的分步指令（如：直走，看到画左转，去浴缸边） |
| DDN (需求驱动导航) | DDN 数据集 | 理解抽象需求（如："我口渴了"），需要常识推理 |

### 性能对比

作者将 **InstructNav** 与各种经过大量数据训练的模型进行了对比：

- **在 ObjectNav 上**：其成功率（**SR**）达到了 **58.0%**，不仅远超其他 Zero-shot 方法（如 **ZSON** 的 25.5%），甚至超过了许多需要训练的模型
- **在 VLN 上**：这是该模型最亮眼的地方。它是第一个在 **R2R-CE** 任务中实现零样本导航的模型（成功率 **31%**），且击败了早期的一些有监督训练模型（如 **Sasra** 的 24%）
- **在 DDN 上**：比之前的 **SOTA（最先进）**模型性能提升了 **86.34%**

### 消融实验

| 实验组 | 成功率影响 | 结论 |
|--------|-----------|------|
| 完整版 InstructNav | 最高 (Baseline) | 协同效果最好 |
| 去掉 DCoN (3.2) | 大幅下降 | 说明"动态规划"非常重要，没有它，机器人无法将指令转化成目标 |
| 去掉动作价值地图 ($m_a$) | 下降 | 机器人会失去对"左转/右转"这类具体动作指令的遵循能力 |
| 去掉直觉价值地图 ($m_i$) | 下降 | 说明 **GPT-4V** 的"视觉直觉"在复杂推理中不可或缺 |

### 直觉价值地图的环视照片数量优化

在 **3.3.4** 节提到的**直觉价值地图 ($m_i$)**中，机器人需要拍几张照片给 **GPT-4V** 看？

作者测试了 $N=4, 6, 12$（即环视拍 4、6、12 张照片），结果：**$N=6$ 效果最好**。

**原理解释**：
- $N=4$ 会丢失视野细节（看得不够细）
- $N=12$ 虽然看得很全，但图片太多会给大模型带来"理解负担"，反而容易判断错误