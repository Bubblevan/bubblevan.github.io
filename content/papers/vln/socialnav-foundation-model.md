---
title: "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation"
---

> 回过头来看，给我的最大启发是就算凑也要凑一个新奇的名字——Socialnav太烂大街了

遵循**社会规范**的**具身导航**仍是一个尚未解决的研究挑战。我们的 **SocialNav** 是一个具有分层"**大脑-行动**"架构的**社交感知导航基础模型**，能够理解高级**社会规范**并生成低级、符合社交规范的**轨迹**。为了实现这种双重能力，我们构建了 **SocNav 数据集**，这是一个包含**700 万个样本**的大规模集合，包括（1）**认知激活数据集（CAD）**，提供**社交推理信号**，如**思维链解释**和**社交可通行性预测**；（2）**专家轨迹金字塔（ETP）**，汇集了来自互联网视频、模拟环境和现实世界机器人的各种导航演示。我们提出了一个**多阶段训练管道**，以逐步注入和完善导航智能：首先通过**模仿学习**向模型注入一般导航技能和**社会规范理解能力**，然后通过精心设计的**社交感知流探索 GRPO（SAFE-GRPO）**来完善这些技能，这是首个基于流的**具身导航强化学习框架**，明确奖励符合社交规范的行为。与最先进的方法相比，**SocialNav** 的**成功率**提高了 **38%**，**社交合规率**提高了 **46%**，在导航性能和社交合规性方面都表现出显著提升。

## Related Work

众所周知，让智能体的行为与**社会规范**保持一致，超出了标准**模仿学习**的表征和推理能力。即使**社会先验**被隐含地嵌入到示范数据中，**行为克隆**也无法捕捉到规范行为背后的**因果结构**。为了解决这一局限性，我们提出了**社交感知流探索 GRPO（SAFE-GRPO）**，这是首个基于流的**强化学习框架**，用于具身导航，它通过**感知规范的奖励机制**明确促进符合社会规范的行为。这种方法使智能体能够内化支配**社会惯例**的基本原则，而不仅仅是模仿表面行为。

> BYD 和隔壁[Citywalker](../navigation/citywalker)用BC/IL的左右脑互搏

### Social Navigation and Human Preference Alignment

**社交导航**需要遵循**社会规范**，这超越了早期的手工构建成本 [7, 31]，迈向了基于**视觉语言模型（VLM）**的推理 [19, 25, 45]。然而，**VLM 推理**往往与低级别的**动作生成**脱节。最近，**流匹配（FM）** [21] 因其能够对**多模态动作分布**进行建模，已被广泛应用于**视觉-语言-动作（VLA）模型** [15, 50] 中，但这些模型通常局限于**行为克隆**。在导航领域，纯粹的**模仿学习**往往缺乏在面对新颖或动态的社交场景时进行稳健适应所需的**因果理解**。因此，我们迫切需要一种不仅能生成动作，还能与复杂的**人类偏好**和**社会规范**保持一致的模型。诸如 **GRPO** [10, 37] 和 **Flow-GRPO** [23] 等基础性研究，展示了将**生成模型**与**在线强化学习**相结合以实现**人类偏好对齐**的方法，这为我们的研究思路提供了启发。

## SocNav Dataset and Benchmark
![](/paper/socialnav-foundation-model-dataset.png)


### SocNav Dataset（700 万样本，分 ETP 与 CAD）

| 组件 | 子类型 | 样本量 | 核心内容与作用 |
| --- | --- | --- | --- |
| **ETP（专家轨迹金字塔）** | **D_video（基础层）** | **200 万** | 互联网城市探索视频→**3D 重建（π³）**→**尺度对齐（MoGe）**→轨迹合成，提供多样真实场景动作先验 |
| | **D_sim（中间层）** | **170 万** | **4490 个高保真场景**（含 **3400 个真实重建场景 SocialGS**、**3.37km² 动态城市 SocCity**），含标准/恢复轨迹，增强鲁棒性 |
| | **D_real（高层）** | **34 万** | 真实机器人数据（**SCAND**、**Huron** 等），提供 **metric 精度**与**物理真实性**，用于微调缩小**仿真-真实差距** |
| **CAD（认知激活数据集）** | **社交可通行性识别** | **120 万** | 手动标注第一视角图像中的合规区域，输出**语义标签**与**多边形坐标**，训练区域识别能力 |
| | **导航 CoT** | **82.5 万** | **Qwen2.5VL-72B** 生成逐步推理文本，训练**导航逻辑推理能力** |
| | **通用 VQA** | **100 万** | 公开数据集（如 **COCO** 衍生），训练**空间关系**/物体属性推理，维持**通用世界知识** |

#### 来自互联网视频的轨迹（$D_{video}$）

$D_{video}$ 作为基础层，包含 **200 万条伪轨迹**，这些轨迹是从大量公开的城市探索视频中精心筛选出来的，以确保涵盖全球城市、天气条件和建筑风格的丰富视觉多样性。这些原始视频流通过一个可扩展的流程被转化为伪轨迹，该流程包括：

1. 使用 **$\pi^3$** 进行密集 **3D 重建** [47]
2. 通过 **MoGe** 进行**度量尺度对齐** [44]
3. 通过沿重建路径对不同的点目标片段进行采样来合成伪轨迹

#### 来自高保真模拟场景的轨迹（$D_{sim}$）

该中间数据层提供了 **170 万条**多样化轨迹。它包含超过 **4490 个高保真 3D 场景**和一个大规模动态城市。我们对这一层的主要贡献包括：

1. **SocialGS**：一个包含 **3400 个真实世界场景**（**4.5 km²**）的新数据集，通过 **3DGS** 重建。它超越了现有的住宅数据集（如 **Matterport3D** [30]、**InteriorGS** [39]），覆盖了购物中心、街道和办公室等社交环境
2. **SocCity**：一个 **3.37 km²** 的动态城市场景，在 **Isaac Sim** 中模拟了车辆和行人。在所有场景中，轨迹都在手动标注的可通行道路网络上生成。这些轨迹不仅包括标准的道路导航，还包括具有挑战性的恢复场景，如近碰撞。这个丰富的数据集使模型能够学习高效的导航和鲁棒的恢复行为

#### 来自真实世界机器人数据的轨迹（$D_{real}$）

该层提供了 **34 万条**高质量轨迹，这些轨迹来自部署在真实环境中的自主机器人（来自公共数据集，包括 **SCAND** [16]、**Huron** [13]、**Recon** [33] 和 **CityWalker** 远程操作数据 [24]）。这些轨迹具有真实的**度量准确性**、**物理真实性**和**传感器一致性**，能够捕捉真实的物理动态、传感器噪声和环境交互。它们非常适合**监督微调（SFT）**和缩小**仿真到现实的差距**。

#### 认知激活数据集（CAD）

为了培养超越**几何路径规划**的高级**空间认知能力**，我们引入了**认知激活数据集（$D_{cog}$）**。它整合了一系列非轨迹辅助任务，旨在灌输对环境规则和以人为中心的规范的语义基础理解。

- **社交可通行性识别**：我们通过在来自互联网视频的第一人称图像中手动标注**社交可通行区域**（如人行道、人行横道、公园小径）的多边形，创建了一个包含 **120 万个样本**的数据集。这直接用于训练模型区分物理上和社交上可接受的路径
- **Navigation Chain-of-Thought (CoT)**：利用第一人称图像和精选的提示模板，我们使用 **Qwen2.5VL-72B** [1] 生成了 **82.5 万个 Chain-of-Thought (CoT)** 样本。这些样本包含导航决策的逐步文本推理，旨在教授智能体显式推理
- **通用视觉问答（VQA）**：为确保模型具备**通用世界知识**，我们从 [5, 6, 11, 20, 22, 36] 中精选了 **100 万个通用 VQA** 样本。这项任务训练智能体对环境中的**空间关系**和**物体属性**进行推理


### SocNav Benchmark（高保真评估平台）

通过将Isaac Sim的物理模拟与3DGS的照片级真实感渲染相结合，实现了独特的真实感融合。该基准建立在我们使用3DGS捕捉和重建的9个新的大规模社交场景之上，总面积为\(73K m^{2}\)。这些新场景包括多样化的、以人为中心的环境：三个公园、三条街道级道路、两个办公室和一个校园。所有评估都采用标准化设置：统一的Unitree Go2机器人模型和一致的运动策略。为了实现逼真的物理交互，每个3DGS场景都被转换为网格并导入Isaac Sim以进行精确的碰撞反馈。此外，为了模拟潜在的动态碰撞，数字人被随机引入场景中。为了进行严格的基准测试，我们在每个场景内距离为20米和100米处各采样10对起点-目标点，每个场景形成20个评估案例。

## SocialNav Foundation Model

### Problem Definition

我们将基础导航任务表述为一个**基于视觉的、历史条件点目标导航问题**，在多样化环境中，类似于 **CityWalker** [24] 的设置。在每个时间步 $t$，智能体接收：

- **最近的单目视觉观测序列**：$O_{t-n:t} = \{o_{t-n}, \ldots, o_t\}$，其中 $o \in \mathbb{R}^{H \times W \times 3}$
- **关联的 2D 位置信息**：$P_{t-n:t} = \{p_{t-n}, \ldots, p_t\}$，其中 $p \in \mathbb{R}^2$
- **指定的 2D 目标位置**：$g \in \mathbb{R}^2$

目标是学习一个策略 $\pi_\theta$，将历史观测和位置映射到未来动作序列：

$$A_{t+1:t+m} = \pi_\theta(O_{t-n:t}, P_{t-n:t}, g)$$

除非另有说明，我们设置 $n = 5$ 和 $m = 5$。

![](/paper/socialnav-foundation-model-architect-pipeline.png)
### Brain Module（认知核心）

**基础模型**：基于 **VLM（Qwen2.5-VL-3B）**，承担高层**语义推理**功能。

**核心输出**（3 类可解释结果，为轨迹生成提供语义先验）：

- **社交可通行区域**：以**多边形坐标**表示（如人行道、斑马线、公园步道）
- **导航 CoT（Chain-of-Thought）**：逐步文本推理（如"当前需走斑马线，因车道有车辆通行"）
- **通用 VQA**：回答环境问题（如"前方是否为无障碍通道"），增强**场景理解**

### Action Expert（轨迹生成模块）

**技术基础**：基于**条件流匹配（Conditional Flow Matching）**，使用 **Diffusion Transformer** 架构实现，建模动作分布。

**输入依赖**：接收 **Brain Module** 输出的 **latent 语义特征**（$Z_{VLM}$）、历史视觉观测（$O_{t-n:t}$）、历史位置（$P_{t-n:t}$）与目标（$g$）。

**Action Expert** 专门用于端到端轨迹生成。受动作预测最新进展 [2, 15] 的启发，我们利用**条件流匹配** [21] 来建模动作分布（实现上采用 **Diffusion Transformer** 架构，推理时通过去噪过程生成轨迹）。该模块以从 **VLM** 最后一层特征提取的 **latent 语义特征**为条件：

$$A_{t+1:t+m} = \pi_{flow}(x_t, t; Z_{VLM})$$

其中：

$$Z_{VLM} = \pi_{VLM}(O_{t-n:t}, P_{t-n:t}, g)$$

实现"**高层推理 - 低层控制**"解耦但语义强关联，确保**轨迹合规性**。

### Training Pipeline

#### 阶段 1：基础训练（从零训练 VLM + Action Expert）

在第一阶段，我们旨在激活**视觉语言模型（VLM）**的导航能力，并训练**流模型**来预测低级航点。这是通过在 **ETP 数据集**（$D_{video}$ 和 $D_{sim}$）以及**认知激活数据集** $D_{cog}$ 上进行基础训练来实现的。

- **$D_{video}$**：提供了具有隐含专家行为的多样化现实世界导航场景
- **$D_{sim}$**：引入了具有挑战性的合成案例，以增强在罕见和复杂情况下的鲁棒性
- **$D_{cog}$**：通过**思维链（CoT）**和**视觉问答（VQA）**任务进一步提升了 **VLM** 的推理和决策能力，并使其具备预测可通行区域的能力，为后续的**社会规范对齐**奠定了坚实基础

#### 阶段 2：使用高质量真实世界数据进行微调

在第二阶段，我们使用从真实世界机器人收集的高质量专家轨迹（$D_{real}$）对模型进行微调，以缩小**仿真到现实的差距**。在此阶段，**VLM** 被冻结，仅优化**动作专家**。这种方法保留了 **VLM** 的语义和社会推理能力，同时允许**流模型**适应现实世界的动态和空间尺度。

#### 阶段 3：用于社会规则对齐的强化学习

尽管前几个阶段为模型配备了强大的导航先验知识和现实世界适应性，但**模仿学习**在社会环境中仍然缺乏**因果推理能力**。因此，我们引入了**SAFE-GRPO（社交感知流探索 GRPO）**，这是一个**强化学习阶段**，能将策略与**人类社会习俗**明确对齐。该模型使用来自 **SocCity** 的专家轨迹在 $D_{sim}$ 内进行训练，这提供了准确且丰富的路径注释，对精确的奖励反馈至关重要。

##### （1）核心改造：ODE→SDE，实现"可控探索"

- **原流策略**：确定性 **ODE（常微分方程）**——轨迹固定，无探索空间，无法适应新社交场景
- **改为**：随机 **SDE（随机微分方程）**：

$$dx_t = v_{flow}(x_t, t; Z_{VLM}) dt + \sigma_t dw_t$$

其中：
- $v_{flow}$：流策略的**速度场**（由当前状态 $x_t$、时间 $t$、**VLM** 语义特征 $Z_{VLM}$ 决定）
- $\sigma_t$：**探索幅度控制器**（控制随机探索的"大胆程度"）
- $dw_t$：**随机噪声**（引入探索性）

##### （2）关键优势：语义接地的"有方向探索"

- **传统 RL**：无结构随机探索（瞎逛找奖励）
- **SAFE-GRPO**：有约束的探索
  - 随机噪声仅在"流积分阶段"加入
  - **VLM（Brain Module）**提供的语义约束 $Z_{VLM}$ 始终固定（比如"人行道是合规区域""草坪不可走"）
  - 探索始终围绕"**VLM 定义的合规范围**"，避免传统 RL 因不懂场景语义，陷入"低效探索"（比如反复踩草坪却找不到奖励）

##### （3）奖励函数：平衡"社交合规"与"导航效率"

**总奖励公式**：

$$\mathcal{R} = \mathcal{R}_{social} + \lambda_{expert} \mathcal{R}_{expert} + \lambda_{smooth} \mathcal{R}_{smooth} + \lambda_{eff} \mathcal{R}_{eff}$$

| 奖励项 | 核心作用 |
| --- | --- |
| **$\mathcal{R}_{social}$（主奖励）** | 基于**语义占据图** $M_{occ}$，奖励"远离非可通行区域"（如草坪、车道），确保**社交合规**与安全 |
| **$\mathcal{R}_{expert}$** | 奖励与**专家轨迹**的一致性，避免探索过度偏离最优路径 |
| **$\mathcal{R}_{smooth}$** | 奖励**轨迹平滑性**（如避免突然变向），符合人类行走习惯 |
| **$\mathcal{R}_{eff}$** | 奖励向目标的高效推进，避免为合规而过度绕路（平衡合规与效率） |

**核心逻辑**：**社交合规优先**（$\mathcal{R}_{social}$ 为主），同时兼顾"走得对、走得顺、走得快"，让模型行为既合规又实用。

**总结**：**Stage 3** 通过"有语义约束的流基 **RL（SAFE-GRPO）**"，让模型在 **VLM** 的"规则指引"下主动探索，再用多维度奖励强化"合规 + 高效"的行为，最终实现"懂规则、会导航"的**社交感知能力**。

## Experiment

为了公平检验模型在不同场景的性能，实验分 **3 类设置**：

1. **开环评估**：用 **CityWalker** 提出的公开基准，只测"**轨迹预测准确性**"（不涉及实时交互）
2. **闭环评估**：用自研的 **SocNav Benchmark**，测"**真实导航能力** + **社交合规性**"（有实时环境反馈、动态干扰）
3. **真实世界部署**：用 **Unitree Go2** 机器人在街道、办公园区、购物中心实测，验证"**仿真到真实的落地能力**"

### Metrics

#### 开环指标

- **MAOE（最大平均方向误差）**：衡量预测轨迹与人类真实轨迹的方向偏差，越小越准

#### 闭环/真实世界指标

**导航性能**：

闭环性能通过**成功率（SR）**、**路线完成率（RC）**以及**路径长度加权成功率（SPL）**来衡量：

**SR（成功率）**：

$$SR = \begin{cases}
1, & \text{if } d_{goal} \leq 3 \text{ m and } n_{collision} < 3 \\
0, & \text{otherwise}
\end{cases}$$

其中：
- $d_{goal}$ 表示到达目标时的最终距离（米）
- $n_{collision}$ 表示碰撞次数

**RC（路线完成率）**：

$$RC = \frac{d_{actual}}{d_{optimal}}$$

其中：
- $d_{actual}$ 表示实际行进的距离
- $d_{optimal}$ 表示从起点到目标的最优路径距离

**SPL（路径长度加权成功率）**：

$$SPL = \frac{1}{N} \sum_{i=1}^{N} S_i \cdot \frac{l_i^*}{\max(l_i, l_i^*)}$$

其中：
- $N$ 表示任务总数
- $S_i$ 表示第 $i$ 个任务的成功指标（成功为 1，失败为 0）
- $l_i$ 表示第 $i$ 个任务的实际路径长度
- $l_i^*$ 表示第 $i$ 个任务的最优路径长度

**核心逻辑**：**SPL** 兼顾"能不能到"和"走得效率"，既考虑任务是否成功，也考虑路径是否接近最优。

**社交合规性**：

为了评估**社交合规性**，我们引入了**距离合规率（DCR）**和**时间合规率（TCR）**：

**DCR（距离合规率）**：

$$DCR = \begin{cases}
\frac{d_{compliant}}{d_{actual}}, & \text{if } s = 1 \\
0, & \text{otherwise}
\end{cases}$$

其中：
- $s$ 是一个二进制指标（成功为 1，失败为 0）
- $d_{compliant}$ 表示在**社交合规区域**内行进的距离
- $d_{actual}$ 代表实际行进的总距离

**TCR（时间合规率）**：

$$TCR = \begin{cases}
\frac{t_{compliant}}{t_{actual}}, & \text{if } s = 1 \\
0, & \text{otherwise}
\end{cases}$$

其中：
- $s$ 是一个二进制指标（成功为 1，失败为 0）
- $t_{compliant}$ 表示在**社交合规区域**内行进的时间
- $t_{actual}$ 代表实际行进的总时间

**核心逻辑**：只有在任务成功（$s = 1$）时，才计算合规率；任务失败时，合规率为 0，强调**合规性与导航成功**的紧密关联。

#### 模型结构

- **Brain Module**：**Qwen2.5-VL**（**3B 参数**，轻量 **VLM**）
- **Action Expert**：**Diffusion Transformer**（**12 层**、**12 个注意力头**、隐藏维度 **1536**，推理时轨迹去噪 **5 步**）

#### 训练配置（硬件 + 超参数）

- **基础训练（阶段1）**：**96 张 H20 GPU**，**batch size=192**，学习率 = **5e-5**，**AdamW** 优化，**3 个 epoch**
- **微调**：**32 张 H20 GPU**，仅优化 **Action Expert**，**batch size=256**，学习率 = **1e-5**
- **SAFE-GRPO**：**16 张 H20 GPU**，仅优化 **Action Expert**，**rollout batch size=128**，学习率 = **5e-7**

### 开环测试

| Method | Mean | Turn 8% | Crossing 12% | Detour 12% | Proximity 6% | Crowd 7% | Other 55% | All 100% |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **GNM** [34] | 16.2 | 31.1 | 14.8 | 12.5 | 14.7 | 12.8 | 11.0 | 12.1 |
| **ViNT** [35] | 16.5 | 31.1 | 15.4 | 12.9 | 14.8 | 13.3 | 11.6 | 12.6 |
| **NoMaD** [40] | 19.1 | 35.1 | 18.5 | 15.6 | 18.1 | 14.3 | 12.8 | 12.1 |
| **CityWalker** [24] | 15.2 | 26.6 | 14.1 | 13.9 | 14.3 | 12.0 | 10.4 | 11.5 |
| **SocialNav (Full)** | **10.2** | **20.1** | **8.8** | **8.4** | **8.9** | **7.6** | **7.2** | **7.8** |

所有指标都是当前最优（**SOTA**），比第二好的 **CityWalker** 强太多——**成功率（SR）**多 **38.3 个百分点**、**路线完成率（RC）**多 **26.5 个百分点**、**路径效率加权成功率（SPL）**多 **32.7 个百分点**，相当于"又快又准到目标"。

### 闭环测试

| Methods | Navigation Performance | | | Social Compliance | |
| --- | --- | --- | --- | --- | --- |
| | **SR** ↑ | **RC** ↑ | **SPL** ↑ | **DCR** ↑ | **TCR** ↑ |
| **GNM*** [34] | 43.3 | 62.4 | 37.0 | 26.5 | 28.7 |
| **ViNT*** [35] | 45.6 | 66.2 | 39.5 | 31.4 | 33.8 |
| **NoMaD*** [40] | 41.1 | 60.5 | 35.4 | 29.5 | 31.6 |
| **CityWalker** [24] | 47.8 | 64.7 | 44.7 | 36.1 | 36.6 |
| **SocialNav*** | 65.0 | 78.4 | 62.3 | 58.0 | 56.7 |
| **SocialNav (Full)** | **86.1** | **91.2** | **77.4** | **82.5** | **82.9** |

**关键发现**：

- **距离合规率（DCR=82.5）**、**时间合规率（TCR=82.9）**，都是 **CityWalker**（36.1/36.6）的**两倍多**，其他基线更差
- 从下图能直观看到：**SocialNav** 乖乖走人行道、指定步道，而基线会抄近路穿草坪、车道，甚至撞玻璃墙/树木

![](/paper/socialnav-foundation-model-qualitative-results.png)

- 把 **SocialNav***（只在真实机器人数据 $D_{real}$ 上用**模仿学习**训练）和同样条件的基线（**NoMaD*/GNM*/ViNT***）比，**SocialNav*** 还是显著更优——这说明"**脑-动作**"分层架构，本身就有更强的泛化能力，不是靠多数据"堆出来"的

### 真实世界

| Method | Street Crossing | Office Park | Shopping Mall | Average SR |
| --- | --- | --- | --- | --- |
| **GNM*** [34] | 9/20 | 10/20 | 8/20 | 45.0 |
| **ViNT*** [35] | 7/20 | 12/20 | 8/20 | 45.0 |
| **NoMaD*** [40] | 9/20 | 11/20 | 10/20 | 50.0 |
| **CityWalker** [24] | 12/20 | 13/20 | 12/20 | 62.5 |
| **SocialNav (Full)** | **18/20** | **16/20** | **17/20** | **85.0** |

用 **NVIDIA A10 GPU** 的云服务器，控制 **Unitree Go2** 机器人在 **3 个真实环境**（过街、办公园区、商场）测试，每个环境 **20 次**，所有方法起点/终点完全一致：

- **平均成功率 85%**，远超 **NoMaD**（50%）、**CityWalker**（62.5%）
- 最难的"**过街场景**" **18/20** 成功，说明在有人流、车流的**社交约束场景**下，表现依然稳健
- **实时性达标**：控制频率超 **5Hz**，能满足真实机器人导航的"即时响应"需求

### 消融实验

| No. | Model | D real | D video | D sim | D cog | IL | RL | **SR** ↑ | **RC** ↑ | **SPL** ↑ | **DCR** ↑ | **TCR** ↑ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | **SocialNav*** | ✓ | | | | ✓ | | 65.0 | 78.4 | 62.3 | 58.0 | 56.7 |
| 2 | + **D video** | ✓ | ✓ | | | ✓ | | 76.7 | 84.8 | 70.1 | 62.9 | 64.6 |
| 3 | + **D sim** | ✓ | ✓ | ✓ | | ✓ | | 82.2 | 86.0 | 77.8 | 69.8 | 68.2 |
| 4 | + **D cog** | ✓ | ✓ | ✓ | ✓ | ✓ | | 84.4 | 88.1 | 79.4 | 78.2 | 78.4 |
| 5 | + **RL** (no **D cog**) | ✓ | ✓ | ✓ | | ✓ | ✓ | 80.0 | 89.1 | 78.9 | 68.1 | 66.9 |
| 6 | + **RL** (full data) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | **86.1** | **91.2** | **77.4** | **82.5** | **82.9** |

实验通过"从少到多叠加数据"（仅用真实数据→+ 互联网视频数据→+ 仿真数据→+ 认知数据），看每个数据组件的作用。

#### 加 D_video（互联网视频轨迹）：补"场景多样性"

对比 **No.2**（有 $D_{video}$）和 **No.1**（无 $D_{video}$）：导航性能明显提升（成功率 **+11.7 个百分点**、路线完成率 **+6.4**、路径效率 **+7.8**），社交合规也变好（**DCR+4.9**、**TCR+7.9**）。

**核心原因**：互联网视频覆盖全球不同城市、天气、建筑，给模型学了各种人类走路的模式，场景适应性更强。

#### 加 D_sim（仿真轨迹）：补"复杂场景鲁棒性"

对比 **No.3**（有 $D_{sim}$）和 **No.2**（无 $D_{sim}$）：导航性能再升级（成功率 **+5.5**、路径效率 **+7.7**），合规性小幅提升。

**核心原因**：仿真数据里有"近碰撞后恢复""起步跑偏修正"这类稀有场景的轨迹，模型学会应对突发情况，不容易"慌神"。

#### 加 D_cog（认知数据）：补"社交规则理解"，合规性暴涨

对比 **No.4**（有 $D_{cog}$）和 **No.3**（无 $D_{cog}$）：社交合规率大幅跳升（**DCR+8.4**、**TCR+10.2**），导航指标也跟着涨。

**核心结论**：$D_{cog}$ 是"教模型懂规则"的关键——通过**可通行区域标注**、**CoT 推理**，模型不再是"模仿表面轨迹"，而是真正理解"草坪不能走、斑马线才合规"，能内化**社交规则**。

#### SAFE-GRPO 的正面作用：优化合规 + 导航，效果拉满

对比 **No.6**（**IL+RL** + 全数据）和 **No.4**（仅 **IL** + 全数据）：社交合规率再升级（**DCR** 从 78.2→82.5，**TCR** 从 78.4→82.9），导航性能也冲到顶（成功率 **86.1**、路线完成率 **91.2**）。

**核心原因**：基于流的 **RL** + **规范感知奖励**，能让模型主动优化"既合规又高效"的行为，而不只是模仿数据里的轨迹。

#### 关键前提：RL 必须配 D_cog，否则越训越差

对比 **No.5**（有 **RL**、无 $D_{cog}$）和 **No.3**（无 **RL**、有 $D_{sim}$）、**No.6**（有 **RL** + 有 $D_{cog}$）：

- **没 $D_{cog}$ 的 RL（No.5）**：社交合规率反而下降（**DCR-1.7**、**TCR-1.3**），比单纯的**模仿学习**还糟
- **有 $D_{cog}$ 的 RL（No.6）**：合规率最优

**核心结论**：$D_{cog}$ 是 **RL** 的"导航图"——没有高层社交理解（比如"哪里合规"），**RL agent** 就是"瞎探索"，不仅找不到合规路径，还会违规；有了 $D_{cog}$，**RL** 才能精准优化符合人类规范的行为。

#### 必然权衡：合规优先，牺牲一点路径效率

对比 **No.6**（有 **RL**）和 **No.4**（无 **RL**）：路径效率（**SPL**）从 79.4 降到 77.4（小幅下降）。

**核心原因**：真实世界里"合规的路往往不是最短的"（比如绕开草坪走人行道），**SAFE-GRPO** 优先保证"像人一样合规走路"，而非单纯追求"最短路径"，这和真实导航的逻辑一致。
