---
title: "SocialNav-SUB Benchmarking VLMs for Scene Understanding in Social Robot Navigation"
---
> CoRL2025
> 这一篇由于是Bench类，值得常看，因为还有一大截Appendix

在动态、以人为中心的环境中，**机器人导航**需要基于可靠**场景理解**的符合**社交规范**的决策。近年来的 **VLMs（视觉语言模型）** 展现出了令人期待的能力，如**物体识别**、**常识推理**和**语境理解**——这些能力与**社交机器人导航**的细微要求相契合。然而，目前尚不清楚 **VLMs** 能否准确理解复杂的**社交导航场景**（例如，推断智能体之间的**时空关系**以及**人类意图**），而这对于安全且符合社交规范的机器人导航至关重要。

尽管最近有一些研究探索了 **VLMs** 在社交导航中的应用，但尚无现有研究系统地评估它们满足这些必要条件的能力。在本文中，我们引入了**社交导航场景理解基准（SocialNav-SUB）**，这是一个**视觉问答（VQA）**数据集和基准，旨在评估 **VLMs** 在现实世界社交机器人导航场景中的场景理解能力。**SocialNav-SUB** 提供了一个统一的框架，用于在需要社交机器人导航中的**空间**、**时空**和**社交推理**的 **VQA** 任务上，将 **VLMs** 与人类和基于规则的基线进行对比评估。通过对最先进的 **VLMs** 进行实验，我们发现，虽然表现最佳的 **VLM** 与人类答案的**一致性概率**令人鼓舞，但它仍然逊色于更简单的**基于规则的方法**和**人类共识基线**，这表明当前 **VLMs** 在社交场景理解方面存在重大差距。我们的基准为**社交机器人导航基础模型**的进一步研究奠定了基础，提供了一个框架来探索如何调整 **VLMs** 以满足现实世界社交机器人导航的需求。

![](/paper/socialnav-sub-example.png)

他对[RA-L这篇工作](./vlm-social-nav)的评价是，将大型 **VLMs** 用于社交导航有潜力，但是他们的评估仅限于少数受控场景，还真是：这篇文章因为说仿真不trivial就只做了实机。

所以说这篇 **Benchmark** 就是针对三个维度的 **VQA**：

- **空间推理**（静态位置关系）
- **时空推理**（动态运动变化）
- **社交推理**（解读复杂人类意图的能力）


该研究采用了 **SCAND 数据集**中的社交导航场景。**SCAND 数据集**是一个**机器人社交导航数据集**，包含在密集人群和多样化社交环境中符合社交规范的导航演示。我们使用全面的人工标记 **VQA 数据集**作为真实标签，系统地评估**视觉语言模型**在现实世界场景中对社交机器人导航场景的理解性能。

**SocialNav-SUB** 是首个此类基准，它使机器人专家能够系统地评估和改进**视觉语言模型**，以适用于现实世界的社交机器人导航场景。通过弥合**视觉语言模型**能力与**社交机器人导航挑战**之间的差距，我们的工作为推动**视觉语言模型**在社交机器人导航中的应用奠定了基础。

![](/paper/social-nav-sub-overview.png)

## 基准设计
### 数据与场景基础

**数据源**：基于 **SCAND 数据集**（社交机器人导航示范数据集），筛选 **60 个高难度场景**，特征包括：

- **人群密度**：平均 **6.65 人/场景**（标准差 2.80，范围 1-13 人）
- **场景类型**：户外通道、狭窄门口、人行道、路口等
- **交互复杂度**：行人遮挡机器人路径，需复杂**社交合规交互**（如避让、礼让）

针对 **VLMs** 难从 **2D 图像**提取空间/细粒度对象信息的问题，优化视觉输入：

- 用 **PHALP 算法**跟踪行人，结合**机器人里程计数据**，估计行人 **3D 姿态**并经**卡尔曼平滑**
- 将 **3D 坐标**投影到**前视图** + **鸟瞰图（BEV）**，用编号彩色圆圈标注行人位置
- 最终图像既保留原始场景上下文，又提供**结构化空间/对象信息**，且可通过机器人现有传感器实时构建，同时为人类标注员提供相同输入以保证公平对比

![](/paper/socialnav-sub-pipeline.png)
### VQA 问题设计（共 4968 个独特问题）

基于处理后的**多视图图像序列**（**2.5 秒片段**，**4Hz 采样**），设计三类**多项选择题**，全面覆盖社交导航核心推理需求：

- **空间推理**：单帧内的位置关系（如"行人初始位置相对于机器人"）
- **时空推理**：随时间的运动变化（如"行人与机器人的距离变化"）
- **社交推理**：人机互动关系（如"机器人是否受行人影响"）

### Robust Human Baseline from Human-Subject Study

**数据收集**：从 **Prolific 平台**招募参与者，每个场景至少 **5 人作答**，获取人类真实标签

**评估指标**：

- **PA（一致概率）**：衡量模型答案与人类答案的整体一致程度，**VLM** 答案的这个得分越高越接近人类判断
- **CWPA（共识加权一致概率）**：对人类意见高度统一的题（比如"明显挡路"），错了罚得更重；人类有分歧的题（比如"轻微影响"），罚得更轻，更贴合实际主观判断场景

**两个基线**：

- **平均人类基线**：单个人类与其他人类的平均一致度，反映普通人类水平
- **人类 Oracle 基线**：取每个问题的人类最共识答案，反映专家级人类水平

## Experiment

### 模型选择

| 模型类型 | 代表模型 | 特点 |
| --- | --- | --- |
| **闭源通用 VLMs** | **GPT-4o**、**Gemini 2.0** | 整体 **VQA** 能力强，适用于通用场景 |
| **推理优化 VLMs** | **OpenAI o4-mini**、**Gemini 2.5** | 专门微调过推理能力，可后续蒸馏成快模型（适配机器人） |
| **开源可部署 VLMs** | **LLaVa-Next-Video** | 能本地运行，符合机器人实时应用需求 |

### 实验流程

![](/paper/socialnav-sub-pipeline.png)

- **给 VLMs 喂的内容**：和人类标注员完全一致的「视觉（前视图 + **BEV 鸟瞰图**）+ 文本提示」
- **用「思维链（CoT）推理」**：让 **VLMs** 按人类思考顺序答题，还会提供它之前的答案辅助推导（比如"行人从头到尾都在左边，目标在右边→行人不挡路"）
- **对比方式**：用 **PA** 和 **CWPA** 两个指标，把 **VLM** 的答案和人类标注答案比一比

### 实验结果

| Category | Model | All | | Spatial Reasoning | | Spatiotemporal Reasoning | | Social Reasoning | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | | PA | CWPA | PA | CWPA | PA | CWPA | PA | CWPA |
| **Baseline** | **Human Oracle** | 0.74 ± 0.00 | 1.0 ± 0.00 | 0.71 ± 0.01 | 1.0 ± 0.00 | 0.73 ± 0.01 | 1.0 ± 0.00 | 0.76 ± 0.01 | 1.0 ± 0.00 |
| | **Average Human** | 0.60 ± 0.00 | 0.80 ± 0.00 | 0.56 ± 0.01 | 0.79 ± 0.00 | 0.59 ± 0.01 | 0.80 ± 0.00 | 0.62 ± 0.00 | 0.81 ± 0.00 |
| | **Rule-Based** | 0.64 ± 0.00 | 0.84 ± 0.00 | 0.57 ± 0.01 | 0.79 ± 0.01 | 0.62 ± 0.01 | 0.84 ± 0.01 | 0.71 ± 0.00 | 0.92 ± 0.00 |
| **VLM** | **Gemini 2.0** | 0.58 ± 0.00 | 0.79 ± 0.00 | 0.55 ± 0.01 | 0.77 ± 0.01 | 0.46 ± 0.01 | 0.64 ± 0.01 | 0.63 ± 0.01 | 0.84 ± 0.01 |
| | **Gemini 2.5** | 0.54 ± 0.00 | 0.73 ± 0.01 | 0.51 ± 0.01 | 0.72 ± 0.01 | 0.52 ± 0.01 | 0.73 ± 0.01 | 0.55 ± 0.01 | 0.73 ± 0.01 |
| | **GPT-4o** | 0.50 ± 0.00 | 0.69 ± 0.01 | 0.56 ± 0.01 | 0.79 ± 0.01 | 0.51 ± 0.01 | 0.71 ± 0.01 | 0.47 ± 0.01 | 0.63 ± 0.01 |
| | **o4-mini** | 0.62 ± 0.01 | 0.82 ± 0.01 | 0.54 ± 0.01 | 0.74 ± 0.01 | 0.59 ± 0.01 | 0.79 ± 0.01 | 0.66 ± 0.01 | 0.87 ± 0.01 |
| | **LLaVa-Next-Video** | 0.46 ± 0.01 | 0.61 ± 0.01 | 0.35 ± 0.01 | 0.46 ± 0.01 | 0.58 ± 0.01 | 0.79 ± 0.01 | 0.48 ± 0.01 | 0.62 ± 0.01 |

### 结果分析

| 对比对象 | 整体 PA（核心指标） |
| --- | --- |
| **人类 Oracle（专家）** | 0.74 ± 0.00 |
| **规则基准（手工规则）** | 0.64 ± 0.00 |
| **最佳 VLM（o4-mini）** | 0.62 ± 0.01 |
| **其他 VLM** | 0.46~0.58 |

**关键发现**：

- **空间推理**：**VLMs** 和人类 **Oracle** 差距最大（最佳 **VLM PA=0.54** vs 人类 0.71），说明 **VLMs** 看不懂空间关系
- **时空推理**：差距也大（最佳 **VLM PA=0.59** vs 人类 0.73），不会捕捉动态变化（比如"行人离机器人越来越近"）
- **社交推理**：表现最好（最佳 **VLM PA=0.66** vs 人类 0.76），甚至略超"平均人类"（0.62），说明 **VLMs** 对**社交 cues** 更敏感

**结论**：最牛的 **VLM（o4-mini）** 也没赶上人类专家和手工规则，尤其在"**高人群密度**"场景下，人类共识度高的题，**VLMs** 经常错；当前 **VLMs** 还没完全准备好支撑社交机器人导航的场景理解，核心短板在**空间**和**时空推理**。


### 消融实验

| Model | Ablation | Spatial Reasoning | Spatiotemporal Reasoning | Social Reasoning |
| --- | --- | --- | --- | --- |
| **GPT-4o** | **CoT+BEV** | 0.56 ± 0.01 | 0.51 ± 0.01 | 0.47 ± 0.01 |
| | **No CoT** | 0.58 ± 0.01 | 0.53 ± 0.01 | 0.35 ± 0.01 |
| | **No BEV** | 0.51 ± 0.01 | 0.44 ± 0.01 | 0.42 ± 0.01 |
| **LLaVa-Next-Video** | **CoT+BEV** | 0.35 ± 0.01 | 0.58 ± 0.01 | 0.48 ± 0.01 |
| | **No CoT** | 0.35 ± 0.01 | 0.58 ± 0.01 | 0.38 ± 0.01 |
| | **No BEV** | 0.35 ± 0.01 | 0.61 ± 0.01 | 0.46 ± 0.01 |
| **Gemini 2.0** | **CoT+BEV** | 0.55 ± 0.01 | 0.46 ± 0.01 | 0.63 ± 0.01 |
| | **No CoT** | 0.56 ± 0.01 | 0.48 ± 0.01 | 0.58 ± 0.01 |
| | **No BEV** | 0.56 ± 0.01 | 0.46 ± 0.01 | 0.64 ± 0.01 |

#### 消融分析

| 模型 | 策略变化 | 空间推理 PA 变化 | 时空推理 PA 变化 | 社交推理 PA 变化（核心） |
| --- | --- | --- | --- | --- |
| **GPT-4o**（受益最明显） | 去掉 **CoT**（仅留 **BEV**） | 0.56→0.58（微升） | 0.51→0.53（微升） | 0.47→0.35（**大幅下降 12 个百分点**） |
| | 去掉 **BEV**（仅留 **CoT**） | 0.56→0.51（下降） | 0.51→0.44（明显下降） | 0.47→0.42（下降） |
| **LLaVa-Next-Video** | 去掉 **CoT**（仅留 **BEV**） | 0.35→0.35（没变化） | 0.58→0.58（没变化） | 0.48→0.38（**下降 10 个百分点**） |
| | 去掉 **BEV**（仅留 **CoT**） | 0.35→0.35（没变化） | 0.58→0.61（微升） | 0.48→0.46（微降） |
| **Gemini 2.0**（影响最小） | 去掉 **CoT**（仅留 **BEV**） | 0.55→0.56（微升） | 0.46→0.48（微升） | 0.63→0.58（**下降 5 个百分点**） |
| | 去掉 **BEV**（仅留 **CoT**） | 0.55→0.56（微升） | 0.46→0.46（没变化） | 0.63→0.64（微升） |

**关键发现**：

- **CoT 的影响**：所有模型去掉 **CoT** 后，**社交推理 PA** 都显著下降（比如 **GPT-4o** 从 0.47→0.35），但空间/时空推理变化不大。原因是社交互动判断（比如"机器人要不要避让行人"）需要多步逻辑，**CoT** 能提供**结构化推理框架**，帮模型理清思路。但是这玩意又会显著提高 **Latency**
- **BEV 的影响**：**BEV** 策略的作用因模型而异
  - 对 **GPT-4o** 去掉 **BEV** 后，三个推理维度都下降（尤其时空推理从 0.51→0.44），说明它依赖 **BEV** 补充空间信息
  - 对 **LLaVa-Next-Video/Gemini 2.0** 去掉 **BEV** 后性能基本没降，甚至时空推理微升，说明这两个模型可能本身对空间信息的解析能力较强，或 **BEV** 提供的信息对它们冗余
- **进一步实验证明**：如果 **VLMs** 的空间/时空推理能力提升（比如用更好的空间数据微调），其社交推理性能也会跟着变强。这说明当前 **VLMs** 的核心短板是"**空间理解**"，制约了整体场景理解能力
- **上下文增强实验**：当 **VLMs** 使用人类标注的**空间推理答案**作为上下文时，**社交推理 PA** 提升约 **24%**（如 **Gemini 2.0** 从 0.63→0.78）
- **指导意义**：实际应用可采用"**混合架构**"——用**专用感知模块**（如**3D 目标检测**、**轨迹预测**）处理空间/时空推理，将结果作为 **VLMs** 的输入上下文，重点发挥 **VLMs** 在**社交推理**上的优势，提升整体**导航决策**的准确性

## Limitations and Future Work

### 局限性

尽管 **SocialNav-SUB** 推动了对社交机器人导航领域中**视觉语言模型（VLMs）**的评估，但它存在两个局限性：

1. **数据集范围限制**：该基准目前依赖于 **SCAND 数据集**的场景，尽管这些场景多样且包含密集人群（示例可在附录中查看），但仅限于**大学校园环境**中的社交导航
2. **实验规模限制**：虽然初步实验提供了有价值的见解，但它们基于有限的模型和场景；为了克服这些挑战并提高基准的适用性，有必要使用更广泛的**大型视觉语言模型**、数据集和改进的方法进行进一步探索

### 未来工作

展望未来，有几个颇具前景的方向可以进一步增强和利用 **SocialNav-SUB** 的能力：

1. **扩展数据集**：纳入更多**社交机器人导航数据集**，能够增加其多样性和稳健性，从而更全面地评估模型能力
2. **模型微调**：在 **SocialNav-SUB** 提供的人类数据集上对**视觉语言模型（VLMs）**进行微调，可能会使这些模型在社交机器人导航方面更具能力
3. **扩展模型范围**：扩展所评估的**视觉语言模型**范围，一些值得关注的**视觉语言模型**包括：
   - 针对**空间推理**微调的模型
   - 针对**社交机器人导航**微调的模型
4. **混合方法**：评估**混合方法**，即通过特定方式（如**社交推理**）利用**视觉语言模型**，同时配备专门的模块来弥补其不足

通过在多个推理类别中提供有针对性的评估框架，**SocialNav-SUB** 不仅能够系统地评估**视觉语言模型**的性能并突出其弱点，还能指导未来在**场景理解**和**社交合规导航**方面对**视觉语言模型**的改进，从而推动更可靠的现实世界机器人系统的发展。由于我们将开源 **SocialNav-SUB** 并计划对其进行可靠维护，开展这些未来工作所需的大部分基础设施和支持都将随时可用。

> https://www.doubao.com/chat/34591728504557570