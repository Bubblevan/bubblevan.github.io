---
title: "VLM-Social-Nav: Socially Aware Robot Navigation through Scoring using Vision-Language Models"
---

我瞅一眼中了RA-L的这玩意是干嘛的，和Falcon区别在哪里


## Related Works

### A. Safety Requirement of Social Navigation

#### DWA (Dynamic Window Approach)

**DWA** 是一种经典的局部路径规划算法，它是一个**无碰撞（collision-free）**的导航方法，其核心是通过在速度空间中搜索最优的动作序列来避障。

**数学描述**：DWA 在每一个采样周期内，通过最大化一个目标函数 $G(v, \omega)$ 来选择最优的线速度 $v$ 和角速度 $\omega$：

$$G(v, \omega) = \sigma(\alpha \cdot \text{heading}(v, \omega) + \beta \cdot \text{dist}(v, \omega) + \gamma \cdot \text{velocity}(v, \omega))$$

**约束条件**：速度必须在机器人的动力学范围内，且必须能在撞上障碍物前停下。

**局限性**：它主要关注几何避障，确实没有显式地建模传感器噪声或行人意图的不确定性，因此在复杂的社会场景中可能表现得不够自然（如只会生硬地绕开人）。

#### VO (Velocity Obstacle)

**VO** 及其变体常用于多智能体或人群行为模拟。它的基本思想是在速度空间中定义一个"碰撞锥"，只要机器人的速度不落在该锥体内，就不会发生碰撞。

**数学描述**：对于机器人 $A$ 和障碍物 $B$，速度障碍集合 $VO_{A|B}$ 定义为：

$$VO_{A|B} = \{ \mathbf{v}_A \mid \lambda(\mathbf{p}_A, \mathbf{v}_A - \mathbf{v}_B) \cap \mathcal{B} \neq \emptyset \}$$

其中：
- $\mathbf{p}_A$：位置
- $\mathbf{v}_A - \mathbf{v}_B$：相对速度
- $\lambda$：射线
- $\mathcal{B}$：障碍物占据的区域

**局限性**：**VO** 虽高效，但通常假设位置和速度是确定的，没有考虑不确定性。

#### PRVO & OFVO (Probabilistic RVO)

为了弥补 **VO** 的缺陷，**PRVO (Probabilistic RVO)** 和 **OFVO** 被提出来处理感知中的不确定性（如传感器误差或行人的随机运动）。

**数学描述**：这些算法通常将障碍物的位置或速度建模为概率分布（如高斯分布），并要求碰撞概率低于某个硬阈值 $\tau$：

$$P(\mathbf{v}_A \in VO_{A|B}) < \tau$$

**局限性**：虽然处理了不确定性，但需要设置一个**硬阈值（hard threshold）**进行规划。这意味着如果环境复杂度超过了阈值设定的安全边界，机器人可能会陷入"冻结"状态（**Frozen Robot Problem**）或无法做出灵活的社会化决策。

**总结**：Social interactions are highly nuanced and context-dependent. Simulating these interactions accurately requires sophisticated models of human behavior and interaction dynamics, which is not trivial.

### B. Contextual Appropriateness of Social Navigation

#### 1. 综述与核心挑战

**R. Mirsky et al., "Conflict avoidance in social navigation-a survey." (2024)**

这是一篇关于社交导航中冲突规避的最新综述论文。它系统地梳理了机器人如何识别并避免与人类在物理空间或社交规范上的冲突，是该领域的背景基石之一。

#### 2. 强化学习与演示学习

**P. Liu et al., "Data-driven hri: Learning social behaviors by example from human-human interaction." (2016)**

该研究探讨了通过观察人与人之间的互动示例来学习社交行为。它强调了"数据驱动"的方法，即让机器人模仿人类在共享空间中的自然反应。

**M. Li et al., "Role playing learning for socially concomitant mobile robot navigation." (2018)**

该工作提出了一种"角色扮演"的学习机制。通过模拟不同角色（如追随者或领航者）的互动，帮助机器人在伴随人类移动时表现得更自然。

**M. Nazeri et al., "VANP: Learning where to see for navigation with self-supervised vision-action pre-training." (2024)**

这篇论文关注视觉感知与行动的预训练。它使用自监督学习方法，教机器人识别环境中哪些视觉特征对于导航决策最重要，从而提高其在复杂场景下的鲁棒性。

#### 3. 大型社交导航数据集

该部分特别强调了两个用于训练机器人表现得"像人一样"的大规模数据集：

**H. Karnan et al., "Socially compliant navigation dataset (SCAND)." (2022)**

**SCAND** 是一个著名的社交导航数据集，包含大量人类在公共空间操作机器人的轨迹数据。它包含了如"靠右行驶"和"等待礼让"等符合社会规范的行为示例。

**D. M. Nguyen et al., "Toward human-like social robot navigation: A large-scale, multi-modal, social human navigation dataset." (2023)**

**MuSoHu** 数据集是一个多模态的数据集，旨在通过记录真实人类在拥挤环境中的导航方式，为机器人提供更加细致、拟人化的社交导航参考。
### C. Large Foundation Models for Navigation

#### 1. SayCan [31]：整合 LLM 进行高级任务规划

**核心理念**：将大模型的"知识（Say）"与机器人的"能力（Can）"相结合。**LLM** 负责把模糊的指令（如"我洒了饮料，能帮帮我吗？"）拆解为步骤，但每一步都会根据机器人当前环境的**成功概率（Affordance）**进行加权。

**逻辑表示**：选择下一步技能 $\pi$ 的概率公式可以简化为：

$$P(\pi) = P(\text{skill } \pi \mid \text{instruction}) \cdot P(\text{success } \pi \mid \text{state})$$

- $P(\text{skill} \mid \text{instruction})$：**LLM** 认为这一步对完成任务有多大帮助（Say）
- $P(\text{success} \mid \text{state})$：底层算法（如价值函数）判断当前环境下机器人是否能完成这一步（Can）

#### 2. GPT-Driver [32]：将运动规划视为语言建模

**核心理念**：将自动驾驶的轨迹预测直接转化为一个"文字接龙"问题。它不使用传统的数值回归，而是把坐标点变成 **Token**（字符），让 **GPT-3.5** 预测接下来的 **Token**。

**逻辑表示**：运动规划被建模为最大化下一个轨迹点 $w_i$ 的对数似然：

$$\max \sum_{i=1}^{n} \log P(\hat{w}_i \mid w_1, \dots, w_{i-1})$$

其中 $\hat{w}$ 是人类司机的真实驾驶坐标转换成的 **Token**。

#### 3. L3MVN [33]：语义地图与长期目标实现

**核心理念**：利用 **LLM** 的"常识"来寻找目标。例如，如果你让机器人找"马桶"，**LLM** 会告诉机器人"马桶通常在浴室"，机器人就会在语义地图中优先探索看起来像浴室边界的区域（**Frontiers**）。

**工作流**：
1. **构建地图**：通过传感器数据实时生成语义地图
2. **常识推理**：**LLM** 分析地图中已知的物体，判断哪个未知区域最可能包含目标物体
3. **边界选择**：将 **LLM** 的建议作为长期目标点（**Long-term Goal**）交给底层规划器

#### 4. LLaDA [34]：适应不同地区的交通规则

**核心理念**：这是一个**无需重新训练（Training-free）**的框架，专门解决"换个城市就不会开车"的问题。它通过 **LLM** 读取当地的《驾驶手册》（文本），然后实时修改原本的驾驶计划。

**关键组件**：**交通规则提取器（Traffic Rule Extractor, TRE）**
- **输入**：当前场景描述 + 意外情况 + 当地法规手册
- **输出**：符合当地规则的修正版运动计划

#### 5. LM-Nav [16]：基于自然语言的室外导航

**核心理念**：它是三个独立预训练模型的组合，不需要任何针对机器人的微调。

**三剑客组合**：
- **LLM (GPT-3)**：将复杂的指令（"穿过树林，停在红房子前"）解析成一系列路标（**Landmarks**）
- **VLM (CLIP)**：将这些文字路标与机器人摄像头看到的图像进行匹配
- **VNM (视觉导航模型)**：负责具体的行走动作

**数学目标**：寻找一条最优路径 $\tau^*$，使得在给定指令 $l$ 和拓扑图 $G$ 下的概率最大：

$$\tau^* = \arg\max P(\tau \mid l, G)$$

## Methodology
### Problem Definition

#### 普通导航的基础定义

导航是从起点到终点生成并遵循"高效无碰撞路径"的任务，系统分两层规划器：

- **全局规划器**：负责找到全局无碰撞路径
- **局部规划器**：实时调整动作，应对机器人自身动态和周边障碍物

#### 社交导航的核心差异

人类不再只是"动态障碍物"，而是需互动的"社交实体"，因此机器人行为必须融入社交规范（如礼让、保持距离）。

#### MDP 建模（社交导航的数学框架）

将社交导航抽象为四元组 $\langle S, A, T, C \rangle$，各组件含义如下：

- **S（状态）**：机器人的位姿 $s=(x,y,\theta)$，其中 $x/y$ 是位置坐标，$\theta$ 是朝向
- **A（动作）**：机器人的运动指令 $a=(v,w)$，其中 $v$ 是线速度，$w$ 是角速度
- **T（转移函数）**：描述"执行动作 $a$ 后，状态 $s$ 如何变化"，体现机器人的运动动态
- **C（成本函数）**：规划器的优化目标，需找到使"期望成本最小"的最优动作 $(v^*,w^*)$

#### 总成本函数

综合三类目标，用权重 $\alpha$、$\beta$、$\gamma$（非负）调节重要性：

$$C(s, a) = \alpha \cdot C_{\text{goal}} + \beta \cdot C_{\text{obst}} + \gamma \cdot C_{\text{social}}$$

其中：
- $C_{\text{goal}}$：鼓励机器人向目标移动（避免偏离终点）
- $C_{\text{obst}}$：抑制碰撞（避免撞到障碍物）
- $C_{\text{social}}$：强制遵循社交规范（核心新增项，让机器人"懂社交"）

#### 社交成本 $C_{\text{social}}$

量化机器人行为与人类社交行为的偏差：

$$C_{\text{social}} = \|B - B_h\|$$

其中：
- $B$：机器人当前导航行为
- $B_h$：人类在该场景下符合社交规范的行为
- 两者的偏差值，最小化偏差即让机器人模仿人类的社交合规行为

**关键**：$B_h$ 无需依赖大规模数据集，而是通过 **VLM（视觉语言模型）**，结合图像感知和提示词推理得出。

### Architecture

![](/paper/vlm-social-nav-overview.png)

由「感知层」「VLM-based 评分模块」「优化型运动规划器」三部分组成，核心是将"社交理解"转化为规划器可使用的成本项，最终输出最优机器人动作。

#### 1. 感知层：双传感器协同，分工明确

- **LiDAR（激光雷达）**：检测环境几何信息（如障碍物位置、距离），为避障成本 $C_{\text{obst}}$ 和目标成本 $C_{\text{goal}}$ 提供数据，保障基础导航安全
- **RGB 相机 + 实时感知模型（YOLO）**：捕捉环境上下文细节（如人类、手势、门等社交实体），核心作用是"触发 VLM"——仅当检测到社交实体时，才激活后续 VLM 模块，避免无效查询

#### 2. VLM-based 评分模块：社交合规的"决策大脑"

- **输入**：
  - RGB 图像（场景上下文）
  - 文本提示词（社交规则 + 任务指令）
  - 感知层检测到的社交实体信号
- **输出**：生成人类符合社交规范的导航行为（如"右转 + 减速"），并将其转化为可量化的社交成本 $C_{\text{social}}$
- **核心优势**：**VLMs** 擅长理解社交动态（如人类手势含义、场景礼仪），而非仅看视觉特征，让机器人"懂场景、守规矩"

#### 3. 优化型运动规划器：动作执行的"指挥官"

- **输入**：
  - 感知层提供的 $C_{\text{goal}}$（目标导向）
  - $C_{\text{obst}}$（避障）
  - VLM 模块提供的 $C_{\text{social}}$（社交合规）
- **输出**：通过最小化总成本函数 $\mathcal{C}(s,a)$，计算出最优动作 $(v^*,w^*)$（线速度 + 角速度），驱动机器人运动

#### 实时性优化

大模型连续查询会导致高延迟，无法满足实时导航需求。架构通过"感知模型预筛选"解决：

- 仅当 **YOLO** 检测到人类、手势等社交实体（需要社交互动时），才调用 **VLM** 计算 $C_{\text{social}}$
- 无社交场景时，仅依赖 **LiDAR** 和规划器完成基础导航，大幅降低计算开销，保障实时性

### VLM-based Scoring Module

该模块是 **VLM-Social-Nav** 的"社交决策核心"，核心功能是把 **VLM** 的社交理解能力转化为可量化的社交成本，让机器人模仿人类合规行为，最终辅助规划器输出"懂社交"的动作。

**核心目标**：通过 **VLM** 推理出"人类在当前场景下会采取的社交合规导航行为（$B_h^{t+1}$）"，计算机器人候选动作与该行为的偏差（即社交成本 $C_{\text{social}}$），为运动规划器提供"社交约束"。

#### Algorithm 1: VLM-based Social Navigation

```
Input: RGB image I, LiDAR point cloud L, prompt P, goal position p_g
1: Initialize robot position p_r;
2: while not at goal position p_r ≠ p_g do
3:   I ← Read image sensor data;
4:   L ← Read LiDAR sensor data;
5:   e ← Perception Model(I);
6:   for all possible actions a do
7:     C_social ← 0;
8:     if social entities detected in e then
9:       B_h = VLM(I, P, a);
10:      C_social = VLM-based scoring(B_h);
11:    end if
12:    Calculate the total cost:
13:    C = α · C_goal + β · C_obst + γ · C_social;
14:  end for
15:  Find action a with minimal cost C and execute;
16:  Update robot position p_r;
17: end while
```

#### 输入输出

**输入 3 大关键信息**：

- $I^t$：$t$ 时刻机器人视角的 RGB 图像（含场景上下文，如人类位置、手势、环境布局）
- $P$：文本提示词（精心设计，含 3 部分：任务描述 + 自身状态 + 社交规则，示例见图 3，比如"避让视野中的人，靠右行走、不挡路"）
- $a^t$：$t$ 时刻机器人当前动作（线速度 $v^t$ + 角速度 $w^t$，已映射为文字，如"朝向直线、线速度 0.28m/s"）

**输出**：

$$B_h^{t+1} = \text{VLM}(I^t, P, a^t)$$

$B_h^{t+1}$：$t+1$ 时刻人类会采取的社交合规行为（结构化文字格式，如 "Move RIGHT with SLOWING DOWN"）。

![](/paper/vlm-social-nav-example.png)

#### 核心工作流程

如上图所示，核心工作流程分为 3 步：

##### 第一步：提示词设计（零样本激活 VLM 推理）

基于"上下文学习（ICL）"，无需专门训练，通过提示词让 **VLM** 理解场景：

- **明确任务**（如"如何避让视野中的人"）
- **告知机器人自身状态**（如"当前直线行驶、速度 0.28m/s"）
- **注入社交规则**（如"靠右避让、不挡他人路径"，可适配不同文化，比如左行国家改"靠左避让"）
- **限定输出格式**（如 "Move + 方向（左 / 直 / 右）+ with + 速度（减速 / 加速 / 匀速 / 停止）"），方便后续解析

##### 第二步：动作 - 语言映射（把 VLM 文字输出转机器人可执行参数）

**VLM** 输出是文字（如"右转 + 减速"），需转化为机器人能理解的线速度（$v$）和角速度（$w$），映射规则固定：

- **速度映射**：$v_h^{t+1} = v^t + \delta_s$（$\delta_s$ 来自 **VLM** 输出，如"减速"对应 $\delta_s$ 为负，"加速"为正）
- **方向映射**：$w_h^{t+1} = \delta_d$（$\delta_d$ 来自 **VLM** 输出，如"左转"对应正角速度、"右转"对应负角速度、"直线"对应 0）

##### 第三步：计算社交成本

$$C_{\text{social}}^{t+1} = w_l \cdot \|v - v_h^{t+1}\| + w_a \cdot \|w - w_h^{t+1}\|$$

- **核心逻辑**：机器人候选动作 $(v, w)$ 与人类合规行为 $(v_h^{t+1}, w_h^{t+1})$ 的偏差越小，社交成本越低，动作越"合规"
- **权重说明**：$w_l$（线速度权重）、$w_a$（角速度权重）是预设非负值，用于调节"速度合规"和"方向合规"的重要性

## Experiment

> 哎呦太骚了，这篇文章没有进行仿真实验，而是直接在真实世界的物理环境中进行了测试。我说嘛！怎么可能建模的好！

### 1. 实验硬件平台（机器人与配套设备）

- **核心机器人**：**Turtlebot 2**（常用移动机器人平台，适合室内导航实验）
- **传感器**：
  - **Velodyne VLP16 LiDAR（激光雷达）**：获取环境几何信息，用于计算避障成本 $C_{\text{obst}}$
  - **Zed 2i 相机**：拍摄 RGB 图像，捕捉场景上下文（如人类位置、手势），为 **VLM** 提供视觉输入
- **算力支撑**：搭载 Intel i7 CPU + Nvidia GeForce RTX 2080 GPU 的笔记本电脑，满足 **YOLO** 检测、**GPT-4V** 推理和运动规划的实时计算需求

### 2. 模型配置

- **感知模型**：**YOLO**
  - **核心作用**：检测"关键社交线索"（人类、门、手势）——这些是社交导航的核心交互对象，只有检测到它们，才激活 **VLM** 模块
- **VLM 模型**：**GPT-4V** [14]（带视觉功能的大语言模型）
  - **选择理由**：经初步对比实验，它比其他大小型 **VLM** 更可靠、结果一致性高，且平均推理时间约 3 秒，能满足实时导航需求
  - **核心作用**：理解社交动态（如人类手势含义、场景礼仪），输出人类合规导航行为
- **底层运动规划器**：**DWA** [18]（动态窗口法）
  - **核心作用**：结合 $C_{\text{goal}}$（目标导向）、$C_{\text{obst}}$（避障）、$C_{\text{social}}$（社交成本），计算最优机器人动作（线速度 + 角速度）

### 3. 对比方法

实验选择 2 种主流方法作为基准，形成"传统避障""数据驱动学习" vs "VLM 社交导航"的对比：

- **对比方法 1：DWA（无社交成本 $C_{\text{social}}$）**
  - **特点**：只依赖 **LiDAR** 避障和目标导向，不考虑社交规范，代表"无社交感知的传统导航"
- **对比方法 2：BC [19]（行为克隆）**
  - **训练数据**：基于大规模社交导航数据集 **SCAND** [11]（含人类远程操作的合规行为，如靠右走、礼让行人）
  - **特点**：代表"数据驱动的社交导航"，需依赖大量标注数据，泛化性受数据集限制

### 4. 测试场景设计

为全面验证社交合规性，参考主流社交导航研究 [6][40]，设计 4 个"典型室内社交场景"（覆盖常见人类-机器人交互场景）：

- **场景 1：Frontal Approach（正面接近）**——机器人与人类从直线两端相互靠近
- **场景 2：Frontal Approach with Gesture（带手势的正面接近）**——人类看到机器人后，做出"停止"手势
- **场景 3：Intersection（交叉路口）**——机器人与人类在垂直轨迹上交叉通行
- **场景 4：Narrow Doorway（狭窄门口）**——机器人与人类需通过狭窄门口，路径交叉

### 5. 用户问卷评估

更骚了这玩意。通过人类参与者的直观感受，判断 **VLM-Social-Nav** 是否符合人类对"社交友好机器人"的预期，核心逻辑是"让真实人类体验→量化反馈→对比三种方法的社交表现"。

#### 实验设计（保证结果公平可靠）

- **参与者**：每个场景 7 名参与者，覆盖不同人群的直观感受
- **测试流程**：
  1. 参与者按预设轨迹行走，与机器人在目标场景（正面相遇、带手势相遇等 4 个场景）互动
  2. 互动后填写问卷（表 II），评价机器人行为
  3. 三种方法随机打乱顺序，每种方法重复 3 次，避免参与者主观偏见
- **评价工具**：
  - **5 级李克特量表**（1 = 非常不同意，5 = 非常同意）
  - **问卷内容（表 II）**：针对每个场景设计具体问题，含正向（如"机器人保持安全距离"）和负向（标 *，如"机器人阻碍我的路径"）描述，负向问题需"反向编码"（如 1 分换算为 5 分），确保分数可比

### 6. 核心结果

- **整体表现**：**VLM-Social-Nav** 在所有问题中得分最高（表 I 中 4.04-4.35 分），参与者认同度最高，证明其最符合社交规范
- **其他方法问题**：
  - **BC 方法**：标准差大（如正面相遇场景 $2.80 \pm 1.45$），说明行为不稳定——有时避障过度无法回归路径，有时不避障导致碰撞，参与者感受差异大
  - **DWA 方法**：狭窄门口场景与 **VLM-Social-Nav** 得分差距小，原因是 **DWA** 此时常"找不到规划方案而卡顿"，恰好和 **VLM-Social-Nav**"主动停车礼让"的表现类似，让参与者感受相近

> 这种风格的论文，如果投 CVPR 可能会因为“数据量小”、“没跑大规模 Benchmark”被拒，但在 HRI（人机交互顶会）或 ICRA（机器人顶会）的 cs.RO（机器人学）赛道，它反而是非常正宗的写法。与其在死板的仿真环境中跑出一万组好看的数据，不如在真实走廊里和真人面对面走 21 轮
>
> 用户问卷（User Study）也算一种是**社交导航（Social Navigation）**这个细分领域的“金标准”评估方法

## Future Work

### 1. 实时导航与 VLM 的适配（解决 VLM 高延迟问题）

**核心痛点**：大模型（如 **GPT-4V**）单次响应需几秒，连续查询会导致机器人"反应迟钝"，无法实时导航。

**优化方案（2 个关键设计）**：

1. **格式化提示词 + 预定义选项**（如仅允许"左 / 直 / 右""减速 / 加速"等输出），减少 **VLM** 推理时间
2. **感知模型（YOLO）"筛选查询时机"**：仅检测到人类、手势等社交实体时，才调用 **VLM**，无社交场景时不触发，降低计算开销

**优化效果**：平均响应时间 2-3 秒，满足人类互动场景的实时需求（人类行走速度慢，机器人有足够时间调整）。

> 在我看来直接端侧部署 **Qwen3-VL** 就行，启用 **Non-Thinking Mode**

### 2. VLM 在社交导航中的核心价值与局限

- **核心价值**：**VLMs** 能从单张图像分析社交动态 + 常识推理，比如 Fig3 场景中，**GPT-4V** 能识别"有人迎面走来"，并依据"靠右避让"的常识给出策略，无需大规模数据集训练
- **关键局限**：**VLMs** 可能出错（如误判手势），因此不能单独依赖 **VLM** 做决策——需将其输出转化为"社交成本项"，融入总成本函数，结合避障、目标导向综合决策，保证安全

### 3. 未来可扩展的更具挑战性场景

- **场景 1：室外导航**
  - 目前仅测试室内，**VLMs** 能识别室外关键信息（人行道、斑马线、车辆），未来可扩展到户外全局导航
- **场景 2：多人群交互**
  - 目前仅测试"单个人类"场景，多人群时，简单的"左 / 右"方向描述不够用，需 **VLM** 生成更复杂的社交策略（如"绕人群后方缓慢通过"）

> https://www.doubao.com/chat/34277844605080834