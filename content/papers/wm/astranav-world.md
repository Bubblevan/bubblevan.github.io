---
title: "AstraNav-World: World Model for Foresight Control and Consistency"
---
https://arxiv.org/pdf/2512.21714
目前开源了推理代码 Action Former 策略，但Checkpoint、训练相关代码与数据预处理细节还未开原（2026/1/4）

---


## 一、论文基本信息
- **标题**：AstraNav-World: World Model for Foresight Control and Consistency  
- **作者单位**：1. Amap Alibaba、2. 北京大学（PKU）、3. 清华大学（THU）  
- **发布日期**：2025年12月25日  
- **项目页面**：https://astra-amap.github.io/AstraNav-World.github.io/  
- **核心领域**：具身智能（Embodied Intelligence）、具身导航（Embodied Navigation）


## 二、研究背景与问题
### 1. 具身导航的核心需求
具身导航需让智能体在开放、动态环境中自主决策，但现有方法存在关键瓶颈：  
- **物理与时间动态建模缺失**：小预测偏差随时间累积，破坏全局规划有效性；  
- **“先想象再规划”（envision-then-plan）范式缺陷**：视觉预测（想象未来）与动作规划（规划未来）松散耦合，放大物理不确定性与因果模糊性，导致二者不一致。

### 2. 现有方法局限
- **世界模型（World Model）**：擅长物理一致的短视野视觉预测，但长视野规划能力弱；  
- **视觉语言动作模型（VLA）**：擅长高层语义理解与长视野动作生成，但缺乏环境动态感知；  
- 少数融合方法（如WorldVLA、CoT-VLA）仍未突破“松散耦合”框架，误差累积问题未解决。


## 三、核心贡献
1. **统一生成框架**：在单一概率框架内联合建模未来视觉帧与动作序列，通过双向约束（视觉验证动作合理性、动作约束视觉与任务对齐）和同步推演，强化物理与因果一致性。  
2. **AstraNav-World架构**：以长视野理解的VLM为全局规划器，整合扩散式视频生成器（状态预测）与双动作策略头（轨迹预测），实现视觉-动作深度耦合。  
3. **性能与迁移能力**：在多个导航基准上提升成功率（SR）、降低导航误差（NE），且无需真实世界微调即可实现零样本迁移，验证了模型对空间理解与导航动态的泛化性。


## 四、方法设计：AstraNav-World架构
### 1. 整体架构（三大核心组件）
![AstraNav-World架构](论文图1描述：统一框架下含Action Former和Diffusion Policy双策略流，共享VLM规划器与视频生成器，MMFCA实现跨模态交互)  
架构通过**端到端联合训练**实现视觉预测与动作规划的双向约束，核心组件如下：

#### （1）VLM规划器（Vision-Language Model Planner）
- **基础模型**：Qwen2.5-VL-3B（全参数监督微调）；  
- **输入**：自然语言指令（任务目标）、历史视觉观测（多视角图像序列）；  
- **输出**：视觉语言嵌入（$C \in \mathbb{R}^{L×2048}$），含两类特征：  
  - 目标导向语义特征（编码指令的高层导航目标）；  
  - 空间上下文特征（编码历史与当前视觉的语义、空间信息）；  
- **作用**：为视频生成器与动作策略头提供统一高层指导，确保规划与预测的连贯性。

#### （2）VLM条件视频生成器（VLM-Conditioned Video Generator）
- **基础模型**：Wan2.2-TI2V-5B（扩散式视频生成模型，LoRA微调）；  
- **核心设计**：  
  - **VLM作为条件编码器**：替换原文本编码器，通过交叉注意力将VLM嵌入注入扩散Transformer（DiT），确保生成视觉与规划对齐；  
  - **3D-RoPE重排**：处理多视角输入（左/前/右视图），将当前帧多视角沿宽度轴虚拟排列，保留时空关系（如公式1-3定义坐标转换）；  
  - **优化目标**：条件去噪损失（$\mathcal{L}_{VG}$），区分观测帧（历史+当前，噪声$\sigma_{obs}≈0.05$）与预测帧（未来，Flow Matching加噪），仅优化未来帧的去噪精度（公式4）。

#### （3）动作策略头（Action Policy Head）
提供两种策略实现，满足不同场景需求：

| 策略类型       | 核心原理                          | 输出动作表示                | 损失函数                          |
|----------------|-----------------------------------|-----------------------------|-----------------------------------|
| Action Former  | 基于查询的Transformer（5个可学习查询向量+4层编码器），确定性动作生成 | $A=(X,Y,cos\theta,sin\theta,\alpha)$（$X,Y$：相对位移；$\theta$：航向角；$\alpha$：到达标志） | 多目标加权损失（$\mathcal{L}_{PH}=\lambda_1\mathcal{L}_{pos}+\lambda_2\mathcal{L}_{angle}+\lambda_3\mathcal{L}_{arrive}$，$\lambda_1=\lambda_2=\lambda_3=1.0$）：<br>- $\mathcal{L}_{pos}$：L1损失（位移误差）；<br>- $\mathcal{L}_{angle}$：余弦相似度损失（航向角一致性）；<br>- $\mathcal{L}_{arrive}$：二元交叉熵（到达标志分类） |
| 扩散策略（Diffusion Policy） | 扩散去噪过程生成概率性动作，含**MMFCA模块**（多模态融合交叉注意力） | 同Action Former             | Flow Matching损失（$\mathcal{L}_{PH}$，公式9）：学习从噪声动作到真实动作的 velocity field，MMFCA实现双向信息流：<br>- 动作→视觉注意力：动作锚定视觉合理性；<br>- 视觉→动作注意力：视觉约束动作因果一致性 |

#### （4）速度优化：稀疏远见调度（Sparse Foresight Scheduling, SFS）
- **问题**：扩散视频生成计算量大，影响实时导航；  
- **方案**：非每步生成视觉-动作对，仅在固定间隔（如每10步）执行联合生成；  
- **效果**：推理速度提升最高6.7倍，且成功率（SR）基本保持不变（图3b）。

### 2. 训练策略（两阶段）
#### 阶段1：组件单独预训练
- VLM规划器冻结（初始化自大规模视觉语言预训练 checkpoint）；  
- 先独立训练视频生成器（仅优化$\mathcal{L}_{VG}$）；  
- 再独立训练动作策略头（仅优化$\mathcal{L}_{PH}$）；  
- 目的：避免早期优化冲突，确保各组件具备核心能力。

#### 阶段2：联合微调
- 解冻所有组件，用总损失$\mathcal{L}_{Total}=\mathcal{L}_{VG}+\lambda\mathcal{L}_{PH}$（$\lambda=1.0$）联合优化；  
- 扩散策略的MMFCA模块50%概率启用：平衡策略独立性（无视觉时仍有效）与跨模态交互（视觉-动作一致性）。


## 五、实验结果
### 1. 实验设置
- **数据集**：  
  - 指令目标导航：R2R-CE、RxR-CE（Habitat/Matterport3D场景，基于R2R/RxR指令-路径对）；  
  - 目标物体导航：HM3D-OVON（HM3D场景，开放词汇物体导航，随机采样“起始位姿-目标物体”对）；  
- **评价指标**：成功率（SR，越高越好）、路径长度加权成功率（SPL，越高越好）、导航误差（NE，越低越好）、视觉一致性指标（PSNR越高越好、FVD越低越好）。

### 2. 主要基准对比（SOTA提升）
#### （1）指令目标导航（R2R-CE/RxR-CE Val-Unseen）
| 方法                  | R2R-CE（NE/SR/SPL） | RxR-CE（NE/SR/SPL） |
|-----------------------|---------------------|---------------------|
| 此前SOTA（CorrectNav）| 4.09/64.2/56.9      | 4.09/62.3/51.9      |
| AstraNav-World（Action Former） | 3.93/73.1/67.2 | 3.93/70.4/59.6 |
| AstraNav-World（Diffusion Policy） | 3.86/73.9/67.9 | 3.82/72.9/65.4 |



| 类别                | 基线方法                          | 发表年份 | 核心特点（结合表格“Observation”列及领域背景） | 表格中关键指标表现（以R2R-CE Val-Unseen为例） |
|---------------------|-----------------------------------|----------|----------------------------------------------|-----------------------------------------------|
| 早期经典方法        | Seq2Seq (Krantz et al., 2020)     | 2020     | 首个将导航动作建模为序列生成的VLN方法，无深度/里程计输入 | NE=7.37，SR=40.0，SPL=32.0                    |
|                     | CMA (Krantz et al., 2020)         | 2020     | 基于交叉模态注意力对齐语言与视觉，无深度输入   | NE=12.10，SR=13.9，SPL=11.9（早期性能较低）   |
|                     | AG-CMTP (Chen et al., 2021)       | 2021     | 引入动作-目标关联的交叉模态预测，无视觉观测细节 | SR=19.0（侧重关联建模，精度有限）             |
|                     | R2R-CMTP (Chen et al., 2021)      | 2021     | 针对R2R任务优化的CMTP变体，无视觉观测细节     | SR=24.0                                       |
|                     | LAW (Raychaudhuri et al., 2021)   | 2021     | 基于语言引导的动作加权策略，需全景视觉输入     | NE=10.90，SR=8.0，SPL=8.0（依赖语言权重，鲁棒性弱） |
| 深度/多模态增强方法 | HPN+DN* (Krantz et al., 2021)     | 2021     | 结合深度预测（DN）与分层路径网络（HPN），需RGB输入 | SR=34.0（首次引入深度辅助，性能提升有限）     |
|                     | CM2 (Georgakis et al., 2022)      | 2022     | 双交叉模态注意力（视觉-语言、动作-视觉），需RGB输入 | NE=7.02，SR=41.0，SPL=34.0（多模态对齐优化） |
|                     | CMA* (Hong et al., 2022)          | 2022     | 改进版CMA，新增深度输入，强化模态一致性       | NE=8.76，SR=26.5，SPL=22.1（深度提升定位精度） |
|                     | WS-MGMap (Chen et al., 2022)      | 2022     | 基于加权语义图的多目标路径规划，需RGB输入     | NE=7.06，SR=43.3，SPL=30.5（图结构辅助规划） |
| 近年改进方法（2023+）| Sim2Sim* (Krantz and Lee, 2022)   | 2022     | 模拟-模拟迁移的域适应策略，需RGB输入           | SR=36.0（侧重迁移，精度中等）                 |
|                     | Reborn* (An et al., 2022)         | 2022     | 基于“重生”机制的误差修正，需RGB输入           | NE=5.98，SR=48.6，SPL=42.0（误差修正提升精度） |
|                     | GridMM* (Wang et al., 2023b)      | 2023     | 网格级多模态匹配，强化局部视觉-语言对齐，需RGB输入 | NE=5.11，SR=61.0，SPL=49.0（2023年中SOTA，网格匹配提升精度） |
|                     | DreamWalker* (Wang et al., 2023a) | 2023     | 基于“梦境模拟”的未来视觉预测，无观测细节       | SR=44.0（首次引入视觉预测，但未耦合动作）     |
| 近期SOTA（2024-2025）| InstructNav (Long et al., 2024)   | 2024     | 指令级导航优化，需RGB输入，强化长指令理解     | NE=6.89，SR=31.0，SPL=22.0（侧重指令解析，精度待提升） |
|                     | NaVid (Zhang et al., 2024b)       | 2024     | 基于导航专用视觉编码器，无多模态细节           | SR=42.7（专用编码器提升视觉表征）             |
|                     | Uni-NaVid (Zhang et al., 2024a)   | 2024     | 统一导航框架（支持多任务），需RGB输入         | NE=6.24，SR=48.7，SPL=40.9（通用性强，精度中等） |
|                     | NaVILA (Cheng et al., 2024)       | 2024     | 视觉-语言-动作统一编码器，需RGB输入           | NE=5.22，SR=62.5，SPL=54.0（2024年SOTA，统一表征提升一致性） |
|                     | HNR* (Wang et al., 2024)          | 2024     | 分层导航推理（Hierarchical Navigation Reasoning），需RGB输入 | NE=5.53，SR=59.0，SPL=49.0（推理层级优化）   |
|                     | ETPNav* (An et al., 2024)         | 2024     | 基于事件触发的路径规划，需RGB输入             | NE=5.40，SR=57.0，SPL=50.0（事件驱动提升动态适应） |
|                     | StreamVLN (Wei et al., 2025)      | 2025     | 流式视觉-语言处理（实时更新观测），需RGB输入   | NE=4.98，SR=67.5，SPL=56.9（2025年初SOTA，实时性优化） |
|                     | CorrectNav (Yu et al., 2025)      | 2025     | 此前VLN领域SOTA，强化视觉-动作一致性校正，需RGB输入 | NE=4.09，SR=64.2，SPL=56.9（AstraNav-World的直接对比基线，精度接近但无物理先验） |
|                     | AO-Planner (Chen et al., 2025)    | 2025     | 动作-目标关联规划器，需RGB输入                 | NE=6.28，SR=47.0，SPL=38.0（关联规划，精度低于CorrectNav） |

#### （2）目标物体导航（HM3D-OVON Val-Unseen）
| 方法                  | SR（%） | SPL（%） |
|-----------------------|---------|----------|
| 此前SOTA（MTU3D）     | 40.8    | 12.1     |
| AstraNav-World（Action Former） | 45.7    | 28.7     |
| AstraNav-World（Diffusion Policy） | 45.1    | 28.3     |

### 3. 消融实验（验证核心设计有效性）
| 消融项                | 实验结果（以R2R-CE SR为例） | 结论                          |
|-----------------------|-----------------------------|-------------------------------|
| 移除视频生成器        | 从73.9%降至66.5%            | 视觉预测为动作规划提供关键指导，提升导航精度 |
| SFS（间隔k=10）       | SR保持73.1%，推理速度提升6.7倍 | SFS有效平衡精度与实时性        |

### 4. 一致性与真实世界测试
- **视觉-动作一致性**：  
  - 定量：R2R-CE 5步预测PSNR=13.69、FVD=670；RxR-CE 5步预测PSNR=14.50、FVD=497；  
  - 定性：生成的未来视觉帧与轨迹渲染结果高度一致（图2），物体轮廓、相机运动与规划轨迹匹配。  
- **零样本真实世界迁移**：  
  - 实验：物理机器人在未见过的真实环境中执行导航任务（无任何真实数据微调）；  
  - 结果：优于需域适应的传统方法，能预判门、转角等复杂场景，验证模型对物理规律的泛化理解。


## 六、结论与未来工作
### 1. 结论
AstraNav-World通过**统一生成框架**解决了传统“先想象再规划”范式的物理不一致与误差累积问题，实现了：  
- 视觉预测与动作规划的双向约束，提升导航精度与鲁棒性；  
- 零样本迁移能力，无需真实数据微调即可适应真实环境；  
- 可解释性，生成的视觉帧为动作决策提供直观依据。

### 2. 未来工作
- 扩展更长视野的导航任务；  
- 强化物理与因果关系建模；  
- 提升闭环一致性与实时推理效率；  
- 适配更复杂的真实世界场景（如动态障碍物）。




“Instruction-Goal Navigation（视觉语言导航，VLN）”与传统**PointNav（点导航）** 存在核心差异，二者不属于同一任务，但存在“目标导向”的共性，具体区别如下：

| 维度                | Instruction-Goal Navigation（VLN） | 纯PointNav（点导航） |
|---------------------|------------------------------------|----------------------|
| 目标输入形式        | 自然语言指令（如“从客厅沙发走到卧室门口”） | 终点3D坐标（如(x=5.0, y=2.0, θ=90°)） |
| 核心能力需求        | 语言语义解析（指令理解）+ 视觉-语言对齐 + 导航 | 坐标定位（距离/角度计算）+ 导航 |
| 依赖模态            | 多模态（视觉+语言）                 | 单/双模态（视觉+深度/里程计，无需语言） |
| 任务本质            | “语义化点导航”（语言描述映射为终点位置） | “坐标化点导航”（直接以坐标为目标） |

**结论**：VLN是PointNav的“语义增强变体”，而非纯PointNav——它在PointNav“点到点移动”的基础上，额外增加了“语言指令解析与语义映射”的需求，因此不能等同于纯PointNav。


### 七、AstraNav-World迁移到纯PointNav的具体策略
基于AstraNav-World的核心架构（VLM规划器、VLM条件视频生成器、动作策略头），迁移到纯PointNav需针对“移除语言模态、强化坐标目标”做模块适配，具体步骤如下：

#### 1. 核心模块适配：移除语言依赖，替换为坐标目标特征
AstraNav-World的VLM规划器原本负责“语言指令解析+视觉语义对齐”，迁移到PointNav需简化为“坐标目标编码+视觉定位对齐”，其他模块保持物理先验与双向一致性的核心设计：

| 原模块                | 迁移适配方案                                                                 |
|-----------------------|------------------------------------------------------------------------------|
| VLM规划器（Qwen2.5-VL）| 1. 移除语言指令输入通道，保留视觉观测（RGB/深度/多视角）输入；<br>2. 新增“终点坐标编码器”：将PointNav的终点3D坐标（x, y, θ）编码为特征向量（如通过2层MLP映射到2048维，与原VLM视觉特征维度一致）；<br>3. 输出：视觉特征 + 坐标特征的融合向量（替代原“视觉-语言嵌入”），确保规划与坐标目标对齐。 |
| VLM条件视频生成器（Wan2.2）| 1. 替换条件输入：将原“VLM视觉-语言嵌入”替换为“视觉-坐标融合特征”，通过交叉注意力注入扩散Transformer（DiT）；<br>2. 保留3D-RoPE重排（多视角处理）和Flow Matching加噪策略：确保生成的未来视觉帧与“坐标目标导向的运动”一致（如向x=5.0方向移动时，视觉帧需体现场景沿x轴的变化）。 |
| 动作策略头（Action Former/Diffusion Policy）| 1. 输入增强：在原视觉特征基础上，新增“实时坐标误差特征”（当前位置与终点的距离Δx、Δy，角度差Δθ）；<br>2. 输出调整：动作维度不变（X,Y,cosθ,sinθ,α），但“到达标志α”的判断依据从“语言指令完成”改为“坐标误差≤阈值（如Δx²+Δy²≤0.1m²，Δθ≤5°）”；<br>3. 损失函数优化：保留原位置（L1）、角度（余弦相似度）损失，将“到达标志损失（$\mathcal{L}_{arrive}$）”的标签从“指令完成”改为“坐标达标”。 |

#### 2. 数据与训练适配：替换为PointNav基准数据集
AstraNav-World原用VLN数据集（R2R-CE/RxR-CE），迁移需改用PointNav专用数据集，同时调整数据格式：

| 适配项                | 具体操作                                                                 |
|-----------------------|--------------------------------------------------------------------------|
| 数据集替换            | 采用Habitat PointNav Benchmark（如HM3D-PointNav、Matterport3D-PointNav）：包含“起点-终点坐标对”“多视角视觉观测”“动作序列”标注，无需语言指令。 |
| 数据预处理            | 1. 移除原数据中的语言指令字段，保留视觉（RGB/深度）、里程计（Odo）、动作、坐标标签；<br>2. 新增“坐标误差计算”预处理：对每个训练样本，实时计算当前位置与终点的Δx、Δy、Δθ，作为策略头的输入特征。 |
| 训练策略调整          | 1. 阶段1（组件预训练）：先独立训练“坐标编码器+视频生成器”（优化$\mathcal{L}_{VG}$，确保视觉预测与坐标运动一致），再训练“动作策略头”（优化$\mathcal{L}_{PH}$，确保动作向坐标目标收敛）；<br>2. 阶段2（联合微调）：总损失$\mathcal{L}_{Total}=\mathcal{L}_{VG}+\lambda\mathcal{L}_{PH}$（λ=1.0不变），重点优化“坐标误差最小化”。 |

#### 3. 推理与环境适配：匹配PointNav的评估逻辑
1. **推理输入**：每次推理时，输入“当前视觉观测”“当前位置坐标”“终点坐标”，无需语言指令；
2. **评估指标适配**：采用PointNav标准指标（替代VLN的SR/SPL）：
   - 成功率（SR）：到达终点坐标误差≤阈值的样本占比；
   - 平均导航误差（ANE）：所有样本的最终坐标误差均值；
   - 路径效率（PE）：实际路径长度与理论最短路径长度的比值（越小越好）；
3. **物理环境适配**：保留Wan2.2的物理先验（如碰撞检测、运动动力学）：PointNav常涉及动态障碍物（如移动的家具），可通过Wan2.2的物理建模预判障碍物运动，提升避障鲁棒性。

#### 4. 稀疏远见调度（SFS）保留：平衡精度与实时性
PointNav对实时性要求较高（如机器人需秒级响应），因此保留原SFS策略（每10步联合生成视觉-动作对）：既维持“视觉预测验证动作合理性”的双向一致性，又通过减少生成次数提升推理速度（原6.7倍提速效果可复用）。

AstraNav-World的核心优势（物理先验Wan2.2、双向一致性、多视角处理）在PointNav中仍适用，迁移的关键是“移除语言模态、强化坐标目标”——通过上述模块、数据、训练适配，可实现从“语义化点导航（VLN）”到“坐标化点导航（PointNav）”的有效迁移，且大概率能继承原模型的“零样本sim-to-real”能力（物理先验保障真实环境适配）。
