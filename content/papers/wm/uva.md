---
title: "Unified Video Action Model"
---

https://arxiv.org/pdf/2503.00200

---

## 一、论文概况
- **标题**：Unified Video Action Model  
- **作者**：Shuang Li、Yihuai Gao、Dorsa Sadigh、Shuran Song（斯坦福大学）  
- **核心目标**：解决机器人领域中“视频生成提供场景信息”与“动作预测需高速度”的矛盾，提出统一框架同时实现高精度动作推理与高效视频建模，支持多类机器人任务。  
- **关键贡献**：  
  1. 学习**联合视频-动作 latent 表示**，捕捉视觉与动作域的底层关联；  
  2. 设计**解耦视频-动作扩散头**，推理时可跳过视频生成以加速动作预测；  
  3. 引入**掩码训练**，支持灵活输入输出组合，实现多任务功能（政策、视频生成、正/逆动力学等）。


## 二、核心问题与现有方法局限
### 1. 核心矛盾
- 动作预测需**高时间分辨率**（捕捉细粒度运动），视频生成需**高空间分辨率**（生成高保真视觉），二者需求冲突导致现有方法难以平衡。

### 2. 现有方法缺陷
| 方法类型       | 代表工作       | 局限                                                                 |
|----------------|----------------|----------------------------------------------------------------------|
| 仅动作预测     | [10,25,51]     | 忽略视频的场景动态信息，易过拟合动作历史，对视觉干扰鲁棒性差         |
| 先视频后动作   | [14,28]        | 视频生成速度慢，误差从视频传递到动作预测，推理效率低                 |
| 联合视频-动作  | PAD [19]       | 无法独立于视频生成预测动作，推理速度仍受限                           |
| 掩码训练方法   | [31,34,48]     | 仅针对低维状态观测（非视频），需微调才能适配多任务，功能单一         |


## 三、UVA 模型设计
### 1. 整体架构（图1、图2）
UVA 分为“编码-融合-解码”三阶段，核心是“联合表示+解耦解码”，支持掩码训练灵活适配任务。

### 2. 关键模块详解
#### （1）历史编码（Encode History）
- **图像编码**：用预训练 VAE（kl-f16）将每帧图像转为 `w×h×c` 的 latent 映射，展平后通过全连接层生成 `N` 个 `d` 维视觉 token（`N` 为 token 总数）；  
- **动作编码**：动作采样频率高于图像（图像短期冗余），每个图像对应 `L` 个动作组成的“动作块”，重复动作块 `M` 次以匹配视觉 token 数量，再通过全连接层生成 `N` 个 `d` 维动作 token；  
- 作用：将历史观测与动作转为统一维度的 token，为后续融合做准备。

#### （2）掩码自编码器（Masked Autoencoder for Observation Prediction）
- 借鉴 MaskGIT [8] 和 MAR [27]，针对未来观测帧：  
  1. 用 VAE 编码未来帧为 token，随机掩码部分 token；  
  2. 将“历史视觉 token+历史动作 token+掩码未来 token”按通道拼接，再按时间维度串联为 `N×h` 序列（`h` 为时间步长）；  
  3. 输入 Transformer 融合信息，输出**联合视频-动作 latent 表示** `Z = {Z_{t+1}, ..., Z_{t+h}}`（每个 `Z_t` 含 `N` 个 token）；  
- 语言适配（如 Libero10）：用 CLIP 编码语言指令为 `d` 维 token，重复后附加到 `N×h` 序列，Transformer 输出前 `N×h` 个 token 作为 `Z`。

#### （3）解耦视频-动作扩散（Decoupled Video-Action Diffusions）
- 设计两个**轻量扩散头**，分别解码动作与视频，训练时联合优化，推理时可独立调用：  
  - **动作扩散头**：用卷积层聚合 `Z_t` 中所有 token，通过 MLP 生成动作 latent，再扩散生成动作块 `A_t`；  
  - **视频扩散头**：对 `Z_t` 中每个 token 单独解码，生成视频 patch，经 VAE 解码器重组为完整帧 `O_t`；  
- 损失函数：  
  - 动作扩散损失：`L_action = E[||ε - ε_θ(A^(k)|k,Z)||²]`（`A^(k)` 为带噪动作，`ε` 为真实噪声）；  
  - 视频扩散损失：`L_video = E[(1/N)Σ||ε_i - ε_φ(O^(i,k)|k,z_i)||²]`（`O^(i,k)` 为带噪视觉 token）；  
  - 总损失：`L = L_action + L_video`（按时间步求和）。

#### （4）掩码训练（Masked Training for Flexible Objectives）
- 通过**选择性掩码输入/输出**，让单个模型适配多任务，无需微调：  
  - 例：仅输入视频→逆动力学模型（生成动作）；输入历史观测+动作→前向动力学模型（预测未来视频）；输入历史观测→政策模型（预测动作）；  
  - 优势：支持不完整数据（如无动作标签的视频），减少任务过拟合，提升鲁棒性。


## 四、实验结果
### 1. 实验设置
- **基准数据集**：仿真（PushT、ToolHang、PushT-M、Libero10）、真实世界（UMI 数据集：Cup Arrangement、Towel Folding、Mouse Arrangement）；  
- **对比基线**：Diffusion Policy（DP-C/DP-T）、OpenVLA、π₀、UniPi、DP-UMI、SLAM 等；  
- **关键指标**：成功率（政策）、FVD（视频生成，越低越好）、L2 误差（逆动力学）、推理时间（ms/s）。

### 2. 核心实验结果
#### （1）UVA 作为政策模型
| 场景                | 关键结论                                                                 |
|---------------------|--------------------------------------------------------------------------|
| 仿真单任务          | PushT 成功率 0.98（超 DP-C 的 0.91），ToolHang 0.88（超 OpenVLA 的 0.18） |
| 仿真多任务          | PushT-M 超最佳基线 20%，Libero10 成功率 0.90（超 π₀ 的 0.85）            |
| 真实世界多任务      | Cup 任务成功率 0.65（超 DP-UMI 的 0.50），Mouse 任务 0.80（超 DP-UMI 的 0.40） |
| 推理速度            | 仿真 0.23s/轨迹（接近 DP-T 的 0.36s），真实世界 95ms/轨迹（略慢于 DP-UMI 的 70ms） |
| 鲁棒性              | 视觉干扰（目标颜色变化）下成功率 0.64（超 OpenVLA 的 0.32）；长历史输入性能稳定（DP-C 随历史变长性能下降） |

#### （2）UVA 作为视频生成模型
- 对比 UniPi（表 IV）：  
  - Libero10：UVA（8 步）FVD=51.10（超 UniPi 的 56.55）；  
  - Cup Arrangement：UVA（8 步）FVD=29.72（超 UniPi 的 71.37）；  
- 结论： autoregressive 步数越多（如 8 步），视频保真度越高，且优于视频优先的 UniPi。

#### （3）UVA 作为前向动力学模型
- 指导 DP-C 完成块推送任务：DP-C 单独成功率 38%，结合 UVA 后提升至 60%（接近真实模拟器的 75%）；  
- 作用：通过预测未来视频筛选最优动作轨迹，提升政策决策合理性。

#### （4）UVA 作为逆动力学模型
- 对比 UniPi 和 SLAM（表 VI）：  
  - 位置误差：UVA 0.75cm（UniPi 1.92cm，SLAM 0.41cm）；  
  - 旋转误差：UVA 1.11°（UniPi 2.21°，SLAM 0.30°）；  
- 结论：虽略逊于 SLAM，但无需额外映射校准，且动作时序一致性更优。


## 五、相关工作与讨论
### 1. 相关工作定位
- **视频生成用于政策**：UVA 解耦视频与动作解码，解决现有方法（如 UniPi）推理慢、误差累积问题；  
- **视频生成作为动力学模型**：UVA 统一框架支持政策与动力学建模，无需单独训练（如 GameGen-X、Genie 仅做动力学）；  
- **掩码训练**：UVA 针对视频（高维观测）设计掩码，支持多任务零微调（现有方法如 [31,34] 仅低维状态）。

### 2. 局限性与未来工作
- **局限性**：未利用大规模无动作视频数据，真实世界单任务性能略逊于定制模型（如 DP-UMI）；  
- **未来方向**：  
  1. 基于 web-scale 视频预训练，提升泛化性；  
  2. 扩展扩散头至声音、力等模态，构建更全面的机器人感知-动作框架。


## 六、核心总结
UVA 通过“联合 latent 表示+解耦扩散+掩码训练”，实现了机器人领域的“多任务统一框架”：  
1. **性能优**：多任务（政策、视频生成、正/逆动力学）均超或匹配专用模型；  
2. **效率高**：推理时跳过视频生成，速度接近仅动作模型；  
3. **灵活性强**：掩码训练支持零微调适配多任务，兼容不完整数据；  
4. **实用性**：仿真与真实世界均验证有效性，为机器人通用智能提供新范式。