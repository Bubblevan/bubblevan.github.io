---
title: "World Model-based Perception for Visual Legged Locomotion"
---
https://arxiv.org/pdf/2409.16784

---
## 一、核心信息
- **标题**：基于世界模型的视觉腿式移动感知（World Model-based Perception for Visual Legged Locomotion）
- **作者团队**：来自上海交通大学、字节跳动研究院的Hang Lai等研究者
- **核心目标**：解决视觉腿式机器人在复杂地形移动中“高维视觉输入学习效率低”“传统特权学习存在信息差距”的问题，提出端到端框架实现高效仿真到真实场景的迁移
- **代码与视频**：https://wmp-loco.github.io/


## 二、研究背景与动机
### 1. 腿式移动的感知需求
腿式机器人在多样地形（斜坡、台阶、间隙等）移动需结合两类感知：
- **本体感知（Proprioception）**：关节位置/速度、基座角速度等，仅依赖此的“盲策略”无法应对间隙、低矮障碍等复杂地形
- **视觉感知**：深度图像等，需处理高维数据、记忆历史信息以预测脚下地形，直接学习数据效率极低

### 2. 传统方法的局限性
传统**特权学习框架（Privileged Learning）** 分两阶段训练，存在显著缺陷：
- 第一阶段：用仿真中可获取、真实场景不可得的“特权信息”（如地形扫描点scandots）训练教师策略（Teacher）
- 第二阶段：用视觉输入（深度图）训练学生策略（Student）模仿教师，因“特权信息-视觉输入”存在信息差距，学生性能始终低于教师
- 教师策略需为不同地形设计专属特权信息（如scandots无法区分倾斜地形Tilt、低矮障碍Crawl），扩展性差


## 三、相关工作
| 研究方向 | 核心进展 | 本文关联与区别 |
|----------|----------|----------------|
| 腿式移动强化学习（RL for Legged Locomotion） | 1. 仅依赖本体感知的策略可通过奖励设计、域随机化应对简单地形；2. 特权学习框架通过“教师-学生”迁移整合视觉，但存在性能 gap 和扩展性问题 | 本文摒弃两阶段特权学习，用世界模型直接从视觉+本体感知提取信息，无需设计专属特权信息 |
| 基于模型的强化学习（MBRL） | 1. 通过学习环境动力学模型辅助决策，在机器人控制、游戏等领域高效；2. Dreamer-V3等基于循环状态空间模型（RSSM）的方法在像素输入任务中表现优异，但未广泛应用于视觉腿式移动 | 本文基于RSSM设计世界模型，适配视觉腿式场景（如降低世界模型更新频率），首次将世界模型用于复杂视觉腿式移动 |


## 四、预备知识：问题建模
将视觉腿式移动建模为**部分可观测马尔可夫决策过程（POMDP）**，定义如下：
-  tuple：\((S, O, A, T, r, \gamma)\)
  - **状态\(S\)**：\(s_t=(o_t, s_t^{pri})\)，含观测\(o_t\)和仿真专属的特权信息\(s_t^{pri}\)（如scandots、接触力）
  - **观测\(O\)**：\(o_t=(o_t^p, d_t)\)，含45维本体感知\(o_t^p\)（关节位置/速度、基座角速度等）和64×64维深度图\(d_t\)
  - **动作\(A\)**：\(a_t \in \mathbb{R}^{12}\)，表示12个关节的目标位置（通过PD控制器计算力矩）
  - **目标**：学习最优策略\(\pi^*\)，最大化折扣回报\(\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right]\)


## 五、核心方法：WMP框架
WMP（World Model-based Perception）是**端到端框架**，同步训练世界模型与策略，无需分阶段，核心分三部分：

### 1. 世界模型学习（基于修改的RSSM）
#### （1）架构设计
针对视觉腿式场景优化RSSM（循环状态空间模型），**世界模型更新频率低于策略**（每\(k\)步更新，平衡性能与计算成本），含4个核心组件（参数化 by \(\phi\)）：
| 组件 | 功能 | 数学表达 |
|------|------|----------|
| 循环模型（GRU实现） | 基于历史信息预测确定性循环状态\(h_t\) | \(h_t = f_{\phi}(h_{t-k}, z_{t-k}, a_{t-k:t-1})\)（\(a_{t-k:t-1}\)为\(k\)步动作序列，\(z\)为随机状态） |
| 编码器（CNN+MLP） | 融合观测\(o_t\)，输出随机状态\(z_t\)的后验分布 | \(z_t \sim q_{\phi}(\cdot | h_t, o_t)\) |
| 动态预测器 | 无观测时预测随机状态\(z_t\)的先验分布，支持未来动态预测 | \(\hat{z}_t \sim p_{\phi}(\cdot | h_t)\) |
| 解码器（CNN+MLP） | 从\(h_t\)和\(z_t\)重建高维观测 | \(\hat{o}_t \sim p_{\phi}(\cdot | h_t, z_t)\) |

#### （2）损失函数
联合优化4个组件，最小化轨迹长度为\(L\)的损失（含重建损失+KL正则）：
\[
\mathcal{L}(\phi) = \mathbb{E}_{q_{\phi}}\left[\sum_{t=n \cdot k}^{L} -ln p_{\phi}(o_t | z_t, h_t) + \beta \cdot KL\left[q_{\phi}(\cdot | h_t, o_t)|| p_{\phi}(\cdot | h_t)\right]\right]
\]
- 重建损失：确保\(z_t\)包含观测的关键信息
- KL散度：使先验与后验逼近，支持无观测的开环预测（\(\beta\)为超参数，\(n\)为非负整数）


### 2. 策略学习（基于PPO算法）
采用**非对称演员-评论家框架（Asymmetric Actor-Critic）**，利用世界模型的循环状态\(h_t\)解决部分可观测问题：
- **演员（Actor）**：输入本体感知\(o_{t+i}^p\)和“停止梯度的循环状态\(sg(h_t)\)”（避免策略更新影响世界模型），输出动作分布：
  \[a_{t+i} \sim \pi_{\theta}(\cdot | o_{t+i}^p, sg(h_t)), \forall i \in [0,k-1]\]
- **评论家（Critic）**：可访问特权信息\(s_{t+i}^{pri}\)，更精准评估状态价值：
  \[v(s_{t+i}) \sim V_{\theta}(\cdot | o_{t+i}^p, sg(h_t), s_{t+i}^{pri}), \forall i \in [0,k-1]\]
- 训练方式：用PPO算法在仿真中收集数据训练，不依赖世界模型生成rollout（仿真数据足够高效且准确）


### 3. 训练细节
| 维度 | 具体设置 |
|------|----------|
| 环境 | 基于Isaac Gym仿真器，并行训练4096个Unitree A1机器人实例；6种地形（Slope、Stair、Gap、Climb、Tilt、Crawl），采用“难度递增”课程学习 |
| 频率与延迟 | 策略动作频率50Hz（0.02s/步），深度图每\(k\)步采集，添加100ms延迟模拟真实场景 |
| 奖励函数 | 1. 跟踪奖励：鼓励机器人跟随3维速度指令（\(v_x^{cmd}, v_y^{cmd}, \omega_z^{cmd}\)），支持必要时加速（如跳间隙）；2. AMP风格奖励：使动作更自然（区分参考数据集与智能体的状态转移）；3. 惩罚项：避免卡顿、绕障 |


## 六、实验结果
### 1. 仿真对比：验证性能优势
#### （1）对比基线
- Teacher：用特权信息训练的“最优 oracle”基线（不适用Tilt/Crawl）
- Student：基于ConvNet-RNN模仿Teacher的视觉策略（不适用Tilt/Crawl）
- Blind：仅依赖本体感知的“盲策略”
- WMP w/o Prop：移除本体感知的WMP变体

#### （2）核心结论（表1）
- **回报（越高越好）**：WMP在6种地形中表现最优（如Slope：36.55±0.82，接近Teacher的36.70±1.79；Gap：32.37±8.24，远超Student的27.07±12.38）
- **跟踪误差（越低越好）**：WMP在复杂地形误差更小（如Crawl：0.006±0.002，优于Blind的0.051±0.022）
- **关键洞察**：Blind性能显著落后（Gap回报仅9.80±9.51），证明视觉必要性；WMP w/o Prop略有下降，证明本体感知辅助价值


### 2. 实证研究：解析世界模型作用
- **循环状态\(h_t\)的区分能力**：t-SNE可视化显示，\(h_t\)能清晰区分6种地形（仅Slope与Climb轻微重叠，因二者深度图相似）
- **模型间隔\(k\)的选择**：\(k=5\)（0.1s更新）为最优——小\(k\)响应快但计算成本高，大\(k\)延迟大；
- **训练长度**：6.4秒轨迹片段足够（1秒片段已达可接受性能，过长会增加梯度传播成本）；
- **仿真到真实的预测能力**：仿真训练的世界模型能准确预测真实轨迹（如Crawl场景中，虽障碍物形状与仿真不同，但可通行窄缝的位置/角度高度匹配）


### 3. 真实世界评估：验证迁移能力
#### （1）硬件与设置
- 机器人：Unitree A1；计算：机载Jetson NX（无外部设备）；相机：Intel D435i（60Hz，424×240深度图，预处理后降为64×64）
- 评估地形：Stair、Gap、Climb、Tilt、Crawl（排除简单的Slope）

#### （2）核心结果
- **地形突破**：WMP能穿越超难地形——85cm间隙（约2.1倍机器人长度）、55cm攀爬（约2.2倍机器人高度）、22cm低矮障碍（约0.8倍机器人高度），接近仿真最大难度；
- **成功率**：WMP在Tilt/Crawl的成功率100%，而Student完全失败；
- **户外泛化**：在公园场景中稳定工作，可上下台阶、攀爬45cm平台、穿越草地/碎石路。


## 七、结论与未来方向
### 1. 核心贡献
- 提出首个将世界模型用于复杂视觉腿式移动的框架，摒弃传统特权学习的两阶段设计，消除信息差距；
- 仿真训练的世界模型可高效迁移到真实场景，在Unitree A1上实现当前最优的复杂地形穿越性能；
- 验证了“循环状态\(h_t\)”对地形区分、决策辅助的关键作用，为机器人自然学习提供新范式。

### 2. 未来方向
- 混合“仿真+真实数据”训练世界模型，进一步提升真实场景预测精度；
- 整合触觉等其他感知模态，扩展框架适用场景（如湿滑、松软地形）。


## 八、核心亮点小结
1. **架构创新**：端到端同步训练世界模型与策略，无需特权信息设计，扩展性强；
2. **效率与迁移**：完全仿真训练，却能精准预测真实轨迹，仿真到真实迁移成本低；
3. **性能突破**：在真实A1机器人上实现超难地形穿越，填补视觉腿式移动的技术缺口。