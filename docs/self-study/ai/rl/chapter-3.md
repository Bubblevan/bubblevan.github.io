---
id: rl-chapter-2
title: 第2章 贝尔曼最优公式
sidebar_label: 第2章
---


## 3、Bellman Optimality Equation

在第2章中，我们学习了如何评估一个固定策略$\pi$的好坏 —— 通过计算该策略下的状态价值$v_{\pi}(s)$。但强化学习的最终目标不是评估，而是找到**最优策略**。这一章要回答的核心问题是：**如何找到能够获得最大长期收益的策略？**

### 3.1 最优策略与最优状态价值

让我们从直觉出发理解什么是"最优"。想象你在玩一个迷宫游戏，每个路口都有多个选择，每个选择会带来不同的奖励。最优策略就是：**在所有可能的路口，都选择那个能带来最大长期收益的路**。

用数学语言表达：如果存在一个策略$\pi^*$，使得对于任意状态$s$，都有$v_{\pi^*}(s) \geq v_{\pi}(s)$对所有其他策略$\pi$成立，那么$\pi^*$就是最优策略。最优策略对应的状态价值$v^* = v_{\pi^*}$称为**最优状态价值**。

这里有个关键问题：最优策略一定存在吗？如果存在，如何找到它？这正是贝尔曼最优方程（Bellman Optimality Equation, BOE）要回答的。

### 3.2 贝尔曼最优方程

回想第2章的贝尔曼方程，它告诉我们：**给定策略$\pi$，状态价值等于即时奖励的期望加上未来状态价值的折扣期望**。但贝尔曼方程只能评估固定策略，不能帮我们找最优策略。

贝尔曼最优方程的核心思想是：**如果我们知道了最优状态价值$v^*$，那么最优策略就是在每个状态选择能最大化"即时奖励 + 未来最优状态价值折扣"的动作**。

具体来说，对于状态$s$，最优策略应该选择动作$a$使得：

$$q^*(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v^*(s')$$

达到最大。这里$q^*(s,a)$称为**最优动作价值**，表示在状态$s$选择动作$a$后，按照最优策略行动能获得的期望回报。

最优状态价值$v^*(s)$应该等于所有可能动作中最优动作价值的最大值：

$$v^*(s) = \max_{a \in \mathcal{A}} q^*(s,a) = \max_{a \in \mathcal{A}} \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v^*(s') \right]$$

这就是**贝尔曼最优方程的标量形式**。将所有状态写成向量形式，可以得到：

$$v^* = \max_{\pi \in \Pi}(r_{\pi} + \gamma P_{\pi}v^*)$$

其中$\Pi$表示所有可能策略的集合。这个方程告诉我们：最优状态价值$v^*$必须满足"对所有策略取最大值"的条件。

这个方程看起来很合理，但有两个关键问题需要回答：**这个方程有解吗？如果有解，如何求解？** 这需要用到压缩映射定理。

### 3.3 压缩映射定理

![压缩映射示意图](/img/rl/f3-2.png)

要理解贝尔曼最优方程为什么有唯一解，我们需要先理解**压缩映射**。压缩映射的核心思想是：**映射后两点的距离比映射前更近，且收缩比例固定（小于1）**。

数学定义：若存在常数$\gamma \in [0,1)$，对任意两个向量$x_1, x_2$，都有：

$$\|f(x_1) - f(x_2)\| \leq \gamma \|x_1 - x_2\|$$

则$f$是压缩映射。关键在于**距离收缩比例$\gamma$固定且小于1** —— 无论$x_1, x_2$在哪里，映射后它们的距离都会缩小到原来的$\gamma$倍以内。

举个例子，$f(x) = 0.5x$就是压缩映射，因为对任意$x_1, x_2$，有$|f(x_1) - f(x_2)| = 0.5|x_1 - x_2|$，固定收缩比例$\gamma = 0.5 < 1$。但$f(x) = \ln x$不是压缩映射，因为收缩比例不固定：取$x_1 = 1, x_2 = e$时，收缩比例约为$0.58$；取$x_1 = e, x_2 = e^2$时，收缩比例约为$0.21$，不满足"固定$\gamma < 1$"的要求。

**压缩映射定理（定理3.1）**告诉我们：对于方程$x = f(x)$，如果$f$是压缩映射，那么存在唯一的**不动点**$x^*$满足$f(x^*) = x^*$，且可以通过迭代法$x_{k+1} = f(x_k)$求解，收敛速度是指数级的。

为什么压缩映射能保证收敛？核心逻辑是：**压缩映射让迭代序列的相邻项距离指数级变小，最终汇聚到不动点$x^*$**。

假设我们从初始值$x_0$开始迭代，由于压缩映射的性质，相邻项的距离满足：

$$\|x_{k+1} - x_k\| = \|f(x_k) - f(x_{k-1})\| \leq \gamma \|x_k - x_{k-1}\|$$

递推下去，第$k$步时：$\|x_{k+1} - x_k\| \leq \gamma^k \|x_1 - x_0\|$。由于$\gamma < 1$（比如$\gamma = 0.9$），$\gamma^k$会随着$k$增大而指数级变小（例如$\gamma^{10} \approx 0.35$，$\gamma^{20} \approx 0.12$，$\gamma^{100} \approx 2.66 \times 10^{-5}$），这意味着迭代序列的相邻项会越来越近。

要证明序列一定会汇聚到某个点$x^*$，我们需要说明"当$k$足够大时，序列中所有后面的项都挤在一起"。取任意两个足够大的项$x_m$和$x_n$（$m > n$），它们的距离可以拆成相邻项距离的和。根据三角不等式：

$$\|x_m - x_n\| \leq \|x_m - x_{m-1}\| + \|x_{m-1} - x_{m-2}\| + \dots + \|x_{n+1} - x_n\|$$

代入前面的递推结果，这是一个等比数列求和：

$$\|x_m - x_n\| \leq \gamma^n \cdot \frac{1 - \gamma^{m-n}}{1 - \gamma} \|x_1 - x_0\| \leq \frac{\gamma^n}{1 - \gamma} \|x_1 - x_0\|$$

当$n$足够大时，$\gamma^n$指数级趋近于0，所以$\|x_m - x_n\|$会变得任意小。这意味着序列后面的项都挤在一起，必然会收敛到某个固定点$x^*$。

**结论**：只要$f$是压缩映射，迭代序列$x_{k+1}=f(x_k)$就一定会收敛到唯一的不动点$x^*$，且收敛速度是指数级。这正是贝尔曼最优方程能稳定求解最优状态价值$v^*$的核心原因（因为BOE右侧的$f(v)$是压缩映射）。

### 3.4 从BOE求解最优策略的完整推导

现在我们已经理解了压缩映射定理。接下来要证明：**贝尔曼最优方程右侧的函数$f(v) = \max_{\pi \in \Pi}(r_{\pi} + \gamma P_{\pi}v)$是压缩映射（定理3.2）**，从而保证BOE有唯一解。

这个证明虽然重要，但技术细节较多。核心思路是：通过分析$f(v)$在最大范数下的行为，可以证明对任意两个状态价值向量$v_1, v_2$，有：

$$\|f(v_1) - f(v_2)\|_{\infty} \leq \gamma \|v_1 - v_2\|_{\infty}$$

其中$\|\cdot\|_{\infty}$是最大范数（向量中所有元素绝对值的最大值）。由于$\gamma < 1$，$f$确实是压缩映射。

**定理3.3（BOE解的存在性、唯一性与求解算法）**：由于$f(v)$是压缩映射，根据压缩映射定理，方程$v = f(v)$（即BOE）必有唯一不动点$v^*$（最优状态价值），且可通过迭代算法求解：

$$v_{k+1} = \max_{\pi \in \Pi}(r_{\pi} + \gamma P_{\pi}v_k), \quad k = 0,1,2,...$$

对任意初始值$v_0$，$v_k$会指数级收敛到$v^*$。这就是后续第4章"价值迭代算法"的数学根源。

有了唯一的$v^*$后，下一步是**如何从$v^*$得到最优策略$\pi^*$（定理3.5）**。核心是将"对策略$\pi$的最大化"转化为"对动作$a$的最大化"。

对于状态$s$，策略$\pi$的期望奖励与转移可以拆分为"每个动作的加权和"：

$$r_{\pi}(s) + \gamma (P_{\pi}v^*)(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \cdot q^*(s,a)$$

其中$q^*(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v^*(s')$是最优动作价值。

要最大化$\sum_{a} \pi(a|s)q^*(s,a)$（约束：$\sum_a \pi(a|s)=1$，$\pi(a|s) \geq 0$），根据"权重集中在最大项上"的数学性质，最优策略是**仅选择使$q^*(s,a)$最大的动作**。

对任意状态$s$，若$a^*(s) = \arg\max_{a \in \mathcal{A}} q^*(s,a)$，则最优策略为：

$$\pi^*(a|s) = \begin{cases} 1, & a = a^*(s) \\ 0, & a \neq a^*(s) \end{cases}$$

这就是**确定性贪心策略**：在每个状态，总是选择动作价值最大的动作。

接下来需要验证**$v^*$与$\pi^*$确实是最优的（定理3.4）**。即证明$v^* = v_{\pi^*} \geq v_{\pi}$对任意策略$\pi$成立。

由BOE的定义，对任意策略$\pi$，有：

$$v^* = \max_{\pi'} (r_{\pi'} + \gamma P_{\pi'}v^*) \geq r_{\pi} + \gamma P_{\pi}v^*$$

将$v^* \geq r_{\pi} + \gamma P_{\pi}v^*$与普通贝尔曼方程$v_{\pi} = r_{\pi} + \gamma P_{\pi}v_{\pi}$相减，得：

$$v^* - v_{\pi} \geq \gamma P_{\pi}(v^* - v_{\pi})$$

反复递推，可得：

$$v^* - v_{\pi} \geq \gamma^n P_{\pi}^n (v^* - v_{\pi}) \quad (n=1,2,...)$$

当$n \to \infty$时，因$\gamma < 1$且$P_{\pi}^n$是概率矩阵（元素≤1），$\gamma^n P_{\pi}^n (v^* - v_{\pi}) \to 0$，故$v^* - v_{\pi} \geq 0$（即$v^* \geq v_{\pi}$）。

同时，由于$v^* = r_{\pi^*} + \gamma P_{\pi^*}v^*$，而$v_{\pi^*}$也满足$v_{\pi^*} = r_{\pi^*} + \gamma P_{\pi^*}v_{\pi^*}$，又因普通贝尔曼方程的解唯一，故$v^* = v_{\pi^*}$。

**综上**：$v_{\pi^*} \geq v_{\pi}$对任意$\pi$成立，证明$\pi^*$是最优策略。

关于最优策略的附加性质：**最优策略可能不唯一**。如果存在多个动作$a_1, a_2, ..., a_k$在同一状态$s$下具有相同的最大$q^*(s,a)$，那么"选择$a_1$"、"选择$a_2$"甚至"随机选择这些动作"的策略，都能使$\sum_a \pi(a|s)q^*(s,a)$最大化，因此均为最优策略。但**必然存在确定性最优策略** —— 即使存在随机最优策略，确定性最优策略也一定存在。

还有一个有趣的性质（定理3.6）：**最优策略对奖励的仿射变换具有不变性**。如果将奖励$r$通过仿射变换变为$\alpha r + \beta$（$\alpha > 0$），新的最优状态价值$v' = \alpha v^* + \frac{\beta}{1 - \gamma} \mathbf{1}$，但最优策略保持不变。这意味着策略的"结构"（在哪些状态选哪些动作）不依赖于奖励的绝对大小，只依赖于奖励的相对大小。

### 3.5 Summary

本章的核心概念包括最优策略与最优状态价值：若某一策略的状态价值大于或等于所有其他策略的状态价值，则该策略为最优策略；最优策略对应的状态价值即为最优状态价值。

贝尔曼最优方程是分析最优策略与最优状态价值的核心工具。该方程是一种非线性方程，具有良好的压缩性 —— 其右侧函数是压缩映射。利用压缩映射定理可证明：贝尔曼最优方程的解唯一存在，且解对应于最优状态价值与最优策略。

完整的推导逻辑链为："解的存在性"（通过压缩映射定理确定有唯一的最优状态价值$v^*$）→"解的形式"（推导能最大化$v^*$的策略形式，即贪心策略）→"解的最优性"（验证该策略确实最优）→"解的性质"（补充最优策略的性质），形成了从BOE得到可落地最优策略的完整数学推导闭环。
