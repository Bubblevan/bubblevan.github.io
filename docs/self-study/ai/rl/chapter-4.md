---
id: rl-chapter-4
title: 第4章 价值迭代与策略迭代
sidebar_label: 第4章
---

import { NoteBlock } from '@site/src/components/HighlightBlock';

# Value Iteration and Policy Iteration

> **写在前面**：之前学习贝尔曼方程时确实有些囫囵吞枣，所以这一章我决定换个思路——**用大量例子来理解迭代算法**。通过具体的数值计算，你会看到算法是如何一步步从"瞎猜"走向"最优"的。至于 Chapter 2 和 3 的符号体系，后续再慢慢完善。

## Value iteration

价值迭代（Value Iteration）是强化学习中最直观的算法之一。它的核心思想非常简单：**不断更新每个状态的价值估计，直到找到最优策略**。让我们先看看算法是怎么工作的，然后再通过一个完整的例子来理解它。

### 算法描述

<NoteBlock title="Algorithm 4.1: Value iteration algorithm">

- **Initialization**: The probability models `p(r|s, a)` and `p(s′|s, a)` for all `(s, a)` are known.
- **Initial guess**: `v₀`
- **Goal**: Search for the optimal state value and an optimal policy for solving the Bellman optimality equation.

**While** `vₖ` has not converged in the sense that `‖vₖ − vₖ₋₁‖` is greater than a predefined small threshold, **for the kth iteration, do**:

  - **For every state** `s ∈ S`, **do**:
    - **For every action** `a ∈ A(s)`, **do**:
      - **q-value**: `qₖ(s, a) = ∑ᵣ p(r|s, a)r + γ ∑ₛ' p(s'|s, a)vₖ(s')`
      - **Maximum action value**: `a*ₖ(s) = arg maxₐ qₖ(s, a)`
      - **Policy update**: `πₖ₊₁(a|s) = 1` if `a = a*ₖ(s)`, and `πₖ₊₁(a|s) = 0` otherwise
      - **Value update**: `vₖ₊₁(s) = maxₐ qₖ(s, a)`

</NoteBlock>
### 通过例子理解价值迭代

理论说再多，不如看个例子。让我们用一个 **2×2 的网格世界**来完整走一遍价值迭代的过程。

![网格状态示意图](/img/rl/f4-2.png)

#### 第一步：理解环境设置

在开始计算之前，我们需要先搞清楚这个"游戏"的规则。想象你在一个 2×2 的网格中移动：

**状态空间**：网格是 **2 行 2 列**，对应 4 个状态：
- `s₁`（左上）
- `s₂`（右上）
- `s₃`（左下）
- `s₄`（右下）

**特殊区域**：
- **目标区**：<span style={{color: 'red'}}>**`s₄`**</span>（到达这里能拿到正奖励，就像游戏中的终点）
- **禁区/边界**：网格外的区域（走到网格外算"碰边界"，拿到负奖励，就像撞墙扣分）
- **状态转移是确定性的**：比如在 `s₁` 做"向右"动作（ `a₃`），一定能到 `s₃`（不会随机到其他状态）

#### 第二步：理解动作空间

每个状态有 5 个可选动作（`a₁` 到 `a₅`），每个动作对应"向某个方向移动"或"停留"，比如：
- `a₁`：是"向左"（容易碰边界，q 值常为 <span style={{color: 'red'}}>**-1**</span>）
- `a₅`："停留"（不移动，q 值依赖当前状态的 v 值）

**关键点**：每个动作的效果是固定的（比如 `s₂` 的 `a₃` 动作一定到 `s₄`）。这简化了我们的计算，因为不需要考虑随机性。

#### 第三步：理解奖励机制（<span style={{color: 'red'}}>**这是理解 q 值的关键！**</span>）

奖励规则决定了算法会学到什么策略。在这个例子中：

- **碰边界/进禁区**：奖励 `r_boundary = r_forbidden =` <span style={{color: 'red'}}>**-1**</span>（比如 `s₁` 向左走，出网格，拿 -1）
- **到目标区 `s₄`**：奖励 `r_target =` <span style={{color: 'red'}}>**1**</span>（比如 `s₂` 做 `a₃` 到 `s₄`，拿 1）
- **其他情况**（在网格内移动但没到目标）：奖励看动作是否碰边界（比如 `s₁` 向右到 `s₃`，没碰边界，奖励 0）


#### 第四步：理解折扣率

折扣率 <span style={{color: 'red'}}>**`γ = 0.9`**</span> 用来计算"未来奖励的现值"。简单理解：**未来的奖励不如现在的奖励值钱**。

举个例子：
- "现在拿 1 分" = 1 分
- "下一步拿 1 分" = `0.9 × 1 = 0.9` 分（打了 9 折）
- "下下步拿 1 分" = `0.9² × 1 = 0.81` 分（打了 8.1 折）

#### 第五步：q 值和 v 值

- **q 值（动作价值）**：全称"动作价值"，含义是"在状态 `s` 下做动作 `a`，未来能拿到的总奖励期望"
  - 公式简化（示例是确定性的，不用概率求和）：`q(s,a) = 当前动作的即时奖励 + γ × 动作后到达状态的v值`

- **v 值（状态价值）**：全称"状态价值"，含义是"在状态 `s` 下，未来能拿到的最大总奖励期望"（即选所有动作里 q 值最大的那个）
  - **直观理解**：v 值回答"这个状态本身有多值钱？"

- **迭代逻辑**：用当前 v 值算 q 值 → 选 q 最大的动作（更新策略）→ 用最大 q 值更新 v 值 → 重复，直到 v 值不再变或策略最优

![The expression of q(s, a) for example above](/img/rl/f4-1.png)

### 开始迭代：k=0（第一次计算，从"瞎猜"开始）

好，现在让我们开始第一次迭代。价值迭代的起点是"**初始 v 值猜测**"——你可以随便猜，但为了简单，我们选择最简单的：所有状态的初始 <span style={{color: 'red'}}>**`v₀ = 0`**</span>。

> **为什么从 0 开始？** 这相当于"一开始假设每个状态未来都拿不到奖励"，这是一个中性的起点，算法会通过迭代逐步修正这个估计。

现在，我们需要根据这个初始猜测来计算所有状态-动作对的 q 值。计算依据是表 4.1 的表达式，把 `v₀(s₁) = v₀(s₂) = v₀(s₃) = v₀(s₄) = 0` 和 <span style={{color: 'red'}}>**`γ = 0.9`**</span> 代入，逐个状态、逐个动作算，这样就得到了下表：

![The value of q(s, a) at k = 0](/img/rl/f4-3.png)


#### 更新策略 `π₁`：选"当前看起来最赚的动作"

有了 q 值表，我们就可以更新策略了。策略的核心是"**每个状态选 q 值最大的动作**"（贪心策略，因为选当前看起来最赚的）：

- `s₁`：最大 q 值是 <span style={{color: 'red'}}>**0**</span>（a₃ 或 a₅，示例选 a₅）→ `π₁(a₅|s₁) = 1`（其他动作选 0）
- `s₂`：最大 q 值是 <span style={{color: 'red'}}>**1**</span>（a₃）→ `π₁(a₃|s₂) = 1`
- `s₃`：最大 q 值是 <span style={{color: 'red'}}>**1**</span>（a₂）→ `π₁(a₂|s₃) = 1`
- `s₄`：最大 q 值是 <span style={{color: 'red'}}>**1**</span>（a₅）→ `π₁(a₅|s₄) = 1`

> `s₁` 选 a₅（停留）其实不好 —— 因为停留不能到目标区，所以 `π₁` 不是最优策略，需要继续迭代。这就是为什么我们需要多轮迭代的原因。

#### 更新 v 值到 `v₁`：每个状态的"最大奖励期望"

每个状态的 `v₁`，就是该状态下所有动作的最大 q 值（因为 v 值是"未来最大总奖励"）：

- `s₁`：最大 q 值是 <span style={{color: 'red'}}>**0**</span> → `v₁(s₁) = 0`
- `s₂`：最大 q 值是 <span style={{color: 'red'}}>**1**</span> → `v₁(s₂) = 1`
- `s₃`：最大 q 值是 <span style={{color: 'red'}}>**1**</span> → `v₁(s₃) = 1`
- `s₄`：最大 q 值是 <span style={{color: 'red'}}>**1**</span> → `v₁(s₄) = 1`

> 到这里，k=0 的迭代结束，我们得到了"中间 v 值 `v₁`"和"临时策略 `π₁`"。注意 `v₁` 比 `v₀` 更准确了（至少 `s₂`、`s₃`、`s₄` 的价值不再是 0），但还不够好。

### 继续迭代：k=1（用更准确的 v 值，找到最优策略）

现在进入第二轮迭代。这一轮的计算基础是上一轮的 `v₁`（**不是初始 `v₀` 了！**），步骤和 k=0 完全一样：先算 q 值，再更新策略，最后更新 `v₂`。

> 每一轮迭代都在用"更准确的 v 值"来计算 q 值，所以策略会越来越接近最优。

#### 计算 k=1 时的所有 q 值

还是用表 4.1 的表达式，但这次代入的是更新后的 v 值：`v₁(s₁) = 0`、`v₁(s₂) = 1`、`v₁(s₃) = 1`、`v₁(s₄) = 1`、<span style={{color: 'red'}}>**`γ = 0.9`**</span>，逐个算：

![The value of q(s, a) at k = 1](/img/rl/f4-4.png)

> 对比 k=0 和 k=1 的 q 值表，你会发现 q 值变得更准确了。这是因为我们用了更准确的 v 值来计算。

#### 更新策略 `π₂`：<span style={{color: 'red'}}>**找到最优策略！**</span>

还是选每个状态 q 值最大的动作：
- `s₁`：最大 q 值是 <span style={{color: 'red'}}>**0.9**</span>（a₃）→ `π₂(a₃|s₁) = 1`
- `s₂`：最大 q 值是 <span style={{color: 'red'}}>**1.9**</span>（a₃）→ `π₂(a₃|s₂) = 1`
- `s₃`：最大 q 值是 <span style={{color: 'red'}}>**1.9**</span>（a₂）→ `π₂(a₂|s₃) = 1`
- `s₄`：最大 q 值是 <span style={{color: 'red'}}>**1.9**</span>（a₅）→ `π₂(a₅|s₄) = 1`

现在让我们验证这个策略为什么"最优"：
- `s₁` 选 a₃ → 到 `s₃`
- `s₃` 选 a₂ → 到 `s₄`（目标区，拿 <span style={{color: 'red'}}>**1**</span> 奖励）
- `s₂` 选 a₃ → 直接到 `s₄`
- `s₄` 选 a₅ → 停留（持续拿 <span style={{color: 'red'}}>**1**</span> 奖励）

> 所有状态的动作都能"最快拿到正奖励，不碰负奖励"，所以 `π₂` 是最优策略。注意 `s₁` 的策略从"停留"改成了"向右移动"，这就是迭代的威力！

#### 更新 v 值到 `v₂`：验证收敛趋势

每个状态的 `v₂` 是该状态的最大 q 值：
- `s₁`：最大 q 值 <span style={{color: 'red'}}>**0.9**</span> → `v₂(s₁) = 0.9`
- `s₂`：最大 q 值 <span style={{color: 'red'}}>**1.9**</span> → `v₂(s₂) = 1.9`
- `s₃`：最大 q 值 <span style={{color: 'red'}}>**1.9**</span> → `v₂(s₃) = 1.9`
- `s₄`：最大 q 值 <span style={{color: 'red'}}>**1.9**</span> → `v₂(s₄) = 1.9`

> 到这里 k=1 迭代结束，因为已经得到最优策略，后续迭代（k=2、k=3…）v 值会逐渐收敛到"真实最优 v 值"，但策略不会再变了。这就是价值迭代的收敛性保证。

### 价值迭代的本质：三步骤循环

通过这个例子，我们可以看到 **Value iteration** 的本质是"从'瞎猜'到'最优'的迭代过程"，全程只有 **3 个重复动作**：

1. **猜 v 值**：初始猜 `v₀ = 0`，后续用"前一轮的最大 q 值"更新 v 值
2. **算 q 值**：用当前 v 值代入动作奖励公式，算每个"状态-动作"的未来奖励
3. **选动作**：每个状态选 q 值最大的动作，直到动作能导向目标（最优策略）

> 猜 v → 算 q → 选动作，循环直到收敛。就这么简单！本质上就是"**算术计算 + 贪心选择**"。
## Policy iteration：评估与改进的交替进行

与价值迭代不同，策略迭代采用了另一种思路：**先评估当前策略的好坏，再根据评估结果改进策略**。它不是直接求解贝尔曼最优方程，而是通过"评估-改进"的循环逐步逼近最优策略。

> 价值迭代是"边算边改"（每次迭代都更新策略），策略迭代是"先算清楚再改"（先完整评估策略，再改进）。

在深入算法细节之前，让我们先回答三个关键问题，这些问题的答案将帮助我们理解策略迭代为什么有效。

#### 问题 1：策略评估步骤中，如何计算 `v_πₖ`？

在第 2 章中，我们已介绍过两种求解式（4.3）所示贝尔曼方程的方法，此处简要回顾：

**方法 1：闭式解（Closed-Form Solution）**

`v_πₖ = (I - γP_πₖ)⁻¹ r_πₖ`

这种闭式解对理论分析有帮助，但实现效率较低 —— 因为计算矩阵的逆需要借助其他数值算法。

**方法 2：迭代算法（Iterative Algorithm）**

该方法易于实现，公式如下：

`v_πₖ^(j+1) = r_πₖ + γP_πₖ v_πₖ^(j)`, `j = 0, 1, 2, ...` (4.4)

其中，`v_πₖ^(j)` 表示 `v_πₖ` 的第 j 次估计值。从任意初始猜测值 `v_πₖ^(0)` 出发，可保证当 `j → ∞` 时，`v_πₖ^(j) → v_πₖ`（详细内容见第 2.7 节）。

#### 问题 2：策略改进步骤中，为何 `πₖ₊₁` 优于 `πₖ`？

如下所示，策略改进步骤确实能对给定策略进行优化：

**引理 4.1（策略改进引理，Policy Improvement Lemma）**

若 `πₖ₊₁ = arg max_π (r_π + γP_π v_πₖ)`，则 `v_πₖ₊₁ ≥ v_πₖ`。

此处的 `v_πₖ₊₁ ≥ v_πₖ` 表示：对所有状态 `s`，均有 `v_πₖ₊₁(s) ≥ v_πₖ(s)`。

**证明**：

因此，

```
v_πₖ - v_πₖ₊₁ ≤ γ² P_πₖ₊₁² (v_πₖ - v_πₖ₊₁) ≤ ... ≤ γⁿ P_πₖ₊₁ⁿ (v_πₖ - v_πₖ₊₁)
≤ lim_{n→∞} γⁿ P_πₖ₊₁ⁿ (v_πₖ - v_πₖ₊₁) = 0
```

该极限成立的原因在于：当 `n → ∞` 时，<span style={{color: 'red'}}>**`γⁿ → 0`**</span>（`γ` 为折扣率，通常满足 `0 ≤ γ < 1`）；且对任意 `n`，`P_πₖ₊₁ⁿ` 均为非负随机矩阵（随机矩阵指"所有元素非负且每行元素之和为 1"的矩阵）。

#### 问题 3：策略迭代算法为何最终能找到最优策略？

策略迭代算法会生成两个序列：

- **策略序列**：`{π₀, π₁, ..., πₖ, ...}`
- **状态值序列**：`{v_π₀, v_π₁, ..., v_πₖ, ...}`

设 `v*` 为最优状态值，则对所有 `k`，均有 `v_πₖ ≤ v*`。根据引理 4.1，策略会不断改进，因此可得：

`v_π₀ ≤ v_π₁ ≤ v_π₂ ≤ ... ≤ v_πₖ ≤ ... ≤ v*`

由于 `v_πₖ` 是单调递增序列，且始终以 `v*` 为上界，根据单调收敛定理 [12]（见附录 C），当 `k → ∞` 时，`v_πₖ` 会收敛到某一常数，记为 `v_∞`。

往往这里还需要一个 **Convergence of Policy Iteration** 来证明策略迭代的收敛性，即 `v_∞ = v*`。

不过数学的答辩已经吃够了，现在直接来看 **elementwise form of this algorithm** 吧：

### 算法描述

<NoteBlock title="Algorithm 4.2: Policy iteration algorithm">

- **Initialization**: The system model, `p(r|s, a)` and `p(s'|s, a)` for all `(s, a)`, is known. Initial guess `π₀`.
- **Goal**: Search for the optimal state value and an optimal policy.

**While** `v_πₖ` has not converged, **for the kth iteration, do**:

  - **Policy evaluation**:
    - **Initialization**: an arbitrary initial guess `v_πₖ^(0)`
    - **While** `v_πₖ^(j)` has not converged, **for the jth iteration, do**:
      - **For every state** `s ∈ S`, **do**:
        - `v_πₖ^(j+1)(s) = ∑_a πₖ(a|s) [∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)v_πₖ^(j)(s')]`

  - **Policy improvement**:
    - **For every state** `s ∈ S`, **do**:
      - **For every action** `a ∈ A`, **do**:
        - `q_πₖ(s, a) = ∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)v_πₖ(s')`
      - `a*ₖ(s) = arg max_a q_πₖ(s, a)`
      - `πₖ₊₁(a|s) = 1` if `a = a*ₖ(s)`, and `πₖ₊₁(a|s) = 0` otherwise

</NoteBlock>

### 通过例子理解策略迭代

为了更清楚地理解策略迭代，我们用一个更简单的例子：**两状态三动作**。

![两状态三动作示例示意图](/img/rl/f4-5.png)

#### 环境设置：理解状态和动作

**状态空间**：只有 2 个状态，记为 `s₁` 和 `s₂`。可以想象成"两个相邻的格子"：`s₁` 在左，`s₂` 在右（如图 4.3 所示），没有其他状态。

**动作空间**：`A = {a_ℓ, a₀, a_r}`，每个动作的效果是确定性的（做动作 A 就一定到状态 B，不会随机）：

- **`a_ℓ`**：向左移动
  - 若在 `s₁`（最左边）选 `a_ℓ`：会"碰边界"（左边没有格子了），所以只能留在 `s₁`
  - 若在 `s₂`（右边）选 `a_ℓ`：会从 `s₂` 移到 `s₁`
- **`a₀`**：停留不动
  - 不管在 `s₁` 还是 `s₂`，选 `a₀` 都留在当前状态
- **`a_r`**：向右移动
  - 若在 `s₂`（最右边）选 `a_r`：会"碰边界"，只能留在 `s₂`
  - 若在 `s₁`（左边）选 `a_r`：会从 `s₁` 移到 `s₂`（这是靠近目标的动作！）

#### 奖励规则（<span style={{color: 'red'}}>**核心！公式里的 "-1"、"1" 从这来**</span>）

奖励规则决定了策略的好坏。在这个例子中：

- **碰边界**（`s₁` 选 `a_ℓ`、`s₂` 选 `a_r`）：奖励 `r_boundary =` <span style={{color: 'red'}}>**-1**</span>（扣分）
- **到目标/靠近目标**（只有 `s₁` 选 `a_r` 时，会移到 `s₂`，而 `s₂` 是"靠近目标的状态"）：奖励 `r_target =` <span style={{color: 'red'}}>**1**</span>（加分）
- **其他动作**（`s₁/s₂` 选 `a₀`、`s₂` 选 `a_ℓ`）：没有额外奖励，记为 `r = 0`


### 策略迭代第一步：策略评估（算"笨策略"的真实价值）

现在让我们开始策略迭代。第一步是**策略评估**：搞清楚"用当前策略（`π₀`）玩，每个状态到底值多少分"。

> **为什么叫"笨策略"？** 因为初始策略 `π₀` 可能不是最优的（比如总是选 `a_ℓ` 向左走，容易碰边界），但我们需要先知道它到底有多"笨"，才能改进它。

具体来说，我们需要计算 `v_π₀(s₁)` 和 `v_π₀(s₂)`，这两个值代表"从该状态出发，用 `π₀` 玩，未来能拿到的总奖励期望"。

#### 推导策略评估的方程

让我们一步步推导出策略评估的方程。根据贝尔曼方程，状态价值 = 即时奖励 + 折扣 × 未来状态价值。

**（1）计算 `v_π₀(s₁)`（`s₁` 用 `π₀` 的价值）**

`π₀` 在 `s₁` 只选 `a_ℓ`（向左碰边界）：
- **即时奖励**：碰边界，得 `r =` <span style={{color: 'red'}}>**-1**</span>
- **动作后状态**：留在 `s₁`，所以未来价值是"折扣率 × `s₁` 的价值"（即 `γ × v_π₀(s₁)`）

根据"状态价值 = 即时奖励 + 折扣 × 未来状态价值"，可得：

`v_π₀(s₁) = -1 + γ × v_π₀(s₁)`

> 这个方程是自包含的（右边也包含 `v_π₀(s₁)`），我们需要解这个方程。

**（2）计算 `v_π₀(s₂)`（`s₂` 用 `π₀` 的价值）**

`π₀` 在 `s₂` 选 `a_ℓ`（向左移到 `s₁`）：
- **即时奖励**：没碰边界，得 `r = 0`
- **动作后状态**：到 `s₁`，所以未来价值是 `γ × v_π₀(s₁)`

因此方程为：

`v_π₀(s₂) = 0 + γ × v_π₀(s₁)`

> 这个方程依赖于 `v_π₀(s₁)`，所以我们需要先解出 `s₁` 的价值。

#### 解方程得到真实价值

解这两个方程（代入 `γ = 0.9`）：

- `v_π₀(s₁) =` <span style={{color: 'red'}}>**-10**</span>
- `v_π₀(s₂) = 0.9 × (-10) =` <span style={{color: 'red'}}>**-9**</span>

> 两个状态的价值都是负数！这说明策略 `π₀` 确实很"笨"——从任何状态出发，按这个策略行动，长期来看都会得到负的累积奖励。

从"初始猜测 `v_π₀^(0)(s₁) = v_π₀^(0)(s₂) = 0`"开始，逐步逼近真实值：

**（1）第 1 次迭代（j=1）**

```
v_π₀^(1)(s₁) = -1 + 0.9 × v_π₀^(0)(s₁) = -1 + 0.9 × 0 = -1
v_π₀^(1)(s₂) = 0 + 0.9 × v_π₀^(0)(s₁) = 0 + 0.9 × 0 = 0
```

**（2）第 2 次迭代（j=2）**

```
v_π₀^(2)(s₁) = -1 + 0.9 × v_π₀^(1)(s₁) = -1 + 0.9 × (-1) = -1.9
v_π₀^(2)(s₂) = 0 + 0.9 × v_π₀^(1)(s₁) = 0 + 0.9 × (-1) = -0.9
```

**（3）第 3 次迭代（j=3）**

```
v_π₀^(3)(s₁) = -1 + 0.9 × v_π₀^(2)(s₁) = -1 + 0.9 × (-1.9) = -2.71
v_π₀^(3)(s₂) = 0 + 0.9 × v_π₀^(2)(s₁) = 0 + 0.9 × (-1.9) = -1.71
```

**（4）迭代趋势**

继续算下去会发现：`v_π₀^(j)(s₁)` 逐渐靠近 <span style={{color: 'red'}}>**-10**</span>，`v_π₀^(j)(s₂)` 逐渐靠近 <span style={{color: 'red'}}>**-9**</span> —— 和我们解方程的结果一致！

这说明"策略评估的结果是对的"：用 `π₀` 玩，`s₁` 未来总奖励是 <span style={{color: 'red'}}>**-10 分**</span>，`s₂` 是 <span style={{color: 'red'}}>**-9 分**</span>（都扣分，因为策略笨）。

### 策略迭代第二步 —— 策略改进（把"笨策略"改成"好策略"）

策略改进的目的：基于策略评估的结果（`v_π₀(s₁) = -10`，`v_π₀(s₂) = -9`），给每个状态选"更赚的动作"，得到新策略 `π₁`。

**核心工具是 q 值**：q 值 = "在状态 `s` 选动作 `a` 的即时奖励 + 折扣 × 动作后状态的价值"，代表"选这个动作到底值多少分"。

#### 1. 先明确每个"状态-动作"对的 q 值公式（还是从规则来）

根据"动作效果 + 奖励规则"，我们先列出所有 3 个动作在 `s₁` 和 `s₂` 的 q 值表达式：

![The expression of q_πₖ(s, a)](/img/rl/f4-6.png)

带入上面的 <span style={{color: 'red'}}>**-10**</span> 和 <span style={{color: 'red'}}>**-9**</span> 得到：

| `q_π₀(s, a)` | `a_ℓ` | `a₀` | `a_r` |
| --- | --- | --- | --- |
| `s₁` | <span style={{color: 'red'}}>**-10**</span> | <span style={{color: 'red'}}>**-9**</span> | <span style={{color: 'red'}}>**-7.1**</span> |
| `s₂` | <span style={{color: 'red'}}>**-9**</span> | <span style={{color: 'red'}}>**-7.1**</span> | <span style={{color: 'red'}}>**-9.1**</span> |

对每个状态，选 q 值最大的动作（因为 q 值越大，未来总奖励越多）：

- **`s₁` 的 q 值**：<span style={{color: 'red'}}>**-10**</span>（`a_ℓ`） < <span style={{color: 'red'}}>**-9**</span>（`a₀`） < <span style={{color: 'red'}}>**-7.1**</span>（`a_r`）→ 选 `a_r`
- **`s₂` 的 q 值**：<span style={{color: 'red'}}>**-9.1**</span>（`a_r`） < <span style={{color: 'red'}}>**-9**</span>（`a_ℓ`） < <span style={{color: 'red'}}>**-7.1**</span>（`a₀`）→ 选 `a₀`

#### 2. 得到新策略 `π₁`（<span style={{color: 'red'}}>**这就是最优策略！**</span>）

- 在 `s₁`：一定选 `a_r`（`π₁(a_r|s₁) = 1`）→ 从 `s₁` 移到 `s₂`（靠近目标）
- 在 `s₂`：一定选 `a₀`（`π₁(a₀|s₂) = 1`）→ 留在 `s₂`（保持靠近目标，不碰边界）

### 复杂示例：5×5 网格

"复杂示例"（5×5 网格），核心是在"多状态、多禁区、目标明确"的场景下，展示策略如何从"随机混乱"逐步收敛到"最优"。相比两状态示例，它多了"禁区规避"和"远距离状态导航"，但逻辑依然是 **"策略评估→策略改进"的循环**。

![The evolution processes of the policies generated by the policy iteration algorithm](/img/rl/f4-7.png)

#### 1. 状态类型：3 种状态，功能不同

- **普通状态**：大部分格子，可正常移动
- **禁区**：部分格子（原文图 4.4 中标记为 -100），进入会扣重分，且无法停留（比如 (2,2)、(2,3)、(3,3) 等）
- **目标区**：1 个特定格子（原文中是 (4,3)），进入或停留会拿正奖励（目标是让智能体尽量靠近/停在这）

#### 2. 动作：4 个方向移动

每个状态的动作是"向四个方向移动"（或含停留），动作效果确定性：

- 若移动方向是"边界"（比如 (1,1) 向上）：碰边界，留在原状态，扣 <span style={{color: 'red'}}>**1 分**</span>
- 若移动方向是"禁区"（比如 (4,2) 向右到 (4,3) 是目标，(4,2) 向左到 (4,1) 是普通状态，(4,2) 向上到 (3,2) 是禁区）：进入禁区，扣 <span style={{color: 'red'}}>**10 分**</span>，且强制留在原状态
- 若移动方向是"普通状态/目标区"：正常移动，普通状态无即时奖励，目标区拿 <span style={{color: 'red'}}>**1 分**</span>

#### 3. 奖励规则（比两状态示例更细致）

- **碰边界**：`r_boundary =` <span style={{color: 'red'}}>**-1**</span>（轻微扣分）
- **进入禁区**：`r_forbidden =` <span style={{color: 'red'}}>**-10**</span>（严重扣分）
- **到达/停留目标区**：`r_target =` <span style={{color: 'red'}}>**1**</span>（正奖励）
- **普通状态间移动**：`r = 0`（无奖励）

#### 4. 初始策略：随机策略（"瞎选动作"）

初始策略 `π₀` 是"随机的"—— 每个状态下，智能体随机选择 4 个方向的动作，完全不考虑"是否碰边界、是否进禁区、是否靠近目标"。比如：在 (4,2)（目标区 (4,3) 左边），可能随机选"向左"（到普通状态）、"向右"（到目标区）、"向上"（到禁区），完全无规律。

#### 5. 折扣率：依然是 <span style={{color: 'red'}}>**`γ = 0.9`**</span>

未来奖励的"现值"计算规则不变：比如"现在拿 1 分" = 1 分，"下一步拿 1 分" = 0.9 分，"下下步拿 1 分" = 0.81 分，距离越远，奖励折扣越多。

### 观察现象

**现象 1：靠近目标的状态先找到最优策略**

**原因**：靠近目标的状态"一步就能拿到正奖励"，策略评估时它们的状态值很高，策略改进时很容易发现"选靠近目标的动作 q 值最大"。

**现象 2：状态值随距离目标的远近递减**

**原因**：状态值是"未来总奖励的期望"，远离目标的状态需要多走几步才能到目标，每一步的奖励都会被折扣（<span style={{color: 'red'}}>**`γ = 0.9`**</span>）。

多轮迭代后，最优策略从目标区向外扩散，最终覆盖全网格。

## Truncated Policy iteration

> **value iteration and policy iteration algorithms are two special cases of the truncated policy iteration algorithm**

首先，我们通过梳理两种算法的步骤来对其进行比较。

### 算法步骤对比

#### 策略迭代（Policy Iteration）

选择任意初始策略 `π₀`；在第 k 轮迭代中，执行以下两步：

**步骤 1：策略评估（PE，Policy Evaluation）**

已知当前策略 `πₖ`，通过求解以下贝尔曼方程得到该策略对应的状态值 `v_πₖ`：

`v_πₖ = r_πₖ + γP_πₖ v_πₖ`

**步骤 2：策略改进（PI，Policy Improvement）**

已知策略 `πₖ` 的状态值 `v_πₖ`，通过求解以下优化问题得到新策略 `πₖ₊₁`：

`πₖ₊₁ = arg max_π (r_π + γP_π v_πₖ)`

#### 价值迭代（Value Iteration）

选择任意初始状态值 `v₀`；在第 k 轮迭代中，执行以下两步：

**步骤 1：策略更新（PU，Policy Update）**

已知当前状态值 `vₖ`，通过求解以下优化问题得到新策略 `πₖ₊₁`：

`πₖ₊₁ = arg max_π (r_π + γP_π vₖ)`

**步骤 2：价值更新（VU，Value Update）**

已知新策略 `πₖ₊₁`，通过以下公式计算新的状态值 `vₖ₊₁`：

`vₖ₊₁ = r_πₖ₊₁ + γP_πₖ₊₁ vₖ`

### 算法流程表示

两种算法的上述步骤可直观表示为：

- **策略迭代**：`π₀ →[PE] v_π₀ →[PI] π₁ →[PE] v_π₁ →[PI] π₂ →[PE] v_π₂ →[PI] ...`
- **价值迭代**：`v₀ →[PU] π₁' →[VU] v₁ →[PU] π₂' →[VU] v₂ →[PU] ...`

可见，两种算法的流程极为相似。

### 价值计算步骤的差异分析

为进一步明确两种算法的差异，我们重点分析它们的"价值计算步骤"。具体而言，假设两种算法从相同的初始条件出发：`v₀ = v_π₀`（初始状态值 `v₀` 等于初始策略 `π₀` 的状态值 `v_π₀`），其流程对比如下表所示。

由于 `v₀ = v_π₀`，前三个步骤中两种算法的结果完全一致；**从第四步开始，两者出现差异**。

在第四步中：
- **价值迭代算法**执行的是一步计算：`v₁ = r_π₁ + γP_π₁ v₀`
- **策略迭代算法**则需要求解贝尔曼方程：`v_π₁ = r_π₁ + γP_π₁ v_π₁`，该过程需执行无限次迭代

若将第四步中"求解 `v_π₁ = r_π₁ + γP_π₁ v_π₁`"的迭代过程展开，差异便会一目了然。令 `v_π₁^(0) = v₀`（策略迭代中状态值的初始猜测等于价值迭代的初始状态值），则迭代过程如下：

![迭代过程](/img/rl/f4-9.png)

从上述过程可得出以下结论：

- 若仅执行 <span style={{color: 'red'}}>**1 次迭代**</span>，则 `v_π₁^(1)` 本质上就是价值迭代算法中计算的 `v₁`
- 若执行 <span style={{color: 'red'}}>**无限次迭代**</span>，则 `v_π₁^(∞)` 本质上就是策略迭代算法中计算的 `v_π₁`
- 若执行**有限次迭代**（迭代次数记为 `j_truncate`，即"截断迭代次数"），则这类算法被称为**截断策略迭代（Truncated Policy Iteration）**

称之为"截断"，是因为从 `j_truncate` 到 `∞` 的剩余迭代过程被终止。

由此可见，价值迭代与策略迭代可视为截断策略迭代的两种极端情况：
- **价值迭代**对应"截断迭代次数 <span style={{color: 'red'}}>**`j_truncate = 1`**</span>"（仅执行 1 次迭代便终止）
- **策略迭代**对应"截断迭代次数 <span style={{color: 'red'}}>**`j_truncate = ∞`**</span>"（执行无限次迭代直至收敛）

| 步骤 | Policy iteration algorithm | Value iteration algorithm | Comments |
| --- | --- | --- | --- |
| 1) Policy: | `π₀` | N/A |  |
| 2) Value: | `v_π₀ = r_π₀ + γP_π₀ v_π₀` | `v₀ = v_π₀` |  |
| 3) Policy: | `π₁ = arg max_π (r_π + γP_π v_π₀)` | `π₁ = arg max_π (r_π + γP_π v₀)` | The two policies are the same |
| 4) Value: | `v_π₁ = r_π₁ + γP_π₁ v_π₁` | `v₁ = r_π₁ + γP_π₁ v₀` | `v_π₁ ≥ v₁` since `v_π₁ ≥ v_π₀` |
| 5) Policy: | `π₂ = arg max_π (r_π + γP_π v_π₁)` | `π₂' = arg max_π (r_π + γP_π v₁)` |  |
| ... | ... | ... | ... |

### 算法描述

<NoteBlock title="Algorithm 4.3: Truncated policy iteration algorithm">

- **Initialization**: The probability models `p(r|s, a)` and `p(s'|s, a)` for all `(s, a)` are known. Initial guess `π₀`.
- **Goal**: Search for the optimal state value and an optimal policy.

**While** `vₖ` has not converged, **for the kth iteration, do**:

  - **Policy evaluation**:
    - **Initialization**: select the initial guess as `vₖ^(0) = vₖ₋₁`. The maximum number of iterations is set as `j_truncate`.
    - **While** `j < j_truncate`, **do**:
      - **For every state** `s ∈ S`, **do**:
        - `vₖ^(j+1)(s) = ∑_a πₖ(a|s) [∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)vₖ^(j)(s')]`
    - Set `vₖ = vₖ^(j_truncate)`

  - **Policy improvement**:
    - **For every state** `s ∈ S`, **do**:
      - **For every action** `a ∈ A(s)`, **do**:
        - `qₖ(s, a) = ∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)vₖ(s')`
      - `a*ₖ(s) = arg max_a qₖ(s, a)`
      - `πₖ₊₁(a|s) = 1` if `a = a*ₖ(s)`, and `πₖ₊₁(a|s) = 0` otherwise

</NoteBlock>

### 算法特点

简而言之，截断策略迭代算法与策略迭代算法完全相同，唯一区别在于它仅在策略评估步骤中执行**有限次迭代**。其实现细节总结于算法 4.3 中。

值得注意的是，算法中的 `vₖ` 与 `vₖ^(j)` 并非状态值，而是真实状态值的**近似值** —— 这是因为策略评估步骤仅执行了有限次迭代，未完全收敛到真实状态值。

![An illustration of the relationships between the value iteration, policy iteration, and truncated policy iteration algorithms](/img/rl/f4-8.png)

**与策略迭代相比**：它仅需在策略评估步骤中执行有限次迭代，无需等待状态值完全收敛，因此**计算效率更高**。

**与价值迭代相比**：它可通过在策略评估步骤中多执行几次迭代（而非仅 1 次），让近似状态值更接近真实值，从而**加快整体收敛速度**。

### 通俗理解：三种算法的类比

或者你也可以这样通俗的理解（用"学生考试"的类比）：

- **策略迭代**：给学生 `πₖ` <span style={{color: 'red'}}>**"无限次模拟考"**</span>（`j → ∞`），直到分数 `v_πₖ` 完全准确，再根据分数改进学习方法 → `πₖ₊₁`
- **截断策略迭代**：给学生 `πₖ` <span style={{color: 'red'}}>**"j 次模拟考"**</span>（`j` 是有限数），用 j 次考试的平均/最后一次分数 `vₖ` 近似真实水平，再改进方法 → `πₖ₊₁`
- **价值迭代**：先根据上学期分数 `vₖ₋₁`，给学生制定新学习方法 → `πₖ`，然后只让新方法的学生 <span style={{color: 'red'}}>**"考 1 次试"**</span>（`j = 1`），用这 1 次的分数 `vₖ` 当近似水平，再进入下一轮

显然，<span style={{color: 'red'}}>**"只考 1 次试"**</span> 是 <span style={{color: 'red'}}>**"考 j 次试"**</span> 的极端情况（`j = 1`）—— 这就是价值迭代与截断策略迭代的关系。
### 三种算法对比总结

| 算法名称 | 核心思想 | 迭代步骤 | 关键特点 |
| --- | --- | --- | --- |
| **价值迭代（Value Iteration）** | 直接求解贝尔曼最优方程（基于压缩映射定理） | 策略更新 → 价值更新 | 中间值 `vₖ` 非真实状态值，无需初始策略，仅需初始状态值 `v₀` |
| **策略迭代（Policy Iteration）** | 交替执行"评估当前策略价值"与"改进策略"，逐步提升策略性能 | 策略评估 → 策略改进 | 中间值 `v_πₖ` 是真实状态值，需初始策略 `π₀`，收敛速度快于价值迭代 |
| **截断策略迭代（Truncated PI）** | 对策略迭代的"策略评估"步骤进行有限次迭代截断，平衡效率与收敛速度 | 有限次策略评估 → 策略改进 | 中间值为近似状态值，是前两种算法的统一框架（`j_truncate = 1` 对应价值迭代，`j_truncate = ∞` 对应策略迭代） |

### 核心思想

三者均遵循**广义策略迭代（Generalized Policy Iteration）**思想 —— 即通过"价值更新"与"策略更新"的交互循环，逐步优化状态值与策略。这一思想是后续多数强化学习算法的核心框架。

三种算法均属于**基于模型（Model-Based）的动态规划算法**，需依赖系统模型（即已知状态转移概率 `p(s'|s,a)` 与奖励概率 `p(r|s,a)`）。
