---
id: rl-chapter-5
title: 第5章 蒙特卡洛方法
sidebar_label: 第5章
---

import { NoteBlock } from '@site/src/components/HighlightBlock';

# Monte Carlo Methods

> **写在前面**：在前面的章节中，我们学习了价值迭代和策略迭代，它们都假设我们**已知环境模型**（即状态转移概率 `p(s'|s,a)` 和奖励概率 `p(r|s,a)`）。但在很多实际场景中，我们**没有环境模型**，只有智能体与环境交互产生的**数据**（episode）。这一章我们要学习如何"**用数据替代模型**"。
> 
> 蒙特卡洛方法（Monte Carlo Methods）就是解决这类"无模型"（model-free）问题的经典方法。它的核心思想很简单：**通过收集大量的 episode 数据，用经验平均来估计动作价值，然后基于这些估计改进策略**。
> 
> 通过这一章的学习，你会看到：即使不知道环境的"内部机制"，我们也能通过"试错"和"经验总结"来学习最优策略。这就是蒙特卡洛方法的魅力所在。

> **核心思想**：If we do not have a model, we must have some data. If we do not have data, we must have a model. If we have neither, then we are not able to find optimal policies.
> 
> The "data" in reinforcement learning usually refers to the agent's interaction experiences with the environment

## 从有模型到无模型：为什么需要蒙特卡洛方法？

在深入算法细节之前，让我们先思考一个问题：**为什么我们需要无模型的方法？**

想象一下，你要训练一个机器人学习走路。在价值迭代和策略迭代中，你需要知道"如果机器人向前迈一步，它会以多大的概率摔倒"（这就是环境模型）。但在现实中，你可能根本不知道这个概率是多少，你只能让机器人**实际去走**，然后观察它是否摔倒。

这就是无模型强化学习的核心场景：**我们不知道环境的内部机制，但我们有大量的交互数据**。

蒙特卡洛方法就是利用这些数据来学习的方法。它的名字来源于著名的"蒙特卡洛赌场"——就像通过大量随机试验来估计概率一样，蒙特卡洛方法通过大量 episode 来估计动作价值。

## MC Basic

现在让我们看看如何将策略迭代"改造"成无模型版本。MC Basic 算法可以看作是策略迭代的"无模型版本"。

> **关键区别**：策略迭代需要"解方程"（贝尔曼方程），MC Basic 只需要"算平均值"（经验平均）。这就是为什么 MC 方法更实用 —— 因为它不需要知道环境的内部机制。

**核心思路**：既然我们不知道环境模型，那就用"经验"来替代"理论"。具体来说，我们通过收集大量的 episode，用这些 episode 的回报平均值来估计动作价值，而不是通过求解贝尔曼方程。

### 算法描述

让我们先看看算法的整体框架，然后再通过例子深入理解。

<NoteBlock title="MC Basic (a model-free variant of policy iteration)">

- **Initialization**: Initial guess `π₀`. Goal: Search for an optimal policy.
- **For the kth iteration** (`k = 0, 1, 2, ...`), **do**:
  - **For every state** `s ∈ S`, **do**:
    - **For every action** `a ∈ A(s)`, **do**:
      - Collect sufficiently many episodes starting from `(s, a)` by following `πₖ`
  - **Policy evaluation**:
    - `q_πₖ(s, a) ≈ qₖ(s, a) =` the average return of all the episodes starting from `(s, a)`
  - **Policy improvement**:
    - `a*ₖ(s) = arg max_a qₖ(s, a)`
    - `πₖ₊₁(a|s) = 1` if `a = a*ₖ(s)`, and `πₖ₊₁(a|s) = 0` otherwise

</NoteBlock>

> **关键理解**：与策略迭代不同，MC Basic 不需要求解贝尔曼方程，而是通过**收集 episode 数据**，用**经验平均**来估计动作价值。这就是"无模型"的含义。

### 通过例子理解 MC Basic：从经验中学习

理论说再多，不如看个例子。让我们通过一个具体例子来理解 MC Basic 算法是如何工作的。

> **学习策略**：通过这个例子，你会看到 MC 方法如何用"经验"来估计"理论值"。虽然估计可能不完美，但足够指导策略改进。

我们重点分析"状态 `s₁` 的 5 个动作（`a₁` 到 `a₅`）"，每个动作对应不同的 episode 轨迹，进而推导 Q 值。

![An example for illustrating the MC Basic algorithm](/img/rl/f5-1.png)
#### 1. 动作 `a₁`：循环在 `s₁`，拿负奖励

**步骤 1：确定 `a₁` 的 episode 轨迹**

从 `(s₁, a₁)` 出发，根据初始策略 `π₀`，动作 `a₁` 会让智能体一直停留在 `s₁`（即循环：`s₁ →[a₁] s₁ →[a₁] s₁ →[a₁] ...`）。

所以每一步的奖励 `R_{t+1} = R_{t+2} = ... = -1`（对应 `r_boundary = -1`）。

**步骤 2：写出回报 `G_t` 的表达式**

根据回报定义，从 `s₁` 选 `a₁` 的回报是：

`G_t = R_{t+1} + γ R_{t+2} + γ² R_{t+3} + ...`

代入每一步的奖励 `R = -1`，得到：

`G_t = (-1) + γ(-1) + γ²(-1) + γ³(-1) + ...`

**步骤 3：用等比级数求和算 Q 值**

这是一个"首项 `a = -1`，公比 `r = γ = 0.9`"的无穷等比级数。根据求和公式：

`q_{π₀}(s₁, a₁) = G_t = a/(1 - r) = (-1)/(1 - 0.9) = (-1)/0.1 =` <span style={{color: 'red'}}>**-10**</span>

#### 2. 动作 `a₂`：到达目标状态

从 `(s₁, a₂)` 出发，episode 轨迹为：`s₁ →[a₂] s₂ →[a₃] s₅ →[a₃] ...`（即智能体从 `s₁` 经 `a₂` 到 `s₂`，再经 `a₃` 到目标状态 `s₅`，之后在 `s₅` 中循环）。

该动作的动作值等于该 episode 的折扣回报，计算如下：

`q_{π₀}(s₁, a₂) = 0 + γ·0 + γ²·0 + γ³(1) + γ⁴(1) + ... = γ³/(1 - γ)`

#### 3. 动作 `a₃`：另一条到达目标的路径

从 `(s₁, a₃)` 出发，episode 轨迹为：`s₁ →[a₃] s₄ →[a₂] s₅ →[a₃] ...`（即智能体从 `s₁` 经 `a₃` 到 `s₄`，再经 `a₂` 到目标状态 `s₅`，之后在 `s₅` 中循环）。

该动作的动作值等于该 episode 的折扣回报，计算如下：

`q_{π₀}(s₁, a₃) = 0 + γ·0 + γ²·0 + γ³(1) + γ⁴(1) + ... = γ³/(1 - γ)`

#### 4. 动作 `a₄`：回到 `s₁` 并循环

从 `(s₁, a₄)` 出发，episode 轨迹为：`s₁ →[a₄] s₁ →[a₁] s₁ →[a₁] ...`（即智能体从 `s₁` 经 `a₄` 回到 `s₁`，之后经 `a₁` 在 `s₁` 中循环）。

该动作的动作值等于该 episode 的折扣回报，计算如下：

`q_{π₀}(s₁, a₄) = -1 + γ(-1) + γ²(-1) + ... = (-1)/(1 - γ)`

#### 5. 动作 `a₅`：延迟回到 `s₁` 并循环

从 `(s₁, a₅)` 出发，episode 轨迹为：`s₁ →[a₅] s₁ →[a₁] s₁ →[a₁] ...`（即智能体从 `s₁` 经 `a₅` 回到 `s₁`，之后经 `a₁` 在 `s₁` 中循环）。

该动作的动作值等于该 episode 的折扣回报，计算如下：

`q_{π₀}(s₁, a₅) = 0 + γ(-1) + γ²(-1) + ... = (-γ)/(1 - γ)`

#### 策略改进：选择最优动作

通过比较上述 5 个动作的动作值，我们发现：

`q_{π₀}(s₁, a₂) = q_{π₀}(s₁, a₃) = γ³/(1 - γ) > 0`

这两个动作值是最大的。因此，可得到新策略：

`π₁(a₂|s₁) = 1` 或 `π₁(a₃|s₁) = 1`

> **重要说明**：对于每个动作，理论上需要收集足够多且足够长的 episode，才能有效近似其动作值。但由于本示例中的"策略"与"环境模型"均为确定性的（即每次执行相同动作都会产生相同轨迹），多次运行也会生成完全一致的轨迹，因此每个动作值的估计仅需 1 个 episode 即可。

> **实际应用**：在真实场景中，环境通常是随机的，所以需要收集多个 episode 来平均，以减少随机性的影响。

### Episode 长度的影响：为什么需要足够长的轨迹？

通过上面的例子，我们已经理解了 MC Basic 的基本工作原理。但这里有一个关键问题：**episode 需要多长？**

在实际应用中，episode 的长度对算法的性能有重要影响。太短了，智能体来不及到达目标；太长了，计算成本又太高。让我们通过一个综合例子来理解这一点。

> **核心问题**：episode 太短，智能体来不及到达目标；episode 太长，计算成本高。如何找到平衡点？

![A comprehensive example: Episode length and sparse rewards](/img/rl/f5-2.png)

#### 1. Episode 长度对策略和值估计的影响

在深入分析之前，让我们先明确两个核心概念：

| 概念 | 定义（通俗版） | 示例中的具体表现（5.4.3） |
| --- | --- | --- |
| **step（步）** | 智能体与环境的"一次完整交互"：1. 智能体在当前状态 `s` 选动作 `a`；2. 环境给奖励 `r`，并切换到新状态 `s'`；这 3 个元素（`s → a → r → s'`）构成 1 个 step。 | 1 个 step = 智能体在网格某格（如 `s₁`）选动作（如 `a₂`）→ 环境给奖励（如 0）→ 走到新格子（如 `s₂`）。 |
| **episode（情节）** | 从"起始状态-动作对"开始，到"结束条件"为止的所有 step 的序列。（注：若环境无明确"结束"，会用"固定步数"作为 episode 的终止条件） | 示例中无明确结束（目标状态可循环停留），故用"100 万步"作为 1 个 episode 的长度 → 1 个 episode = 100 万个连续的 step。 |
首先我们将证明：**episode 长度对最终得到的最优策略有显著影响**。具体而言，上图展示了蒙特卡洛基础算法在不同 episode 长度下的最终结果：

- **当 episode 长度过短时**，无论是策略还是值估计都无法达到最优（见上图 (a)~(d)）。在"episode 长度 = 1"的极端情况下，只有与目标状态相邻的状态才有非零值，其他所有状态的值均为 0——这是因为 episode 太短，智能体无法到达目标状态或获得正奖励。
- **随着 episode 长度的增加**，策略和值估计会逐渐趋近于最优。

#### 2. Episode 长度与状态值的空间分布规律

随着 episode 长度增加，会出现一个有趣的空间分布规律：**距离目标状态越近的状态，越早出现非零值**。

**原因分析**：从任意状态出发，智能体需要至少一定步数才能到达目标状态并获得正奖励；若 episode 长度小于这个"最小必要步数"，则该状态的回报必然为 0，其估计值也会为 0。

在本示例中，episode 长度至少需要达到 <span style={{color: 'red'}}>**15**</span>——15 是从左下角状态出发到达目标状态所需的最小步数。

#### 3. Episode 长度的"非无限性"要求

尽管上述分析表明 episode 需要"足够长"，但**并非要求"无限长"**。如图所示，当 episode 长度 = 30 时，算法已能找到最优策略（尽管此时的值估计尚未完全最优）。

#### 4. 与稀疏奖励问题的关联

上述分析涉及一个重要的奖励设计问题——**稀疏奖励（sparse reward）**，即"只有到达目标状态才能获得正奖励"的场景。

**稀疏奖励的问题**：
- 稀疏奖励设置要求 episode 必须足够长（以确保智能体能到达目标）
- 但当状态空间较大时，这一要求很难满足
- 最终会导致算法的学习效率下降

**解决方案：设计非稀疏奖励**

解决稀疏奖励问题的一个简单方法是**设计非稀疏奖励**。例如，在上述网格世界示例中，我们可以重新设计奖励规则：让智能体到达"目标附近的状态"时也能获得少量正奖励。

这样一来，目标状态周围会形成一个"**吸引域（attractive field）**"，帮助智能体更轻松地找到目标。

> **实践建议**：在实际应用中，如果遇到稀疏奖励问题，可以考虑：
> 1. 设计中间奖励（如距离目标的负距离）
> 2. 使用奖励塑形（reward shaping）
> 3. 使用课程学习（curriculum learning）逐步增加难度

## MC Exploring Starts

通过前面的学习，我们已经掌握了 MC Basic 的基本原理。但你可能已经注意到一个问题：**MC Basic 的效率似乎不太高**。

确实，在实际应用中，我们不仅希望算法能找到最优策略，还希望它能**高效地利用样本**和**及时地更新策略**。让我们看看如何改进 MC Basic 算法。

> **改进方向**：我们可以从两个角度来提升效率：
> 1. **更高效地利用样本**：一段 episode 可能访问多个状态-动作对，如何充分利用这些信息？
> 2. **更及时地更新策略**：是否必须等到所有 episode 收集完毕才能更新？

### 更高效地利用样本：一段 episode 的多种用法

让我们先思考一个问题：**一段 episode 只能用来估计一个状态-动作对吗？**

答案是否定的。蒙特卡洛（MC）强化学习的一个重要方面，就是如何更高效地利用样本。具体来说，假设我们遵循某一策略 `π`，得到了如下一段 episode 的样本：

`s₁ →[a₂] s₂ →[a₄] s₁ →[a₂] s₂ →[a₃] s₅ →[a₁] ...` (5.3)

其中，下标代表状态或动作的索引，而非时间步。

> **关键概念**：在一段 episode 中，某一状态-动作对每出现一次，就称为对该状态-动作对的一次"**访问（visit）**"。

**观察**：这段 episode 不仅访问了起始状态-动作对 `(s₁, a₂)`，还访问了 `(s₂, a₄)`、`(s₂, a₃)`、`(s₅, a₁)` 等多个其他状态-动作对。我们能否利用这些信息来估计更多状态-动作对的动作值呢？

#### 访问策略：三种不同的样本利用方式

我们可以采用不同的策略来利用这些访问信息。在深入细节之前，让我们先明确三种访问策略的定义：

> **Q：什么是初始访问策略（initial-visit）、首次访问策略（first-visit）和每次访问策略（every-visit）？**
> 
> **A：** 它们是利用 episode 中样本的不同策略。一段 episode 可能会访问多个状态-动作对，三种策略的定义如下：
> 
> - **初始访问策略**：仅利用整个 episode 来估计"episode 起始状态-动作对"的动作值
> - **每次访问策略**：每当某个状态-动作对在 episode 中被访问时，就利用该访问之后的 episode 轨迹来估计该状态-动作对的动作值，能充分利用样本
> - **首次访问策略**：仅在某个状态-动作对"第一次被访问"时，利用该访问之后的 episode 轨迹来估计其动作值，同样能高效利用样本

**第一种策略：初始访问（initial visit）**

即一段 episode 仅用于估计该 episode 起始的状态-动作对的动作值。以式（5.3）的 episode 为例，初始访问策略仅会估计状态-动作对 `(s₁, a₂)` 的动作值。

蒙特卡洛基础算法（MC Basic）采用的就是初始访问策略。然而，这种策略的样本效率较低 —— 因为该 episode 还访问了 `(s₂, a₄)`、`(s₂, a₃)`、`(s₅, a₁)` 等多个其他状态-动作对，这些访问信息同样可用于估计对应状态-动作对的动作值。

**第二种策略：每次访问（every-visit）**

我们可以将式（5.3）的 episode 分解为多个"**子情节（subepisode）**"：

![子情节分解示意图](/img/rl/f5-3.png)

某一状态-动作对被访问后所生成的轨迹，可视为一段新的 episode。这些新 episode 能用于估计更多状态-动作对的动作值，从而更高效地利用原始 episode 中的样本。

**第三种策略：首次访问（first-visit）**

此外，某一状态-动作对在一段 episode 中可能被访问多次。例如，在式（5.3）的 episode 中，状态-动作对 `(s₁, a₂)` 就被访问了两次。

- **首次访问策略（first-visit strategy）**：若仅统计该状态-动作对在 episode 中的"第一次访问"
- **每次访问策略（every-visit strategy）**：若统计该状态-动作对在 episode 中的"每一次访问"

> **效率对比**：从样本利用效率来看，每次访问策略是最优的。若一段 episode 足够长，能多次访问所有状态-动作对，那么仅通过这一段 episode，采用每次访问策略就可能完成对所有状态-动作对动作值的估计。

> **注意事项**：每次访问策略所获取的样本存在相关性 —— 因为从某状态-动作对"第二次访问"开始的轨迹，本质是其"第一次访问"起始轨迹的子集。但如果两次访问在轨迹中相距较远，这种相关性会较弱，对估计结果的影响也有限。

### 更高效地更新策略：何时更新？

现在让我们思考另一个问题：**我们必须在收集完所有 episode 之后才能更新策略吗？**

蒙特卡洛强化学习的另一个重要方面，是"**何时更新策略**"。目前有两种可用策略：

**第一种策略：批量更新**

在策略评估步骤中，先收集所有从"同一状态-动作对"起始的 episode，再利用这些 episode 的回报平均值来近似该状态-动作对的动作值。

蒙特卡洛基础算法（MC Basic）采用的就是这种策略。其缺点在于，智能体必须等到所有 episode 都收集完毕后，才能更新动作值的估计结果。

> ⚠️ **问题**：这意味着我们需要等待很长时间才能看到算法的改进，这在实时学习中是不可接受的。

**第二种策略：逐情节更新**

可克服上述缺点，即利用"单段 episode 的回报"来近似对应状态-动作对的动作值。这样一来，每获取一段 episode，就能立即得到一个粗略的动作值估计，进而以"**逐情节（episode-by-episode）**"的方式更新策略。

> **疑问解答**：有人可能会疑问：单段 episode 的回报无法准确近似动作值，这种策略是否可行？事实上，该策略属于上一章介绍的"**广义策略迭代（generalized policy iteration）**"范畴 —— 即便动作值的估计不够精确，我们仍然可以更新策略。关键在于：**不完美的估计 + 及时的更新 > 完美的估计 + 延迟的更新**。

### MC Exploring Starts 算法：综合两种改进

综合上述两种技术（高效样本利用和高效策略更新），我们可以得到**蒙特卡洛探索起点算法（MC Exploring Starts）**。

> **改进总结**：MC Exploring Starts = MC Basic + 每次访问策略 + 逐情节更新。这样既提高了样本利用效率，又加快了策略更新速度。

<NoteBlock title="MC Exploring Starts (an efficient variant of MC Basic)">

- **Initialization**: Initial policy `π₀(a|s)` and initial value `q(s, a)` for all `(s, a)`. `Returns(s, a) = 0` and `Num(s, a) = 0` for all `(s, a)`.
- **Goal**: Search for an optimal policy.
- **For each episode, do**:
  - **Episode generation**: Select a starting state-action pair `(s₀, a₀)` and ensure that all pairs can be possibly selected (this is the exploring-starts condition). Following the current policy, generate an episode of length `T`: `s₀, a₀, r₁, ..., s_{T-1}, a_{T-1}, r_T`.
  - **Initialization for each episode**: `g ← 0`
  - **For each step of the episode**, `t = T-1, T-2, ..., 0`, **do**:
    - `g ← γg + r_{t+1}`
    - `Returns(s_t, a_t) ← Returns(s_t, a_t) + g`
    - `Num(s_t, a_t) ← Num(s_t, a_t) + 1`
  - **Policy evaluation**:
    - `q(s_t, a_t) ← Returns(s_t, a_t) / Num(s_t, a_t)`
  - **Policy improvement**:
    - `π(a|s_t) = 1` if `a = arg max_a q(s_t, a)` and `π(a|s_t) = 0` otherwise

</NoteBlock>

#### 探索起点条件的要求与限制

"**探索起点条件**"要求：从每一个状态-动作对起始，都要生成足够多的 episode。根据大数定律，只有当所有状态-动作对都被充分探索时，我们才能准确估计它们的动作值，进而成功找到最优策略。

> 反之，若某一动作未被充分探索，其动作值的估计可能存在偏差 —— 即便该动作实际上是最优动作，也可能不被策略选中。

蒙特卡洛基础算法（MC Basic）和蒙特卡洛探索起点算法（MC Exploring Starts）均需满足探索起点条件。然而，在许多实际应用中（尤其是涉及与环境物理交互的场景），这一条件很难满足。

> **实际困难**：想象你在训练一个机器人，你无法随意"重置"到任意状态-动作对。你只能从环境的初始状态开始，这大大限制了探索的灵活性。

**那么，有没有办法移除这个限制呢？**

答案是肯定的。接下来，我们对蒙特卡洛探索起点算法（MC Exploring Starts）进行扩展，移除其"探索起点条件"。该条件本质上要求所有状态-动作对都能被充分访问，而这一要求也可通过"**软策略（soft policies）**"实现。

> **核心思路**：如果我们能让策略在任意状态下都有概率选择任意动作，那么即使从固定起点开始，只要 episode 足够长，我们也能访问到所有状态-动作对。

### ϵ-贪婪策略：实现软策略的经典方法

**软策略的定义**：若某一策略在"任意状态下选择任意动作的概率均为正"，则该策略称为软策略。

> **为什么叫"软策略"？** 因为它不是"硬性"地只选一个动作（概率为 1），而是"软性"地给所有动作都分配一定的概率。

考虑一种极端情况：我们仅有一段 episode。在软策略下，只要这段 episode 足够长，就能多次访问所有状态-动作对。因此，我们无需生成大量从不同状态-动作对起始的 episode，进而可移除"探索起点"的要求。

**那么，如何设计一个软策略呢？**

ϵ-贪婪策略就是实现软策略的经典方法。

**ϵ-贪婪策略的特点**：

ϵ-贪婪策略是一类常见的软策略。它是一种随机策略，其特点是：选择"贪婪动作"的概率更高，同时选择其他任意动作的概率均为非零且相等。其中，"**贪婪动作（greedy action）**"指的是"动作值最大的动作"。

**数学形式**：

具体来说，设参数 `ε ∈ [0, 1]`，对应的 ϵ-贪婪策略概率形式如下：

```
π(a|s) = {
  1 - (|A(s)| - 1)ε / |A(s)|,  若 a 为贪婪动作
  ε / |A(s)|,                   若 a 为其他 |A(s)| - 1 个动作
}
```

其中，`|A(s)|` 表示"状态 `s` 对应的动作数量"。

**特殊情况**：
- 当 `ε = 0` 时，ϵ-贪婪策略退化为贪婪策略（仅选择贪婪动作，概率为 1）
- 当 `ε = 1` 时，选择任意动作的概率均等于 `1/|A(s)|`（即均匀随机策略）

**概率性质**：

对任意 `ε ∈ [0, 1]`，选择贪婪动作的概率始终大于选择其他任意动作的概率，推导如下：

`1 - (|A(s)| - 1)ε / |A(s)| = 1 - ε + ε/|A(s)| ≥ ε/|A(s)|`

**实现方式**：

尽管 ϵ-贪婪策略是随机策略，但我们可通过以下方式遵循该策略选择动作：

1. 首先生成一个服从 `[0, 1]` 区间均匀分布的随机数 `x`
2. 若 `x ≥ ε`，则选择贪婪动作
3. 若 `x < ε`，则从状态 `s` 的动作集合 `A(s)` 中随机选择一个动作（选择概率均为 `1/|A(s)|`，可能再次选中贪婪动作）

通过这种方式，选择贪婪动作的总概率为 `1 - ε + ε/|A(s)|`，选择其他任意单个动作的概率均为 `ε/|A(s)|`。

### 将 ϵ-贪婪策略融入蒙特卡洛学习

现在我们已经理解了 ϵ-贪婪策略的原理，接下来要做的就是将它融入蒙特卡洛学习。

**核心思路**：要将 ϵ-贪婪策略融入蒙特卡洛学习，只需将"策略改进步骤"中的"贪婪策略"替换为"ϵ-贪婪策略"即可。

> **关键改变**：之前我们总是选择"最好的动作"（贪婪策略），现在我们要在"最好的动作"和"其他动作"之间做平衡（ϵ-贪婪策略）。

**原始策略改进**：

蒙特卡洛基础算法（MC Basic）或蒙特卡洛探索起点算法（MC Exploring Starts）中的策略改进步骤，目标是求解以下问题：

`π_{k+1}(s) = arg max_{π ∈ Π} ∑_a π(a|s) q_{π_k}(s, a)` (5.4)

其中，`Π` 表示"所有可能策略的集合"。我们已知式（5.4）的解是一个贪婪策略：

```
π_{k+1}(a|s) = {
  1,  若 a = a_k^*
  0,  若 a ≠ a_k^*
}
```

这里，`a_k^* = arg max_a q_{π_k}(s, a)`（即状态 `s` 下的贪婪动作）。

**修改后的策略改进**：

如今，我们将策略改进步骤修改为求解以下问题：

`π_{k+1}(s) = arg max_{π ∈ Π_ε} ∑_a π(a|s) q_{π_k}(s, a)` (5.5)

其中，`Π_ε` 表示"给定参数 `ε` 下，所有 ϵ-贪婪策略的集合"。通过这种修改，我们强制策略为 ϵ-贪婪策略。

式（5.5）的解为：

```
π_{k+1}(a|s) = {
  1 - (|A(s)| - 1)ε / |A(s)|,  若 a = a_k^*
  ε / |A(s)|,                   若 a ≠ a_k^*
}
```

这里，`a_k^* = arg max_a q_{π_k}(s, a)`。

经过上述修改，我们得到了一种新算法——**蒙特卡洛 ϵ-贪婪算法（MC ϵ-Greedy）**。

> **算法优势**：MC ϵ-Greedy 不需要探索起点条件，这意味着它可以在更实际的环境中应用。你只需要从环境的初始状态开始，让智能体按照 ϵ-贪婪策略行动，就能逐步学习到最优策略。

### 算法描述

让我们看看完整的算法流程：

<NoteBlock title="MC ϵ-Greedy (a variant of MC Exploring Starts)">

- **Initialization**: Initial policy `π₀(a|s)` and initial value `q(s, a)` for all `(s, a)`. `Returns(s, a) = 0` and `Num(s, a) = 0` for all `(s, a)`. `ε ∈ (0, 1]`
- **Goal**: Search for an optimal policy.
- **For each episode, do**:
  - **Episode generation**: Select a starting state-action pair `(s₀, a₀)` (the exploring starts condition is not required). Following the current policy, generate an episode of length `T`: `s₀, a₀, r₁, ..., s_{T-1}, a_{T-1}, r_T`.
  - **Initialization for each episode**: `g ← 0`
  - **For each step of the episode**, `t = T-1, T-2, ..., 0`, **do**:
    - `g ← γg + r_{t+1}`
    - `Returns(s_t, a_t) ← Returns(s_t, a_t) + g`
    - `Num(s_t, a_t) ← Num(s_t, a_t) + 1`
  - **Policy evaluation**:
    - `q(s_t, a_t) ← Returns(s_t, a_t) / Num(s_t, a_t)`
  - **Policy improvement**:
    - Let `a* = arg max_a q(s_t, a)` and
    - `π(a|s_t) = 1 - (|A(s_t)| - 1)ε / |A(s_t)|` if `a = a*`
    - `π(a|s_t) = ε / |A(s_t)|` if `a ≠ a*`

</NoteBlock>

![The evolution process of the MC ϵ-Greedy algorithm based on single episodes](/img/rl/f5-4.png)

### Step 级更新：理解 MC ϵ-Greedy 的细粒度更新机制

在深入理解算法之前，让我们先澄清一个常见的误解。

> **常见误解**：你以为"1 个 episode 对应 1 次策略更新"，但 MC ϵ-Greedy 的策略更新是"**逐 step 反向更新**" —— 1 个 episode 包含 N 个 step（示例中 N=100 万），这 N 个 step 会逐个参与策略更新，相当于 1 个 episode 里有 N 次"细粒度更新"，而不是 1 次"粗粒度更新"。

**为什么这样设计？** 因为每个 step 都包含了从该 step 到 episode 结束的所有信息，所以我们可以立即利用这些信息来更新策略，而不需要等到整个 episode 结束。

#### 为什么是反向更新？

这要结合 MC 算法的核心机制 —— 用"**回报（G）**"估计 Q 值，而回报需要从"当前 step 之后的所有奖励"计算。为了高效计算，MC ϵ-Greedy 会在 1 个 episode 结束后，从最后 1 个 step（T-1 步）反向遍历到第 1 个 step（0 步），每遍历 1 个 step 就做一次更新。

> **记忆技巧**：回报是从"当前时刻"到"未来"的所有奖励，所以从后往前算最方便 —— 先算最后一步的回报，再往前累加。

#### 具体操作流程

我们用示例中"1 个 100 万步的 episode"为例，拆解"step 级更新"的具体操作（对应算法 5.3 的核心步骤）：

**假设 1 个 episode 的 step 序列是**：

`s₀, a₀, r₁ → s₁, a₁, r₂ → s₂, a₂, r₃ → ... → s₉₉₉₉₉₈, a₉₉₉₉₉₈, r₉₉₉₉₉₉ → s₉₉₉₉₉₉, a₉₉₉₉₉₉, r₁₀₀₀₀₀₀`

（共 100 万步，记为 `t = 0` 到 `t = 999999`，最后一步是 `t = 999999`）

**策略更新的流程（反向遍历每个 step）**：

**初始化**：每个 episode 开始前，重置折扣回报 `g = 0`（用于累计当前 step 之后的所有折扣奖励）。

**从最后 1 步（`t = 999999`）反向算到第 1 步（`t = 0`）**：

- **对 `t = 999999`（最后 1 步）**：
  1. **算回报**：`g = γ·g + r₁₀₀₀₀₀₀`（此时 `g = 0`，故 `g = r₁₀₀₀₀₀₀`，即最后一步的奖励）
  2. **累积累计回报**：`Returns(s₉₉₉₉₉₉, a₉₉₉₉₉₉) += g`
  3. **累加访问次数**：`Num(s₉₉₉₉₉₉, a₉₉₉₉₉₉) += 1`
  4. **更新 Q 值**：`q(s₉₉₉₉₉₉, a₉₉₉₉₉₉) = Returns / Num`（用平均回报估计 Q 值）
  5. **更新策略**：根据当前 Q 值选贪婪动作 `a*`，按 ε-贪婪公式调整 `π(a|s₉₉₉₉₉₉)` 的概率

- **对 `t = 999998`（倒数第二步）**：
  1. **算回报**：`g = γ·g + r₉₉₉₉₉₉`（此时 `g` 已包含 `t = 999999` 步的折扣奖励，现在加上当前步的 `r₉₉₉₉₉₉`，就是 `t = 999998` 步之后的总折扣回报）
  2. **重复上述 2-5 的操作**，更新 `(s₉₉₉₉₉₈, a₉₉₉₉₉₈)` 的 `Returns`、`Num`、Q 值和策略

- **... 以此类推**，直到遍历完 `t = 0`（第一步），完成整个 episode 的更新。

#### 结论

> **核心要点**：1 个 episode（100 万步）里，每个 step 都会触发一次"Q 值更新 + 策略更新"，相当于 1 个 episode 包含 100 万次细粒度的策略调整，而不是 1 次更新。这就是 step 比 episode 细粒度更小的核心体现。

> **为什么这样设计？** 因为每个 step 的回报都包含了"从该 step 到 episode 结束"的所有信息，所以每处理一个 step，我们就能立即更新对应的 Q 值和策略，不需要等到整个 episode 结束。这样可以让算法更快地学习和适应。

## Exploration and exploitation of ϵ -greedy policies

通过前面的学习，我们已经掌握了 MC ϵ-Greedy 算法的基本流程。但你可能已经注意到一个关键参数：**ε 值**。这个参数控制着探索与利用的平衡，是算法性能的关键。

在深入分析之前，让我们先理解一个强化学习中的核心问题：**探索与利用的矛盾**。

在强化学习中，**探索（exploration）** 和 **利用（exploitation）** 是一对永恒的矛盾：

- **探索**：策略应尽可能尝试更多不同的动作，通过这种方式，所有动作都能被访问并得到充分评估
- **利用**：改进后的策略应选择"动作值最大的贪婪动作"

然而，由于当前时刻得到的动作值可能因探索不充分而存在偏差，因此我们在进行利用的同时必须保持探索，以避免错过潜在的最优动作。

> **核心矛盾**：如果只探索不利用，我们永远学不到最优策略；如果只利用不探索，我们可能陷入局部最优。

**那么，如何平衡探索和利用呢？**

这就是 ϵ-贪婪策略要解决的问题。

**ϵ-贪婪策略（ϵ-greedy policies）** 为平衡探索与利用提供了一种解决方案：

- **一方面**，ϵ-贪婪策略选择贪婪动作的概率更高，能够基于已估计的动作值进行"利用"
- **另一方面**，该策略也会以一定概率选择其他动作，从而维持"探索"能力

**利用与最优性的关系**：

利用与最优性密切相关 —— 因为最优策略本质上是贪婪策略（始终选择当前动作值最大的动作）。ϵ-贪婪策略的核心思想是"**通过牺牲部分最优性/利用效率来增强探索能力**"：

- 若想提升利用效率与策略最优性，需**减小 ε 值**
- 若想增强探索能力，则需**增大 ε 值**

> **直观理解**：ε 值就像探索的"温度计"—— ε 越大，探索越"热"（尝试更多动作）；ε 越小，探索越"冷"（更偏向利用已知的好动作）。

接下来，我们通过几个具体示例来分析这种权衡关系，看看不同的 ε 值会带来什么影响。

### ϵ-贪婪策略的最优性分析：ε 值如何影响性能？

让我们通过具体例子来看看 ε 值如何影响策略的最优性。

> **关键问题**：ε 值越大，探索能力越强，但这是否意味着性能越好？答案可能出乎你的意料。

![The state values of some ϵ-greedy policies](/img/rl/f5-5.png)

**图（a）**展示了"贪婪最优策略"及其对应的最优状态值；**图（b）-（d）**则展示了若干"一致 ε-贪婪策略"的状态值。

> **一致策略的定义**：若两个 ε-贪婪策略中"概率最高的动作"相同，则称这两个策略是"一致的"。

**观察结果**：

从图中可观察到：随着 ε 值增大，ε-贪婪策略的状态值逐渐降低，这表明其最优性在不断变差。尤其当 ε 增大至 <span style={{color: 'red'}}>**0.5**</span> 时，目标状态的状态值达到最小 —— 这是因为 ε 值越大，智能体从目标区域出发时"进入周围禁止区域"的概率越高，从而更易获得负奖励。

#### 最优 ε-贪婪策略 vs 贪婪最优策略

![The optimal ϵ-greedy policies and their corresponding state values under different values of ϵ](/img/rl/f5-6.png)

上图展示了"**最优 ε-贪婪策略**"（即在 ε-贪婪策略集合 `Π_ε` 中最优的策略）及其对应的状态值：

- **当 `ε = 0` 时**：该策略即为贪婪策略，且在所有策略中均最优
- **当 ε 较小时**（如 `ε = 0.1`）：最优 ε-贪婪策略与"贪婪最优策略"保持一致
- **但当 ε 增大到一定程度**（如 `ε = 0.2`）：得到的最优 ε-贪婪策略与"贪婪最优策略"不再一致

> 若想让最优 ε-贪婪策略与"贪婪最优策略"保持一致，ε 值需设置得足够小。

**为什么 ε 值较大时策略会不一致？**

我们可从目标状态的策略选择来解释：

- **在贪婪策略下**：目标状态的最优动作是"停留不动"，以持续获得正奖励
- **但当 ε 值较大时**："停留不动"仍有较高概率因探索而误入禁止区域、获得负奖励
- **结果**：此时，目标状态的最优动作会从"停留"变为"逃离"，导致策略一致性被打破

> **直观理解**：ε 值太大 → 探索太强 → 即使在目标区域也会随机走 → 容易走到禁区 → 不如主动"逃离"目标区域更安全 → 策略改变

### ϵ-贪婪策略的探索能力分析：如何选择 ε 值？

通过上面的分析，我们已经看到了 ε 值对策略最优性的影响。现在让我们从另一个角度来分析：**不同 ε 值下的探索能力如何？**

> **思考**：探索能力越强，是否意味着算法性能越好？答案取决于你的目标——是快速探索整个状态空间，还是快速收敛到最优策略？

![Exploration abilities of ϵ-greedy policies with different values of ϵ](/img/rl/f5-7.png)

#### ε = 1：最强探索（均匀随机策略）

考虑 `ε = 1` 的 ε-贪婪策略（如上图（a）所示）：此时策略在任意状态下选择任意动作的概率均为 `0.2`（均匀随机策略），探索能力最强。

**特点**：
- 由于探索能力极强，只要 episode 长度足够，单一段 episode 就能多次访问所有状态-动作对（见上图（c））
- 所有状态-动作对的访问次数几乎均匀分布

> **适用场景**：适合学习初期，需要快速探索整个状态空间。

#### ε = 0.5：中等探索

再考虑 `ε = 0.5` 的 ε-贪婪策略（如上图（d）所示）：尽管 episode 足够长时仍能访问所有动作，但各动作的访问次数分布极不均衡。

> **观察**：ε 值减小 → 探索能力下降 → 更偏向利用 → 访问次数集中在"看起来更好"的动作上。

#### 实践建议：动态调整 ε 值

在实际应用中，一种常用的策略是"**初始设置较大的 ε 值以增强探索，随后逐步减小 ε 值以保证最终策略的最优性**"—— 这种动态调整方式能在学习初期充分探索状态空间，后期则聚焦于利用已学习到的最优动作。

>  **记忆技巧**：ε 值 = 探索的"温度"—— 开始时"高温"（高探索），逐渐"降温"（高利用），最终"冷却"（最优策略）。

## Summary

通过这一章的学习，我们已经掌握了蒙特卡洛方法的核心思想和技术细节。让我们来总结一下关键要点。

### 核心思想

总结一下，**Monte Carlo estimation** 是一类广义的技术方法，指通过"随机样本"来解决近似问题的技术总称。

**无模型蒙特卡洛强化学习的核心思想**：将"有模型的策略迭代算法"转化为"无模型算法"。具体而言，策略迭代算法依赖系统模型计算值函数，而蒙特卡洛强化学习则将策略迭代中"基于模型的策略评估步骤"，替换为"无模型的蒙特卡洛策略评估步骤"。

> **核心转变**：从"知道环境如何工作"到"通过试错来学习"。这就是蒙特卡洛方法的革命性意义。

### 三种 MC 算法对比

| 算法名称 | 核心特点 | 主要改进 |
| --- | --- | --- |
| **蒙特卡洛基础算法（MC Basic）** | 最简单的基于蒙特卡洛的强化学习算法 | 将策略迭代算法中"基于模型的策略评估步骤"，替换为"无模型的蒙特卡洛估计模块"。在样本足够多的前提下，可保证该算法收敛到最优策略和最优状态值。 |
| **蒙特卡洛探索起点算法（MC Exploring Starts）** | MC Basic 的变体 | 通过在 MC Basic 中采用"首次访问策略（first-visit strategy）"或"每次访问策略（every-visit strategy）"，能更高效地利用样本。 |
| **蒙特卡洛 ε-贪婪算法（MC ε-Greedy）** | MC Exploring Starts 的变体 | 在"策略改进步骤"中，不再搜索"贪婪策略"，而是搜索"最优 ε-贪婪策略"。通过这种方式，策略的探索能力得到增强，从而可移除"探索起点"这一约束条件。 |

### 关键要点回顾

1. **无模型 vs 有模型**：MC 方法不需要环境模型，只需要 episode 数据
2. **样本效率**：每次访问策略比初始访问策略更高效
3. **更新频率**：逐情节更新比批量更新更及时
4. **探索与利用**：ε-贪婪策略通过调整 ε 值来平衡探索和利用
5. **Step 级更新**：MC ϵ-Greedy 采用反向遍历，每个 step 都会触发更新

> **记忆口诀**：MC = 无模型 + 经验平均 + 探索利用平衡。从 MC Basic → MC Exploring Starts → MC ϵ-Greedy，每一步都在提升算法的实用性和效率。
