<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – 博客</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in 博客 on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 10 Nov 2025 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>在 Docusaurus 3 里搭建内容生产流水线</title>
      <link>http://localhost:1313/blog/2025/2025-11-10-docusaurus-content-workflow/</link>
      <pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-10-docusaurus-content-workflow/</guid>
      <description>
        
        
        &lt;p&gt;作为一套以文档驱动为核心的前端框架，Docusaurus 3 天然把「内容生产」拆成两个入口：&lt;code&gt;docs&lt;/code&gt; 和 &lt;code&gt;blog&lt;/code&gt;。最近整理站点时，我也顺手理了一遍这套流程，写下来算是备忘。&lt;/p&gt;
&lt;!-- truncate --&gt;
&lt;p&gt;最初我完全没有博客的需求，因此从 &lt;code&gt;docs&lt;/code&gt; 入手，把课程笔记、强化学习章节放进 &lt;code&gt;docs/self-study/ai&lt;/code&gt; 一类的目录。&lt;/p&gt;
&lt;p&gt;每个 Markdown 或 MDX 文件都可以用 front matter 决定 &lt;code&gt;id&lt;/code&gt;、标题和侧边栏标签，然后通过 &lt;code&gt;sidebars.ts&lt;/code&gt; 统一组织结构。&lt;/p&gt;
&lt;p&gt;只要在某个类别下列出文档路径，访问 &lt;code&gt;/docs/...&lt;/code&gt; 时就能看到层级清晰的目录树。&lt;/p&gt;
&lt;p&gt;但是文档的内容太多太长了。如果想让一篇文档拆成多个小章节，除了常规的多文件方案，这里另一个常规解决方案是吧父级文件改成 &lt;code&gt;.mdx&lt;/code&gt;，像 React 组件一样 &lt;code&gt;import&lt;/code&gt; 子文档，再内联渲染，你就能在同一个页面里拼装章节，同时保留侧边栏导航。&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;import Chapter1 from &amp;#39;./rl/chapter-1.md&amp;#39;;
import Chapter2 from &amp;#39;./rl/chapter-2.md&amp;#39;;
import Chapter3 from &amp;#39;./rl/chapter-3.md&amp;#39;;

&amp;lt;Chapter1 /&amp;gt;

&amp;lt;Chapter2 /&amp;gt;

&amp;lt;Chapter3 /&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;写到这里就不可避免要碰 &lt;code&gt;docusaurus.config.ts&lt;/code&gt;。&lt;code&gt;classic&lt;/code&gt; 预设里 &lt;code&gt;docs&lt;/code&gt; 与 &lt;code&gt;blog&lt;/code&gt; 的配置都在这里：&lt;code&gt;routeBasePath&lt;/code&gt; 决定入口路径，&lt;code&gt;editUrl&lt;/code&gt; 可以连到 GitHub。&lt;/p&gt;
&lt;p&gt;想让数学公式在博客和文档里都生效，就把 &lt;code&gt;remark-math&lt;/code&gt;、&lt;code&gt;rehype-katex&lt;/code&gt; 同时塞进两个插件配置。&lt;/p&gt;
&lt;p&gt;导航栏是在 &lt;code&gt;themeConfig.navbar.items&lt;/code&gt; 里布置的，像 &lt;code&gt;type: &#39;docSidebar&#39;&lt;/code&gt; 这样的条目，可以把整个文档侧边栏挂到顶栏里；而 &lt;code&gt;to: &#39;/blog&#39;&lt;/code&gt; 则直接跳转到博客首页。&lt;/p&gt;
&lt;p&gt;说到博客，Docusaurus 会自动扫描 &lt;code&gt;./blog&lt;/code&gt; 目录，按照日期或 slug 生成路由。文章同样支持 Markdown、MDX，还有 &lt;code&gt;authors.yml&lt;/code&gt;、&lt;code&gt;tags.yml&lt;/code&gt; 管理作者与标签。比如这篇文章就使用 front matter 里的 &lt;code&gt;tags: [docusaurus, docs, blog]&lt;/code&gt;，最终会渲染成标签页链接。&lt;/p&gt;
&lt;p&gt;写作体验跟文档差不多，但因为博客默认暴露 RSS、Atom 订阅，还能在配置文件里打开阅读时长统计，所以我常把学习心得、踩坑记录放在这里。&lt;/p&gt;
&lt;p&gt;MDX 则是打通 React 组件和 Markdown 的桥梁：你可以引入现成的组件，也可以在 &lt;code&gt;src/components&lt;/code&gt; 写一个自定义卡片，然后在任意文档、博客中 &lt;code&gt;&amp;lt;Highlight&amp;gt;&lt;/code&gt; 一下。如果要引用别的 Markdown 片段，直接 &lt;code&gt;import Section from &#39;../foo/bar.md&#39;&lt;/code&gt;，接着 &lt;code&gt;&amp;lt;Section /&amp;gt;&lt;/code&gt;，Docusaurus 在构建时会内联处理。这在编写重复的导言、FAQ 时很好用，既不会复制粘贴，也不怕链接失效。&lt;/p&gt;
&lt;p&gt;管理多章节文档时，我越来越喜欢把大纲拆成子目录，再在 &lt;code&gt;_category_.json&lt;/code&gt; 里加入 &lt;code&gt;generated-index&lt;/code&gt;。这样一来，访问目录本身就能看到自动生成的索引页，还能写简介。必要时也能通过 &lt;code&gt;link&lt;/code&gt; 配置生成手写的 md 页面。随着文档越来越多，还可以在类别层级上应用 &lt;code&gt;collapsed: false&lt;/code&gt; 让关键模块默认展开，读者体验会好很多。&lt;/p&gt;
&lt;p&gt;最后补一笔 React 组件的故事。Docusaurus 的主题本质是 React 应用，所以自带的布局、&lt;code&gt;DocItem&lt;/code&gt; 等部件都能 override。只要在 &lt;code&gt;src/theme&lt;/code&gt; 下按需复制对应组件，就能自定义渲染逻辑，例如替换文档页眉、给博客加上分享按钮。我自己常把一些重复的提示条、总结卡片抽成小组件，既能保持风格统一，又方便在 MDX 中复用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>3D 场景理解工作</title>
      <link>http://localhost:1313/blog/2025/2025-11-19/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-19/</guid>
      <description>
        
        
        &lt;h1&gt;3D 场景理解工作&lt;/h1&gt;&lt;h2&gt;Depth Anything 3&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;depth-anything-3&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#depth-anything-3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ByteDance-Seed/depth-anything-3&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Depth Anything 3 (DA3)&lt;/strong&gt;&lt;/a&gt; 是字节跳动提出的 3D 视觉模型，核心目标是从&lt;strong&gt;任意数量&lt;/strong&gt;（单张 / 多张 / 视频流）、&lt;strong&gt;已知或未知相机姿态&lt;/strong&gt;的视觉输入中恢复空间一致的几何结构；其采用&lt;strong&gt;单 Transformer 骨干网络&lt;/strong&gt;（如 vanilla DINOv2 编码器）和&lt;strong&gt;深度-射线（depth-ray）表示&lt;/strong&gt;作为最小预测目标，避免复杂多任务学习，通过&lt;strong&gt;师生训练范式&lt;/strong&gt;（教师模型用合成数据生成高质量伪标签指导学生模型）实现与 DA2 相当的细节度和泛化性；同时构建了涵盖相机姿态估计、任意视图几何重建、视觉渲染的 &lt;strong&gt;Visual Geometry Benchmark&lt;/strong&gt;，在该基准上 DA3 刷新所有任务 SOTA，平均超越此前 SOTA 模型 VGGT &lt;strong&gt;35.7%&lt;/strong&gt; 的相机姿态精度和 &lt;strong&gt;23.6%&lt;/strong&gt; 的几何精度，且在单目深度估计任务上优于 DA2；此外，DA3 可通过微调扩展至前馈 &lt;strong&gt;3D 高斯 splatting（3DGS）&lt;/strong&gt; 等下游任务，为通用 3D 感知提供基础模型支持。&lt;/p&gt;
&lt;!-- truncate --&gt;
&lt;h3&gt;架构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;架构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9e%b6%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/da3-pipeline.png&#34; alt=&#34;DA3 Pipeline&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模块&lt;/th&gt;
          &lt;th&gt;核心设计&lt;/th&gt;
          &lt;th&gt;作用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;单 Transformer 骨干&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;基于预训练 ViT（如 DINOv2），分 L_s（视图内注意力）和 L_g（交替跨视图 / 视图内）层，L_s:L_g=2:1&lt;/td&gt;
          &lt;td&gt;继承预训练特征，支持任意视图数（单图自动降级为单目）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;相机条件注入&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;每个视图前缀相机 token：已知姿态→MLP 编码（E_c (f,q,t)）；未知→共享可学习 token&lt;/td&gt;
          &lt;td&gt;无缝处理有 / 无姿态输入，提供几何上下文&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;双 DPT 头（Dual-DPT）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;共享重组模块（Reassemble），独立融合层 + 输出层，分别预测深度和射线&lt;/td&gt;
          &lt;td&gt;保证两任务特征交互，避免中间表示冗余，提升预测一致性&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;深度图（Depth Map）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;深度图depth-map&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b7%b1%e5%ba%a6%e5%9b%bedepth-map&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在 &lt;strong&gt;Depth Anything 3（DA3）&lt;/strong&gt; 的 3D 几何重建框架中，&lt;strong&gt;深度图&lt;/strong&gt;是用于描述 &amp;ldquo;图像像素与相机之间物理距离&amp;rdquo; 的稠密矩阵，是构建 3D 结构的核心基础之一。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义与数学形式：&lt;/strong&gt; 深度图用符号 $D \in \mathbb{R}^{H \times W}$ 表示，其中 $H$ 和 $W$ 分别对应输入图像的高度和宽度，每个元素 $D(u,v)$ 代表图像中坐标 $(u,v)$ 的像素到相机光心的真实距离（单位通常为米），且与输入图像严格像素对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt; 深度图直接提供 &amp;ldquo;像素位置的远近信息&amp;rdquo;，是将 2D 图像映射到 3D 空间的关键桥梁 —— 结合相机姿态或射线信息，可通过 $P = t + D(u,v) \cdot d$（$t$ 为射线原点，$d$ 为射线方向）计算出该像素在世界坐标系中的 3D 坐标 $P$，进而生成完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DA3 中的特性：&lt;/strong&gt; DA3 通过 &lt;strong&gt;&amp;ldquo;师生训练范式&amp;rdquo;&lt;/strong&gt; 优化深度图质量 —— 教师模型（仅用合成数据训练）生成高质量伪深度标签，对齐真实场景的稀疏 / 噪声深度，确保深度图的细节完整性与几何一致性，避免近距区域（如物体表面）的距离预测偏差。&lt;/p&gt;
&lt;h3&gt;射线图（Ray Map）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;射线图ray-map&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%b0%84%e7%ba%bf%e5%9b%beray-map&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;射线图&lt;/strong&gt;是 DA3 提出的创新表示方式，用于隐式编码 &amp;ldquo;相机观测方向与位置&amp;rdquo;，避免直接预测相机姿态（如旋转矩阵、平移向量）的复杂约束，是实现 &lt;strong&gt;&amp;ldquo;最小建模&amp;rdquo;&lt;/strong&gt; 的关键设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义与数学形式：&lt;/strong&gt; 射线图用符号 $M \in \mathbb{R}^{H \times W \times 6}$ 表示，同样与输入图像像素对齐，6 个通道分为两组：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;前 3 通道&lt;/strong&gt;（$M(:,:,:3)$）：&lt;strong&gt;射线原点&lt;/strong&gt; $t \in \mathbb{R}^3$，代表该像素对应的相机光心在世界坐标系中的位置&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后 3 通道&lt;/strong&gt;（$M(:,:,3:)$）：&lt;strong&gt;射线方向&lt;/strong&gt; $d \in \mathbb{R}^3$，代表该像素从相机光心出发，指向世界空间目标点的单位向量（未归一化，保留投影尺度）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;规避姿态预测约束：&lt;/strong&gt; 传统方法需直接预测相机外参（旋转矩阵 $R$ 需满足正交性），而射线图通过 &amp;ldquo;像素级射线&amp;rdquo; 隐式包含姿态信息，简化建模&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反解相机参数：&lt;/strong&gt; 若需显式获取相机姿态，可从射线图推导 —— 相机中心 $t_c$ 为射线原点的平均值（$t_c = \frac{1}{H \times W} \sum M(h,w,:3)$），通过单应性矩阵 $H=KR$（DLT 算法求解）和 RQ 分解，进一步得到内参 $K$（上三角矩阵）和外参 $R$（正交矩阵）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成 3D 点云：&lt;/strong&gt; 与深度图协同，通过 $P = t + D(u, v) \cdot d$ 直接计算 3D 点，无需额外任务目标（如点云、相机姿态），实现 &amp;ldquo;最小预测目标&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;DA3 中的特性：&lt;/strong&gt; 射线图与深度图通过 &lt;strong&gt;&amp;ldquo;双 DPT 头&amp;rdquo;&lt;/strong&gt; 联合预测 —— 共享特征重组模块，独立融合层分别输出两者，确保射线方向与深度的空间一致性，避免多任务目标的纠缠（实验验证 &amp;ldquo;深度 + 射线&amp;rdquo; 组合的性能优于 &amp;ldquo;深度 + 点云 + 相机姿态&amp;rdquo; 等冗余方案）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;详细解释：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第一部分：射线原点（前 3 个通道）&lt;/strong&gt; 记录 &amp;ldquo;这条光线的起点&amp;rdquo;—— 也就是相机光心在 3D 空间中的位置（比如拍桌子时，手机所在的坐标：x=2 米、y=1 米、z=0.5 米）。所有像素的射线原点通常很接近（因为都来自同一台相机），所以射线图前 3 通道的数值差异很小，本质是 &amp;ldquo;相机位置的像素级体现&amp;rdquo;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第二部分：射线方向（后 3 个通道）&lt;/strong&gt; 记录 &amp;ldquo;这条光线的指向&amp;rdquo;—— 比如 &amp;ldquo;桌子角像素&amp;rdquo; 的光线指向是 &amp;ldquo;向右下方 15°&amp;quot;，&amp;ldquo;桌子边缘像素&amp;rdquo; 的光线指向是 &amp;ldquo;向左下方 10°&amp;quot;。用 3 个数值（x/y/z 方向分量）描述这个指向，比如（0.2, -0.3, 0.1）就代表 &amp;ldquo;在 x 轴正方向、y 轴负方向、z 轴正方向有一定延伸&amp;rdquo;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;为什么使用射线图？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;传统 3D 模型需要直接预测 &amp;ldquo;相机姿态&amp;rdquo;（比如相机朝哪个方向转、在哪个位置），但姿态计算有严格约束（比如旋转矩阵必须满足 &amp;ldquo;正交性&amp;rdquo;，算错一点就会严重偏差）。而射线图绕开了这个复杂问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它不直接算 &amp;ldquo;相机整体姿态&amp;rdquo;，而是给每个像素算 &amp;ldquo;专属光线&amp;rdquo;—— 这些光线天然包含了姿态信息（比如所有光线的原点平均下来就是相机中心，光线方向的规律就是相机朝向）&lt;/li&gt;
&lt;li&gt;后续要显式获取相机姿态时，只需从射线图反推（比如用文档 1-44-46 节的方法：原点求平均得相机中心，方向算单应性矩阵得内参 / 外参），无需在训练时额外优化姿态目标&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;点云&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;点云&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%82%b9%e4%ba%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;点云生成公式：&lt;/strong&gt; 3D 点坐标 = 射线原点 + 深度值 × 射线方向&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt; 比如某像素的射线原点是（2,1,0.5）、方向是（0.2,-0.3,0.1）、深度是 2 米，那么它的 3D 坐标就是（2+2×0.2, 1+2×(-0.3), 0.5+2×0.1）=（2.4, 0.4, 0.7）。所有像素的 3D 坐标拼起来，就是一张完整的点云（比如桌子的点云就是无数个 &amp;ldquo;桌子表面 3D 点&amp;rdquo; 组成的）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;点云评估指标：&lt;/strong&gt; 把生成的点云和 &amp;ldquo;真实点云&amp;rdquo;（比如用 LiDAR 扫描的精准点云）对比：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Chamfer 距离&lt;/strong&gt;：点云之间的平均距离，越小越准&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F1 分数&lt;/strong&gt;：点云的精度和召回率，越高越完整&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;实验&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;实验&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ae%9e%e9%aa%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;DA3 的实验核心围绕 &lt;strong&gt;&amp;ldquo;统一视觉几何基准&amp;rdquo;&lt;/strong&gt; 展开，覆盖 &lt;strong&gt;&amp;ldquo;相机姿态估计、几何重建、视觉渲染&amp;rdquo;&lt;/strong&gt; 三大任务，实验流程严格区分 &lt;strong&gt;&amp;ldquo;数据准备、任务执行、指标评估&amp;rdquo;&lt;/strong&gt; 三阶段。&lt;/p&gt;
&lt;h4&gt;（1）实验基础：构建 Visual Geometry Benchmark&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1实验基础构建-visual-geometry-benchmark&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1%e5%ae%9e%e9%aa%8c%e5%9f%ba%e7%a1%80%e6%9e%84%e5%bb%ba-visual-geometry-benchmark&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;为避免不同任务评估标准不统一的问题，DA3 首先建立了标准化基准，明确实验的 &lt;strong&gt;&amp;ldquo;输入 - 输出 - 评估&amp;rdquo;&lt;/strong&gt; 链路：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基准覆盖范围：&lt;/strong&gt; 包含 5 个场景多样化的数据集（共 89+ 场景），涵盖合成数据、真实 LiDAR 数据、低清噪声数据，确保实验泛化性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;合成数据：&lt;/strong&gt; HiRoom（29 个室内场景，Blender 渲染，用于验证几何细节）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真实 LiDAR 数据：&lt;/strong&gt; ETH3D（11 个室内外场景，激光雷达采集，高分辨率）、DTU（22 个物体级场景，控制条件下的真实点云）、ScanNet++（20 个室内场景，iPhone LiDAR + 激光重建深度）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;低清噪声数据：&lt;/strong&gt; 7Scenes（7 个室内场景，低分辨率 + 运动模糊，模拟真实复杂场景）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;基准流程：&lt;/strong&gt; 所有任务遵循 &lt;strong&gt;&amp;ldquo;输入→模型预测→结果对齐→指标计算&amp;rdquo;&lt;/strong&gt; 的统一逻辑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; 随机采样图像（若图像数超 100 张，固定随机种子采样 100 张，保证实验可复现）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测：&lt;/strong&gt; DA3 输出深度图、射线图、相机姿态（可选）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对齐：&lt;/strong&gt; 几何重建时用 RANSAC+evo 工具将预测姿态与真实姿态对齐，确保点云在同一坐标系&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估：&lt;/strong&gt; 按任务计算对应指标（姿态用 AUC3/AUC30，几何用 F1/Chamfer 距离，渲染用 PSNR/SSIM）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;（2）核心任务实验流程（以 &amp;ldquo;几何重建&amp;rdquo; 为例）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2核心任务实验流程以-几何重建-为例&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2%e6%a0%b8%e5%bf%83%e4%bb%bb%e5%8a%a1%e5%ae%9e%e9%aa%8c%e6%b5%81%e7%a8%8b%e4%bb%a5-%e5%87%a0%e4%bd%95%e9%87%8d%e5%bb%ba-%e4%b8%ba%e4%be%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;用户关心的 &lt;strong&gt;&amp;ldquo;图片流生成连续点云建模&amp;rdquo;&lt;/strong&gt;，正是 DA3 &lt;strong&gt;&amp;ldquo;多视图 / 视频几何重建&amp;rdquo;&lt;/strong&gt; 任务的核心，实验流程在论文 7.1 节和 6.1 节明确：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt; 支持 &lt;strong&gt;&amp;ldquo;图片流（视频帧）&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;多视图图像集&amp;rdquo;&lt;/strong&gt;（无数量限制，单帧即单目，多帧即多视图）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;点云生成逻辑：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型先为每张图像预测 &lt;strong&gt;&amp;ldquo;深度图 + 射线图&amp;rdquo;&lt;/strong&gt;（通过双 DPT 头联合输出，像素级对齐）&lt;/li&gt;
&lt;li&gt;用公式 $P = t + D(u,v) \cdot d$（$t$ 为射线原点，$d$ 为射线方向，$D$ 为深度）计算每个像素的 3D 坐标，生成单帧稀疏点云&lt;/li&gt;
&lt;li&gt;对图片流的连续帧，用 &lt;strong&gt;TSDF 融合&lt;/strong&gt;（Truncated Signed Distance Function）将多帧稀疏点云合并为稠密、连续的场景点云（不同数据集的 TSDF 参数不同，如 HiRoom 体素大小 0.007m，ETH3D 为 0.039m）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实验验证：&lt;/strong&gt; 论文 7.1 节用 ScanNet++、ETH3D 的图片流测试，&lt;strong&gt;DA3-Giant&lt;/strong&gt; 生成的连续点云在 F1 分数上超 VGGT &lt;strong&gt;23.6%&lt;/strong&gt;，且能保留桌角、墙面边缘等细粒度细节。&lt;/p&gt;
&lt;h4&gt;（1）评估阶段：用 &amp;ldquo;真实场景的标注数据&amp;rdquo; 作为 Ground Truth&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1评估阶段用-真实场景的标注数据-作为-ground-truth&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1%e8%af%84%e4%bc%b0%e9%98%b6%e6%ae%b5%e7%94%a8-%e7%9c%9f%e5%ae%9e%e5%9c%ba%e6%99%af%e7%9a%84%e6%a0%87%e6%b3%a8%e6%95%b0%e6%8d%ae-%e4%bd%9c%e4%b8%ba-ground-truth&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;实验中模型性能对比的是真实世界的物理测量结果，而非教师模型输出，具体来源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;姿态真值：&lt;/strong&gt; ETH3D、DTU 等数据集用 &lt;strong&gt;COLMAP&lt;/strong&gt;（基于特征匹配的 SfM 方法）** 或激光跟踪仪获取相机姿态&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;几何真值：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;ETH3D/ScanNet++：用 TSDF 融合 LiDAR 采集的稠密深度图，生成真实场景点云&lt;/li&gt;
&lt;li&gt;DTU：在实验室控制条件下，用高精度 3D 扫描仪获取物体的真实点云&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;渲染真值：&lt;/strong&gt; NVS 任务（视觉渲染）用 &lt;strong&gt;&amp;ldquo;真实场景的多视图图像&amp;rdquo;&lt;/strong&gt; 作为渲染目标真值（如 DL3DV 的测试帧）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;（2）训练阶段：用 &amp;ldquo;教师模型伪标签&amp;rdquo; 作为监督（非 Ground Truth）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2训练阶段用-教师模型伪标签-作为监督非-ground-truth&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5%e7%94%a8-%e6%95%99%e5%b8%88%e6%a8%a1%e5%9e%8b%e4%bc%aa%e6%a0%87%e7%ad%be-%e4%bd%9c%e4%b8%ba%e7%9b%91%e7%9d%a3%e9%9d%9e-ground-truth&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;教师模型的作用是 &lt;strong&gt;&amp;ldquo;为真实噪声数据生成高质量监督信号&amp;rdquo;&lt;/strong&gt;，而非作为评估的真值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实数据（如 ARKitScenes、WildRGBD）的深度标注常稀疏 / 噪声大，无法直接用&lt;/li&gt;
&lt;li&gt;教师模型（仅用合成数据训练）为这些真实图像生成 &lt;strong&gt;&amp;ldquo;伪深度图&amp;rdquo;&lt;/strong&gt;，再通过 RANSAC 对齐真实稀疏深度，得到 &lt;strong&gt;&amp;ldquo;干净的监督信号&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;论文 7.2.3 节验证：用教师伪标签训练的 DA3，深度图细节比无教师监督的版本丰富 &lt;strong&gt;30%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Navigation/Tracking 适配&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;navigationtracking-适配&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#navigationtracking-%e9%80%82%e9%85%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;DA3 适配机器人 SLAM 避障需解决 &lt;strong&gt;动态物体过滤、回环检测、语义融合&lt;/strong&gt; 三大核心问题。这些问题的本质是 &lt;strong&gt;&amp;ldquo;DA3 作为&amp;rsquo;几何重建模型&amp;rsquo;的功能边界，与机器人&amp;rsquo;自主环境交互&amp;rsquo;的实际需求之间的差距&amp;rdquo;&lt;/strong&gt;——DA3 仅负责输出高精度几何信息（深度、点云），但机器人避障需要 &lt;strong&gt;&amp;ldquo;干净的静态环境地图&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;长期建图的一致性&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;可解释的障碍决策依据&amp;rdquo;&lt;/strong&gt;，这三个适配问题正是填补这一差距的关键。以下结合你提供的新搜索资源（尤其是动态物体滤除、SLAM 实践相关内容），从 &lt;strong&gt;&amp;ldquo;问题本质、DA3 局限、解决方案、落地细节&amp;rdquo;&lt;/strong&gt; 四个维度详细拆解：&lt;/p&gt;
&lt;h4&gt;一、动态物体过滤：从 &amp;ldquo;无差别几何重建&amp;rdquo; 到 &amp;ldquo;精准静态环境提取&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一动态物体过滤从-无差别几何重建-到-精准静态环境提取&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%8a%a8%e6%80%81%e7%89%a9%e4%bd%93%e8%bf%87%e6%bb%a4%e4%bb%8e-%e6%97%a0%e5%b7%ae%e5%88%ab%e5%87%a0%e4%bd%95%e9%87%8d%e5%bb%ba-%e5%88%b0-%e7%b2%be%e5%87%86%e9%9d%99%e6%80%81%e7%8e%af%e5%a2%83%e6%8f%90%e5%8f%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 问题本质与影响&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人在真实场景（如商场、街道）中，环境充满动态物体（行人、移动车辆、开合的门）。DA3 会将这些动态物体与静态场景（地面、墙壁）一同重建为点云，导致两个严重问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;建图污染：&lt;/strong&gt; 动态物体的点云会被误判为静态障碍（如行人走过的区域，地图会残留 &amp;ldquo;人形障碍&amp;rdquo;），后续机器人再次经过时会误触发避障，无法通行&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;避障误判：&lt;/strong&gt; 若动态物体突然出现在路径上（如行人横穿），DA3 生成的点云包含该物体，但机器人无法区分 &lt;strong&gt;&amp;ldquo;静态障碍&amp;rdquo;&lt;/strong&gt; 和 &lt;strong&gt;&amp;ldquo;动态障碍&amp;rdquo;&lt;/strong&gt;，可能出现 &lt;strong&gt;&amp;ldquo;过度避障&amp;rdquo;&lt;/strong&gt;（避开缓慢移动的行人导致绕远）或 &lt;strong&gt;&amp;ldquo;避障不及时&amp;rdquo;&lt;/strong&gt;（未识别快速移动物体）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. DA3 当前的核心局限&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 的设计目标是 &lt;strong&gt;&amp;ldquo;恢复空间一致的几何结构&amp;rdquo;&lt;/strong&gt;，完全不区分 &lt;strong&gt;&amp;ldquo;静态&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;动态&amp;rdquo;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入图像中的动态物体，会被 DA3 当作 &lt;strong&gt;&amp;ldquo;场景几何的一部分&amp;rdquo;&lt;/strong&gt;，输出对应的深度图和射线图，进而生成动态物体的点云&lt;/li&gt;
&lt;li&gt;无任何时序动态检测模块（如光流跟踪、多帧运动一致性判断），无法从连续帧中识别 &lt;strong&gt;&amp;ldquo;运动的物体&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. 解决方案：结合 &amp;ldquo;几何 + 语义 + 时序&amp;rdquo; 多维度滤除，适配 DA3 输出&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据搜索资源（摘要 3：动态物体滤除算法、摘要 6：Dynablox 几何方法），可将 DA3 的输出与以下技术结合，实现动态物体精准滤除：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;技术路径&lt;/th&gt;
          &lt;th&gt;核心原理（结合 DA3）&lt;/th&gt;
          &lt;th&gt;优势与优化方向（来自搜索资源）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;2D 语义分割反投影&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. DA3 输出 &amp;ldquo;RGB 图像 + 深度图&amp;rdquo;；&lt;br&gt;&lt;/br&gt;2. 用语义分割模型（如 Mask R-CNN、DeepLab，摘要 3）分割 RGB 图像中的动态物体（如 &amp;ldquo;行人&amp;quot;&amp;ldquo;车辆&amp;rdquo;），得到动态区域掩码；&lt;br&gt;&lt;/br&gt;3. 将掩码反投影到 DA3 生成的 3D 点云中，滤除掩码对应的点（即动态物体点）。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 直接利用 DA3 的 RGB + 深度输出，无需额外传感器；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 结合时序一致性融合（摘要 3），对连续 3-5 帧的分割结果取交集，减少单帧误分割（如将静态停靠的车辆误判为动态）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;光流跟踪 + 运动一致性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. 对 DA3 处理的连续帧 RGB 图像，用光流模型（如 RAFT、FlowNet，摘要 3）计算像素运动向量；&lt;br&gt;&lt;/br&gt;2. 筛选运动向量大于阈值的像素（动态物体区域）；&lt;br&gt;&lt;/br&gt;3. 结合 DA3 的深度图，将动态像素反投影到点云滤除。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 无需训练语义模型，适用于 &amp;ldquo;未知动态物体&amp;rdquo;（如未见过的包裹）；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 补偿相机运动（Ego-motion）（摘要 3），用 DA3 估计的相机姿态（extrinsics）修正光流，避免将 &amp;ldquo;相机移动导致的静态物体运动&amp;rdquo; 误判为动态。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;纯几何时序检测（Dynablox）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. 用 DA3 生成的连续帧点云构建 TSDF 体素地图（摘要 6）；&lt;br&gt;&lt;/br&gt;2. 跟踪每个体素的 &amp;ldquo;占用状态时序&amp;rdquo;（如 &amp;ldquo;最近 10 帧是否持续被占用&amp;rdquo;）；&lt;br&gt;&lt;/br&gt;3. 若体素从 &amp;ldquo;空闲&amp;rdquo; 突然变为 &amp;ldquo;占用&amp;rdquo;（如行人闯入），标记为动态体素，滤除对应点云。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 无依赖外观，适用于黑暗、低纹理场景（DA3 深度图仍有效）；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 引入高置信度自由空间（摘要 6），通过激光射线穿透检测，排除 &amp;ldquo;静态物体遮挡导致的误判&amp;rdquo;（如行人挡住墙壁，墙壁仍为静态）。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;4. 落地示例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人在商场走廊移动时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 实时输出 640×480 分辨率的深度图 + RGB 图像（摘要 1、4 中 Jetson 平台配置）&lt;/li&gt;
&lt;li&gt;同步运行 Mask R-CNN 分割 RGB 图像，得到 &lt;strong&gt;&amp;ldquo;行人&amp;rdquo;&lt;/strong&gt; 掩码（动态区域）&lt;/li&gt;
&lt;li&gt;将掩码与 DA3 的深度图结合，计算动态区域的 3D 坐标范围，从点云中删除这些坐标的点&lt;/li&gt;
&lt;li&gt;同时用连续 5 帧的分割结果取交集，避免将 &lt;strong&gt;&amp;ldquo;短暂停留的行人&amp;rdquo;&lt;/strong&gt; 误判为静态障碍，最终得到 &lt;strong&gt;&amp;ldquo;仅含墙壁、地面的静态点云&amp;rdquo;&lt;/strong&gt;，供避障决策使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;二、回环检测：从 &amp;ldquo;短期几何一致&amp;rdquo; 到 &amp;ldquo;长期地图全局一致&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二回环检测从-短期几何一致-到-长期地图全局一致&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8c%e5%9b%9e%e7%8e%af%e6%a3%80%e6%b5%8b%e4%bb%8e-%e7%9f%ad%e6%9c%9f%e5%87%a0%e4%bd%95%e4%b8%80%e8%87%b4-%e5%88%b0-%e9%95%bf%e6%9c%9f%e5%9c%b0%e5%9b%be%e5%85%a8%e5%b1%80%e4%b8%80%e8%87%b4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 问题本质与影响&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人在 &lt;strong&gt;&amp;ldquo;长期建图&amp;rdquo;&lt;/strong&gt;（如探索整个办公楼）时，会因&lt;strong&gt;位姿漂移&lt;/strong&gt;（相机姿态估计误差累积）导致地图扭曲：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如机器人从 &lt;strong&gt;&amp;ldquo;1 楼走廊&amp;rdquo;&lt;/strong&gt; 出发，绕一圈回到起点，若没有回环检测，DA3 生成的地图会出现 &lt;strong&gt;&amp;ldquo;起点与终点不重合&amp;rdquo;&lt;/strong&gt;（如走廊两端错开 1 米），严重时会导致机器人 &lt;strong&gt;&amp;ldquo;迷路&amp;rdquo;&lt;/strong&gt;（不知道自己的真实位置）&lt;/li&gt;
&lt;li&gt;DA3 仅能保证 &lt;strong&gt;&amp;ldquo;单帧 / 短序列帧&amp;rdquo;&lt;/strong&gt; 的几何一致性（如连续 10 帧的点云对齐），但无法判断 &lt;strong&gt;&amp;ldquo;当前场景是否与 10 分钟前访问过的场景相同&amp;rdquo;&lt;/strong&gt;—— 这一判断能力正是回环检测的核心&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. DA3 当前的核心局限&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 的定位是 &lt;strong&gt;&amp;ldquo;几何重建模型&amp;rdquo;&lt;/strong&gt;，无任何回环检测与位姿修正模块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;仅能根据输入图像估计 &lt;strong&gt;&amp;ldquo;相对姿态&amp;rdquo;&lt;/strong&gt;（相邻帧之间的姿态变化），无法估计 &lt;strong&gt;&amp;ldquo;绝对姿态&amp;rdquo;&lt;/strong&gt;（相对于全局地图的位置）&lt;/li&gt;
&lt;li&gt;长期运行后，相对姿态误差会累积，导致点云地图出现 &lt;strong&gt;&amp;ldquo;漂移&amp;rdquo;&lt;/strong&gt;（如直线走廊建图后变成曲线），进而影响避障精度（如机器人认为前方有障碍，实际是地图漂移导致的虚警）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. 解决方案：DA3 作为 &amp;ldquo;几何 backbone&amp;rdquo;，集成到成熟 SLAM 框架&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据搜索资源（摘要 5：DA3-Long 提升 SLAM 性能、摘要 3：SLAM 位姿优化），需将 DA3 的输出接入具备回环检测的 SLAM 框架，利用 SLAM 的回环模块修正漂移，具体流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;选择 SLAM 框架：&lt;/strong&gt; 优先选择支持 &lt;strong&gt;&amp;ldquo;深度图输入&amp;rdquo;&lt;/strong&gt; 的开源框架，如 &lt;strong&gt;ORB-SLAM3&lt;/strong&gt;（适用于单目 / RGB-D）、&lt;strong&gt;LDSO&lt;/strong&gt;（直接法 SLAM，高精度）、&lt;strong&gt;VINS-Mono&lt;/strong&gt;（视觉惯性融合，抗抖动）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DA3 与 SLAM 的数据交互：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 将 &lt;strong&gt;&amp;ldquo;深度图（prediction.depth）+ 相机内参（prediction.intrinsics）+ 相对姿态（prediction.extrinsics）&amp;rdquo;&lt;/strong&gt; 输出给 SLAM 框架&lt;/li&gt;
&lt;li&gt;SLAM 框架将 DA3 的深度图作为 &lt;strong&gt;&amp;ldquo;视觉观测数据&amp;rdquo;&lt;/strong&gt;，替代传统的 &lt;strong&gt;&amp;ldquo;单目相机的三角化深度&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;LiDAR 点云&amp;rdquo;&lt;/strong&gt;，提升姿态估计精度（摘要 5 提到 DA3-Long 替换 VGGT 后，SLAM 漂移显著降低）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SLAM 回环检测模块修正漂移：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 SLAM 框架检测到 &lt;strong&gt;&amp;ldquo;当前场景与历史场景相似&amp;rdquo;&lt;/strong&gt;（如 ORB-SLAM3 通过 ORB 特征匹配判断回环），会触发 &lt;strong&gt;&amp;ldquo;回环优化&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;优化过程中，SLAM 会根据 DA3 生成的深度图重新计算 &lt;strong&gt;&amp;ldquo;回环帧&amp;rdquo;&lt;/strong&gt; 的姿态，修正之前累积的漂移，并更新全局地图，确保长期建图的一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4. 落地示例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人探索办公楼时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 在 Jetson AGX Orin 上实时输出深度图（6.35 FPS，摘要 1、4），并将数据传入 ORB-SLAM3&lt;/li&gt;
&lt;li&gt;ORB-SLAM3 用 DA3 的深度图优化相邻帧的姿态（减少初始漂移），同时提取图像的 ORB 特征，与历史帧特征库比对&lt;/li&gt;
&lt;li&gt;当机器人回到 &lt;strong&gt;&amp;ldquo;1 楼大厅&amp;rdquo;&lt;/strong&gt;（之前访问过的场景），ORB-SLAM3 检测到 ORB 特征匹配度超过阈值（回环触发），用 DA3 的深度图验证 &lt;strong&gt;&amp;ldquo;当前深度与历史深度是否一致&amp;rdquo;&lt;/strong&gt;，若一致则优化全局位姿，修正之前的漂移&lt;/li&gt;
&lt;li&gt;最终生成的地图无明显扭曲，机器人能准确判断自己的位置，避障决策更可靠&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;三、语义融合：从 &amp;ldquo;纯几何点云&amp;rdquo; 到 &amp;ldquo;可解释的避障决策&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三语义融合从-纯几何点云-到-可解释的避障决策&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e8%af%ad%e4%b9%89%e8%9e%8d%e5%90%88%e4%bb%8e-%e7%ba%af%e5%87%a0%e4%bd%95%e7%82%b9%e4%ba%91-%e5%88%b0-%e5%8f%af%e8%a7%a3%e9%87%8a%e7%9a%84%e9%81%bf%e9%9a%9c%e5%86%b3%e7%ad%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 问题本质与影响&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 生成的点云仅包含 &lt;strong&gt;&amp;ldquo;3D 坐标 + 深度&amp;rdquo;&lt;/strong&gt; 等几何信息，但机器人避障需要语义理解—— 即 &lt;strong&gt;&amp;ldquo;知道障碍物是什么，以及是否需要避开&amp;rdquo;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如点云中的 &lt;strong&gt;&amp;ldquo;地面点&amp;rdquo;&lt;/strong&gt;（可通行）和 &lt;strong&gt;&amp;ldquo;桌子腿点&amp;rdquo;&lt;/strong&gt;（需避开），几何上都是 &lt;strong&gt;&amp;ldquo;空间中的点&amp;rdquo;&lt;/strong&gt;，但对机器人的意义完全不同&lt;/li&gt;
&lt;li&gt;若仅依赖几何信息，机器人可能出现 &lt;strong&gt;&amp;ldquo;荒谬避障&amp;rdquo;&lt;/strong&gt;（如避开地面上的阴影点云）或 &lt;strong&gt;&amp;ldquo;漏避障&amp;rdquo;&lt;/strong&gt;（如未识别细长的栏杆，几何上点云稀疏易被忽略）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. DA3 当前的核心局限&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 是 &lt;strong&gt;&amp;ldquo;纯几何模型&amp;rdquo;&lt;/strong&gt;，无任何语义输出能力：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无法给点云添加 &lt;strong&gt;&amp;ldquo;类别标签&amp;rdquo;&lt;/strong&gt;（如 &amp;ldquo;地面&amp;quot;&amp;ldquo;行人&amp;quot;&amp;ldquo;栏杆&amp;rdquo;）&lt;/li&gt;
&lt;li&gt;无法区分 &lt;strong&gt;&amp;ldquo;可通行区域&amp;rdquo;&lt;/strong&gt;（如平坦地面）和 &lt;strong&gt;&amp;ldquo;不可通行障碍&amp;rdquo;&lt;/strong&gt;（如台阶、桌椅），仅能通过 &lt;strong&gt;&amp;ldquo;点云是否在路径上&amp;rdquo;&lt;/strong&gt; 判断是否避障，精度极低&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. 解决方案：DA3 深度图 + 多模态语义分割，给点云 &amp;ldquo;贴标签&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据搜索资源（摘要 3：RGB-D 语义分割、多模态融合），可利用 DA3 的深度图生成 &lt;strong&gt;&amp;ldquo;RGB-D 数据&amp;rdquo;&lt;/strong&gt;，输入语义分割模型，实现 &lt;strong&gt;&amp;ldquo;几何 + 语义&amp;rdquo;&lt;/strong&gt; 融合：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;技术路径&lt;/th&gt;
          &lt;th&gt;核心原理（结合 DA3）&lt;/th&gt;
          &lt;th&gt;优势与优化方向（来自搜索资源）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RGB-D 语义分割&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. DA3 输出 &amp;ldquo;RGB 图像 + 深度图&amp;rdquo;，组合为 RGB-D 数据（每个像素含 RGB 颜色 + 深度值）；&lt;br&gt;&lt;/br&gt;2. 将 RGB-D 数据输入专门的语义分割网络（如 Cylinder3D、RangeNet++，摘要 3）；&lt;br&gt;&lt;/br&gt;3. 分割网络输出 &amp;ldquo;语义标签图&amp;rdquo;（每个像素对应类别，如 &amp;ldquo;0 = 地面，1 = 行人，2 = 栏杆&amp;rdquo;）；&lt;br&gt;&lt;/br&gt;4. 将语义标签反投影到 DA3 生成的 3D 点云中，每个点云添加 &amp;ldquo;语义属性&amp;rdquo;。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; DA3 的深度图能弥补 RGB 图像的遮挡 / 光照问题（如阴影区域，深度图仍能区分地面与障碍）；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 多模态特征融合（摘要 3），将 DA3 的深度特征与 RGB 特征拼接输入分割网络，提升 &amp;ldquo;细长物体（栏杆）&amp;ldquo;&amp;ldquo;低纹理物体（白色墙壁）&amp;rdquo; 的分割精度。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;几何辅助语义修正&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. 用 DA3 的深度图计算 &amp;ldquo;点云曲率&amp;rdquo;（高曲率区域多为物体边缘，如桌子角）；&lt;br&gt;&lt;/br&gt;2. 结合语义分割结果，若 &amp;ldquo;语义标签为地面，但曲率过高&amp;rdquo;（如地面上的石头），修正标签为 &amp;ldquo;障碍&amp;rdquo;；&lt;br&gt;&lt;/br&gt;3. 若 &amp;ldquo;语义标签为障碍，但深度值过大&amp;rdquo;（如远处的树木，超出避障范围），标记为 &amp;ldquo;无需避障&amp;rdquo;。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 利用 DA3 的几何精度修正语义分割的误判；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 高度阈值约束（摘要 3 提到的 ERASORS 算法），根据机器人高度（如 0.1-1.5 米为障碍范围），过滤掉 &amp;ldquo;过高（天花板）&amp;rdquo; 或 &amp;ldquo;过低（小石子）&amp;rdquo; 的点，减少无效避障。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;4. 落地示例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;服务机器人在办公室避障时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 生成 RGB-D 数据（RGB 图像 + 深度图），输入 &lt;strong&gt;Cylinder3D&lt;/strong&gt; 语义分割网络&lt;/li&gt;
&lt;li&gt;分割网络输出语义标签图：&lt;strong&gt;&amp;ldquo;地面（绿色）&amp;ldquo;&amp;ldquo;办公桌（红色）&amp;ldquo;&amp;ldquo;行人（蓝色）&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;点云被标记为三类：&lt;strong&gt;绿色点&lt;/strong&gt;（可通行）、&lt;strong&gt;红色点&lt;/strong&gt;（静态障碍，需绕开）、&lt;strong&gt;蓝色点&lt;/strong&gt;（动态障碍，需实时跟踪）&lt;/li&gt;
&lt;li&gt;机器人的避障算法优先避开红色 / 蓝色点，且仅在 &lt;strong&gt;&amp;ldquo;障碍点位于机器人运动范围内（0.1-1.5 米高）&amp;rdquo;&lt;/strong&gt; 时触发避障，忽略地面小石子（过低）和天花板管道（过高），避免无效决策&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;GPT4Scene&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;gpt4scene&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gpt4scene&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;3D 视觉-语言任务&lt;/strong&gt;旨在将 3D 场景理解与自然语言处理相融合。然而，我们希望更进一步：将 3D 内容融入大型语言模型（LLMs），以实现更自然的人机交互。这一方向的研究最初始于 &lt;strong&gt;3D 点云大型语言模型（3D Point Cloud LLMs）&lt;/strong&gt;。此类模型以点云为输入，能够在 3D 场景中实现自然语言生成与交互。&lt;/p&gt;
&lt;p&gt;早期的 3D 大型语言模型主要关注物体级别的几何结构与外观；后续研究扩展至室内场景，开始侧重物体间的空间关系及场景整体特征，通常会利用场景点云，并结合辅助性 2D 多视图图像。为了更精准地捕捉物体间关系，近期的 3D 大型语言模型会先对场景中的物体进行解耦，再将其输入至 LLM。此外，部分方法会更依赖视觉输入来判断场景上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://gpt4scene.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT4Scene&lt;/a&gt;&lt;/strong&gt; 是旨在探索纯视觉输入的 VLM 是否能更有效地处理室内 3D 场景理解任务的框架，仅依赖视频输入（无需 3D 点云），核心通过 &lt;strong&gt;3D 重建生成鸟瞰图（BEV）&lt;/strong&gt; 提供全局场景布局，以及 &lt;strong&gt;时空对象标记（STO-markers）&lt;/strong&gt; 建立 BEV 与视频帧的全局-局部关联，解决 VLMs 缺全局表示、帧与时空上下文错位的问题；同时构建含 &lt;strong&gt;165K 文本标注&lt;/strong&gt;的 &lt;strong&gt;ScanAlign 数据集&lt;/strong&gt;，用于微调开源 VLMs。在零样本设置下，该框架显著提升 GPT-4o、Gemini-1.5-Pro 等大模型性能（如 GPT-4o 在 ScanQA 的 ROUGE 指标从 32.6 提升至 &lt;strong&gt;37.7&lt;/strong&gt;）；微调后 &lt;strong&gt;Qwen2-VL-7B&lt;/strong&gt; 在多任务达 SOTA，如 3D 问答（SQA3D）EM1 指标从 40.7 提升至 &lt;strong&gt;60.7&lt;/strong&gt;（相对 &lt;strong&gt;+48%&lt;/strong&gt;），超过此前 SOTA 模型 Chat-Scene &lt;strong&gt;11.0%&lt;/strong&gt;，且能让 VLMs 形成内在 3D 理解能力，为 VLMs 扩展 3D 场景理解提供无缝方案。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/gpt4scene-pipeline.png&#34; alt=&#34;GPT4Scene Pipeline&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;整体工作流程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;整体工作流程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%95%b4%e4%bd%93%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;VLMs 直接处理视频时存在两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;缺全局场景表示&lt;/strong&gt;（第一视角视频看不到房间整体布局）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;帧与时空上下文错位&lt;/strong&gt;（不知道不同帧的同一物体是同一个）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此，GPT4Scene 的框架设计围绕 &lt;strong&gt;&amp;ldquo;补全局&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;建关联&amp;rdquo;&lt;/strong&gt; 展开：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Reconstruct 路径&amp;rdquo;&lt;/strong&gt; 负责 &lt;strong&gt;&amp;ldquo;补全局&amp;rdquo;&lt;/strong&gt;（生成 BEV）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Partition 路径&amp;rdquo;&lt;/strong&gt; 负责 &lt;strong&gt;&amp;ldquo;提细节&amp;rdquo;&lt;/strong&gt;（采样帧）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;STO-markers&amp;rdquo;&lt;/strong&gt; 负责 &lt;strong&gt;&amp;ldquo;建关联&amp;rdquo;&lt;/strong&gt;（让 BEV 和帧的物体对应）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;起点：&lt;/strong&gt; 一段围绕室内场景拍摄的第一视角视频 $V = {I_1, I_2, \ldots, I_N}$（比如 1000 帧的房间漫游视频）。这段视频会被 &lt;strong&gt;&amp;ldquo;复用&amp;rdquo;&lt;/strong&gt; 到两个预处理环节，而非拆分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;给 &amp;ldquo;Partition（采样）路径&amp;rdquo; 用：&lt;/strong&gt; 取 &lt;strong&gt;&amp;ldquo;部分帧&amp;rdquo;&lt;/strong&gt;（采样），目的是减少 VLM 的输入 token 数量（避免 1000 帧计算量过大），同时保留场景局部细节（如物体外观、局部视角）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;给 &amp;ldquo;Reconstruct（重建）路径&amp;rdquo; 用：&lt;/strong&gt; 用 &lt;strong&gt;&amp;ldquo;完整帧&amp;rdquo;&lt;/strong&gt;，目的是通过连续时序的图像 + 相机外参，还原场景的 3D 结构（点云），进而生成全局布局（BEV）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/gpt4scene-framework.png&#34; alt=&#34;GPT4Scene Framework&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如上图，给定一段围绕室内场景移动拍摄的视频 $V = {I_1, \dots, I_N}$，我们首先通过索引近似均匀地采样 $n$ 帧，采样公式为：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;⌊&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo stretchy=&#34;false&#34;&gt;⌋&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;s_i = \lfloor (i-1) \frac{N}{n} \rfloor + 1&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.5806em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;⌊(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:2.0463em;vertical-align:-0.686em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3603em;&#34;&gt;&lt;span style=&#34;top:-2.314em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.677em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.686em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;⌋&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6444em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中 $\forall i \in {1, \dots, n}$，由此形成采样后的视频帧集合 $V^* = {I_{s_1}, \dots, I_{s_n}}$。该策略在保留场景上下文且无显著信息损失的前提下，减少了视觉语言模型（VLMs）的令牌（token）数量和计算开销。随后，我们利用完整的时间序列进行 3D 场景重建，生成全局鸟瞰图（BEV）。通过后续的 3D 实例分割，可实现物体的精准定位；将该定位结果投影到 BEV 地图和 2D 视频帧上，即可建立时空对象标记（STO-markers）。&lt;/p&gt;
&lt;h4&gt;Reconstruct → 生成 &amp;ldquo;带 STO-markers 的 BEV 图像 $\mathcal{I}_b&amp;rsquo;$&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;reconstruct--生成-带-sto-markers-的-bev-图像-mathcali_b&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#reconstruct--%e7%94%9f%e6%88%90-%e5%b8%a6-sto-markers-%e7%9a%84-bev-%e5%9b%be%e5%83%8f-mathcali_b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt; 给 VLM 提供 &lt;strong&gt;&amp;ldquo;全局场景布局&amp;rdquo;&lt;/strong&gt;（如房间里桌子、椅子的相对位置，墙壁的形状），解决 &lt;strong&gt;&amp;ldquo;缺全局表示&amp;rdquo;&lt;/strong&gt; 问题，同时通过 STO-markers 与采样帧关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 1：3D 重建生成点云&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;用 &lt;strong&gt;&amp;ldquo;完整原始视频&amp;rdquo;&lt;/strong&gt;（而非采样帧）+ 每帧的 &lt;strong&gt;&amp;ldquo;相机外参 $E = {E_1, \ldots, E_N}$&amp;rdquo;&lt;/strong&gt;（描述相机在现实中的位置和姿态），通过 3D 重建技术（如 BundleFusion）生成 3D 点云 $\mathcal{P}$：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;script&#34;&gt;P&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;script&#34;&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{P} = \mathcal{R}\left(\{(I_t, E_t)\}\right)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.08222em;&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;{(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)}&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;步骤 2：点云生成 BEV 图像&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设定 &lt;strong&gt;&amp;ldquo;俯视图相机外参 $E_{top}$&amp;rdquo;&lt;/strong&gt;（模拟从房间正上方往下拍的相机姿态，$E_{top} \in SE(3)$），通过渲染函数 $\mathcal{T}(\cdot)$ 将 3D 点云投影为 2D 的 BEV 图像 $\mathcal{I}_b$：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;script&#34;&gt;I&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;script&#34;&gt;T&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi mathvariant=&#34;script&#34;&gt;P&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{I}_b = \mathcal{T}(\mathcal{P}, E_{top})&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.07382em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0738em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.25417em;&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.08222em;&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;步骤 3：叠加 STO-markers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;与 &lt;strong&gt;&amp;ldquo;路径 1&amp;rdquo;&lt;/strong&gt; 共享 3D 实例掩码 $M$，保证标记一致性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 3D 掩码 $M$ 投影到 BEV 的 xy 平面（忽略 z 轴高度，只保留平面位置），提取每个物体投影后的 &lt;strong&gt;&amp;ldquo;边界框中心坐标&amp;rdquo;&lt;/strong&gt; $C^{xy} = {C_1^{xy}, \ldots, C_K^{xy}}$&lt;/li&gt;
&lt;li&gt;用函数 $\mathcal{F}(\cdot)$ 将这些坐标作为标记叠加到 BEV 图像上，生成带 STO-markers 的 BEV $\mathcal{I}_b&amp;rsquo; = \mathcal{F}(\mathcal{I}_b, C^{xy})$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键：&lt;/strong&gt; BEV 上的标记 ID 与 &lt;strong&gt;&amp;ldquo;路径 1&amp;rdquo;&lt;/strong&gt; 采样帧的标记 ID 完全一致（比如 BEV 上的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 和采样帧上的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 是同一个桌子），建立 &lt;strong&gt;&amp;ldquo;全局布局&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;局部细节&amp;rdquo;&lt;/strong&gt; 的关联。&lt;/p&gt;
&lt;h4&gt;Partition（视频帧采样）→ 生成 &amp;ldquo;带 STO-markers 的采样帧 $\mathcal{V}^{* \prime}$&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;partition视频帧采样-生成-带-sto-markers-的采样帧-mathcalv-prime&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#partition%e8%a7%86%e9%a2%91%e5%b8%a7%e9%87%87%e6%a0%b7-%e7%94%9f%e6%88%90-%e5%b8%a6-sto-markers-%e7%9a%84%e9%87%87%e6%a0%b7%e5%b8%a7-mathcalv-prime&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt; 给 VLM 提供 &lt;strong&gt;&amp;ldquo;局部物体细节&amp;rdquo;&lt;/strong&gt;（如椅子的颜色、杯子的形状），并通过 STO-markers 标记物体 ID，解决 &lt;strong&gt;&amp;ldquo;帧间物体错位&amp;rdquo;&lt;/strong&gt; 问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 1：帧采样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按公式 $s_i = \lfloor (i-1)\frac{N}{n} \rfloor + 1$（$n$ 默认 8 帧）从原始视频中 &lt;strong&gt;&amp;ldquo;均匀采样&amp;rdquo;&lt;/strong&gt; $n$ 帧，得到采样帧集合 $V^* = {I_{s_1}, \ldots, I_{s_n}}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 2：叠加 STO-markers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这一步需要 &lt;strong&gt;&amp;ldquo;Reconstruct 路径&amp;rdquo;&lt;/strong&gt; 的输出（3D 实例掩码）作为前提，并非独立完成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从 &lt;strong&gt;&amp;ldquo;Reconstruct 路径&amp;rdquo;&lt;/strong&gt; 获取 3D 实例掩码 $M = {M_1, &amp;hellip;, M_K}$（每个 $M_k$ 对应 1 个物体的 3D 轮廓）&lt;/li&gt;
&lt;li&gt;根据每帧采样帧的 &lt;strong&gt;&amp;ldquo;相机外参&amp;rdquo;&lt;/strong&gt;，将 3D 掩码 $M$ 投影到该 2D 帧上，提取每个物体的 &lt;strong&gt;&amp;ldquo;2D 质心坐标&amp;rdquo;&lt;/strong&gt; $C_i^{uv}$（第 $i$ 帧中第 $k$ 个物体的标记是 $C_{i,k}^{uv}$）&lt;/li&gt;
&lt;li&gt;用函数 $\mathcal{F}(\cdot)$ 将这些坐标作为 &lt;strong&gt;&amp;ldquo;标记&amp;rdquo;&lt;/strong&gt;（如数字 ID &amp;ldquo;物体 1&amp;quot;&amp;ldquo;物体 2&amp;rdquo;）叠加到采样帧上，生成带 STO-markers 的采样帧 $\mathcal{V}^{* \prime} = {\mathcal{F}(I_i, C_i^{uv})}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键：&lt;/strong&gt; 不同采样帧中，同一物体的标记 ID（如 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt;）保持一致，解决 &lt;strong&gt;&amp;ldquo;帧间物体错位&amp;rdquo;&lt;/strong&gt;（比如第 1 帧的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 和第 5 帧的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 都是同一个桌子）。&lt;/p&gt;
&lt;h4&gt;VLM 输入&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vlm-输入&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vlm-%e8%be%93%e5%85%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;The resulting frames and BEV image, enhanced with STO-markers, are inputs for VLM training and inference&amp;rdquo;&lt;/strong&gt;—— 即最终输入 VLM 的是两个核心组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;em&gt;$\mathcal{V}^{&lt;/em&gt; \prime}$：&lt;/em&gt;* 带 STO-markers 的采样帧（提供 &lt;strong&gt;&amp;ldquo;局部物体外观 + 帧间物体一致性&amp;rdquo;&lt;/strong&gt;）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$\mathcal{I}_b&amp;rsquo;$：&lt;/strong&gt; 带 STO-markers 的 BEV 图像（提供 &lt;strong&gt;&amp;ldquo;全局场景布局 + 物体空间位置&amp;rdquo;&lt;/strong&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两个组件结合，恰好解决了 VLMs 的 3D 理解缺陷：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缺全局？&lt;/strong&gt; BEV 提供房间整体布局&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;帧错位？&lt;/strong&gt; STO-markers 保证同一物体在 BEV 和所有采样帧中 ID 一致，让 VLM 能关联 &lt;strong&gt;&amp;ldquo;局部看到的物体&amp;rdquo;&lt;/strong&gt; 和 &lt;strong&gt;&amp;ldquo;全局中的位置&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>AI 上下游工作概念</title>
      <link>http://localhost:1313/blog/2025/2025-12-19-ai-work/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-19-ai-work/</guid>
      <description>
        
        
        &lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/13476251758/answer/1914837861510934794?utm_psn=1985438588473741431&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;如何评价当前的 AI Agent 落地效果普遍不佳的问题？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇回答给我玩小黄文这一块，**&amp;ldquo;做AI&amp;rdquo;**是吧&lt;/p&gt;
&lt;p&gt;不过也让我很好的了解到了从模型生产到部署AI各阶段具体干什么的概念：&lt;/p&gt;
&lt;h3&gt;1. Pre-training (预训练)：造脑工程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-pre-training-预训练造脑工程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-pre-training-%e9%a2%84%e8%ae%ad%e7%bb%83%e9%80%a0%e8%84%91%e5%b7%a5%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;这是 AI 的 &lt;strong&gt;&amp;ldquo;基建&amp;quot;阶段&lt;/strong&gt;，目标是从海量数据中学习通用知识。&lt;/p&gt;
&lt;h4&gt;数据工程 (Data Curation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;数据工程-data-curation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8b-data-curation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;清洗与去重&lt;/strong&gt;：处理成百上千 T 的互联网数据，剔除垃圾信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据配比&lt;/strong&gt;：决定书本、代码、网页、数学题各自占多少比例（这是各家模型的核心秘密）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;算力基础设施 (Infrastructure)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;算力基础设施-infrastructure&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ae%97%e5%8a%9b%e5%9f%ba%e7%a1%80%e8%ae%be%e6%96%bd-infrastructure&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;分布式训练&lt;/strong&gt;：如何让几万张显卡同时跑一个模型（&lt;strong&gt;3D 并行&lt;/strong&gt;：数据并行、算力并行、流水线并行）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算力优化&lt;/strong&gt;：提高显卡利用率（&lt;strong&gt;MFU&lt;/strong&gt;），防止训练过程中突然崩溃（&lt;strong&gt;Checkpoint 恢复&lt;/strong&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;架构设计 (Architecture)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;架构设计-architecture&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1-architecture&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MoE (混合专家模型)&lt;/strong&gt;：像 &lt;strong&gt;DeepSeek&lt;/strong&gt; 那样，让模型只激活部分参数以节省算力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;长文本窗口&lt;/strong&gt;：让模型一次能读完一整本书。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Post-training (后训练/对齐)：教育工程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-post-training-后训练对齐教育工程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-post-training-%e5%90%8e%e8%ae%ad%e7%bb%83%e5%af%b9%e9%bd%90%e6%95%99%e8%82%b2%e5%b7%a5%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;预训练出的模型只是一个**&amp;ldquo;满腹经纶但满嘴胡话&amp;quot;的学者**，Post-training 是为了让它变乖、变有用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SFT (有监督微调)&lt;/strong&gt;：喂给模型高质量的 Q&amp;amp;A 对，教会它听从指令。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Alignment (对齐/价值观)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;alignment-对齐价值观&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#alignment-%e5%af%b9%e9%bd%90%e4%bb%b7%e5%80%bc%e8%a7%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RLHF (强化学习)&lt;/strong&gt;：让模型根据人类的打分来优化回答。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DPO (直接偏好优化)&lt;/strong&gt;：目前最流行的替代 RLHF 的方案，更高效。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;推理能力强化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;推理能力强化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a8%e7%90%86%e8%83%bd%e5%8a%9b%e5%bc%ba%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;思维链 (CoT) 激发&lt;/strong&gt;：通过特定的训练让模型学会**&amp;ldquo;想好了再说&amp;rdquo;**。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reflection (反思)&lt;/strong&gt;：教会模型在输出前自我检查错误。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合成数据 (Synthetic Data)&lt;/strong&gt;：当人类数据不够用时，让模型生成高质量数据来训练模型自己。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Multimodal (多模态)：五官工程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-multimodal-多模态五官工程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-multimodal-%e5%a4%9a%e6%a8%a1%e6%80%81%e4%ba%94%e5%ae%98%e5%b7%a5%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;让 AI 不仅能看懂文字，还能看图、听声音、甚至看视频。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模态对齐 (Modality Alignment)&lt;/strong&gt;：将图像编码器（如 &lt;strong&gt;ViT&lt;/strong&gt;）捕捉到的特征，翻译成大语言模型能听懂的&amp;quot;语言&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统一表示 (Unified Tokenization)&lt;/strong&gt;：尝试把声音、图像、文本都变成同一种数字序列进行处理（如 &lt;strong&gt;Chameleon&lt;/strong&gt; 或 &lt;strong&gt;GPT-4o&lt;/strong&gt; 的思路）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时序理解&lt;/strong&gt;：针对视频流，如何让模型理解动作的先后顺序和逻辑。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Inference Optimization (推理优化)：落地工程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-inference-optimization-推理优化落地工程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-inference-optimization-%e6%8e%a8%e7%90%86%e4%bc%98%e5%8c%96%e8%90%bd%e5%9c%b0%e5%b7%a5%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;模型训练好后，如何让它运行得更快、更便宜、更稳。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;量化 (Quantization)&lt;/strong&gt;：将 &lt;strong&gt;16 位浮点数&lt;/strong&gt;压成 &lt;strong&gt;8 位或 4 位&lt;/strong&gt;，模型体积缩小一倍，速度飞快，但精度损失很小。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算子优化&lt;/strong&gt;：比如 &lt;strong&gt;FlashAttention&lt;/strong&gt;，通过底层数学技巧极大提升显卡的计算效率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度系统&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;vLLM / TensorRT-LLM&lt;/strong&gt;：并发处理成千上万个请求，提高吞吐量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KV Cache 管理&lt;/strong&gt;：解决模型在生成长文本时内存占用过高的问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. Agent 开发&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-agent-开发&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-agent-%e5%bc%80%e5%8f%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;虽然表面上是鄙视链最底层，很大程度上依赖**&amp;ldquo;调教 Prompt&amp;rdquo;**，不过下面这篇回答我觉得说的挺好：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/657739588/answer/1959347964674809996?utm_psn=1985429646507004959&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI agent到底有多大创新？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个知乎提问主要谈到了 &lt;strong&gt;AI agent&lt;/strong&gt; 的缺陷：&lt;/p&gt;
&lt;h4&gt;1. Planning 阶段带来了巨大的耗时&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-planning-阶段带来了巨大的耗时&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-planning-%e9%98%b6%e6%ae%b5%e5%b8%a6%e6%9d%a5%e4%ba%86%e5%b7%a8%e5%a4%a7%e7%9a%84%e8%80%97%e6%97%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;当 tool 变多后，&lt;strong&gt;turbo 系列模型&lt;/strong&gt;的准确率堪忧，因此不得不使用旗舰模型，这让延时进一步增加。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本质原因&lt;/strong&gt;：&lt;strong&gt;组合优化问题&lt;/strong&gt;。工具多了以后，搜索空间呈指数级膨胀。弱模型搞不定，强模型 Token 多、推理慢。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;分层治理（缩小搜索空间）&lt;/strong&gt;：意图分类 -&amp;gt; 路由到特定域（&lt;strong&gt;Domain&lt;/strong&gt;） -&amp;gt; 仅暴露少量工具（类似 &lt;strong&gt;MCP 协议&lt;/strong&gt;思路）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;并行化（工程优化）&lt;/strong&gt;：将串行链改为 &lt;strong&gt;DAG（有向无环图）&lt;/strong&gt;，无依赖的任务并行执行（参考 &lt;strong&gt;LLMCompiler&lt;/strong&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;路由策略（成本优化）&lt;/strong&gt;：简单任务给小模型（&lt;strong&gt;SLM&lt;/strong&gt;）/硬编码，复杂任务给大模型（参考 &lt;strong&gt;RouteLLM&lt;/strong&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. Planning 的质量不够高&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-planning-的质量不够高&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-planning-%e7%9a%84%e8%b4%a8%e9%87%8f%e4%b8%8d%e5%a4%9f%e9%ab%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;原来的 task bot 做任务所使用的 workflow 是人工决定的，现在改成了模型自助决定，从目前的测试来看，由模型构建的复杂工作流的可用率远远不及人类水平。简单工作流使用判别式小模型反而性能更好。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本质原因&lt;/strong&gt;：自然语言生成的计划缺乏**&amp;ldquo;可执行性&amp;rdquo;&lt;strong&gt;和&lt;/strong&gt;&amp;ldquo;全局约束&amp;rdquo;**。模型线性思维（Step A-&amp;gt;B-&amp;gt;C）难以应对复杂多变场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;解耦规划（HiPlan）&lt;/strong&gt;：战略（里程碑）与战术（执行细节）分离。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结构化约束（Routine）&lt;/strong&gt;：强制输出 &lt;strong&gt;DSL（领域特定语言）&lt;/strong&gt; 而非自然语言，由语法保证正确性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;搜索式规划（LATS）&lt;/strong&gt;：引入 &lt;strong&gt;MCTS（蒙特卡洛树搜索）&lt;/strong&gt;，不是赌一把，而是模拟多条路径+打分（&lt;strong&gt;Verifier&lt;/strong&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多轮 RL 训练&lt;/strong&gt;：让模型在多轮交互中&amp;quot;学会&amp;quot;长程规划，而不是仅靠 Prompt。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. Reflection 是一种时间换准确度的策略&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-reflection-是一种时间换准确度的策略&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-reflection-%e6%98%af%e4%b8%80%e7%a7%8d%e6%97%b6%e9%97%b4%e6%8d%a2%e5%87%86%e7%a1%ae%e5%ba%a6%e7%9a%84%e7%ad%96%e7%95%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;然而这个策略非常容易重复进行自我内耗，和死循环。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本质原因&lt;/strong&gt;：反馈信号太弱（**&amp;ldquo;我觉得不对&amp;rdquo;**太主观），缺乏明确的停机条件，导致错误假设被不断强化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型侧&lt;/strong&gt;：训练模型学会**&amp;ldquo;诊断错误&amp;rdquo;&lt;strong&gt;并&lt;/strong&gt;&amp;ldquo;提出修复方案&amp;rdquo;**（&lt;strong&gt;Failure Makes the Agent Stronger&lt;/strong&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工程侧（兜底）&lt;/strong&gt;：设置硬性上限（&lt;strong&gt;Max rounds&lt;/strong&gt;）、状态去重（&lt;strong&gt;State-hash&lt;/strong&gt;）、预算控制。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;这么看来，其实 agent 的 &lt;strong&gt;Prompt Engineering&lt;/strong&gt; 已经臭了，应该转向成 &lt;strong&gt;Flow Engineering&lt;/strong&gt;，使用 &lt;strong&gt;HiPlan（分层）&lt;/strong&gt;、&lt;strong&gt;DAG（并行）&lt;/strong&gt;、&lt;strong&gt;Router（路由）&lt;/strong&gt; 等手段。&lt;strong&gt;DSL（结构化语言）&lt;/strong&gt; 依然很重要，即输出 JSON 或特定代码，主要就看你一个 Schema 定义能力本身。然后 &lt;strong&gt;MCP&lt;/strong&gt; 即插即用，&lt;strong&gt;Multi-agent System (MAS)&lt;/strong&gt; 组成一组&amp;rsquo;专家 Agent&amp;rsquo;的协作网络。&lt;/p&gt;
&lt;h2&gt;相关论文&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关论文&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%ae%ba%e6%96%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;路由（Routing）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;路由routing&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%af%e7%94%b1routing&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.18665&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RouteLLM: Learning to Route LLMs with Preference Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.07571&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MoMA: Multimodal LLM Adapter for Mobile Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;规划优化（Planning Optimization）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划优化planning-optimization&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92%e4%bc%98%e5%8c%96planning-optimization&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.19076&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HiPlan: Hierarchical Planning for Complex Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.14447&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Routine: Structured Instruction for Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.04406&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LATS: Language Agent Tree Search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;多轮强化学习（Multi-turn RL）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;多轮强化学习multi-turn-rl&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%a4%9a%e8%bd%ae%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0multi-turn-rl&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.20073&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAGEN (StarPO-S)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.08755&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AgentGym: Evolving Agents via RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.18669&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MUA-RL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;反思与强化学习（Reflection &amp;amp; RL）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;反思与强化学习reflection--rl&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8f%8d%e6%80%9d%e4%b8%8e%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0reflection--rl&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://unary-feedback.github.io/assets/pdf/UFO_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UFO: Unary Feedback as Observation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.arxiv.org/pdf/2509.1884&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Failure Makes the Agent Stronger&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Commit Message</title>
      <link>http://localhost:1313/blog/2025/2025-11-13/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-13/</guid>
      <description>
        
        
        &lt;h1&gt;COMMIT MESSAGE&lt;/h1&gt;&lt;p&gt;我完全把&lt;code&gt;blog&lt;/code&gt;当成杂碎知识点的记录了&amp;hellip;&amp;hellip;不过COMMIT MESSAGE还是很有必要规范一下的，规范的COMMIT MESSAGE是团队协作的“沟通密码”，能让代码变更清晰可追溯，那些只写一行空消息或“改东西”“修复bug”的做法，简直是给未来埋坑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;规范的COMMIT MESSAGE需遵循「类型(可选范围)：简洁描述」格式，搭配正文和脚注，常用类型包括feat、fix、docs等&lt;/strong&gt;，目的是让任何人快速看懂变更目的和影响。&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;一、为什么不能只写一行模糊消息？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一为什么不能只写一行模糊消息&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e8%83%bd%e5%8f%aa%e5%86%99%e4%b8%80%e8%a1%8c%e6%a8%a1%e7%b3%8a%e6%b6%88%e6%81%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;后续排查问题时，翻log像“开盲盒”，找不到对应变更的上下文。&lt;/li&gt;
&lt;li&gt;团队协作时，同事不知道你改了啥，需要反复沟通确认。&lt;/li&gt;
&lt;li&gt;自动化工具（如生成CHANGELOG）无法识别，失去版本迭代的清晰记录。&lt;/li&gt;
&lt;li&gt;隔几个月自己回头看，完全忘了当初为啥改这段代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;二、核心规范结构（通用Angular规范，最常用）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二核心规范结构通用angular规范最常用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8c%e6%a0%b8%e5%bf%83%e8%a7%84%e8%8c%83%e7%bb%93%e6%9e%84%e9%80%9a%e7%94%a8angular%e8%a7%84%e8%8c%83%e6%9c%80%e5%b8%b8%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;完整格式分3部分，一行消息只占“简洁描述”，核心是前半段的「类型」：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;类型(可选范围)：简洁描述（不超过50字）

可选正文：详细说明变更原因、实现方式、影响范围（换行写，每行不超过72字）

可选脚注：关联Issue、PR或Breaking Change（如：Fixes #123 / BREAKING CHANGE: 接口参数变更）&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3&gt;三、常用类型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三常用类型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e5%b8%b8%e7%94%a8%e7%b1%bb%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;1. feat：新增功能（新特性、新功能、新接口）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-feat新增功能新特性新功能新接口&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-feat%e6%96%b0%e5%a2%9e%e5%8a%9f%e8%83%bd%e6%96%b0%e7%89%b9%e6%80%a7%e6%96%b0%e5%8a%9f%e8%83%bd%e6%96%b0%e6%8e%a5%e5%8f%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;feat(登录模块)：新增手机验证码登录功能&lt;/li&gt;
&lt;li&gt;feat(订单页)：添加物流轨迹实时查询接口&lt;/li&gt;
&lt;li&gt;feat(用户中心)：支持第三方账号（微信/QQ）绑定&lt;/li&gt;
&lt;li&gt;feat(支付模块)：新增支付宝分期支付选项&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. fix：修复bug（线上/测试环境问题、逻辑错误）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-fix修复bug线上测试环境问题逻辑错误&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-fix%e4%bf%ae%e5%a4%8dbug%e7%ba%bf%e4%b8%8a%e6%b5%8b%e8%af%95%e7%8e%af%e5%a2%83%e9%97%ae%e9%a2%98%e9%80%bb%e8%be%91%e9%94%99%e8%af%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;fix(首页)：修复轮播图在iOS15下自动播放失效的问题&lt;/li&gt;
&lt;li&gt;fix(购物车)：解决商品数量为0时仍可结算的bug&lt;/li&gt;
&lt;li&gt;fix(搜索框)：修复输入特殊字符导致接口报错的问题&lt;/li&gt;
&lt;li&gt;fix(个人资料)：更正手机号格式校验逻辑（支持境外号码）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. docs：文档变更（README、注释、接口文档）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-docs文档变更readme注释接口文档&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-docs%e6%96%87%e6%a1%a3%e5%8f%98%e6%9b%b4readme%e6%b3%a8%e9%87%8a%e6%8e%a5%e5%8f%a3%e6%96%87%e6%a1%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;docs(API文档)：补充用户登录接口的错误码说明&lt;/li&gt;
&lt;li&gt;docs(README)：更新项目启动步骤（新增依赖安装命令）&lt;/li&gt;
&lt;li&gt;docs(注释)：为工具类函数添加参数说明和使用示例&lt;/li&gt;
&lt;li&gt;docs(部署文档)：修正生产环境nginx配置示例&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4. style：代码格式调整（不影响逻辑，仅格式/排版）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-style代码格式调整不影响逻辑仅格式排版&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-style%e4%bb%a3%e7%a0%81%e6%a0%bc%e5%bc%8f%e8%b0%83%e6%95%b4%e4%b8%8d%e5%bd%b1%e5%93%8d%e9%80%bb%e8%be%91%e4%bb%85%e6%a0%bc%e5%bc%8f%e6%8e%92%e7%89%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;style(utils.js)：统一代码缩进为2个空格&lt;/li&gt;
&lt;li&gt;style(login.vue)：删除多余空行和未使用的注释&lt;/li&gt;
&lt;li&gt;style(api.js)：调整函数参数顺序，优化代码可读性&lt;/li&gt;
&lt;li&gt;style(全局)：统一变量命名规范（下划线转小驼峰）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;5. refactor：重构代码（不新增功能、不修复bug，优化结构）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-refactor重构代码不新增功能不修复bug优化结构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-refactor%e9%87%8d%e6%9e%84%e4%bb%a3%e7%a0%81%e4%b8%8d%e6%96%b0%e5%a2%9e%e5%8a%9f%e8%83%bd%e4%b8%8d%e4%bf%ae%e5%a4%8dbug%e4%bc%98%e5%8c%96%e7%bb%93%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;refactor(支付逻辑)：拆分过大的pay()函数，提取工具方法&lt;/li&gt;
&lt;li&gt;refactor(列表组件)：用TS重构JS代码，添加类型定义&lt;/li&gt;
&lt;li&gt;refactor(请求封装)：优化axios拦截器结构，简化错误处理&lt;/li&gt;
&lt;li&gt;refactor(数据处理)：替换for循环为数组方法，提升代码简洁度&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;6. test：测试相关（新增/修改测试用例、测试代码）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;6-test测试相关新增修改测试用例测试代码&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#6-test%e6%b5%8b%e8%af%95%e7%9b%b8%e5%85%b3%e6%96%b0%e5%a2%9e%e4%bf%ae%e6%94%b9%e6%b5%8b%e8%af%95%e7%94%a8%e4%be%8b%e6%b5%8b%e8%af%95%e4%bb%a3%e7%a0%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;test(登录模块)：新增验证码登录的单元测试用例&lt;/li&gt;
&lt;li&gt;test(订单接口)：补充异常场景（参数为空、权限不足）的测试&lt;/li&gt;
&lt;li&gt;test(工具函数)：修复测试用例中过期的断言逻辑&lt;/li&gt;
&lt;li&gt;test(全局)：新增E2E测试，覆盖核心业务流程&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;7. chore：构建/依赖/工具相关（不影响业务代码）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;7-chore构建依赖工具相关不影响业务代码&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#7-chore%e6%9e%84%e5%bb%ba%e4%be%9d%e8%b5%96%e5%b7%a5%e5%85%b7%e7%9b%b8%e5%85%b3%e4%b8%8d%e5%bd%b1%e5%93%8d%e4%b8%9a%e5%8a%a1%e4%bb%a3%e7%a0%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;chore(package.json)：更新axios依赖版本至1.6.0&lt;/li&gt;
&lt;li&gt;chore(webpack)：新增打包分析插件，优化构建体积&lt;/li&gt;
&lt;li&gt;chore(CI)：配置GitHub Actions自动部署测试环境&lt;/li&gt;
&lt;li&gt;chore(脚本)：新增数据库备份脚本（daily-backup.sh）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;8. perf：性能优化（提升代码运行速度、减少资源占用）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;8-perf性能优化提升代码运行速度减少资源占用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#8-perf%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%e6%8f%90%e5%8d%87%e4%bb%a3%e7%a0%81%e8%bf%90%e8%a1%8c%e9%80%9f%e5%ba%a6%e5%87%8f%e5%b0%91%e8%b5%84%e6%ba%90%e5%8d%a0%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;perf(列表渲染)：使用虚拟滚动优化长列表加载性能&lt;/li&gt;
&lt;li&gt;perf(图片加载)：新增图片懒加载，减少首屏加载时间&lt;/li&gt;
&lt;li&gt;perf(接口请求)：添加请求缓存，减少重复接口调用&lt;/li&gt;
&lt;li&gt;perf(本地存储)：优化localStorage读写逻辑，提升响应速度&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;9. revert：回滚代码（撤销之前的提交）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;9-revert回滚代码撤销之前的提交&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#9-revert%e5%9b%9e%e6%bb%9a%e4%bb%a3%e7%a0%81%e6%92%a4%e9%94%80%e4%b9%8b%e5%89%8d%e7%9a%84%e6%8f%90%e4%ba%a4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;revert: feat(支付模块)：回滚“新增支付宝分期”功能（因兼容性问题）&lt;/li&gt;
&lt;li&gt;revert: fix(首页轮播)：撤销#123提交的修复（导致其他功能异常）&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;四、反面例子&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四反面例子&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9b%e5%8f%8d%e9%9d%a2%e4%be%8b%e5%ad%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;错误1：&lt;code&gt;git commit -m &amp;quot;&amp;quot;&lt;/code&gt;（空消息，完全不知道改了啥）&lt;/li&gt;
&lt;li&gt;错误2：&lt;code&gt;git commit -m &amp;quot;改东西&amp;quot;&lt;/code&gt;（模糊到极致，毫无意义）&lt;/li&gt;
&lt;li&gt;错误3：&lt;code&gt;git commit -m &amp;quot;修复bug&amp;quot;&lt;/code&gt;（没说清哪个bug、哪个模块）&lt;/li&gt;
&lt;li&gt;错误4：&lt;code&gt;git commit -m &amp;quot;今天的修改&amp;quot;&lt;/code&gt;（时间维度没用，无法追溯）&lt;/li&gt;
&lt;li&gt;错误5：&lt;code&gt;git commit -m &amp;quot;新增功能+修复bug+改文档&amp;quot;&lt;/code&gt;（一个提交干多件事，应该拆分）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这就把&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Falcon</title>
      <link>http://localhost:1313/blog/2025/2025-11-29-falcon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-29-falcon/</guid>
      <description>
        
        
        &lt;p&gt;来拜读&lt;strong&gt;梁俊卫老师&lt;/strong&gt;和&lt;strong&gt;龚泽颖学长&lt;/strong&gt;的工作，没准之后红鸟面试还能用上。&lt;/p&gt;
&lt;h2&gt;论文研读&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;论文研读&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%ae%ba%e6%96%87%e7%a0%94%e8%af%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;为解决&lt;strong&gt;社交导航（SocialNav）&lt;strong&gt;中机器人&lt;/strong&gt;&amp;ldquo;短视避障&amp;rdquo;&lt;strong&gt;和现有评估基准不真实的问题，论文提出&lt;/strong&gt;Falcon 框架&lt;/strong&gt;（一种基于强化学习的未来感知社交导航架构），通过&lt;strong&gt;社交认知惩罚（SCP）&lt;/strong&gt;（含障碍碰撞、人类距离、轨迹阻碍三类惩罚）和&lt;strong&gt;时空预知模块（SPM）&lt;/strong&gt;（含人类计数估计、当前位置跟踪、未来轨迹预测三个辅助任务）实现主动避障与社交合规；同时构建&lt;strong&gt;SocialNav 基准&lt;/strong&gt;，包含&lt;strong&gt;Social-HM3D&lt;/strong&gt;（844 个场景）和&lt;strong&gt;Social-MP3D&lt;/strong&gt;（72 个场景）两个高真实感室内数据集，平衡人类密度与自然运动模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实验结果：&lt;/strong&gt; Falcon 在该基准上实现&lt;strong&gt;55% 的任务成功率&lt;/strong&gt;，同时保持约&lt;strong&gt;90% 的个人空间合规率&lt;/strong&gt;，显著优于 &lt;strong&gt;A&lt;/strong&gt;*、&lt;strong&gt;ORCA&lt;/strong&gt; 等规则算法及 &lt;strong&gt;Proximity-Aware&lt;/strong&gt; 等 RL 方法，且具备&lt;strong&gt;零样本泛化能力&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;问题背景&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题背景&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e8%83%8c%e6%99%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;**社交导航（SocialNav）**要求机器人在人类共享环境中遵守社交规范并安全导航，但现有方案存在两大关键问题：&lt;/p&gt;
&lt;h4&gt;1. 算法短视性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-算法短视性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e7%ae%97%e6%b3%95%e7%9f%ad%e8%a7%86%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;传统 &lt;strong&gt;RL 方法&lt;/strong&gt;仅依赖当前环境信息，易出现**&amp;ldquo;短视避障&amp;rdquo;*&lt;em&gt;问题；规则类算法（A&lt;/em&gt;/ORCA）或依赖全局地图，或无法动态适应人类运动。下面以 &lt;strong&gt;Proximity-Aware&lt;/strong&gt; 为例详细说明传统方法的局限性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;传统 RL 方法的基本流程：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;状态表示：&lt;/strong&gt; 通常使用当前时刻的传感器数据（如深度图像、激光雷达）和人类当前位置作为状态输入&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励设计：&lt;/strong&gt; 设计基于&lt;strong&gt;当前距离&lt;/strong&gt;的奖励函数，例如：
&lt;ul&gt;
&lt;li&gt;当机器人与人类距离过近时给予惩罚&lt;/li&gt;
&lt;li&gt;当机器人成功到达目标时给予奖励&lt;/li&gt;
&lt;li&gt;考虑当前时刻的碰撞风险&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略学习：&lt;/strong&gt; 通过强化学习（如 &lt;strong&gt;PPO&lt;/strong&gt;、&lt;strong&gt;A3C&lt;/strong&gt;）训练策略网络，学习在当前状态下选择最优动作&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作执行：&lt;/strong&gt; 策略网络直接输出下一步动作（如前进、转向、停止）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;传统方法的局限性：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;仅考虑当前状态：&lt;/strong&gt; 只基于当前时刻的人类位置和距离进行决策，无法预测人类未来的移动轨迹&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反应式避障：&lt;/strong&gt; 当人类突然改变方向时，机器人只能被动反应，容易出现&amp;quot;短视避障&amp;quot;（即只关注眼前障碍，导致后续路径不佳）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缺乏前瞻性：&lt;/strong&gt; 无法提前规划路径以避免与人类未来轨迹发生冲突，导致效率低下或碰撞风险增加&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Proximity-Aware&lt;/strong&gt; 是一个典型的基于当前距离的 RL 方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;状态空间：&lt;/strong&gt; $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前时刻机器人与人类的距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励函数：&lt;/strong&gt; $r_t = r_{goal} + \alpha \cdot r_{proximity}$，其中：
&lt;ul&gt;
&lt;li&gt;$r_{goal}$ 是到达目标的奖励&lt;/li&gt;
&lt;li&gt;$r_{proximity} = -1/d_t$ 是基于当前距离的惩罚（距离越近惩罚越大）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;问题：&lt;/strong&gt; 这个奖励函数只考虑&lt;strong&gt;当前时刻&lt;/strong&gt;的距离 $d_t$，无法考虑人类未来可能移动到的位置
假设机器人在走廊中需要绕过前方正在移动的人类。&lt;strong&gt;Proximity-Aware&lt;/strong&gt;可能：&lt;/li&gt;
&lt;li&gt;看到人类在左侧，选择向右避让&lt;/li&gt;
&lt;li&gt;但人类可能正在向左移动，导致机器人向右避让后反而与人类未来位置冲突&lt;/li&gt;
&lt;li&gt;需要多次调整，效率低下&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. 基准不真实性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-基准不真实性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%9f%ba%e5%87%86%e4%b8%8d%e7%9c%9f%e5%ae%9e%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;现有数据集（如 &lt;strong&gt;iGibson&lt;/strong&gt;、&lt;strong&gt;Habicrowd&lt;/strong&gt;）场景类型单一、人类行为简化（随机行走 / 无动画）、人类密度失衡，且常假设机器人可获取全局信息，与真实场景脱节。&lt;/p&gt;
&lt;h3&gt;核心创新&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;核心创新&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a0%b8%e5%bf%83%e5%88%9b%e6%96%b0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;因此 &lt;strong&gt;Falcon 框架&lt;/strong&gt;的核心贡献在于是&lt;strong&gt;首个融合显式人类轨迹预测的未来感知 RL 架构&lt;/strong&gt;，通过 &lt;strong&gt;SCP&lt;/strong&gt; 和 &lt;strong&gt;SPM&lt;/strong&gt; 实现主动避障与社交合规。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;一方面&lt;/strong&gt;，该框架引入**&amp;ldquo;社交认知惩罚&amp;rdquo;**（含轨迹阻碍惩罚），鼓励智能体主动规避潜在碰撞并遵守社交礼仪&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;另一方面&lt;/strong&gt;，框架搭载**&amp;ldquo;时空预知模块&amp;rdquo;**，该模块融入包含轨迹预测在内的社交感知辅助任务，以增强智能体在训练过程中对未来动态的理解&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; 这里提到社交导航最初是在 &lt;strong&gt;iGibson 社交导航挑战赛&lt;/strong&gt;提出，是 &lt;strong&gt;PointNav&lt;/strong&gt;基础上增加了移动人类这一元素，那这个领域真的非常新颖了，还是这篇文章首创的&lt;strong&gt;Habitat替代这一静态人形模型&lt;/strong&gt;。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;相关工作&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关工作&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;根据 &lt;strong&gt;Related Works&lt;/strong&gt;，往期人类轨迹预测方法可分为三类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;基于物理学的方法：&lt;/strong&gt; 此类方法从牛顿运动定律中推导显式动力学模型，用于轨迹预测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于学习的方法：&lt;/strong&gt; 聚焦于从观测到的历史轨迹中学习运动模式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于规划的方法：&lt;/strong&gt; 其核心目标是推理理性智能体的运动意图，通过理解智能体的目标及其决策过程来预测轨迹&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Falcon&lt;/strong&gt;借鉴了这些研究思路，提出的方法不仅能预测人类轨迹，还能将社交感知信息融入智能体的导航策略，从而确保智能体在动态场景中实现安全、高效的导航。&lt;/p&gt;
&lt;h3&gt;方法论&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;方法论&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%96%b9%e6%b3%95%e8%ae%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;考虑这样一个社交导航任务：机器人 $a$ 在存在 $N$ 个动态人类（记为 $i \in {1, \ldots, N}$）的环境中导航。机器人从初始位形 $q_a \in Q$ 出发，需持续选择动作以生成一条通往目标位形 $g_a \in Q$ 的路径 $\tau_a$，同时避免与静态障碍及动态人类发生碰撞。&lt;/p&gt;
&lt;p&gt;其总体目标可建模如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable rowspacing=&#34;0.25em&#34; columnalign=&#34;right left&#34; columnspacing=&#34;0em&#34;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;msub&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;arg&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;munder&gt;&lt;mrow&gt;&lt;mi&gt;min&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;mtext&gt;s.t.&lt;/mtext&gt;&lt;mspace width=&#34;1em&#34;/&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo mathvariant=&#34;normal&#34;&gt;∉&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mspace width=&#34;1em&#34;/&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;∩&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∅&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mspace width=&#34;1em&#34;/&gt;&lt;msub&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
\begin{aligned}
\tau_a &amp;amp;= \arg \min_{\tau \in T} \left( c_a(\tau) + \lambda_a c_a^s(\tau, \tau_{1:N}) \right) \\
\text{s.t.} \quad &amp;amp;A_a(\tau_a) \notin C_{obs}, \quad A_a(\tau_a) \cap A_i(\tau_i) = \emptyset, \\
&amp;amp;\tau_a(0) = q_a, \quad \tau_a(T) = g_a
\end{aligned}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:4.9117em;vertical-align:-2.2059em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mtable&#34;&gt;&lt;span class=&#34;col-align-r&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:2.7059em;&#34;&gt;&lt;span style=&#34;top:-4.8659em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.9541em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;s.t.&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:1em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-1.4541em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:2.2059em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;col-align-l&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:2.7059em;&#34;&gt;&lt;span style=&#34;top:-4.8659em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop&#34;&gt;ar&lt;span style=&#34;margin-right:0.01389em;&#34;&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6679em;&#34;&gt;&lt;span style=&#34;top:-2.3557em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;∈&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.13889em;&#34;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop&#34;&gt;min&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7717em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;λ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.9541em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mrel&#34;&gt;∈&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord vbox&#34;&gt;&lt;span class=&#34;thinbox&#34;&gt;&lt;span class=&#34;llap&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;inner&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.0556em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;fix&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07153em;&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:1em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;∩&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∅&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-1.4541em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;q&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:1em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:2.2059em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$c_a$ 为引导机器人前往目标的路径成本&lt;/li&gt;
&lt;li&gt;$c_a^s$ 为考虑社会规范的成本项&lt;/li&gt;
&lt;li&gt;$A(\tau)$ 表示轨迹 $\tau$ 所占据的体积&lt;/li&gt;
&lt;li&gt;$C_{obs}$ 表示静态障碍&lt;/li&gt;
&lt;li&gt;$T$ 为任务回合结束时间&lt;/li&gt;
&lt;li&gt;$\lambda_a$ 为权重因子&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;约束条件确保机器人在到达目标前，不会与静态障碍或人类发生碰撞。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/falcon-overview.png&#34; alt=&#34;Falcon Overview&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如上图，&lt;strong&gt;主策略网络（Main Policy Network）&lt;strong&gt;在每个时间步以&lt;/strong&gt;深度图像（depth image）&lt;strong&gt;和&lt;/strong&gt;点目标（point goal）&lt;strong&gt;为输入，直接输出机器人下一步的动作。该网络的训练结合了&lt;/strong&gt;点目标导航（PointNav）&lt;strong&gt;的奖励与本文提出的&lt;/strong&gt;社交认知惩罚（Social Cognition Penalty, SCP）&lt;/strong&gt;。此外，主策略网络还搭配了&lt;strong&gt;时空预知模块（Spatial-Temporal Precognition Module）&lt;/strong&gt;，该模块在训练过程中支持多个辅助任务的执行。&lt;/p&gt;
&lt;h3&gt;主策略网络（Main Policy Network）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;主策略网络main-policy-network&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%bb%e7%ad%96%e7%95%a5%e7%bd%91%e7%bb%9cmain-policy-network&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Main Policy Network（主策略网络）&lt;/strong&gt;—— 导航的&amp;quot;核心决策大脑&amp;quot;&lt;/p&gt;
&lt;p&gt;其实这个模块是整个系统的&amp;quot;&lt;strong&gt;指挥官&lt;/strong&gt;&amp;quot;，核心作用是：接收传感器数据，分析当前环境和自身状态，最终输出具体的导航动作（比如向前走、左转、右转、减速），同时通过奖励/惩罚机制不断学习&amp;quot;更优的导航策略&amp;quot;。&lt;/p&gt;
&lt;h4&gt;1. 核心定位&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-核心定位&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e6%a0%b8%e5%bf%83%e5%ae%9a%e4%bd%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;它是智能体的&amp;quot;&lt;strong&gt;行动决策中心&lt;/strong&gt;&amp;quot;—— 既要保证导航效率（尽快到达目标），又要遵守社交规则（不碰撞、不挡路、保持安全距离），是直接指导智能体移动的核心模块。&lt;/p&gt;
&lt;h4&gt;2. 内部结构与工作流程（输入→处理→输出）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-内部结构与工作流程输入处理输出&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%86%85%e9%83%a8%e7%bb%93%e6%9e%84%e4%b8%8e%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b%e8%be%93%e5%85%a5%e5%a4%84%e7%90%86%e8%be%93%e5%87%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;可以把它想象成&amp;quot;一个带&amp;rsquo;翻译官&amp;rsquo;&amp;lsquo;记忆大师&amp;rsquo;和&amp;rsquo;决策+评估团队&amp;rsquo;的指挥系统&amp;quot;，步骤如下：&lt;/p&gt;
&lt;h5&gt;（1）输入：智能体&amp;quot;看到&amp;quot;和&amp;quot;知道&amp;quot;的信息&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1输入智能体看到和知道的信息&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1%e8%be%93%e5%85%a5%e6%99%ba%e8%83%bd%e4%bd%93%e7%9c%8b%e5%88%b0%e5%92%8c%e7%9f%a5%e9%81%93%e7%9a%84%e4%bf%a1%e6%81%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心输入 1：深度图像（RGBD 中的 Depth 部分）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;相当于智能体的&amp;quot;&lt;strong&gt;眼睛&lt;/strong&gt;&amp;quot;，能看到周围的墙壁、障碍物、人类的轮廓和距离（比如&amp;quot;前方 3 米有个人，左侧 2 米有个柜子&amp;quot;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心输入 2：相对目标坐标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;相当于智能体的&amp;quot;&lt;strong&gt;导航目的地&lt;/strong&gt;&amp;quot;，比如&amp;quot;目标在我前方 5 米、偏右 10 度的位置&amp;quot;（不需要全局地图，只需要自己和目标的相对位置）。&lt;/p&gt;
&lt;h5&gt;（2）处理：把&amp;quot;原始信息&amp;quot;变成&amp;quot;决策依据&amp;quot;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2处理把原始信息变成决策依据&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2%e5%a4%84%e7%90%86%e6%8a%8a%e5%8e%9f%e5%a7%8b%e4%bf%a1%e6%81%af%e5%8f%98%e6%88%90%e5%86%b3%e7%ad%96%e4%be%9d%e6%8d%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;这个过程分 3 步，对应模块里的关键组件：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ResNet-50 编码器：视觉翻译官&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;深度图像是&amp;quot;像素组成的图片&amp;quot;，机器看不懂，&lt;strong&gt;ResNet-50&lt;/strong&gt; 的作用就是把图片&amp;quot;翻译&amp;quot;成机器能理解的&amp;quot;数字特征向量&amp;quot;（比如用一串数字代表&amp;quot;前方 3 米有人类&amp;quot;&amp;ldquo;左侧是静态柜子&amp;rdquo;）。&lt;/p&gt;
&lt;p&gt;简单说：它负责提取环境的&amp;quot;&lt;strong&gt;关键视觉信息&lt;/strong&gt;&amp;quot;，过滤无用细节（比如墙壁的纹理、人类的衣服颜色），只保留和导航相关的核心特征（距离、障碍物类型、人类位置）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2 层 LSTM：时间记忆大师&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;导航是&amp;quot;连续的过程&amp;quot;，不是单张图片能决定的（比如&amp;quot;前一秒那个人在走，这一秒停了，下一秒可能继续走&amp;quot;）。&lt;strong&gt;LSTM&lt;/strong&gt; 的作用是&amp;quot;记住时间序列的变化&amp;quot;，处理&amp;quot;&lt;strong&gt;时序依赖&lt;/strong&gt;&amp;quot;。&lt;/p&gt;
&lt;p&gt;比如：它会整合&amp;quot;过去 5 个时间步的视觉特征&amp;quot;，判断人类的移动趋势（&amp;ldquo;这个人一直在朝我这边走，速度大概 0.5 米/秒&amp;rdquo;），而不是只看&amp;quot;当下这一帧&amp;quot;的静态位置 —— 这能避免智能体&amp;quot;短视&amp;quot;，比如不会因为当下距离够远就忽视正在靠近的人类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Actor-Critic（演员-评论家）头：决策+评估团队&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;经过 &lt;strong&gt;ResNet-50&lt;/strong&gt; 和 &lt;strong&gt;LSTM&lt;/strong&gt; 处理后，得到了&amp;quot;当前环境特征 + 历史变化趋势&amp;quot;，接下来由这个&amp;quot;团队&amp;quot;输出最终决策：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;演员头（Actor Head）&lt;/strong&gt;：&amp;quot;&lt;strong&gt;决策者&lt;/strong&gt;&amp;quot;—— 输出具体的导航动作，比如&amp;quot;向前移动 0.3 米&amp;quot;&amp;ldquo;左转 15 度&amp;quot;&amp;ldquo;保持静止&amp;rdquo;（动作是连续的，不是固定的几个选项）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评论家头（Critic Head）&lt;/strong&gt;：&amp;rdquo;&lt;strong&gt;评估师&lt;/strong&gt;&amp;quot;—— 不直接做决策，而是评估&amp;quot;演员头做出的这个动作好不好&amp;quot;，输出一个&amp;quot;价值分数&amp;quot;（比如&amp;quot;这个动作能让你更快到目标，且不会撞人，得分 8 分&amp;quot;&amp;ldquo;这个动作会挡路，得分 2 分&amp;rdquo;）&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;（3）输出：具体的导航动作&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3输出具体的导航动作&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3%e8%be%93%e5%87%ba%e5%85%b7%e4%bd%93%e7%9a%84%e5%af%bc%e8%88%aa%e5%8a%a8%e4%bd%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;最终由演员头输出&amp;quot;&lt;strong&gt;连续的控制指令&lt;/strong&gt;&amp;quot;（比如速度、转向角度），直接驱动智能体移动。&lt;/p&gt;
&lt;h4&gt;3. 训练逻辑：怎么让&amp;quot;大脑&amp;quot;学会&amp;quot;好策略&amp;quot;？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-训练逻辑怎么让大脑学会好策略&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e8%ae%ad%e7%bb%83%e9%80%bb%e8%be%91%e6%80%8e%e4%b9%88%e8%ae%a9%e5%a4%a7%e8%84%91%e5%ad%a6%e4%bc%9a%e5%a5%bd%e7%ad%96%e7%95%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;主策略网络是通过&amp;quot;&lt;strong&gt;奖励机制&lt;/strong&gt;&amp;ldquo;学习的 —— 就像训练宠物：做得好就给奖励，做得差就给惩罚，慢慢形成条件反射。核心是之前提到的 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;R_t^{socialnav}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1461em;vertical-align:-0.247em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.00773em;&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8991em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.0077em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;soc&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ia&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;na&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 奖励函数，简化理解就是：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;加分项：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;靠近目标（&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;Δ&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;-\beta_d \Delta_d&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;Δ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，距离目标越近，加分越多）&lt;/li&gt;
&lt;li&gt;成功到达目标（&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\beta_{succ} \cdot I_{succ}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;cc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;cc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，直接加大额奖励）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;扣分项：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无意义动作（&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;-r_{slack}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，比如原地打转）&lt;/li&gt;
&lt;li&gt;碰撞（&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;-r_{coll}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;ll&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，撞墙或撞人扣大分）&lt;/li&gt;
&lt;li&gt;离人太近（&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;-r_{prox}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ro&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，距离小于 2 米扣分，越近扣越多）&lt;/li&gt;
&lt;li&gt;挡人类轨迹（&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;-r_{traj}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，挡住别人要走的路扣分）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过不断迭代训练（&lt;strong&gt;7500 万步&lt;/strong&gt;），主策略网络会逐渐学会&amp;rdquo;&lt;strong&gt;平衡效率和合规&lt;/strong&gt;&amp;quot;—— 既不会为了快而撞人，也不会为了合规而绕远路。&lt;/p&gt;
&lt;h4&gt;4. 与时空预认知模块的协作关系&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-与时空预认知模块的协作关系&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e4%b8%8e%e6%97%b6%e7%a9%ba%e9%a2%84%e8%ae%a4%e7%9f%a5%e6%a8%a1%e5%9d%97%e7%9a%84%e5%8d%8f%e4%bd%9c%e5%85%b3%e7%b3%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;主策略网络：&lt;/strong&gt; 负责&amp;quot;&lt;strong&gt;当下该做什么动作&lt;/strong&gt;&amp;quot;（比如现在走还是转）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时空预认知模块：&lt;/strong&gt; 负责&amp;quot;&lt;strong&gt;未来会发生什么&lt;/strong&gt;&amp;quot;（比如人类会走到哪）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;协作逻辑：&lt;/strong&gt; 主策略网络根据&amp;quot;当下状态 + 未来预测&amp;quot;做决策 —— 比如当下离人 3 米（安全），但预测 1 秒后会到 1.5 米（危险），主策略网络就会提前调整轨迹，而不是等进入危险区再反应。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;主策略网络包含两个核心组件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;状态编码器（State Encoders）&lt;/strong&gt;：从观测中提取视觉与时序特征&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;社交认知惩罚（SCP）&lt;/strong&gt;：促进社交合规性的惩罚机制&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;网络架构（技术细节）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;网络架构技术细节&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%bd%91%e7%bb%9c%e6%9e%b6%e6%9e%84%e6%8a%80%e6%9c%af%e7%bb%86%e8%8a%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;主策略网络的处理流程分为四个步骤，具体技术实现如下：&lt;/p&gt;
&lt;h5&gt;第一步：输入编码（Linear Encoder + Visual Encoder）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第一步输入编码linear-encoder--visual-encoder&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%80%e6%ad%a5%e8%be%93%e5%85%a5%e7%bc%96%e7%a0%81linear-encoder--visual-encoder&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;点目标 + GPS+Compass → 线性编码器（Linear Encoder）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;点目标的坐标（如 $g_a \in Q$）和 GPS 定位的机器人当前位置（$q_a \in Q$）&lt;/li&gt;
&lt;li&gt;先计算&amp;quot;目标相对距离与方向&amp;quot;&lt;/li&gt;
&lt;li&gt;通过线性变换转化为低维特征向量 $f_{goal}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;深度图像 → 视觉编码器（Visual Encoder：ResNet-50）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过预训练的 &lt;strong&gt;ResNet-50 模型 [51]&lt;/strong&gt; 提取视觉特征 $f_{depth}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第二步：时序建模（Recurrent State Encoder：2 层 LSTM）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第二步时序建模recurrent-state-encoder2-层-lstm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%ba%8c%e6%ad%a5%e6%97%b6%e5%ba%8f%e5%bb%ba%e6%a8%a1recurrent-state-encoder2-%e5%b1%82-lstm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt; 将线性编码器输出的 $f_{goal}$ 和 ResNet-50 输出的 $f_{depth}$ 拼接，得到融合特征：
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
f_{fusion} = [f_{goal}, f_{depth}]
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9805em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;pt&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;输出：&lt;/strong&gt; LSTM 会输出两个关键结果：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;时序特征 $h_t$&lt;/strong&gt;：捕捉&amp;quot;前 t 步环境变化&amp;quot;的动态信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;潜在变量 $\delta_R$&lt;/strong&gt;：LSTM 的隐藏状态，其核心作用是&amp;quot;将主网络的时序记忆传递给 SPM&amp;quot;，让 SPM 能基于主网络的&amp;quot;观察记忆&amp;quot;学习辅助任务&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;第三步：输出决策（Actor Head + Value Head）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第三步输出决策actor-head--value-head&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%89%e6%ad%a5%e8%be%93%e5%87%ba%e5%86%b3%e7%ad%96actor-head--value-head&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;时序特征 $h_t$ 会输入到两个&amp;quot;头网络&amp;quot;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Actor Head（动作头）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于 $h_t$ 输出机器人的下一步动作，决定了机器人的实际导航路径 $\tau_a$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Value Head（价值头）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于 $h_t$ 预测当前动作的&amp;quot;预期奖励&amp;quot; $V(h_t)$，辅助强化学习的训练（PPO 算法需要通过&amp;quot;预测奖励&amp;quot;与&amp;quot;实际奖励&amp;quot;的差异更新策略）&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第四步：主损失 $L_{main}$ 的来源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第四步主损失-l_main-的来源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e5%9b%9b%e6%ad%a5%e4%b8%bb%e6%8d%9f%e5%a4%b1-l_main-%e7%9a%84%e6%9d%a5%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;主策略网络的训练目标是&amp;quot;让机器人既快又安全地抵达目标&amp;quot;，其损失 $L_{main}$ 来自 &lt;strong&gt;DD-PPO 算法 [53]&lt;/strong&gt; 的 PPO 损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PPO 损失的核心逻辑：&lt;/strong&gt; &amp;ldquo;约束策略更新的幅度，避免更新过快导致不稳定&amp;rdquo;&lt;/p&gt;
&lt;p&gt;其简化公式为：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;double-struck&#34;&gt;E&lt;/mi&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;[&lt;/mo&gt;&lt;mi&gt;min&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msup&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mtext&gt;clip&lt;/mtext&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo fence=&#34;true&#34;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
L_{PPO} = \mathbb{E}_{\hat{\pi}} \left[ \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a), \text{clip}\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}, 1-\epsilon, 1+\epsilon \right) A^{\pi_{\theta_{old}}}(s,a) \right) \right]
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;PPO&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:2.4em;vertical-align:-0.95em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathbb&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord accent mtight&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6944em;&#34;&gt;&lt;span style=&#34;top:-2.7em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.7em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.25em;&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size3&#34;&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mop&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size3&#34;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.427em;&#34;&gt;&lt;span style=&#34;top:-2.314em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3448em;&#34;&gt;&lt;span style=&#34;top:-2.3488em;margin-left:-0.0278em;margin-right:0.0714em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.5em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size3 size1 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1512em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2559em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.677em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9419em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3448em;&#34;&gt;&lt;span style=&#34;top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.5em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size3 size1 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3448em;&#34;&gt;&lt;span style=&#34;top:-2.3448em;margin-left:-0.0278em;margin-right:0.1em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.6944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3496em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.401em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;clip&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size3&#34;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.427em;&#34;&gt;&lt;span style=&#34;top:-2.314em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3448em;&#34;&gt;&lt;span style=&#34;top:-2.3488em;margin-left:-0.0278em;margin-right:0.0714em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.5em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size3 size1 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1512em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2559em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.677em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9419em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;ϵ&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;ϵ&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size3&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3448em;&#34;&gt;&lt;span style=&#34;top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.5em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size3 size1 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3448em;&#34;&gt;&lt;span style=&#34;top:-2.3448em;margin-left:-0.0278em;margin-right:0.1em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.6944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3496em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.401em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size3&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size3&#34;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\pi_{\theta}(a|s)$ 是当前策略（主网络）在状态 $s$ 下选择动作 $a$ 的概率&lt;/li&gt;
&lt;li&gt;$\pi_{\theta_{old}}(a|s)$ 是上一轮策略的概率（用于约束更新幅度）&lt;/li&gt;
&lt;li&gt;$A^{\pi_{\theta_{old}}}(s,a)$ 是优势函数（衡量&amp;quot;当前动作比平均动作好多少&amp;quot;，与奖励函数 $R_t^{socialnav}$ 直接相关，而 $R_t^{socialnav}$ 包含了 SCP 惩罚，因此 SCP 会间接影响 $L_{main}$）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;简言之，&lt;/strong&gt; $L_{main}$ 是&amp;quot;主策略网络决策质量的量化指标&amp;quot;：动作越接近&amp;quot;抵达目标 + 遵守社交规则&amp;quot;，$L_{main}$ 越小。&lt;/p&gt;
&lt;h4&gt;PointNav 奖励函数&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;pointnav-奖励函数&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#pointnav-%e5%a5%96%e5%8a%b1%e5%87%bd%e6%95%b0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;训练过程中，策略的更新由一个鼓励&amp;quot;达成目标&amp;quot;行为的奖励函数引导。每个时间步 $t$ 采用经典的 &lt;strong&gt;PointNav 奖励函数&lt;/strong&gt;：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;Δ&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
R_t^{pointnav} = -\beta_d \Delta_d - r_{slack} + \beta_{succ} \cdot I_{succ}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1883em;vertical-align:-0.2458em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.00773em;&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9426em;&#34;&gt;&lt;span style=&#34;top:-2.4542em;margin-left:-0.0077em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.1809em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;in&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;na&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2458em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;Δ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;cc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;cc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Delta_d$ 为机器人到目标的测地线距离变化量&lt;/li&gt;
&lt;li&gt;$r_{slack}$ 为防止不必要动作的步长惩罚项&lt;/li&gt;
&lt;li&gt;$I_{succ}$ 为导航成功的指示变量（成功时为 1，否则为 0）&lt;/li&gt;
&lt;li&gt;$\beta_d$ 与 $\beta_{succ}$ 为权重系数&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;社交认知惩罚（SCP）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;社交认知惩罚scp&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a4%be%e4%ba%a4%e8%ae%a4%e7%9f%a5%e6%83%a9%e7%bd%9ascp&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;然而，&lt;strong&gt;PointNav 奖励函数&lt;/strong&gt;未考虑动态环境与社交交互，无法满足社交导航（SocialNav）的需求。为此，本文引入&lt;strong&gt;社交认知惩罚（SCP）&lt;/strong&gt;——一套用于促进机器人遵守社会规范的惩罚机制，具体包含以下三类惩罚：&lt;/p&gt;
&lt;h5&gt;1. 障碍碰撞惩罚（Obstacle Collision Penalty）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-障碍碰撞惩罚obstacle-collision-penalty&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e9%9a%9c%e7%a2%8d%e7%a2%b0%e6%92%9e%e6%83%a9%e7%bd%9aobstacle-collision-penalty&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;该惩罚针对机器人与静态障碍或人类发生碰撞的行为，计算公式如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;_&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;_&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
r_{coll} = \beta_s \cdot I_{s\_coll} + \beta_h \cdot I_{h\_coll}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.5806em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;ll&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0503em;vertical-align:-0.367em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mord mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;ll&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.367em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0503em;vertical-align:-0.367em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;mord mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;ll&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.367em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$I_{s_coll}$ 和 $I_{h_coll}$ 分别为表示&amp;quot;与静态障碍碰撞&amp;quot;和&amp;quot;与人类碰撞&amp;quot;的指示变量（发生碰撞时为 1，否则为 0）&lt;/li&gt;
&lt;li&gt;$\beta_s$ 和 $\beta_h$ 为对应的惩罚权重&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2. 人类距离惩罚（Human Proximity Penalty）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-人类距离惩罚human-proximity-penalty&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e4%ba%ba%e7%b1%bb%e8%b7%9d%e7%a6%bb%e6%83%a9%e7%bd%9ahuman-proximity-penalty&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;该惩罚确保机器人与人类保持安全距离，计算公式如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;{&lt;/mo&gt;&lt;mtable rowspacing=&#34;0.36em&#34; columnalign=&#34;left left&#34; columnspacing=&#34;1em&#34;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mrow&gt;&lt;mtext&gt;若 &lt;/mtext&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;2.0&lt;/mn&gt;&lt;mtext&gt; m&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mrow&gt;&lt;mtext&gt;若 &lt;/mtext&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;≥&lt;/mo&gt;&lt;mn&gt;2.0&lt;/mn&gt;&lt;mtext&gt; m&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
r_{prox} = \sum_{i=1}^{N} \begin{cases}
\beta_{prox} \cdot \exp(-d_i^t) &amp;amp; \text{若 } d_i^t &amp;lt; 2.0 \text{ m} \\
0 &amp;amp; \text{若 } d_i^t \geq 2.0 \text{ m}
\end{cases}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ro&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.106em;vertical-align:-1.2777em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.8283em;&#34;&gt;&lt;span style=&#34;top:-1.8723em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-4.3em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.2777em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size4&#34;&gt;{&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mtable&#34;&gt;&lt;span class=&#34;col-align-l&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.69em;&#34;&gt;&lt;span style=&#34;top:-3.69em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ro&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop&#34;&gt;exp&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7936em;&#34;&gt;&lt;span style=&#34;top:-2.4413em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2587em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.25em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.19em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;arraycolsep&#34; style=&#34;width:1em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;col-align-l&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.69em;&#34;&gt;&lt;span style=&#34;top:-3.69em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord cjk_fallback&#34;&gt;若&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7936em;&#34;&gt;&lt;span style=&#34;top:-2.4413em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2587em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;2.0&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt; m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.25em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord cjk_fallback&#34;&gt;若&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7936em;&#34;&gt;&lt;span style=&#34;top:-2.4413em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2587em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;≥&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;2.0&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt; m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.19em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;式中，$d_i^t = |\tau_a(t) - \tau_i(t)|$ 表示时间步 $t$ 时机器人与第 $i$ 个人类的欧氏距离。当 $d_i^t$ 减小时，惩罚呈指数增长，以促使机器人主动避开人类；当机器人距离目标不足 2.0 米时，该惩罚自动取消。&lt;/p&gt;
&lt;h5&gt;3. 轨迹阻碍惩罚（Trajectory Obstruction Penalty）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-轨迹阻碍惩罚trajectory-obstruction-penalty&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e8%bd%a8%e8%bf%b9%e9%98%bb%e7%a2%8d%e6%83%a9%e7%bd%9atrajectory-obstruction-penalty&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;该惩罚用于阻止机器人阻碍人类未来 $H$ 步的轨迹。它通过同时考虑&amp;quot;当前与未来位置&amp;quot;预判潜在阻碍，且对&amp;quot;早期轨迹重叠&amp;quot;的惩罚更重，计算公式如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;{&lt;/mo&gt;&lt;mtable rowspacing=&#34;0.36em&#34; columnalign=&#34;left left&#34; columnspacing=&#34;1em&#34;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mrow&gt;&lt;mtext&gt;若 &lt;/mtext&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;0.05&lt;/mn&gt;&lt;mtext&gt; m&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;false&#34;&gt;&lt;mrow&gt;&lt;mtext&gt;若 &lt;/mtext&gt;&lt;msubsup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;≥&lt;/mo&gt;&lt;mn&gt;0.05&lt;/mn&gt;&lt;mtext&gt; m&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
r_{traj} = \sum_{k=t+1}^{t+H} \sum_{i=1}^{N} \begin{cases}
\beta_{traj} \cdot \frac{1}{k-t+1} &amp;amp; \text{若 } d_{traj-i}^k &amp;lt; 0.05 \text{ m} \\
0 &amp;amp; \text{若 } d_{traj-i}^k \geq 0.05 \text{ m}
\end{cases}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.1888em;vertical-align:-1.3604em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.8283em;&#34;&gt;&lt;span style=&#34;top:-1.8479em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-4.3em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.08125em;&#34;&gt;H&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3604em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.8283em;&#34;&gt;&lt;span style=&#34;top:-1.8723em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-4.3em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.2777em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size4&#34;&gt;{&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mtable&#34;&gt;&lt;span class=&#34;col-align-l&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.69em;&#34;&gt;&lt;span style=&#34;top:-3.69em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8451em;&#34;&gt;&lt;span style=&#34;top:-2.655em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.394em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.4033em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.25em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.19em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;arraycolsep&#34; style=&#34;width:1em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;col-align-l&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.69em;&#34;&gt;&lt;span style=&#34;top:-3.69em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord cjk_fallback&#34;&gt;若&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8491em;&#34;&gt;&lt;span style=&#34;top:-2.4413em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3948em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;0.05&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt; m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.25em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.008em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord cjk_fallback&#34;&gt;若&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8491em;&#34;&gt;&lt;span style=&#34;top:-2.4413em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3948em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;≥&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;0.05&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt; m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.19em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$d_{traj-i}^k = |\tau_a(k) - \tilde{\tau}_i(k)|$ 表示时间步 $k$ 时机器人与第 $i$ 个人类&amp;quot;未来轨迹&amp;quot;的距离&lt;/li&gt;
&lt;li&gt;$\frac{1}{k-t+1}$ 为时间衰减因子，确保距离当前时间越近的轨迹重叠，惩罚权重越大&lt;/li&gt;
&lt;li&gt;当机器人距离目标不足 2.0 米时，该惩罚同样取消&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;总奖励函数&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;总奖励函数&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%bb%e5%a5%96%e5%8a%b1%e5%87%bd%e6%95%b0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;社交导航的总奖励函数为&amp;quot;目标导向奖励&amp;quot;与&amp;quot;社交认知惩罚&amp;quot;的差值：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
R_t^{socialnav} = R_t^{pointnav} - R_t^{scp}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1461em;vertical-align:-0.247em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.00773em;&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8991em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.0077em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;soc&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ia&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;na&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1883em;vertical-align:-0.2458em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.00773em;&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9426em;&#34;&gt;&lt;span style=&#34;top:-2.4542em;margin-left:-0.0077em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.1809em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;in&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;na&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2458em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0281em;vertical-align:-0.2458em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.00773em;&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7823em;&#34;&gt;&lt;span style=&#34;top:-2.4542em;margin-left:-0.0077em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.1809em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;sc&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2458em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中，$R_t^{scp}$ 为社交认知惩罚的总和：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
R_t^{scp} = r_{coll} + r_{prox} + r_{traj}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0281em;vertical-align:-0.2458em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.00773em;&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7823em;&#34;&gt;&lt;span style=&#34;top:-2.4542em;margin-left:-0.0077em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.1809em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;sc&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2458em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;ll&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ro&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;主策略网络采用 &lt;strong&gt;DD-PPO 算法&lt;/strong&gt;进行训练，优化目标为 PPO 损失 $L_{main}$。&lt;/p&gt;
&lt;h3&gt;时空预知模块（Spatial-Temporal Precognition Module）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;时空预知模块spatial-temporal-precognition-module&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%97%b6%e7%a9%ba%e9%a2%84%e7%9f%a5%e6%a8%a1%e5%9d%97spatial-temporal-precognition-module&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;SPM 的核心作用：&lt;/strong&gt; &amp;ldquo;利用主网络的记忆（$\delta_R$），学习与人类相关的&amp;rsquo;经验&amp;rsquo;，辅助主网络更聪明地决策&amp;rdquo;。&lt;/p&gt;
&lt;h4&gt;模块架构与输入设计&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;模块架构与输入设计&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a8%a1%e5%9d%97%e6%9e%b6%e6%9e%84%e4%b8%8e%e8%be%93%e5%85%a5%e8%ae%be%e8%ae%a1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;**时空预认知模块（Spatial-Temporal Precognition Module）**的核心输入完全来自主策略网络的&amp;quot;中间处理结果&amp;quot;，没有额外新增传感器数据。核心目的是&amp;quot;&lt;strong&gt;复用特征、节省计算&lt;/strong&gt;&amp;quot;，同时让&amp;quot;预判能力&amp;quot;和&amp;quot;决策能力&amp;quot;基于同一套环境理解，避免信息脱节。&lt;/p&gt;
&lt;h5&gt;一、核心输入：主策略网络的状态编码 $\delta_R$&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一核心输入主策略网络的状态编码-delta_r&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e6%a0%b8%e5%bf%83%e8%be%93%e5%85%a5%e4%b8%bb%e7%ad%96%e7%95%a5%e7%bd%91%e7%bb%9c%e7%9a%84%e7%8a%b6%e6%80%81%e7%bc%96%e7%a0%81-delta_r&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;来源：&lt;/strong&gt; 主策略网络中&amp;quot;&lt;strong&gt;ResNet-50 编码器 + 2 层 LSTM&lt;/strong&gt;&amp;ldquo;的输出结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先由 &lt;strong&gt;ResNet-50&lt;/strong&gt; 把&amp;quot;深度图像&amp;quot;翻译成&amp;quot;视觉特征向量&amp;rdquo;（比如&amp;quot;前方 3 米有人类、左侧 2 米是柜子&amp;quot;）&lt;/li&gt;
&lt;li&gt;再经 &lt;strong&gt;2 层 LSTM&lt;/strong&gt; 处理&amp;quot;时序依赖&amp;quot;（比如&amp;quot;这个人前 3 步朝我走，速度 0.5 米/秒&amp;quot;）&lt;/li&gt;
&lt;li&gt;最终输出的 $\delta_R$ 是&amp;quot;浓缩了当前环境 + 历史变化&amp;quot;的高维数字特征，相当于主策略网络整理好的&amp;quot;&lt;strong&gt;环境情报汇总&lt;/strong&gt;&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 时空预认知模块直接用这个&amp;quot;情报汇总&amp;quot;做预测，不用再重复处理原始深度图像，既高效又能保证&amp;quot;预判和决策基于同一套环境理解&amp;quot;（比如主策略网络认为&amp;quot;那是个人&amp;quot;，预认知模块不会误判为&amp;quot;障碍物&amp;quot;）。&lt;/p&gt;
&lt;h5&gt;二、辅助输入：Auxiliary Information（$S_R, N, P$）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二辅助输入auxiliary-informations_r-n-p&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8c%e8%be%85%e5%8a%a9%e8%be%93%e5%85%a5auxiliary-informations_r-n-p&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;这部分是从 $\delta_R$ 中解析出的&amp;quot;具体结构化信息&amp;quot;，不是额外输入，而是对 $\delta_R$ 的&amp;quot;拆解和明确&amp;quot;，方便模块针对性预测：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$S_R$（Spatial Relationship）&lt;/strong&gt;：人类与智能体的&amp;quot;相对空间关系&amp;quot;，比如&amp;quot;人类在智能体前方 30 度、距离 2.5 米&amp;quot;，从 $\delta_R$ 中的视觉特征和目标坐标推导而来&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$N$（Number of Humans）&lt;/strong&gt;：当前环境中检测到的人类数量（初始值来自主策略网络的初步特征解析，后续会被辅助任务的 classifier 优化）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$P$（Human Positions）&lt;/strong&gt;：人类相对于智能体的&amp;quot;当前位置坐标&amp;quot;（比如&amp;quot;人类 A 在 $(x=2.3, y=1.5)$ 米处&amp;quot;），同样从 $\delta_R$ 中提取的视觉特征回归得到&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;简单说：&lt;/strong&gt; 核心输入是&amp;quot;浓缩情报&amp;quot; $\delta_R$，辅助输入是&amp;quot;拆解后的具体情报&amp;quot; $S_R, N, P$，两者都来自主策略网络，没有额外传感器开销。&lt;/p&gt;
&lt;h5&gt;三、Auxiliary Information 的具体来源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三auxiliary-information-的具体来源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89auxiliary-information-%e7%9a%84%e5%85%b7%e4%bd%93%e6%9d%a5%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;这三个信息不是&amp;quot;凭空产生&amp;quot;，而是主策略网络处理原始数据后，从&amp;quot;视觉特征 + 时序特征&amp;quot;中解析出来的，流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;原始数据&lt;/strong&gt;（深度图像 + 相对目标坐标）→ &lt;strong&gt;ResNet-50&lt;/strong&gt; → 提取&amp;quot;静态视觉特征&amp;quot;（包含障碍物、人类的轮廓、距离信息）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;静态视觉特征&lt;/strong&gt; → &lt;strong&gt;2 层 LSTM&lt;/strong&gt; → 融合&amp;quot;时序特征&amp;quot;（人类移动趋势、环境变化）→ 生成 $\delta_R$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;从 $\delta_R$ 中进一步解析：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;通过&amp;quot;空间关系提取器&amp;quot;得到 $S_R$（相对角度、距离）&lt;/li&gt;
&lt;li&gt;通过&amp;quot;初步人数检测&amp;quot;得到 $N$（比如先判断&amp;quot;有 2 个人类&amp;quot;，后续由 classifier 优化精度）&lt;/li&gt;
&lt;li&gt;通过&amp;quot;初步位置回归&amp;quot;得到 $P$（比如先粗略预测&amp;quot;人类在前方 2-3 米处&amp;quot;，后续由 regressor 优化坐标精度）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键结论：&lt;/strong&gt; Auxiliary Information 是从主策略网络的状态编码 $\delta_R$ 中解析出的结构化信息，不是额外传感器数据，也不是预定义的固定值——会随着 $\delta_R$ 的更新（每个时间步都更新）而动态变化，比如人类移动后，$S_R$ 和 $P$ 会实时更新。&lt;/p&gt;
&lt;h5&gt;四、Classifier 和两个 Regressor 的具体对应&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四classifier-和两个-regressor-的具体对应&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bclassifier-%e5%92%8c%e4%b8%a4%e4%b8%aa-regressor-%e7%9a%84%e5%85%b7%e4%bd%93%e5%af%b9%e5%ba%94&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;这三个组件是时空预认知模块的&amp;quot;核心工作单元&amp;quot;，分别对应 Falcon 框架的三个辅助任务，本质是&amp;quot;通过多任务学习，让模块同时掌握&amp;rsquo;数人数、定位置、猜轨迹&amp;rsquo;三种能力&amp;quot;，最终提升时空理解能力。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;组件类型&lt;/th&gt;
          &lt;th&gt;对应辅助任务&lt;/th&gt;
          &lt;th&gt;核心作用&lt;/th&gt;
          &lt;th&gt;输入（均为 $\delta_R$）&lt;/th&gt;
          &lt;th&gt;输出结果&lt;/th&gt;
          &lt;th&gt;损失函数&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Classifier（分类器）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;人数估计（Population Estimation）&lt;/td&gt;
          &lt;td&gt;预测当前环境中&amp;quot;真实人类数量&amp;quot;（0-M）&lt;/td&gt;
          &lt;td&gt;状态编码 $\delta_R$&lt;/td&gt;
          &lt;td&gt;每个可能人数的概率（比如&amp;quot;0 人：5%、1 人：30%、2 人：65%&amp;quot;）&lt;/td&gt;
          &lt;td&gt;交叉熵损失 $\mathcal{L}_{count}$&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Regressor 1（回归器）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;位置估计（Position Estimation）&lt;/td&gt;
          &lt;td&gt;精准回归&amp;quot;人类当前相对智能体的坐标&amp;quot;&lt;/td&gt;
          &lt;td&gt;状态编码 $\delta_R$&lt;/td&gt;
          &lt;td&gt;每个人类的 $(x, y)$ 坐标（比如&amp;quot;人类 A：$(2.3, 1.5)$ 米&amp;quot;）&lt;/td&gt;
          &lt;td&gt;MSE 损失 $\mathcal{L}_{pos}$&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Regressor 2（回归器）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;轨迹预测（Trajectory Forecasting）&lt;/td&gt;
          &lt;td&gt;预测&amp;quot;人类未来 $H$ 个时间步的坐标&amp;quot;&lt;/td&gt;
          &lt;td&gt;状态编码 $\delta_R$&lt;/td&gt;
          &lt;td&gt;每个人类的未来位置序列（比如&amp;quot;人类 A：$t+1$ 步 $(2.1,1.6)$、$t+2$ 步 $(1.9,1.7)$&amp;quot;）&lt;/td&gt;
          &lt;td&gt;MSE 损失 $\mathcal{L}_{traj}$&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;逐个拆解（通俗理解）：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classifier（分类器）= &amp;ldquo;人数计数器&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务本质：&lt;/strong&gt; 分类问题（比如&amp;quot;当前人数是 0、1、2、…、$M$&amp;quot;，$M$ 是最大预测人数，比如 6 人）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作逻辑：&lt;/strong&gt; 输入 $\delta_R$（包含当前环境的视觉 + 时序特征），输出&amp;quot;每个人数选项的概率&amp;quot;，最终选择概率最高的作为预测人数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;举个例子：&lt;/strong&gt; $\delta_R$ 中包含&amp;quot;两个移动的人形轮廓特征&amp;quot;，分类器会输出&amp;quot;2 人：90% 概率&amp;quot;，通过交叉熵损失（惩罚预测概率与真实人数的差异）不断优化，让&amp;quot;计数&amp;quot;更准确&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第一个 Regressor（位置估计回归器）= &amp;ldquo;实时定位仪&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务本质：&lt;/strong&gt; 回归问题（预测连续的坐标值，不是离散类别）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作逻辑：&lt;/strong&gt; 输入 $\delta_R$，针对每个检测到的人类，输出精准的 $(x, y)$ 相对坐标&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心价值：&lt;/strong&gt; 解决&amp;quot;视觉特征只能判断&amp;rsquo;有人类&amp;rsquo;，但不知道&amp;rsquo;具体在哪&amp;rsquo;&amp;ldquo;的问题——比如主策略网络知道&amp;quot;前方有人&amp;rdquo;，这个回归器能精准告诉它&amp;quot;在前方 2.3 米、偏右 10 度的位置&amp;quot;，为避障提供精确依据&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第二个 Regressor（轨迹预测回归器）= &amp;ldquo;未来预言家&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务本质：&lt;/strong&gt; 序列回归问题（预测未来多个时间步的连续坐标）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作逻辑：&lt;/strong&gt; 输入 $\delta_R$（包含人类的历史移动趋势，比如&amp;quot;前 3 步朝智能体移动，速度 0.5 米/秒&amp;quot;），输出未来 $H$ 个时间步（比如 $H=5$）的人类位置序列&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心价值：&lt;/strong&gt; 解决&amp;quot;只能看当下，看不到未来&amp;quot;的短视问题——比如现在人类在 3 米外，但回归器预测&amp;quot;1 秒后会到 1.8 米处&amp;quot;，主策略网络就能提前调整轨迹，而不是等靠近了才反应&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键补充：三个组件的协同关系&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;共享输入：&lt;/strong&gt; 都用主策略网络的 $\delta_R$，确保&amp;quot;基于同一套环境理解&amp;quot;做预测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果互补：&lt;/strong&gt; 人数估计（classifier）确定&amp;quot;有多少人要预测&amp;quot;，位置估计（regressor1）确定&amp;quot;现在在哪&amp;quot;，轨迹预测（regressor2）确定&amp;quot;未来会到哪&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失合并：&lt;/strong&gt; 三个组件的损失（$\mathcal{L}&lt;em&gt;{count} + \mathcal{L}&lt;/em&gt;{pos} + \mathcal{L}&lt;em&gt;{traj}$）乘以权重 $\beta&lt;/em&gt;{aux}$ 后，加入总损失函数（公式 12），一起优化——比如人数预测不准、位置回归偏差大，都会让总损失上升，倒逼模型同时提升三个任务的精度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;总结：核心逻辑链&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;主策略网络生成&amp;quot;环境情报&amp;quot; $\delta_R$ → 拆解出辅助信息 $S_R, N, P$ → 时空预认知模块用 $\delta_R$ 驱动三个组件（classifier + 两个 regressor）→ 分别输出&amp;quot;人数、当前位置、未来轨迹&amp;quot; → 三个组件的损失共同优化模型 → 提升智能体的时空理解能力，最终帮助主策略网络做出更精准的导航决策。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;根据论文第三章 3.3 节，SPM 包含 3 个辅助任务，每个任务的网络结构都是**&amp;ldquo;LSTM/BiLSTM + 自注意力（Self-Attention） + 分类器/回归器&amp;rdquo;**，且输入均依赖主网络的潜在变量 $\delta_R$。&lt;/p&gt;
&lt;h4&gt;1. 人类计数估计（Human Count Estimation）——&amp;ldquo;数清楚周围有几个人&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-人类计数估计human-count-estimation数清楚周围有几个人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%ba%ba%e7%b1%bb%e8%ae%a1%e6%95%b0%e4%bc%b0%e8%ae%a1human-count-estimation%e6%95%b0%e6%b8%85%e6%a5%9a%e5%91%a8%e5%9b%b4%e6%9c%89%e5%87%a0%e4%b8%aa%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;任务目标：&lt;/strong&gt; 估计场景中人类的总数&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt; 主网络的潜在变量 $\delta_R$（包含前几步的环境记忆，如&amp;quot;前 3 步看到 2 个人类轮廓&amp;quot;）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;网络结构：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;LSTM 编码器：&lt;/strong&gt; 将 $\delta_R$ 编码为时序特征 $\Phi_R$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力层 [54]：&lt;/strong&gt; 以 $Q=K=V=\Phi_R$ 处理特征，目的是&amp;quot;聚焦与人类数量相关的关键记忆&amp;quot;（如&amp;quot;忽略墙的特征，重点关注人类轮廓的变化&amp;quot;），输出注意力特征 $A_t$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分类器 $\phi_{count}$：&lt;/strong&gt; 基于 $A_t$ 预测&amp;quot;场景中人类数量为 $k$&amp;ldquo;的概率 $\hat{n}_k$：&lt;/li&gt;
&lt;/ol&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;ϕ&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mspace width=&#34;1em&#34;/&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
\hat{n}_k = \phi_{count}(A_t), \quad k \in \{0, 1, \ldots, M\}, M=6
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8444em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6944em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.25em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;ϕ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:1em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;∈&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;…&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;M&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;M&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6444em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;6&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;损失函数：&lt;/strong&gt; 采用交叉熵损失（衡量&amp;quot;预测概率&amp;quot;与&amp;quot;真实数量&amp;quot;的差异）：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
L_{count} = -\sum_{k=0}^{M} n_k \log(\hat{n}_k)
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.1304em;vertical-align:-1.3021em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.8283em;&#34;&gt;&lt;span style=&#34;top:-1.8479em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-4.3em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3021em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop&#34;&gt;lo&lt;span style=&#34;margin-right:0.01389em;&#34;&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6944em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.25em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中 $n_k$ 是&amp;quot;真实人类数量为 $k$&amp;ldquo;的指示变量（如真实有 2 人，则 $n_2=1$，其余 $n_k=0$）。&lt;/p&gt;
&lt;h4&gt;2. 当前位置跟踪（Current Position Tracking）——&amp;ldquo;知道每个人在哪&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-当前位置跟踪current-position-tracking知道每个人在哪&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%bd%93%e5%89%8d%e4%bd%8d%e7%bd%ae%e8%b7%9f%e8%b8%aacurrent-position-tracking%e7%9f%a5%e9%81%93%e6%af%8f%e4%b8%aa%e4%ba%ba%e5%9c%a8%e5%93%aa&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;任务目标：&lt;/strong&gt; 跟踪人类相对于机器人的二维位置&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt; 主网络的 $\delta_R$ + 场景中人类的真实数量 $N$（Oracle 信息，仅用于训练，推理时无需）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;网络结构：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;LSTM 编码器：&lt;/strong&gt; 将 $\delta_R$ 和 $N$ 融合编码为特征 $\Phi_{R;N}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力层：&lt;/strong&gt; 以 $Q=K=V=\Phi_{R;N}$ 处理，聚焦&amp;quot;人类位置相关的特征&amp;rdquo;（如&amp;quot;分辨&amp;rsquo;人类 A 在左前方&amp;rsquo;和&amp;rsquo;人类 B 在右后方&amp;rsquo;&amp;quot;），输出 $A_t$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回归器 $\phi_{pos}$：&lt;/strong&gt; 基于 $A_t$ 预测第 $i$ 个人类的相对 2D 位置 $\hat{P}_i^t$：&lt;/li&gt;
&lt;/ol&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;ϕ&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
\hat{P}_i^t = \phi_{pos}(A_t)
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1938em;vertical-align:-0.247em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8436em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;ϕ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;os&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;损失函数：&lt;/strong&gt; 采用均方误差（MSE，衡量&amp;quot;预测位置&amp;quot;与&amp;quot;真实位置&amp;quot;的距离）：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∥&lt;/mi&gt;&lt;msubsup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;msup&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∥&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
L_{pos} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^t - P_i^t\|^2
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;os&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:2.6431em;vertical-align:-1.3217em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3214em;&#34;&gt;&lt;span style=&#34;top:-2.314em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;M&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.677em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.936em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.05em;&#34;&gt;&lt;span style=&#34;top:-1.8557em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;∈&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3217em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∥&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8436em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1141em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8436em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;∥&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P_i^t$ 是人类 $i$ 的真实位置&lt;/li&gt;
&lt;li&gt;$M$ 是&amp;quot;真实存在的人类&amp;quot;的掩码（如场景中只有 2 人，就只计算这 2 人的位置损失）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. 未来轨迹预测（Future Trajectory Forecasting）——&amp;ldquo;预判人类未来走哪&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-未来轨迹预测future-trajectory-forecasting预判人类未来走哪&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e6%9c%aa%e6%9d%a5%e8%bd%a8%e8%bf%b9%e9%a2%84%e6%b5%8bfuture-trajectory-forecasting%e9%a2%84%e5%88%a4%e4%ba%ba%e7%b1%bb%e6%9c%aa%e6%9d%a5%e8%b5%b0%e5%93%aa&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;任务目标：&lt;/strong&gt; 预测人类未来多个时间步的轨迹&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; 这是 SPM 中&lt;strong&gt;最重要的任务&lt;/strong&gt;（实验证明其对性能提升最大），因&amp;quot;预测未来轨迹&amp;quot;需要处理更复杂的时序关系，所以用 **BiLSTM（双向 LSTM）**替代普通 LSTM。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt; 主网络的 $\delta_R$ + 人类真实数量 $N$ + 当前人类位置 $P_i^t$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;网络结构：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;BiLSTM 编码器：&lt;/strong&gt; 双向处理 $\delta_R$、$N$、$P_i^t$ 的融合信息，输出特征 $\Phi_{R;N;P}$（双向 LSTM 能同时利用&amp;quot;过去记忆&amp;quot;和&amp;quot;未来趋势&amp;rdquo;，更适合长时序预测）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力层：&lt;/strong&gt; 以 $Q=K=V=\Phi_{R;N;P}$ 处理，聚焦&amp;quot;人类运动趋势相关的特征&amp;quot;（如&amp;quot;人类 A 前 2 步朝电梯走，预判他会继续向电梯移动&amp;quot;），输出 $A_t$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回归器 $\phi_{traj}$：&lt;/strong&gt; 基于 $A_t$ 预测未来 $H$ 步的人类轨迹 $\hat{P}_i^{t+1:t+H}$：&lt;/li&gt;
&lt;/ol&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;ϕ&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
\hat{P}_i^{t+1:t+H} = \phi_{traj}(A_t)
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2138em;vertical-align:-0.267em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8913em;&#34;&gt;&lt;span style=&#34;top:-2.433em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.08125em;&#34;&gt;H&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.267em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;ϕ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;损失函数：&lt;/strong&gt; 采用 MSE（衡量&amp;quot;预测轨迹&amp;quot;与&amp;quot;真实轨迹&amp;quot;的差异）：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∥&lt;/mi&gt;&lt;msubsup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;msup&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∥&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
L_{traj} = \frac{1}{|M|} \sum_{i \in M} \|\hat{P}_i^{t+1:t+H} - P_i^{t+1:t+H}\|^2
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:2.6431em;vertical-align:-1.3217em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3214em;&#34;&gt;&lt;span style=&#34;top:-2.314em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;M&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.677em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.936em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.05em;&#34;&gt;&lt;span style=&#34;top:-1.8557em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;∈&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3217em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∥&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8913em;&#34;&gt;&lt;span style=&#34;top:-2.433em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.08125em;&#34;&gt;H&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.267em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1583em;vertical-align:-0.267em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8913em;&#34;&gt;&lt;span style=&#34;top:-2.433em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.08125em;&#34;&gt;H&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.267em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;∥&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中 $P_i^{t+1:t+H}$ 为第 $i$ 个人类未来 $H$ 步的真实轨迹。&lt;/p&gt;
&lt;h4&gt;辅助损失与总损失&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;辅助损失与总损失&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%be%85%e5%8a%a9%e6%8d%9f%e5%a4%b1%e4%b8%8e%e6%80%bb%e6%8d%9f%e5%a4%b1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;3 个任务的损失相加，得到 SPM 的辅助损失：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
L_{aux} = L_{count} + L_{pos} + L_{traj}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ux&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;co&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;os&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;aj&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;训练过程中，主策略网络与辅助任务的优化同步进行，模型的总损失为&amp;quot;主策略损失&amp;quot;与&amp;quot;辅助损失&amp;quot;的加权和：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
L_{total} = \beta_{main} L_{main} + \beta_{aux} L_{aux}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;main&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;main&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8889em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05278em;&#34;&gt;β&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ux&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;ux&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中，$\beta_{main}$ 和 $\beta_{aux}$ 分别为主策略损失与辅助损失的权重系数。&lt;/p&gt;
&lt;h3&gt;实验与结果&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;实验与结果&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ae%9e%e9%aa%8c%e4%b8%8e%e7%bb%93%e6%9e%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/falcon-experiment.png&#34; alt=&#34;Falcon Experiment&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;评估指标&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;评估指标&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;任务完成度指标：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;成功率（Success Rate, Suc.）&lt;/strong&gt;：机器人成功到达目标的比例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;路径长度加权成功率（Success weighted by Path Length, SPL）&lt;/strong&gt;：考虑路径效率的成功率&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时间长度加权成功率（Success weighted by Time Length, STL）&lt;/strong&gt;：考虑时间效率的成功率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;社交合规性指标：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人机碰撞率（Human-Robot Collision Rate, H-Coll）&lt;/strong&gt;：机器人与人类发生碰撞的比例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;个人空间合规率（Personal Space Compliance, PSC）&lt;/strong&gt;：机器人保持适当社交距离的比例&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;考虑到人类碰撞半径为 &lt;strong&gt;0.3 米&lt;/strong&gt;、机器人碰撞半径为 &lt;strong&gt;0.25 米&lt;/strong&gt;，本实验将个人空间合规（PSC）的距离阈值设定为 &lt;strong&gt;1.0 米&lt;/strong&gt;。&lt;/p&gt;
&lt;h4&gt;基线方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基线方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e7%ba%bf%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;Proximity-Aware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;proximity-aware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#proximity-aware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;Proximity-Aware&lt;/strong&gt; 是一个基于强化学习的社交导航方法，该方法通过两个辅助任务建模人类与机器人的距离和方向，能有效捕捉当前时刻的人机距离关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心思想：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用&lt;strong&gt;当前时刻&lt;/strong&gt;的人机距离和相对方向作为状态输入&lt;/li&gt;
&lt;li&gt;通过两个辅助任务学习人机距离关系：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;距离预测任务&lt;/strong&gt;：预测机器人与人类的距离&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方向预测任务&lt;/strong&gt;：预测人类相对于机器人的方向&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;奖励函数基于当前距离：$r_{proximity} = -1/d_t$，其中 $d_t$ 是当前时刻的距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;工作原理：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;状态表示：&lt;/strong&gt; $s_t = [d_t, \theta_t, g_t]$，其中 $d_t$ 是当前距离，$\theta_t$ 是相对角度，$g_t$ 是目标方向&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;辅助任务：&lt;/strong&gt; 在训练过程中同时学习预测距离和方向，增强对当前人机关系的理解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略学习：&lt;/strong&gt; 使用 PPO 算法训练策略网络，学习在当前状态下选择最优动作&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;局限性：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只考虑&lt;strong&gt;当前时刻&lt;/strong&gt;的距离和方向，无法预测人类未来的移动轨迹&lt;/li&gt;
&lt;li&gt;当人类突然改变方向时，只能被动反应，容易出现&amp;quot;短视避障&amp;quot;&lt;/li&gt;
&lt;li&gt;无法提前规划路径以避免与人类未来位置发生冲突&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;A* 算法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-算法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-%e7%ae%97%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;em&gt;&lt;em&gt;A&lt;/em&gt; 算法&lt;/em&gt;*是一个经典的静态路径规划算法，广泛应用于机器人导航和游戏AI中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心思想：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用&lt;strong&gt;启发式搜索&lt;/strong&gt;在静态地图上找到从起点到终点的最优路径&lt;/li&gt;
&lt;li&gt;综合考虑&lt;strong&gt;已走路径成本&lt;/strong&gt;（$g(n)$）和&lt;strong&gt;预估剩余成本&lt;/strong&gt;（$h(n)$）&lt;/li&gt;
&lt;li&gt;评估函数：$f(n) = g(n) + h(n)$，其中 $h(n)$ 通常是欧氏距离或曼哈顿距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;工作原理：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;初始化：&lt;/strong&gt; 将起点加入开放列表（open list）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代搜索：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;从开放列表中选择 $f(n)$ 值最小的节点&lt;/li&gt;
&lt;li&gt;将该节点移入关闭列表（closed list）&lt;/li&gt;
&lt;li&gt;检查该节点的所有邻居节点&lt;/li&gt;
&lt;li&gt;对于每个邻居节点，计算新的 $g$ 值，如果更优则更新&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;终止条件：&lt;/strong&gt; 当目标节点被加入关闭列表时，回溯路径&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;在社交导航中的应用：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预先计算一条从起点到终点的&lt;strong&gt;固定路径&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;假设环境是&lt;strong&gt;静态的&lt;/strong&gt;，不考虑动态人类的存在&lt;/li&gt;
&lt;li&gt;当遇到人类时，需要重新规划路径&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;局限性：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;无法适应动态环境&lt;/strong&gt;：预先确定的路径无法应对人类移动&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;易导致碰撞&lt;/strong&gt;：当人类移动到规划路径上时，机器人可能直接碰撞&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要全局地图&lt;/strong&gt;：算法需要完整的静态地图信息&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;ORCA 算法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;orca-算法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#orca-%e7%ae%97%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;ORCA（Optimal Reciprocal Collision Avoidance）算法&lt;/strong&gt;是一个基于速度障碍的多智能体避障算法，可获取智能体的位置和速度&amp;quot;先知信息&amp;quot;（oracle access），以动态调整规划路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心思想：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于**速度障碍（Velocity Obstacle）**概念&lt;/li&gt;
&lt;li&gt;每个智能体选择一个新的速度，使得在假设其他智能体也选择最优速度的情况下，避免碰撞&lt;/li&gt;
&lt;li&gt;使用&lt;strong&gt;线性规划&lt;/strong&gt;在速度空间中寻找可行速度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;工作原理：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;速度障碍计算：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于每个其他智能体，计算一个速度障碍区域&lt;/li&gt;
&lt;li&gt;该区域包含所有会导致碰撞的速度&lt;/li&gt;
&lt;li&gt;速度障碍是一个&lt;strong&gt;圆锥形区域&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ORCA 半平面：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将速度障碍转换为&lt;strong&gt;ORCA 半平面&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;每个半平面定义了一个速度约束：$v \cdot n \geq u \cdot n$&lt;/li&gt;
&lt;li&gt;其中 $n$ 是半平面的法向量，$u$ 是参考速度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;线性规划求解：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在满足所有 ORCA 半平面约束的条件下，选择最接近期望速度的速度&lt;/li&gt;
&lt;li&gt;优化目标：$\min |v - v_{pref}|$，其中 $v_{pref}$ 是期望速度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;在社交导航中的应用：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以获取人类的位置和速度信息（oracle access）&lt;/li&gt;
&lt;li&gt;实时计算速度障碍，动态调整机器人速度&lt;/li&gt;
&lt;li&gt;假设所有智能体都遵循 ORCA 规则，实现&lt;strong&gt;相互避让&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;局限性：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;假设运动不受限制&lt;/strong&gt;：ORCA 假设智能体可以在任意方向移动，但实际机器人可能有运动学约束（如非完整约束）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可能导致与静态障碍物碰撞&lt;/strong&gt;：算法主要关注动态避障，可能忽略静态障碍&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要精确的位置和速度信息&lt;/strong&gt;：在实际应用中，这些信息可能难以准确获取&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算复杂度&lt;/strong&gt;：当环境中智能体数量较多时，线性规划的计算成本较高&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;实验设置&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;实验设置&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ae%9e%e9%aa%8c%e8%ae%be%e7%bd%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练算法：&lt;/strong&gt; 强化学习智能体采用 &lt;strong&gt;DD-PPO 算法 [53]&lt;/strong&gt; 训练，且所有模型使用相同超参数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;随机性控制：&lt;/strong&gt; 每种算法均采用 3 个不同随机种子独立运行 3 次，最终结果取各指标的均值与标准差&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型初始化：&lt;/strong&gt; 模型初始化权重来自预训练的 &lt;strong&gt;PointNav 模型 [57]&lt;/strong&gt;，并在社交导航任务上进行 &lt;strong&gt;1000 万步微调&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练硬件：&lt;/strong&gt; 训练过程使用 4 块 &lt;strong&gt;Nvidia RTX 3090&lt;/strong&gt; 显卡，同时运行 8 个并行环境&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集划分：&lt;/strong&gt; 模型在 &lt;strong&gt;Social-HM3D 训练集&lt;/strong&gt;上训练，在 &lt;strong&gt;Social-HM3D 测试集&lt;/strong&gt;和 &lt;strong&gt;Social-MP3D 测试集&lt;/strong&gt;上测试&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;泛化评估：&lt;/strong&gt; &lt;strong&gt;Social-MP3D&lt;/strong&gt; 的测试结果用于评估 Falcon 的&lt;strong&gt;零样本泛化能力&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/falcon-results.png&#34; alt=&#34;Falcon Results&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;实验结果与分析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;实验结果与分析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c%e4%b8%8e%e5%88%86%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;结论 1：具备未来感知能力的方法比静态方法和情境感知方法更高效、更安全&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;结论-1具备未来感知能力的方法比静态方法和情境感知方法更高效更安全&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%bb%93%e8%ae%ba-1%e5%85%b7%e5%a4%87%e6%9c%aa%e6%9d%a5%e6%84%9f%e7%9f%a5%e8%83%bd%e5%8a%9b%e7%9a%84%e6%96%b9%e6%b3%95%e6%af%94%e9%9d%99%e6%80%81%e6%96%b9%e6%b3%95%e5%92%8c%e6%83%85%e5%a2%83%e6%84%9f%e7%9f%a5%e6%96%b9%e6%b3%95%e6%9b%b4%e9%ab%98%e6%95%88%e6%9b%b4%e5%ae%89%e5%85%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;* 等静态路径规划算法会预先确定一条固定路径，无法适应动态环境，易导致人机碰撞（见图 4 (a)）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ORCA&lt;/strong&gt;、&lt;strong&gt;Proximity-Aware&lt;/strong&gt; 等情境感知避障方法虽能通过调整路径避开当前人类与障碍物，但存在局限性：
&lt;ul&gt;
&lt;li&gt;路径重规划需耗时，延迟响应会增加碰撞风险&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ORCA&lt;/strong&gt; 因假设运动不受限制，导致与静态障碍物碰撞（见图 4 (b)）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proximity-Aware&lt;/strong&gt; 因无法预判人类运动，短期调整失效，最终发生碰撞（见图 4 (c)）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Falcon&lt;/strong&gt; 能主动适应人类动态运动，高效抵达目标位置&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;结论 2：辅助任务可提升模型性能，其中轨迹预测任务作用最显著&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;结论-2辅助任务可提升模型性能其中轨迹预测任务作用最显著&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%bb%93%e8%ae%ba-2%e8%be%85%e5%8a%a9%e4%bb%bb%e5%8a%a1%e5%8f%af%e6%8f%90%e5%8d%87%e6%a8%a1%e5%9e%8b%e6%80%a7%e8%83%bd%e5%85%b6%e4%b8%ad%e8%bd%a8%e8%bf%b9%e9%a2%84%e6%b5%8b%e4%bb%bb%e5%8a%a1%e4%bd%9c%e7%94%a8%e6%9c%80%e6%98%be%e8%91%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;表 3 展示了不同辅助任务组合的实验结果。与 &lt;strong&gt;PointNav 基线模型 [57]&lt;/strong&gt; 相比，单个辅助任务即可提升导航性能；其中，&lt;strong&gt;轨迹预测任务（SPM.Traj）&lt;strong&gt;效果最突出——将成功率从 &lt;strong&gt;40.94%&lt;/strong&gt; 显著提升至 &lt;strong&gt;54.00%&lt;/strong&gt;。这一结果证明，在社交导航中引入&lt;/strong&gt;显式轨迹预测&lt;/strong&gt;具有重要价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表 3：Falcon 消融实验结果&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;表格说明：&lt;/strong&gt; 仅使用 PointNav 算法 [57] 训练的模型作为基线；SPM.Count、SPM.Pos、SPM.Traj 分别指&amp;quot;人类计数估计&amp;quot;&amp;ldquo;当前位置跟踪&amp;quot;&amp;ldquo;未来轨迹预测&amp;quot;三个辅助任务；数据以百分比表示，&amp;quot;↑&amp;rdquo; 表示指标值越高越好，&amp;quot;↓&amp;rdquo; 表示指标值越低越好&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;SPM. 计数（Count）&lt;/th&gt;
          &lt;th&gt;SPM. 位置（Pos）&lt;/th&gt;
          &lt;th&gt;SPM. 轨迹（Traj）&lt;/th&gt;
          &lt;th&gt;SCP&lt;/th&gt;
          &lt;th&gt;成功率（Suc.）↑&lt;/th&gt;
          &lt;th&gt;路径长度加权成功率（SPL）↑&lt;/th&gt;
          &lt;th&gt;时间长度加权成功率（STL）↑&lt;/th&gt;
          &lt;th&gt;个人空间合规率（PSC）↑&lt;/th&gt;
          &lt;th&gt;人机碰撞率（H-Coll）↓&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;（无辅助任务）&lt;/td&gt;
          &lt;td&gt;（无辅助任务）&lt;/td&gt;
          &lt;td&gt;（无辅助任务）&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;40.94&lt;/td&gt;
          &lt;td&gt;34.14&lt;/td&gt;
          &lt;td&gt;11.50&lt;/td&gt;
          &lt;td&gt;90.82&lt;/td&gt;
          &lt;td&gt;53.54&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;51.43&lt;/td&gt;
          &lt;td&gt;51.42&lt;/td&gt;
          &lt;td&gt;51.16&lt;/td&gt;
          &lt;td&gt;90.53&lt;/td&gt;
          &lt;td&gt;46.46&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;53.17&lt;/td&gt;
          &lt;td&gt;53.17&lt;/td&gt;
          &lt;td&gt;52.95&lt;/td&gt;
          &lt;td&gt;90.06&lt;/td&gt;
          &lt;td&gt;44.07&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;54.00&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;53.99&lt;/td&gt;
          &lt;td&gt;53.92&lt;/td&gt;
          &lt;td&gt;89.46&lt;/td&gt;
          &lt;td&gt;43.88&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;51.24&lt;/td&gt;
          &lt;td&gt;51.24&lt;/td&gt;
          &lt;td&gt;51.08&lt;/td&gt;
          &lt;td&gt;90.41&lt;/td&gt;
          &lt;td&gt;48.11&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;无&lt;/td&gt;
          &lt;td&gt;53.63&lt;/td&gt;
          &lt;td&gt;53.63&lt;/td&gt;
          &lt;td&gt;53.40&lt;/td&gt;
          &lt;td&gt;89.33&lt;/td&gt;
          &lt;td&gt;44.89&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;✓&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;55.15&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;55.15&lt;/td&gt;
          &lt;td&gt;54.94&lt;/td&gt;
          &lt;td&gt;89.56&lt;/td&gt;
          &lt;td&gt;42.96&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h5&gt;结论 3：SCP 与 SPM 协同互补，显著提升模型性能并加快训练收敛&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;结论-3scp-与-spm-协同互补显著提升模型性能并加快训练收敛&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%bb%93%e8%ae%ba-3scp-%e4%b8%8e-spm-%e5%8d%8f%e5%90%8c%e4%ba%92%e8%a1%a5%e6%98%be%e8%91%97%e6%8f%90%e5%8d%87%e6%a8%a1%e5%9e%8b%e6%80%a7%e8%83%bd%e5%b9%b6%e5%8a%a0%e5%bf%ab%e8%ae%ad%e7%bb%83%e6%94%b6%e6%95%9b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;表 3 显示，&lt;strong&gt;SCP&lt;/strong&gt; 对模型性能提升至关重要，尤其在与 &lt;strong&gt;SPM&lt;/strong&gt; 结合时效果更明显：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;仅使用 SPM&lt;/strong&gt; 的三个辅助任务（计数、位置、轨迹）时，模型性能较单个辅助任务无显著提升（SPM 组合的成功率为 &lt;strong&gt;53.63%&lt;/strong&gt;，而 SPM.Pos 为 &lt;strong&gt;53.17%&lt;/strong&gt;、SPM.Traj 为 &lt;strong&gt;54.00%&lt;/strong&gt;）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加入 SCP 后&lt;/strong&gt;，完整模型的成功率提升至 &lt;strong&gt;55.15%&lt;/strong&gt;，显著优于单独使用 SPM 的情况&lt;/li&gt;
&lt;li&gt;图 5 显示，同时包含 &lt;strong&gt;SPM&lt;/strong&gt; 和 &lt;strong&gt;SCP&lt;/strong&gt; 的模型在训练过程中收敛速度更快（&lt;strong&gt;1400K 步前&lt;/strong&gt;即可体现）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些结果表明，若缺乏 &lt;strong&gt;SCP&lt;/strong&gt; 的引导，&lt;strong&gt;SPM&lt;/strong&gt; 的辅助任务无法有效整合——&lt;strong&gt;SCP&lt;/strong&gt; 能帮助模型平衡各项任务，更充分地利用现有信息。&lt;/p&gt;
&lt;h4&gt;局限性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;局限性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%b1%80%e9%99%90%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Falcon&lt;/strong&gt; 虽能实现较高的成功率，但 &lt;strong&gt;Proximity-Aware&lt;/strong&gt;（成功率约 20%）在避障方面表现更优。这一现象暴露了现有评估指标的局限性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在拥挤环境中，指标可能过度优先考虑**&amp;ldquo;社交舒适度&amp;rdquo;&lt;strong&gt;，而忽视&lt;/strong&gt;&amp;ldquo;任务完成度&amp;rdquo;**&lt;/li&gt;
&lt;li&gt;本基准目前未涵盖**&amp;ldquo;礼让&amp;rdquo;**等更高阶的人类社交行为&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;在 Autodl 上复现 Falcon&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;在-autodl-上复现-falcon&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9c%a8-autodl-%e4%b8%8a%e5%a4%8d%e7%8e%b0-falcon&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda create -n falcon &lt;span class=&#34;nv&#34;&gt;python&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;3.9 &lt;span class=&#34;nv&#34;&gt;cmake&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;3.14.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 在base环境安装mamba（仅需一次）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda install mamba -n base -c conda-forge -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 用mamba给falcon环境安装habitat-sim（核心：-n falcon 指定目标环境）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mamba install habitat-sim&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0.3.1 withbullet headless -n falcon -c conda-forge -c aihabitat -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda activate falcon&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;接下来使用Robosense比赛所提供的代码而不是原生。原版 SocialNav 侧重于让人形机器人（Humanoid）在环境中跟随或者寻找人，它的 Reward（奖励函数）和 Episode（任务集）定义是通用的；RoboSense 比赛版 SocialNav 目标是从 A 点走到 B 点，同时避开动态的人，它使用的是 Social-HM3D 这个特定的数据集，里面的 NPC（路人）的轨迹是经过特殊生成的（基于 ORCA 算法），且有人口密度分级。
但是说白了就是 Robosense 是对其的发展，直接战未来：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone --recurse-submodules https://github.com/robosense2025/track2.git
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mv track2 Falcon
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; Falcon/Falcon
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装Python依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -e habitat-lab
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -e habitat-baselines
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -r requirements.txt&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;下面是数据集部分：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; /root/Falcon/Falcon
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p /root/autodl-fs/habitat_data/scene_datasets
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 创建软链接&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; /root/Falcon/Falcon/data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ln -s /root/autodl-fs/habitat_data/scene_datasets ./scene_datasets&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;接下来是场景数据集：
点击&lt;a href=&#34;https://my.matterport.com/settings/account/devtools&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这个链接&lt;/a&gt;登录，但是我一直卡在最终的这个地方：
&lt;img src=&#34;http://localhost:1313/blog/2025/MatterPort-login.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;
在页面上找到 &amp;ldquo;API Tokens&amp;rdquo; 部分。
点击 &amp;ldquo;Generate&amp;rdquo; (生成)。
Token ID 就是你的 &amp;ndash;username。
Token Secret 就是你的 &amp;ndash;password。
中途面临一个&lt;code&gt;ImportError: libEGL.so.1&lt;/code&gt;的问题，直接&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y libegl1-mesa libgl1-mesa-glx libgl1-mesa-dev&lt;/code&gt;掉&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;实际上这里不应该装mesa，详见&lt;a href=&#34;./2025-12-17-daily.md&#34;&gt;SocialNav-map的复现&lt;/a&gt;
首先下载HM3D：&lt;/p&gt;

&lt;/blockquote&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -m habitat_sim.utils.datasets_download --username xxx --password xxx --uids hm3d_minival&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;接下来下载任务数据 (Episode Datasets)，这是告诉机器人“从哪里走到哪里”的指令包。给的是 ™ HuggingFace 的链接&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ch&#34;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;使用 Python huggingface_hub 库下载数据，支持镜像站和SSL配置
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ssl&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pathlib&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 配置镜像站&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;HF_ENDPOINT&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;https://hf-mirror.com&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 如果遇到SSL证书问题，可以临时禁用验证（不推荐生产环境）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# ssl._create_default_https_context = ssl._create_unverified_context&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;huggingface_hub&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;snapshot_download&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 设置下载目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;local_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;vm&#34;&gt;__file__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parent&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;track2-social-navigation&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;local_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mkdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parents&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exist_ok&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 下载指定路径的文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 注意：allow_patterns 和 ignore_patterns 同时使用时，需要正确配置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;snapshot_download&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;repo_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;robosense/datasets&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;repo_type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dataset&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;local_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;local_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;allow_patterns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;track2-social-navigation/**&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;resume_download&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;token!!!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;下载完成，文件保存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;local_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;ImportError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;请先安装: pip install huggingface_hub&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;下载失败: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;如果遇到SSL证书错误，可以取消注释脚本中的SSL禁用行（第15行）&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;然后是下载机器人动画 (Leg Animation)，即 Spot 机器人的走路动作文件&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 回到项目根目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; /root/Falcon/Falcon
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 创建目录并下载&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p data/robots/spot_data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://github.com/facebookresearch/habitat-lab/files/12502177/spot_walking_trajectory.csv -O data/robots/spot_data/spot_walking_trajectory.csv&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;再之后是下载多智能体资产 (Multi-agent Assets)，这里包含人类模型（Humanoids）和一些仿真资产，而被 Git LFS 折磨惨了，选择不用&lt;code&gt;python -m habitat_sim.utils.datasets_download --uids habitat_humanoids hab3_bench_assets hab_spot_arm&lt;/code&gt;而是额外写一个Python下：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ch&#34;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;使用 huggingface_hub 直接下载数据集，避免 Git LFS 问题
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;实现与 habitat_sim.utils.datasets_download 相同的功能
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sys&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pathlib&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 配置镜像站（可选）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;HF_ENDPOINT&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;HF_ENDPOINT&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;https://hf-mirror.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;huggingface_hub&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;snapshot_download&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;ImportError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;错误：请先安装 huggingface_hub&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;运行: pip install huggingface_hub&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 数据集配置（从 habitat_sim 的数据源配置中提取）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;DATASETS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;habitat_humanoids&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;repo_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ai-habitat/habitat_humanoids&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;main&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;link_path&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;humanoids/humanoid_data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;version_dir&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;habitat_humanoids&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;hab3_bench_assets&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;repo_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ai-habitat/hab3_bench_assets&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;main&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;link_path&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hab3_bench_assets&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;version_dir&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hab3_bench_assets&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;hab_spot_arm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;repo_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ai-habitat/hab_spot_arm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;v2.0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;link_path&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;robots/hab_spot_arm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;version_dir&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hab_spot_arm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 默认数据路径（相对于执行脚本时的当前工作目录）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 与 habitat_sim.utils.datasets_download 的行为一致&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;DEFAULT_DATA_PATH&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;./data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;resolve&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;download_dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    下载指定的数据集
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Args:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        uid: 数据集唯一标识符
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        data_path: 数据存储根目录（默认为 ./data）
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        token: HuggingFace token（如果需要认证）
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DATASETS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;错误：未知的数据集 ID: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;支持的数据集: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;, &amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DATASETS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DEFAULT_DATA_PATH&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;resolve&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DATASETS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;versioned_data&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;version_dir&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;link_path&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;下载数据集: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;仓库: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;repo_id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;版本目录: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;符号链接: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 检查是否已存在&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;警告：版本目录已存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;是否删除并重新下载？(y/n): &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;shutil&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;shutil&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rmtree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_symlink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_symlink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unlink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;shutil&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rmtree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;跳过下载&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 创建版本目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parent&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mkdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parents&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exist_ok&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 使用 huggingface_hub 下载&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;开始下载 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;repo_id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;snapshot_download&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;repo_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;repo_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;repo_type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dataset&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;local_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;local_dir_use_symlinks&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;revision&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;resume_download&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;下载完成: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 创建符号链接&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parent&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mkdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parents&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exist_ok&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_symlink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_symlink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unlink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;shutil&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;shutil&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rmtree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;symlink_to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_is_directory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;符号链接已创建: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; -&amp;gt; &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;数据集 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; 下载成功！&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;源目录: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;符号链接: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;link_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;下载失败: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;traceback&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;traceback&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;print_exc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;主函数&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;argparse&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;argparse&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;使用 huggingface_hub 下载 Habitat 数据集（无需 Git LFS）&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;--uids&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;nargs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;+&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;habitat_humanoids&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hab3_bench_assets&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hab_spot_arm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;要下载的数据集 ID 列表（默认: habitat_humanoids hab3_bench_assets hab_spot_arm）&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;--data-path&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;数据存储根目录（默认: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DEFAULT_DATA_PATH&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;）&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;--token&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;HuggingFace token（如果需要认证）&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;--list&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;store_true&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;列出所有支持的数据集&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parse_args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;支持的数据集:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DATASETS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;repo_id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; (版本: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;version&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;)&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DEFAULT_DATA_PATH&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;数据路径: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;要下载的数据集: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;, &amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;success_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;download_dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;success_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;总计: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;success_count&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; 个数据集下载成功&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;success_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;对了还不能忘记 &lt;a href=&#34;https://drive.google.com/drive/folders/1Bx1L9U345P_9pUfADk3Tnj7uK01EpxZY&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pretrained_models&lt;/a&gt;，但是它很烦是Google云盘里的，好在只有243MB，可以下下来之后再传到autodl上去&lt;/p&gt;
&lt;p&gt;万事俱备，开始运行评测脚本：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -u -m habitat_baselines.eval --config-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;social_nav_v2/falcon_hm3d.yaml&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;运行后立即遇到了 &lt;strong&gt;OpenGL 上下文创建失败&lt;/strong&gt;的问题：&lt;code&gt;GL::Context: cannot retrieve OpenGL version: GL::Renderer::Error::InvalidValue&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;问题分析&lt;/strong&gt;：habitat-sim 试图在 AutoDL 容器里创建一个图形渲染上下文，但是失败了。随后的 BrokenPipeError 和 core dumped 只是因为主进程试图跟一个已经崩溃的渲染进程通信导致的后果。这个问题通常是 &lt;strong&gt;EGL（无头渲染接口）配置不当&lt;/strong&gt;引起的。在 AutoDL 这种无显示器的服务器上，必须强制指定渲染模式为无头模式。&lt;/p&gt;
&lt;p&gt;虽然系统确实找到了 NVIDIA 显卡（found 3 EGL devices, choosing EGL device 0），但是在创建 OpenGL 上下文时失败了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;初步尝试&lt;/strong&gt;：先尝试设置简单的环境变量：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_PLATFORM&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;surfaceless
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;MAGNUM_LOG&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;verbose 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;HABITAT_SIM_LOG&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;verbose&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;但问题依然存在，出现了内存错误：&lt;code&gt;malloc_consolidate(): unaligned fastbin chunk detected&lt;/code&gt;，随后进程崩溃。&lt;/p&gt;
&lt;p&gt;简单的环境变量设置不够，需要更完整的 NVIDIA EGL 配置。编写了一个脚本来强制使用 NVIDIA 的 EGL 库：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么需要强制使用 NVIDIA 库？Mesa vs NVIDIA 冲突详解&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mesa 是什么&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mesa 是开源的 OpenGL/EGL/Vulkan 实现&lt;/li&gt;
&lt;li&gt;提供软件渲染（纯 CPU）或通过 DRI 使用集成显卡（Intel/AMD）&lt;/li&gt;
&lt;li&gt;是 Linux 发行版的默认图形库&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NVIDIA 驱动&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NVIDIA 提供专有的硬件加速 OpenGL/EGL 实现&lt;/li&gt;
&lt;li&gt;直接访问 NVIDIA GPU，性能远高于 Mesa 软件渲染&lt;/li&gt;
&lt;li&gt;库文件：&lt;code&gt;libEGL_nvidia.so.0&lt;/code&gt;、&lt;code&gt;libGLX_nvidia.so.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;冲突原因&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux 动态链接器默认按库搜索路径顺序加载库&lt;/li&gt;
&lt;li&gt;系统通常先找到 Mesa 的 &lt;code&gt;libEGL.so.1&lt;/code&gt;（在 &lt;code&gt;/usr/lib/x86_64-linux-gnu/&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;Mesa 无法访问 NVIDIA GPU，只能使用 CPU 或集成显卡&lt;/li&gt;
&lt;li&gt;导致 habitat-sim 无法创建硬件加速的 OpenGL 上下文&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;为什么先安装 Mesa？&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解决 &lt;code&gt;ImportError: libEGL.so.1&lt;/code&gt; 缺失问题（提供符号链接）&lt;/li&gt;
&lt;li&gt;但 Mesa 只是&amp;quot;占位符&amp;quot;，实际渲染需要 NVIDIA 库&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 强制优先加载 NVIDIA 库&lt;/li&gt;
&lt;li&gt;设置 &lt;code&gt;__EGL_VENDOR_LIBRARY_FILENAMES&lt;/code&gt; 指向 NVIDIA 配置&lt;/li&gt;
&lt;li&gt;确保子进程也继承这些设置（多进程环境）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 强制使用 NVIDIA EGL 运行 Habitat-Sim&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;=== 设置 NVIDIA EGL 环境 ===&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1. 强制指定 NVIDIA 的 EGL 配置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__EGL_VENDOR_LIBRARY_FILENAMES&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/usr/share/glvnd/egl_vendor.d/10_nvidia.json
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2. 设置无头模式&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_PLATFORM&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;surfaceless
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 3. 确保使用 NVIDIA 的 GL 库（GLX用于OpenGL）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__GLX_VENDOR_LIBRARY_NAME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nvidia
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 4. 设置额外的 EGL 相关环境变量，确保子进程也能正确初始化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_DEVICE_ID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__GL_SYNC_TO_VBLANK&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 5. 确保 NVIDIA 驱动相关的环境变量被设置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__NVIDIA_BUG_REPORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 6. 使用 LD_PRELOAD 强制加载 NVIDIA 的 EGL 库，确保子进程也能正确初始化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查找 NVIDIA 库的实际位置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_GLX_LIB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 尝试多个可能的库位置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; egl_lib in /lib/x86_64-linux-gnu/libEGL_nvidia.so.0 /usr/lib/x86_64-linux-gnu/libEGL_nvidia.so.0&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -f &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$egl_lib&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nv&#34;&gt;NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$egl_lib&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; glx_lib in /usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0 /usr/lib/libGLX_nvidia.so.0&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -f &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$glx_lib&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nv&#34;&gt;NVIDIA_GLX_LIB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$glx_lib&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 如果找到了库，设置 LD_PRELOAD&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -n &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -n &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NVIDIA_GLX_LIB&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;LD_PRELOAD&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_GLX_LIB&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  设置 LD_PRELOAD 强制使用 NVIDIA 库&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -n &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;LD_PRELOAD&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  设置 LD_PRELOAD 强制使用 NVIDIA EGL 库&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;环境变量设置：&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  __EGL_VENDOR_LIBRARY_FILENAMES=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$__EGL_VENDOR_LIBRARY_FILENAMES&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  EGL_PLATFORM=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$EGL_PLATFORM&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  EGL_DEVICE_ID=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$EGL_DEVICE_ID&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  LD_PRELOAD=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$LD_PRELOAD&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  __GLX_VENDOR_LIBRARY_NAME=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$__GLX_VENDOR_LIBRARY_NAME&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 检查 NVIDIA 库是否存在&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -z &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;警告：找不到 NVIDIA EGL 库，可能无法正常工作&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;  请确保已安装 NVIDIA 驱动和相应的库文件&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;开始运行命令...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 获取 falcon conda 环境的 Python 路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;FALCON_PYTHON&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/root/miniconda3/envs/falcon/bin/python&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 如果第一个参数是 python，替换为 falcon 环境的 python&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 这样可以确保使用正确的 Python 环境和所有环境变量都被继承&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;python3&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 替换 python 为 falcon 环境的 python，保持其他参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;shift&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$FALCON_PYTHON&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$@&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 如果不是 python 命令，尝试使用 conda run（环境变量可能需要额外处理）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 这里我们假设用户知道自己在做什么&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$@&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;使用方法&lt;/strong&gt;：将脚本保存为 &lt;code&gt;run_with_nvidia_egl.sh&lt;/code&gt; 并赋予执行权限，然后通过脚本运行评测命令：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;./run_with_nvidia_egl.sh python -u -m habitat_baselines.eval &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    --config-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;social_nav_v2/falcon_hm3d.yaml &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    habitat_baselines.num_environments&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;这个脚本其实是最终版本了，在这之前还有下列一系列问题的，第一个问题：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;ModuleNotFoundError: No module named &amp;#39;habitat_baselines&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;检查发现 &lt;code&gt;run_with_nvidia_egl.sh&lt;/code&gt; 脚本使用了系统的 Python，而 &lt;code&gt;habitat_baselines&lt;/code&gt; 模块安装在 &lt;code&gt;falcon&lt;/code&gt; conda 环境中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;修复方案&lt;/strong&gt;：修改 &lt;code&gt;run_with_nvidia_egl.sh&lt;/code&gt;，自动使用 falcon conda 环境的 Python：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;FALCON_PYTHON&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/root/miniconda3/envs/falcon/bin/python&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;python3&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;shift&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$FALCON_PYTHON&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$@&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;修复 conda 环境后，程序可以导入模块，但在初始化多进程环境时崩溃：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;malloc_consolidate(): unaligned fastbin chunk detected
ConnectionResetError: [Errno 104] Connection reset by peer
Aborted (core dumped)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;问题在于多进程环境下，子进程无法正确继承 EGL 相关环境变量。虽然主进程设置了 &lt;code&gt;EGL_PLATFORM=surfaceless&lt;/code&gt; 等环境变量，但子进程在初始化 EGL/OpenGL 时仍然失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;修复方案&lt;/strong&gt;：在 &lt;code&gt;run_with_nvidia_egl.sh&lt;/code&gt; 中添加 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 强制加载 NVIDIA 库，并添加额外的 EGL 环境变量：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查找并设置 NVIDIA 库的 LD_PRELOAD&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/lib/x86_64-linux-gnu/libEGL_nvidia.so.0&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_GLX_LIB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;LD_PRELOAD&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_EGL_LIB&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;NVIDIA_GLX_LIB&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 额外的 EGL 环境变量确保子进程正确初始化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_DEVICE_ID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__GL_SYNC_TO_VBLANK&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__NVIDIA_BUG_REPORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;修复 EGL 问题后，程序可以初始化 Simulator，但遇到数据集路径错误：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;FileNotFoundError: Could not find dataset file `data/datasets/pointnav/social-hm3d/val`&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;坑爹的是，这个地方的报错和上面的一模一样，然后日志没有专门写出导一个文件中，轻易就超出了终端的最大长度，导致排查了好久，这里进行了最小元测试：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;test_egl_step_by_step.py&lt;/strong&gt;: 测试 EGL/OpenGL 初始化，验证环境变量、habitat_sim 导入、Simulator 创建、多进程环境&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ch&#34;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;最小元测试：逐步验证每个配置环节
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sys&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;json&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gzip&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;test_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;step_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;打印测试步骤标题&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;步骤 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step_num&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step_name&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;最小元测试：逐步验证配置&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 步骤1: 检查数据集文件是否存在&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;test_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;检查数据集文件&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dataset_file&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;data/datasets/pointnav/hm3d/val/val.json.gz&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] 数据集文件不存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_file&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 数据集文件存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_file&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 步骤2: 检查数据集文件内容&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;test_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;检查数据集文件内容&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gzip&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;rt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 数据集文件可以读取&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  Episodes 数量: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;episodes&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]))&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;episodes&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [WARN] 数据集文件为空（没有episodes）&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 获取第一个episode的场景路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;first_episode&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;episodes&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first_episode&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;scene_id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  第一个episode的场景ID: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] 读取数据集文件失败: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;traceback&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;traceback&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;print_exc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 步骤3: 检查场景文件是否存在&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;test_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;检查场景文件路径&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] 场景ID为空&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 场景ID可能是相对路径或绝对路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  数据集中的场景路径: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 尝试多个可能的路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;possible_paths&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 原始路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getcwd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 相对于当前目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;replace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 去掉data前缀&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;scene_found&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;path&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;possible_paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 场景文件存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  文件大小: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getsize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; MB&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;scene_found&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scene_found&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] 场景文件不存在&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  尝试过的路径:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;path&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;possible_paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;    - &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 检查场景目录结构&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;  检查场景目录结构...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;scene_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dirname&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 场景目录存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;files&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  目录中的文件: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;files&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] 场景目录不存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_dir&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 检查hm3d目录结构&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;hm3d_base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;data/scene_datasets/hm3d&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hm3d_base&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;  检查 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hm3d_base&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; 目录结构...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;subdirs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hm3d_base&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  子目录: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subdirs&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;c1&#34;&gt;# 检查val目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;val_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hm3d_base&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;val&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;val_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] val目录存在&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;val_subdirs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;val_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  val目录中的前5个条目: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;val_subdirs&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] val目录不存在&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 步骤4: 检查配置文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;test_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;检查配置文件&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 检查数据集配置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dataset_config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 数据集配置文件存在&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;hm3d&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 配置使用hm3d路径&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] 数据集配置文件不存在&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 检查任务配置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;task_config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 任务配置文件存在&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;social_nav_v2: hm3d&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;social_nav_v2:hm3d&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;replace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 任务配置引用hm3d数据集&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [FAIL] 任务配置文件不存在&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 步骤5: 测试实际加载数据集&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;test_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;测试加载数据集（使用habitat API）&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;habitat&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;omegaconf&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DictConfig&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 创建配置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;dataset_cfg&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;s1&#34;&gt;&amp;#39;type&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;PointNav-v1&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;s1&#34;&gt;&amp;#39;split&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;val&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;s1&#34;&gt;&amp;#39;data_path&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;data/datasets/pointnav/hm3d/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{split}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{split}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;.json.gz&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  尝试加载数据集...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 这里只是测试配置，不实际加载（因为可能需要更多依赖）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [OK] 数据集配置格式正确&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  [WARN] 无法测试habitat API: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;测试完成&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;success&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;success&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;test_minimal_step_by_step.py&lt;/strong&gt;: 最小元测试，验证数据集文件、场景文件路径、配置文件&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;test_scene_paths.py&lt;/strong&gt;: 专门诊断场景路径问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;test_complete_flow.py&lt;/strong&gt;: 完整流程测试&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ch&#34;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;完整流程测试：模拟实际运行环境
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sys&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;完整流程测试&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 测试1: 场景文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;1. 测试场景文件路径...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;scene_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;data/scene_datasets/hm3d/val/00808-y9hTuugGdiq/y9hTuugGdiq.basis.glb&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   [OK] 场景文件存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   文件大小: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getsize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; MB&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   [FAIL] 场景文件不存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scene_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 测试2: 数据集文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;2. 测试数据集文件...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dataset_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;data/datasets/pointnav/hm3d/val/val.json.gz&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   [OK] 数据集文件存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 数据集文件可能为空，但这不是致命错误&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   [WARN] 数据集文件可能为空，但程序可能从其他地方获取场景信息&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   [FAIL] 数据集文件不存在: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_path&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 测试3: 配置文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;3. 测试配置文件...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;configs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;configs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exists&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   [OK] &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;   [FAIL] &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;所有基本检查通过！&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;=&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;现在可以尝试运行评估命令:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;  ./run_with_nvidia_egl.sh python -u -m habitat_baselines.eval &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\\&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;    --config-name=social_nav_v2/falcon_hm3d.yaml &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\\&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;    habitat_baselines.num_environments=1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;success&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;success&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;中间的两步一方面是最小元测试的过程中通过&lt;strong&gt;排除法&lt;/strong&gt;检查发现配置文件引用了 &lt;code&gt;social-hm3d&lt;/code&gt;，但实际数据集目录是 &lt;code&gt;hm3d&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;修复方案&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建新的数据集配置文件 &lt;code&gt;habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml&lt;/code&gt;，设置 &lt;code&gt;data_path: data/datasets/pointnav/hm3d/{split}/{split}.json.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;修改 &lt;code&gt;habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml&lt;/code&gt;，将数据集引用从 &lt;code&gt;social-hm3d&lt;/code&gt; 改为 &lt;code&gt;hm3d&lt;/code&gt;
修复数据集配置后，程序可以加载数据集，但场景文件路径错误：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;AssertionError: ESP_CHECK failed: No Stage Attributes exists for requested scene 
&amp;#39;data/scene_datasets/hm3d/val/00808-y9hTuugGdiq/y9hTuugGdiq.basis.glb&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;检查发现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data/scene_datasets/hm3d&lt;/code&gt; 是指向 &lt;code&gt;/root/autodl-fs/habitat_data/versioned_data/hm3d-0.2/hm3d&lt;/code&gt; 的符号链接&lt;/li&gt;
&lt;li&gt;实际目录中只有 &lt;code&gt;minival&lt;/code&gt; 子目录，没有 &lt;code&gt;val&lt;/code&gt; 子目录&lt;/li&gt;
&lt;li&gt;场景文件实际存储在 &lt;code&gt;minival&lt;/code&gt; 目录下
&lt;strong&gt;修复方案&lt;/strong&gt;：在场景数据实际目录中创建符号链接：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; /root/autodl-fs/habitat_data/versioned_data/hm3d-0.2/hm3d
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ln -sf minival val&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;这样 &lt;code&gt;data/scene_datasets/hm3d/val/&lt;/code&gt; 就可以正确访问到场景文件了。&lt;/p&gt;
&lt;p&gt;最后程序总算成功启动：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EGL/OpenGL 初始化成功（检测到 NVIDIA RTX 4090，OpenGL 3.0.0）&lt;/li&gt;
&lt;li&gt;场景文件加载成功&lt;/li&gt;
&lt;li&gt;多进程环境正常工作&lt;/li&gt;
&lt;li&gt;评估进度正常（2/1087 episodes）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：运行过程中会出现大量 Debug 日志 &lt;code&gt;Can&#39;t project end-point to navmesh&lt;/code&gt;，这是正常的调试信息，表示某些 episode 的坐标无效，程序会自动跳过，不影响评估结果。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Gaussian World Model</title>
      <link>http://localhost:1313/blog/2025/2025-12-9-3dgswm-manipulation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-9-3dgswm-manipulation/</guid>
      <description>
        
        
        &lt;p&gt;装模甲样阅读一下文献。
不过说来也巧，如果我选择去港科广读PhD的话通过connection这个组的研究方向就是这个，然后经过他们的几个月的考核，这样就不用被该死的committee折磨了，不过对我来说还是大三的PTSD更impressive一些，所以先冲个Master吧&lt;/p&gt;
&lt;p&gt;黄思源是通讯作者，隶属于香港科技大学（广州）人工智能与数字经济实验室（&lt;strong&gt;LAMDA&lt;/strong&gt;），主要研究方向为通用人工智能、机器人学习与 3D 视觉，但是一作是&lt;strong&gt;THU&lt;/strong&gt;、&lt;strong&gt;NTU&lt;/strong&gt;还有&lt;strong&gt;BIGAI&lt;/strong&gt;，该团队聚焦通过 3D 表征与生成模型解决机器人操作的世界建模问题，即文章中 &lt;strong&gt;GWM&lt;/strong&gt; 的研究方向&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GWM: Towards Scalable Gaussian World Models for Robotic Manipulation&lt;/strong&gt;这篇文章中了&lt;strong&gt;ICCV 2025&lt;/strong&gt;
&lt;img src=&#34;http://localhost:1313/blog/2025/gwm-overview.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;
该模型通过推断机器人动作作用下&lt;strong&gt;高斯基元&lt;/strong&gt;（Gaussian primitives）的传播过程，实现对未来状态的重建。其核心是一个 latent &lt;strong&gt;扩散 Transformer&lt;/strong&gt;（&lt;strong&gt;Diffusion Transformer, DiT&lt;/strong&gt;），并结合了 &lt;strong&gt;3D 变分自动编码器&lt;/strong&gt;（3D variational autoencoder），能够借助&lt;strong&gt;高斯溅射&lt;/strong&gt;（Gaussian Splatting）技术完成细粒度的场景级未来状态重建&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GWM&lt;/strong&gt; 不仅能通过自监督未来预测训练，为模仿学习智能体增强视觉表征能力，还可作为支持&lt;strong&gt;模型基强化学习&lt;/strong&gt;（model-based reinforcement learning, &lt;strong&gt;MBRL&lt;/strong&gt;）的神经模拟器。模拟实验与真实世界实验均表明：&lt;strong&gt;GWM&lt;/strong&gt; 能在不同机器人动作的条件下精准预测未来场景，且进一步用于策略训练时，其性能能以显著优势超越当前&lt;strong&gt;SOTA&lt;/strong&gt;，充分展现了 3D 世界模型在初始数据扩展方面的潜力&lt;/p&gt;
&lt;p&gt;基于视频的生成模型依赖图像输入，且缺乏 3D 几何与空间理解能力，因此易受未见过的视觉变化（如光照、相机姿态、纹理等）影响，尽管 RGB-D（彩色 - 深度）与多视图设置试图弥补这一差距，但在连贯的 3D 空间内隐式对齐图像块特征仍面临挑战&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3DGS&lt;/strong&gt;将点云等高效 3D 表示与高保真渲染相结合，然而，这些方法主要依赖离线逐场景重建，其计算需求给在机器人操作（尤其是&lt;strong&gt;MBRL&lt;/strong&gt;）中的应用带来了重大挑战&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/gwm-pipeline.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;Methodology&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;methodology&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#methodology&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;将真实世界的视觉输入编码为潜在 3DGS 表示，并利用基于扩散的条件生成模型，在给定机器人状态与动作的情况下，学习表示层面的动态特性&lt;/p&gt;
&lt;h3&gt;World State Encoding&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-state-encoding&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-state-encoding&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;Feed-forward 3D Gaussian Splatting&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;feed-forward-3d-gaussian-splatting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#feed-forward-3d-gaussian-splatting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;给定某一世界状态的单视图或双视图图像输入&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;I = \{I\}_{i=\{1,2\}}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1052em;vertical-align:-0.3552em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;mclose&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3448em;&#34;&gt;&lt;span style=&#34;top:-2.5198em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mopen mtight&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mpunct mtight&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;mclose mtight&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3552em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，的核心目标是先将场景编码为 3D 高斯表示，为后续动态学习与预测提供基础&lt;/p&gt;
&lt;p&gt;3DGS采用多个非结构化 3D 高斯核&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;Σ&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G = \{x_p, \sigma_p, \Sigma_p, C_p\}_{p \in P}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;Σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07153em;&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;mclose&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;∈&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;表示 3D 场景&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;每个高斯核是一个小的 3D 椭球，包含位置、大小、颜色等信息&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;其中：
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;：高斯核中心；（3D 坐标）
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;：高斯核不透明度；（0-1，控制是否可见）
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;Σ&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\Sigma_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;Σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;：高斯核协方差矩阵；（控制椭球的形状和方向）
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;C_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07153em;&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;：高斯核球谐系数。（存储颜色信息，支持视角相关颜色）&lt;/p&gt;
&lt;p&gt;为从特定视角获取每个像素的颜色，3DGS 会将 3D 高斯核投影到图像平面，并按以下公式计算像素颜色：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mtext&gt;SH&lt;/mtext&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;munderover&gt;&lt;mo&gt;∏&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(1)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;C(G) = \sum_{p \in P} \alpha_p \cdot \text{SH}(d_p; C_p) \cdot \prod_{j=1}^{p-1} (1 - \alpha_j) \tag{1}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07153em;&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:2.4804em;vertical-align:-1.4304em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.05em;&#34;&gt;&lt;span style=&#34;top:-1.8557em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;∈&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.13889em;&#34;&gt;P&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.4304em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.0037em;&#34;&gt;α&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;SH&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07153em;&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.262em;vertical-align:-1.4138em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.8482em;&#34;&gt;&lt;span style=&#34;top:-1.8723em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∏&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-4.3471em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.4138em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.0037em;&#34;&gt;α&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.05724em;&#34;&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.2787em;vertical-align:-1.4304em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;blockquote&gt;
  &lt;p&gt;像素颜色 = 所有高斯核的贡献叠加&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;其中：
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\alpha_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.0037em;&#34;&gt;α&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;：按 z 深度排序的&lt;strong&gt;有效不透明度&lt;/strong&gt;，即由协方差矩阵&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;Σ&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\Sigma_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9694em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;Σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;得到的 2D 高斯权重与整体不透明度&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;的乘积；
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;d_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9805em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;：从相机到高斯核中心&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_p&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7167em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;的视角方向；
&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mtext&gt;SH&lt;/mtext&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\text{SH}(\cdot)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;SH&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;：球谐函数（spherical harmonics function）。由于基础版 &lt;strong&gt;3D-GS&lt;/strong&gt;（vanilla 3D-GS）依赖耗时的逐场景离线优化，采用&lt;strong&gt;可泛化 3D-GS&lt;/strong&gt; 学习 &amp;ldquo;从图像到 3D 高斯&amp;rdquo; 的前馈映射，以提升效率。&lt;/p&gt;
&lt;p&gt;具体而言，通过 &lt;strong&gt;Splatt3R&lt;/strong&gt; 模型获取 3D 高斯世界状态 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，该模型的实现流程为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先利用立体重建模型 &lt;strong&gt;Mast3R&lt;/strong&gt; 从输入图像生成 3D 点图&lt;/li&gt;
&lt;li&gt;再通过额外的预测头，基于这些 3D 点图预测每个 3D 高斯核的参数&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;流程示意：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;输入图像 → Mast3R → 3D 点云
3D 点云 → 预测头 → 每个高斯核的参数&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4&gt;3D Gaussian VAE&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3d-gaussian-vae&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3d-gaussian-vae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;由于不同场景与任务中，每个世界状态对应的已学习 3D 高斯核数量差异显著，引入 &lt;strong&gt;3D 高斯变分自动编码器&lt;/strong&gt;（&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;E_\theta, D_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8778em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;），将重建得到的 3D 高斯核 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 编码为长度固定为 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;N&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 的潜在嵌入 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&#34;double-struck&#34;&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x \in \mathbb{R}^{N \times D}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.5782em;vertical-align:-0.0391em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;∈&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8913em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathbb&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8913em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;×&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，具体步骤如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下采样&lt;/strong&gt;：采用&lt;strong&gt;最远点采样&lt;/strong&gt;（Farthest Point Sampling, &lt;strong&gt;FPS&lt;/strong&gt;）将重建的 3D 高斯核 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 下采样为固定数量 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;N&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 的高斯核 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G_N&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，即：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;FPS&lt;/mtext&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G_N = \text{FPS}(G)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;FPS&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;编码&lt;/strong&gt;：将下采样后的高斯核 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G_N&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 作为查询（query），通过一个 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;L&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 层基于交叉注意力的编码器 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;E_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;（参考 [94] 的设计），从所有高斯核 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 中聚合信息并生成潜在嵌入 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.4306em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，公式如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtable rowspacing=&#34;0.25em&#34; columnalign=&#34;right left&#34; columnspacing=&#34;0em&#34;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;∘&lt;/mo&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;mo&gt;∘&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&#34;0&#34; displaystyle=&#34;true&#34;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;LayerNorm&lt;/mtext&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;mtext&gt;CrossAttn&lt;/mtext&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mtext&gt;PosEmbed&lt;/mtext&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(2)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\begin{aligned}
X &amp;amp;= E_\theta (G_N, G) = E_\theta^{(L)} \circ \cdots \circ E_\theta^{(1)} (G_N, G), \\
E_\theta^{(l)} (Q, G) &amp;amp;= \text{LayerNorm}\left(\text{CrossAttn}\left(Q, \text{PosEmbed}(G)\right)\right) \tag{2}
\end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.4096em;vertical-align:-1.4548em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mtable&#34;&gt;&lt;span class=&#34;col-align-r&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.9548em;&#34;&gt;&lt;span style=&#34;top:-3.9548em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.0448em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.25em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.0448em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.0448em;&#34;&gt;&lt;span style=&#34;top:-2.3987em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2198em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mopen mtight&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.01968em;&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;mclose mtight&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3013em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;Q&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.4548em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;col-align-l&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.9548em;&#34;&gt;&lt;span style=&#34;top:-3.9548em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.0448em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.0448em;&#34;&gt;&lt;span style=&#34;top:-2.3987em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2198em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mopen mtight&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;mclose mtight&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3013em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;∘&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;⋯&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;∘&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.0448em;&#34;&gt;&lt;span style=&#34;top:-2.3987em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2198em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mopen mtight&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mclose mtight&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3013em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.25em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.0448em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;LayerNorm&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;CrossAttn&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;Q&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;PosEmbed&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.4548em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.4096em;vertical-align:-1.4548em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;解码&lt;/strong&gt;：利用一个结构对称的基于 Transformer 的解码器 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;D_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，在潜在编码集合内传播并聚合信息，最终重建得到高斯核 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\hat{G}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，公式如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;LayerNorm&lt;/mtext&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;mtext&gt;SelfAttn&lt;/mtext&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(3)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\hat{G} = D_\theta (x) = \text{LayerNorm}\left(\text{SelfAttn}(x, x)\right) \tag{3}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;LayerNorm&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;SelfAttn&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1968em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;在 &lt;strong&gt;3D 高斯变分自动编码器&lt;/strong&gt;（&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;E_\theta, D_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8778em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;）的训练过程中，采用两种损失函数进行监督：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;倒角损失&lt;/strong&gt;（Chamfer Loss）：约束重建高斯核 &lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\hat{G}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 与原始高斯核 &lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;G&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 的中心对齐&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;渲染损失&lt;/strong&gt;（Rendering Loss）：确保重建高斯核 &lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\hat{G}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 的渲染效果，为基于图像的策略提供高保真视觉输入&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总损失函数公式如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;script&#34;&gt;L&lt;/mi&gt;&lt;mtext&gt;VAE&lt;/mtext&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;Chamfer&lt;/mtext&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;∥&lt;/mo&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;∥&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(4)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{L}_{\text{VAE}} = \text{Chamfer}(\hat{G}, G) + \left\| C(\hat{G}) - C(G) \right\|_1 \tag{4}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathcal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3283em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;VAE&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1968em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;Chamfer&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.8497em;vertical-align:-0.6997em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen&#34;&gt;&lt;span class=&#34;delimsizing mult&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.15em;&#34;&gt;&lt;span style=&#34;top:-3.15em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.8em;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;width:0.556em;height:1.800em;&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;0.556em&#34; height=&#34;1.800em&#34; viewBox=&#34;0 0 556 1800&#34;&gt;&lt;path d=&#34;M145 15 v585 v600 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-600 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v600 v585 h43z
M367 15 v585 v600 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-600 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M410 15 H367 v585 v600 v585 h43z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.65em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07153em;&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9468em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.2523em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07153em;&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;delimsizing mult&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.15em;&#34;&gt;&lt;span style=&#34;top:-3.15em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.8em;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;width:0.556em;height:1.800em;&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;0.556em&#34; height=&#34;1.800em&#34; viewBox=&#34;0 0 556 1800&#34;&gt;&lt;path d=&#34;M145 15 v585 v600 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-600 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v600 v585 h43z
M367 15 v585 v600 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-600 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M410 15 H367 v585 v600 v585 h43z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.65em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:-0.2486em;&#34;&gt;&lt;span style=&#34;top:-2.0003em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6997em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.8497em;vertical-align:-0.6997em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;4&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;h3&gt;Diffusion-based Dynamics Modeling&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;diffusion-based-dynamics-modeling&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#diffusion-based-dynamics-modeling&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;已知时刻 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;t&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6151em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 的编码世界状态嵌入 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_t&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.5806em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 及其未来状态 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_{t+1}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6389em;vertical-align:-0.2083em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2083em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，的目标是学习世界动态 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;p(x_{t+1} | x_{\leq t}, a_{\leq t})&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2083em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2952em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2452em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2952em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2452em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;（其中 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_{\leq t}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6757em;vertical-align:-0.2452em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2952em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2452em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;、&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;a_{\leq t}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6757em;vertical-align:-0.2452em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2952em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2452em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 分别表示历史状态与历史动作）。&lt;/p&gt;
&lt;p&gt;具体而言，构建基于扩散的动态模型，将动态学习转化为条件生成问题：以历史状态与动作 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;y_t = (x_{\leq t}, a_{\leq t})&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.625em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2952em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2452em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2952em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2452em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 为条件，从噪声中生成未来状态 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_{t+1}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6389em;vertical-align:-0.2083em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2083em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;。&lt;/p&gt;
&lt;h4&gt;加噪过程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;加噪过程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8a%a0%e5%99%aa%e8%bf%87%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;对真实未来状态 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_{t+1}^0 = x_{t+1}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1694em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6389em;vertical-align:-0.2083em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2083em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 添加噪声，通过高斯扰动核得到带噪未来状态样本 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_{t+1}^\tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0197em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，公式如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;script&#34;&gt;N&lt;/mi&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(5)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;p^{0 \to \tau} (x_{t+1}^\tau | x_{t+1}^0) = \mathcal{N}\left(x_{t+1}^\tau; x_{t+1}^0, \sigma^2(\tau) \cdot I\right) \tag{5}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1694em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;→&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2141em;vertical-align:-0.35em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.14736em;&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2141em;vertical-align:-0.35em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;5&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.4306em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：噪声步数索引&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma(\tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：&lt;strong&gt;噪声调度&lt;/strong&gt;（noise schedule），用于控制不同步骤的噪声强度&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;script&#34;&gt;N&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mo separator=&#34;true&#34;&gt;;&lt;/mo&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;Σ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{N}(\cdot; \mu, \Sigma)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.14736em;&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;μ&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;Σ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：正态分布，均值为 &lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.625em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;μ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，协方差矩阵为 &lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;Σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\Sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;Σ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上述扩散过程可表示为如下&lt;strong&gt;随机微分方程&lt;/strong&gt;（SDE）的解：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(6)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;dx = f(x, \tau) d\tau + g(\tau) dw \tag{6}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02691em;&#34;&gt;w&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;6&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;w&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.4306em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02691em;&#34;&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：标准维纳过程（Wiener process）&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;f(x, \tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：漂移系数（drift coefficient）&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;g(\tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：扩散系数（diffusion coefficient）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在高斯扰动核的设定下，漂移系数 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;f(x, \tau) = 0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6444em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，扩散系数 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo&gt;˙&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;g(\tau) = \sqrt{2 \cdot \dot{\sigma}(\tau) \cdot \sigma(\tau)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.24em;vertical-align:-0.2561em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord sqrt&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9839em;&#34;&gt;&lt;span class=&#34;svg-align&#34; style=&#34;top:-3.2em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.2em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34; style=&#34;padding-left:1em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6679em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1389em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;˙&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.9439em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.2em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;hide-tail&#34; style=&#34;min-width:1.02em;height:1.28em;&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;400em&#34; height=&#34;1.28em&#34; viewBox=&#34;0 0 400000 1296&#34; preserveAspectRatio=&#34;xMinYMin slice&#34;&gt;&lt;path d=&#34;M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2561em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;（&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo&gt;˙&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\dot{\sigma}(\tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6679em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1389em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;˙&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 为 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma(\tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 的导数）。&lt;/p&gt;
&lt;h4&gt;逆时采样&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;逆时采样&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%80%86%e6%97%b6%e9%87%87%e6%a0%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;为从噪声中生成未来状态，通过&lt;strong&gt;逆时 SDE&lt;/strong&gt; 逆转公式（6）的扩散过程，实现采样：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;[&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∇&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;ˉ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(7)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;dx = \left[ f(x, \tau) - g(\tau)^2 \cdot \nabla_x \log p^\tau(x) \right] d\tau + g(\tau) d\bar{w} \tag{7}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2141em;vertical-align:-0.35em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10764em;&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;∇&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop&#34;&gt;lo&lt;span style=&#34;margin-right:0.01389em;&#34;&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.5678em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02691em;&#34;&gt;w&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;ˉ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2141em;vertical-align:-0.35em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;7&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;ˉ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\bar{w}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.5678em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.5678em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02691em;&#34;&gt;w&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.1667em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;ˉ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：逆时维纳过程&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∇&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\nabla_x \log p^\tau(x)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;∇&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop&#34;&gt;lo&lt;span style=&#34;margin-right:0.01389em;&#34;&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6644em;&#34;&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：&lt;strong&gt;得分函数&lt;/strong&gt;（score function），即对数边缘概率关于 &lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.4306em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 的梯度，可通过网络估计&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;BTW 这里引用的文献是真多，将来对 SDE 这里感兴趣的话看看它的 Reference 吧&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;条件去噪模型训练&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;条件去噪模型训练&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9d%a1%e4%bb%b6%e5%8e%bb%e5%99%aa%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;通过最小化 &amp;ldquo;采样未来状态 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\hat{x}_{t+1}^0 = D_\theta(x_{t+1}^\tau, y_t)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1694em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord accent&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6944em;&#34;&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;accent-body&#34; style=&#34;left:-0.2222em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0553em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&amp;rdquo; 与 &amp;ldquo;真实未来状态 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x_{t+1}^0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1694em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&amp;rdquo; 的差异，学习条件去噪模型 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;D_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，损失函数如下：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;script&#34;&gt;L&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;double-struck&#34;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;[&lt;/mo&gt;&lt;msubsup&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;∥&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo fence=&#34;true&#34;&gt;∥&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo fence=&#34;true&#34;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(8)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{L}(\theta) = \mathbb{E}\left[ \left\| D_\theta(x_{t+1}^\tau, y_t^\tau) - x_{t+1}^0 \right\|_2^2 \right] \tag{8}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.8em;vertical-align:-0.65em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathbb&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size2&#34;&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen&#34;&gt;&lt;span class=&#34;delimsizing mult&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.85em;&#34;&gt;&lt;span style=&#34;top:-2.85em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.2em;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;width:0.556em;height:1.200em;&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;0.556em&#34; height=&#34;1.200em&#34; viewBox=&#34;0 0 556 1200&#34;&gt;&lt;path d=&#34;M145 15 v585 v0 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v0 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v0 v585 h43z
M367 15 v585 v0 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v0 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M410 15 H367 v585 v0 v585 h43z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.35em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;delimsizing mult&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.85em;&#34;&gt;&lt;span style=&#34;top:-2.85em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.2em;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;width:0.556em;height:1.200em;&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;0.556em&#34; height=&#34;1.200em&#34; viewBox=&#34;0 0 556 1200&#34;&gt;&lt;path d=&#34;M145 15 v585 v0 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v0 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v0 v585 h43z
M367 15 v585 v0 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v0 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M410 15 H367 v585 v0 v585 h43z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.35em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.0681em;&#34;&gt;&lt;span style=&#34;top:-2.3003em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.317em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3997em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size2&#34;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.8em;vertical-align:-0.65em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;8&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;double-struck&#34;&gt;E&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;[&lt;/mo&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathbb{E}[\cdot]&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathbb&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 表示期望，&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∥&lt;/mi&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msubsup&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∥&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\|\cdot\|_2^2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∥&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1141em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;∥&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 表示 L2 平方范数。&lt;/p&gt;
&lt;p&gt;如文献 [[33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565–26577, 2022. 4, A1] 所指出，直接学习去噪器 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;D_\theta(x_{t+1}^\tau, y_t)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0553em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 易受噪声幅度变化的影响。&lt;/p&gt;
&lt;p&gt;因此，参考 [[1] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari] 的思路，采用 &lt;strong&gt;EDM&lt;/strong&gt;（Elucidating the Design Space of Diffusion-Based Generative Models）[33] 中的预处理策略，转而学习网络 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;F_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;。&lt;/p&gt;
&lt;p&gt;具体而言，将去噪器 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;D_\theta(x_{t+1}^\tau, y_t^\tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0553em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 参数化为：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;skip&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;out&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;noise&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(9)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;D_\theta(x_{t+1}^\tau, y_t^\tau) = c_{\text{skip}}^\tau \cdot x_{t+1}^\tau + c_{\text{out}}^\tau \cdot F_\theta\left( c_{\text{in}}^\tau \cdot x_{t+1}^\tau, y_t^\tau; c_{\text{noise}}^\tau \right) \tag{9}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0553em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0975em;vertical-align:-0.3831em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;skip&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3831em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0197em;vertical-align:-0.3053em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9614em;vertical-align:-0.247em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2em;vertical-align:-0.35em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;in&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;noise&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2331em;vertical-align:-0.3831em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;9&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;c_{\text{in}}^\tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9289em;vertical-align:-0.2645em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6644em;&#34;&gt;&lt;span style=&#34;top:-2.4355em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;in&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2645em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;、&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;out&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;c_{\text{out}}^\tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9114em;vertical-align:-0.247em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6644em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：输入 / 输出幅度缩放预处理器&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;skip&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;c_{\text{skip}}^\tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0836em;vertical-align:-0.4192em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6644em;&#34;&gt;&lt;span style=&#34;top:-2.4169em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;skip&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.4192em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：跳跃连接调节预处理器&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;noise&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;c_{\text{noise}}^\tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.9289em;vertical-align:-0.2645em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6644em;&#34;&gt;&lt;span style=&#34;top:-2.4355em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;noise&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2645em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：噪声水平映射预处理器，将噪声强度作为额外条件输入到 &lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;F_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;预处理器的详细设计见附录 B.1。&lt;/p&gt;
&lt;p&gt;基于上述参数化，可将公式（8）的损失函数重写为：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mtable width=&#34;100%&#34;&gt;&lt;mtr&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;script&#34;&gt;L&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;double-struck&#34;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;[&lt;/mo&gt;&lt;msubsup&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;∥&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;out&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;skip&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo fence=&#34;true&#34;&gt;∥&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo fence=&#34;true&#34;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd width=&#34;50%&#34;&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtext&gt;(10)&lt;/mtext&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{L}(\theta) = \mathbb{E}\left[ \left\| F_\theta\left( c_{\text{in}}^\tau \cdot x_{t+1}^\tau, y_t^\tau \right) - \frac{1}{c_{\text{out}}^\tau} \left( x_{t+1}^0 - c_{\text{skip}}^\tau \cdot x_{t+1}^\tau \right) \right\|_2^2 \right] \tag{10}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3em;vertical-align:-1.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathbb&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size4&#34;&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen&#34;&gt;&lt;span class=&#34;delimsizing mult&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.45em;&#34;&gt;&lt;span style=&#34;top:-3.45em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:4.4em;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;width:0.556em;height:2.400em;&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;0.556em&#34; height=&#34;2.400em&#34; viewBox=&#34;0 0 556 2400&#34;&gt;&lt;path d=&#34;M145 15 v585 v1200 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-1200 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v1200 v585 h43z
M367 15 v585 v1200 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-1200 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M410 15 H367 v585 v1200 v585 h43z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.95em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;in&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3214em;&#34;&gt;&lt;span style=&#34;top:-2.314em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.6462em;&#34;&gt;&lt;span style=&#34;top:-2.4542em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.0448em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2458em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.677em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9318em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;skip&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3831em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;⋅&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3053em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size1&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;delimsizing mult&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.45em;&#34;&gt;&lt;span style=&#34;top:-3.45em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:4.4em;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;width:0.556em;height:2.400em;&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;0.556em&#34; height=&#34;2.400em&#34; viewBox=&#34;0 0 556 2400&#34;&gt;&lt;path d=&#34;M145 15 v585 v1200 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-1200 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v1200 v585 h43z
M367 15 v585 v1200 v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v-1200 v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M410 15 H367 v585 v1200 v585 h43z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.95em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.654em;&#34;&gt;&lt;span style=&#34;top:-1.7003em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.9029em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.9997em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size4&#34;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;tag&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3em;vertical-align:-1.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord text&#34;&gt;&lt;span class=&#34;mord&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord&#34;&gt;10&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;这一转化的核心价值在于：根据噪声调度 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma(\tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 自适应混合信号与噪声，为网络 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;F_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 构建更优的训练目标 —— 在高噪声水平下（&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;≫&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mtext&gt;data&lt;/mtext&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma(\tau) \gg \sigma_{\text{data}}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;≫&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.5806em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;data&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;），&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;skip&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;c_{\text{skip}}^\tau \to 0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0975em;vertical-align:-0.3831em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;skip&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3831em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;→&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6444em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，网络主要学习预测干净信号；在低噪声水平下（&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma(\tau) \to 0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;σ&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;→&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6444em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;），&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;skip&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;c_{\text{skip}}^\tau \to 1&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0975em;vertical-align:-0.3831em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;skip&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3831em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;→&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6444em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，网络目标转为预测噪声分量，避免损失函数趋于 trivial（无学习价值）。&lt;/p&gt;
&lt;h4&gt;技术实现&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技术实现&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e6%9c%af%e5%ae%9e%e7%8e%b0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;在技术实现上，采用&lt;strong&gt;扩散 Transformer&lt;/strong&gt;（&lt;strong&gt;DiT&lt;/strong&gt;）构建网络 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;F_\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.13889em;&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，具体流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;带噪潜在生成&lt;/strong&gt;：给定真实世界状态潜在嵌入序列 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\{x_t^0 = x_t\}_{t=1}^T&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1141em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1413em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;mclose&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8913em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.13889em;&#34;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，根据公式（5）的高斯扰动生成带噪潜在序列 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\{x_t^\tau\}_{t=1}^T&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.1413em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;&lt;span class=&#34;mclose&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8913em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.13889em;&#34;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;输入构建&lt;/strong&gt;：将带噪潜在嵌入与&lt;strong&gt;旋转位置嵌入&lt;/strong&gt;（&lt;strong&gt;RoPE&lt;/strong&gt;）拼接，作为 &lt;strong&gt;DiT&lt;/strong&gt; 的输入&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;条件融入&lt;/strong&gt;：对于条件 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mtext&gt;noise&lt;/mtext&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;y_t = (x_{\leq t}^0, a_{\leq t}, c_{\text{noise}}^\tau)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.625em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.2063em;vertical-align:-0.3422em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8641em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3422em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2952em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mrel mtight&#34;&gt;≤&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2452em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7144em;&#34;&gt;&lt;span style=&#34;top:-2.453em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord text mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;noise&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.1132em;&#34;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.247em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，通过&lt;strong&gt;自适应层归一化&lt;/strong&gt;（&lt;strong&gt;AdaLN&lt;/strong&gt;）调制时序嵌入，并将当前机器人动作作为 &lt;strong&gt;DiT&lt;/strong&gt; 交叉注意力层的键（key）与值（value），实现条件生成&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练稳定性优化&lt;/strong&gt;：为保证所有注意力机制的稳定性与效率，采用带可学习缩放因子的&lt;strong&gt;均方根归一化&lt;/strong&gt;（&lt;strong&gt;RMSNorm&lt;/strong&gt;），在处理空间表示的同时，将时序动作序列作为条件融入训练&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;GWM for Policy Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;gwm-for-policy-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gwm-for-policy-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;GWM for RL&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;gwm-for-rl&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gwm-for-rl&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;从形式化定义来看，&lt;strong&gt;马尔可夫决策过程&lt;/strong&gt;（&lt;strong&gt;MDP&lt;/strong&gt;）由元组 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;(S, A, p, r, \gamma, \rho_0)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;S&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05556em;&#34;&gt;γ&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;ρ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 描述，其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;S&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;S&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;、&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;A&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：分别为状态空间与动作空间&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.625em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05556em;&#34;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：折扣因子&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;r(s, a)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：奖励函数&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\rho_0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.625em;vertical-align:-0.1944em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;ρ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;：初始状态分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;MBRL&lt;/strong&gt; 的核心目标是：在通过策略滚动采样（policy roll-outs）构建动态模型 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;p_\theta(s_{t+1}, r_t | s_t, a_t)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2083em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; 的同时，学习使 &amp;ldquo;期望折扣奖励和&amp;rdquo; 最大化的策略 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo&gt;∗&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\pi^*&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7387em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7387em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mbin mtight&#34;&gt;∗&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;，即：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo&gt;∗&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;arg&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;munder&gt;&lt;mrow&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;double-struck&#34;&gt;E&lt;/mi&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;[&lt;/mo&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∞&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo fence=&#34;true&#34;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\pi^* = \arg\max_\pi \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.7387em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7387em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mbin mtight&#34;&gt;∗&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:3.0171em;vertical-align:-1.2671em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop&#34;&gt;ar&lt;span style=&#34;margin-right:0.01389em;&#34;&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.4306em;&#34;&gt;&lt;span style=&#34;top:-2.4em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop&#34;&gt;max&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.7em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathbb&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.1514em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03588em;&#34;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size4&#34;&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mop op-limits&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.6514em;&#34;&gt;&lt;span style=&#34;top:-1.8829em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mrel mtight&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&#34;mop op-symbol large-op&#34;&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-4.3em;margin-left:0em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3.05em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;∞&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.2671em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05556em;&#34;&gt;γ&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8436em;&#34;&gt;&lt;span style=&#34;top:-3.113em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;&lt;span class=&#34;delimsizing size4&#34;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;Algorithm 1: Monotonic Model-Based Policy Optimization (MBPO) with Gaussian World Model&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;for N epochs do
    Initialize policy π(at|st), Gaussian world model pθ(st&amp;#43;1, rt|st, at), empty replay buffer B;
    Collect data with π in real environment:
        B = B ∪ {(st, at, st&amp;#43;1, rt)}t;
    Train Gaussian world model pθ on dataset B via maximum likelihood:
        θ ← arg maxθ EB[log pθ(st&amp;#43;1, rt|st, at)];
    Optimize policy under predictive model:
        π ← arg maxπ Eπ[∑t≥0 γ^t rt];
end&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;在该框架下，在 &lt;strong&gt;GWM&lt;/strong&gt; 基础上新增一个&lt;strong&gt;奖励预测头&lt;/strong&gt;，用于参数化动态模型 &lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;∣&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;p_\theta(s_{t+1}, r_t | s_t, a_t)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.02778em;&#34;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3011em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mbin mtight&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mord mtight&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2083em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.02778em;&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;∣&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;。为提升视觉操作任务的性能，参考 [Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. Proceedings of Advances in Neural Information Processing Systems (NeurIPS)] 的设计选择构建基础强化学习策略。&lt;/p&gt;
&lt;h4&gt;GWM for IL&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;gwm-for-il&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gwm-for-il&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;在&lt;strong&gt;模仿学习&lt;/strong&gt;（&lt;strong&gt;IL&lt;/strong&gt;）场景中，将 &lt;strong&gt;GWM&lt;/strong&gt; 作为更高效的编码器，为策略学习提供更优特征。&lt;/p&gt;
&lt;p&gt;具体而言，提取扩散过程中 &amp;ldquo;&lt;strong&gt;第一步去噪后的特征向量&lt;/strong&gt;&amp;quot;，作为下游策略模型（如&lt;strong&gt;行为克隆 Transformer&lt;/strong&gt;（&lt;strong&gt;BC-transformer&lt;/strong&gt;）、&lt;strong&gt;扩散策略&lt;/strong&gt;（diffusion policy））的输入 —— 第一步去噪后的特征能保留具有代表性的空间信息，可有效应对较高的噪声水平。&lt;/p&gt;
&lt;p&gt;在实现过程中，采用 &amp;ldquo;&lt;strong&gt;按序列块预测动作&lt;/strong&gt;&amp;rdquo; 的方式，确保机器人控制的一致性。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Habitat 相关项目常识</title>
      <link>http://localhost:1313/blog/2025/2025-12-14-habitat-common-sense/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-14-habitat-common-sense/</guid>
      <description>
        
        
        &lt;p&gt;今天做的另一件事情是去autodl里复现falcon，具体可见blog\2025-11-29-falcon.md下半部分，图形学驱动这些真的很难绷的住。再就是今天我疑似罹患流感，晚上昏昏沉沉的，效率不高，RT-1没有看。&lt;/p&gt;
&lt;h2&gt;复盘：3D 视觉仿真与图形渲染基础科普&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;复盘3d-视觉仿真与图形渲染基础科普&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%a4%8d%e7%9b%983d-%e8%a7%86%e8%a7%89%e4%bb%bf%e7%9c%9f%e4%b8%8e%e5%9b%be%e5%bd%a2%e6%b8%b2%e6%9f%93%e5%9f%ba%e7%a1%80%e7%a7%91%e6%99%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;一、Habitat-Sim 是什么？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一habitat-sim-是什么&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80habitat-sim-%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Habitat-Sim&lt;/strong&gt; 是 Meta（Facebook）开发的 3D 场景仿真器，主要用于训练和评估机器人导航算法。它的核心作用是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;3D 场景渲染&lt;/strong&gt;：加载真实的 3D 场景（如 HM3D 数据集中的室内场景），生成机器人&amp;quot;看到&amp;quot;的图像&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;物理仿真&lt;/strong&gt;：模拟机器人的移动、碰撞检测等物理行为&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;传感器仿真&lt;/strong&gt;：模拟 RGB 相机、深度相机、激光雷达等传感器数据&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多智能体仿真&lt;/strong&gt;：可以同时模拟多个机器人或 NPC（如行人）的行为&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;为什么需要图形渲染？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机器人需要通过&amp;quot;视觉&amp;quot;感知环境，这需要从 3D 场景生成 2D 图像&lt;/li&gt;
&lt;li&gt;即使不显示图像，也需要在 GPU 上渲染以获取传感器数据（RGB、深度图等）&lt;/li&gt;
&lt;li&gt;这就是为什么即使是无头（headless）模式，也需要 GPU 和图形驱动&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;二、OpenGL、EGL、GLX 是什么？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二opengleglglx-是什么&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8copengleglglx-%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;OpenGL（Open Graphics Library）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;openglopen-graphics-library&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#openglopen-graphics-library&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：跨平台的图形渲染 API，用于在 GPU 上绘制 3D 图形&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：告诉 GPU 如何渲染三角形、纹理、光照等&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类比&lt;/strong&gt;：就像告诉画家&amp;quot;用红色画笔在画布上画一个圆&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;EGL（Embedded-System Graphics Library）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;eglembedded-system-graphics-library&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#eglembedded-system-graphics-library&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：OpenGL 和底层显示系统之间的接口层&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：创建 OpenGL 上下文（context），连接 OpenGL 和 GPU 驱动&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么重要&lt;/strong&gt;：在服务器上（没有显示器），EGL 是创建 OpenGL 上下文的唯一方式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无头模式&lt;/strong&gt;：&lt;code&gt;EGL_PLATFORM=surfaceless&lt;/code&gt; 告诉 EGL &amp;ldquo;我不需要显示窗口，只需要在内存中渲染&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;GLX（OpenGL Extension to the X Window System）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;glxopengl-extension-to-the-x-window-system&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#glxopengl-extension-to-the-x-window-system&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：Linux 上连接 OpenGL 和 X11 窗口系统的接口&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：在有显示器的 Linux 系统上创建 OpenGL 上下文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;服务器场景&lt;/strong&gt;：在无头服务器上通常不需要，但 NVIDIA 驱动可能需要它来初始化&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;X11（X Window System）是什么？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;x11x-window-system是什么&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#x11x-window-system%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：Linux/Unix 系统上的图形窗口系统，用于管理窗口、鼠标、键盘等&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;提供图形界面基础：窗口管理、事件处理、输入输出&lt;/li&gt;
&lt;li&gt;客户端-服务器架构：X Server（显示服务器）和 X Client（应用程序）分离&lt;/li&gt;
&lt;li&gt;支持远程显示：可以通过网络在远程机器上显示图形界面&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么服务器上没有 X11？&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;服务器通常没有显示器，不需要窗口系统&lt;/li&gt;
&lt;li&gt;X11 服务器需要显示器硬件支持&lt;/li&gt;
&lt;li&gt;无头服务器不需要图形界面，节省资源&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与 OpenGL 的关系&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;GLX 是 X11 的扩展，用于在 X11 窗口系统中创建 OpenGL 上下文&lt;/li&gt;
&lt;li&gt;有显示器时：应用程序 → GLX → X11 → 显示器&lt;/li&gt;
&lt;li&gt;无头服务器：无法使用 GLX，必须使用 EGL 的无头模式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;三、为什么服务器上需要特殊配置？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三为什么服务器上需要特殊配置&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a%e9%9c%80%e8%a6%81%e7%89%b9%e6%ae%8a%e9%85%8d%e7%bd%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;问题本质&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题本质&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e6%9c%ac%e8%b4%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;服务器（如 AutoDL）通常：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;没有显示器&lt;/strong&gt;：无法创建传统的窗口来显示图形&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;没有 X11 服务器&lt;/strong&gt;：无法使用 GLX 创建 OpenGL 上下文（GLX 依赖 X11）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要 GPU 加速&lt;/strong&gt;：仍然需要 GPU 来加速渲染计算&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;X11 vs EGL 的区别&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;X11 + GLX&lt;/strong&gt;：需要显示器，创建可见窗口，适合桌面环境
&lt;ul&gt;
&lt;li&gt;流程：应用程序 → GLX → X11 Server → 显示器&lt;/li&gt;
&lt;li&gt;示例：在本地 Linux 桌面运行图形程序&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EGL（无头模式）&lt;/strong&gt;：不需要显示器，直接在 GPU 内存渲染，适合服务器
&lt;ul&gt;
&lt;li&gt;流程：应用程序 → EGL → GPU（无窗口）&lt;/li&gt;
&lt;li&gt;示例：在服务器上运行 3D 仿真，渲染结果保存为图像或用于计算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;解决方案：无头渲染（Headless Rendering）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;解决方案无头渲染headless-rendering&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e6%97%a0%e5%a4%b4%e6%b8%b2%e6%9f%93headless-rendering&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;使用 EGL 的 &lt;code&gt;surfaceless&lt;/code&gt; 模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不创建可见窗口&lt;/li&gt;
&lt;li&gt;直接在 GPU 内存中渲染&lt;/li&gt;
&lt;li&gt;渲染结果可以保存为图像或用于计算&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;关键环境变量解释&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;关键环境变量解释&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%b3%e9%94%ae%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e8%a7%a3%e9%87%8a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1. 指定 EGL 平台为无头模式（不需要窗口）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_PLATFORM&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;surfaceless
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2. 强制使用 NVIDIA 的 EGL 实现（而不是 Mesa 软件渲染）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__EGL_VENDOR_LIBRARY_FILENAMES&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/usr/share/glvnd/egl_vendor.d/10_nvidia.json
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 3. 指定使用哪个 GPU（多 GPU 系统）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_DEVICE_ID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 4. 强制加载 NVIDIA 的图形库（确保子进程也使用正确的库）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;LD_PRELOAD&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/lib/x86_64-linux-gnu/libEGL_nvidia.so.0:/usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0&amp;#34;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;为什么需要 &lt;code&gt;LD_PRELOAD&lt;/code&gt;？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux 的动态链接器默认会按顺序查找库文件&lt;/li&gt;
&lt;li&gt;系统可能有多个 EGL 实现（NVIDIA、Mesa 等）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LD_PRELOAD&lt;/code&gt; 强制优先加载指定的库，确保使用 NVIDIA 的硬件加速版本&lt;/li&gt;
&lt;li&gt;在多进程环境下，子进程也需要继承这个设置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;关于 Mesa vs NVIDIA 冲突的说明&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mesa&lt;/strong&gt; 是开源的 OpenGL/EGL 实现，提供软件渲染（CPU）或通过 DRI（Direct Rendering Infrastructure）使用集成显卡&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NVIDIA 驱动&lt;/strong&gt; 提供专有的硬件加速 OpenGL/EGL 实现，直接使用 NVIDIA GPU&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;冲突原因&lt;/strong&gt;：Linux 系统默认会优先加载 Mesa 库（&lt;code&gt;libEGL.so.1&lt;/code&gt;），但 Mesa 无法访问 NVIDIA GPU，导致无法创建硬件加速的 OpenGL 上下文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：虽然安装了 Mesa 库解决了 &lt;code&gt;libEGL.so.1&lt;/code&gt; 缺失的问题，但后续需要通过 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 和环境变量强制使用 NVIDIA 的 EGL 库（&lt;code&gt;libEGL_nvidia.so.0&lt;/code&gt;），这样才能利用 GPU 硬件加速&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;EGL/OpenGL 配置（核心难点）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;eglopengl-配置核心难点&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#eglopengl-%e9%85%8d%e7%bd%ae%e6%a0%b8%e5%bf%83%e9%9a%be%e7%82%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;问题诊断流程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;检查错误信息&lt;/strong&gt;：&lt;code&gt;GL::Context: cannot retrieve OpenGL version&lt;/code&gt; → EGL 初始化失败&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检查 GPU&lt;/strong&gt;：&lt;code&gt;nvidia-smi&lt;/code&gt; 确认 GPU 可用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检查库文件&lt;/strong&gt;：确认 NVIDIA EGL 库存在&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;设置环境变量&lt;/strong&gt;：配置 EGL 平台和库路径&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多进程问题&lt;/strong&gt;：使用 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 确保子进程继承设置&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;调试技巧&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设置 &lt;code&gt;MAGNUM_LOG=verbose&lt;/code&gt; 和 &lt;code&gt;HABITAT_SIM_LOG=verbose&lt;/code&gt; 查看详细日志&lt;/li&gt;
&lt;li&gt;使用最小测试脚本逐步验证每个环节&lt;/li&gt;
&lt;li&gt;检查日志中的 &lt;code&gt;found X EGL devices&lt;/code&gt; 确认 GPU 被检测到&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;配置文件路径问题&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;配置文件路径问题&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%b7%af%e5%be%84%e9%97%ae%e9%a2%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;常见问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置文件引用的数据集名称与实际目录不匹配&lt;/li&gt;
&lt;li&gt;场景数据集的实际目录结构与配置期望不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;排查方法&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;检查配置文件中的 &lt;code&gt;data_path&lt;/code&gt; 和 &lt;code&gt;scene_id&lt;/code&gt; 路径&lt;/li&gt;
&lt;li&gt;使用符号链接统一路径结构&lt;/li&gt;
&lt;li&gt;创建最小测试脚本验证路径是否正确&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;五、复现其他 3D 视觉工作的通用检查清单&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五复现其他-3d-视觉工作的通用检查清单&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94%e5%a4%8d%e7%8e%b0%e5%85%b6%e4%bb%96-3d-%e8%a7%86%e8%a7%89%e5%b7%a5%e4%bd%9c%e7%9a%84%e9%80%9a%e7%94%a8%e6%a3%80%e6%9f%a5%e6%b8%85%e5%8d%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;环境检查&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;环境检查&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%8e%af%e5%a2%83%e6%a3%80%e6%9f%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; GPU 驱动已安装：&lt;code&gt;nvidia-smi&lt;/code&gt; 能正常显示&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; CUDA 版本匹配：检查项目要求的 CUDA 版本&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; EGL 库已安装：&lt;code&gt;apt-get install libegl1-mesa&lt;/code&gt;（或使用 NVIDIA 版本）&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Python 环境正确：conda/venv 环境已激活&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;图形渲染配置&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;图形渲染配置&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%be%e5%bd%a2%e6%b8%b2%e6%9f%93%e9%85%8d%e7%bd%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 设置无头模式：&lt;code&gt;export EGL_PLATFORM=surfaceless&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 指定 GPU 设备：&lt;code&gt;export EGL_DEVICE_ID=0&lt;/code&gt;（多 GPU 时）&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 强制使用硬件加速：设置 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 指向 NVIDIA 库&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 多进程环境：确保子进程继承环境变量&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;数据集路径&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;数据集路径&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86%e8%b7%af%e5%be%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 场景数据集路径正确：检查 &lt;code&gt;scene_datasets/&lt;/code&gt; 目录&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Episode 数据集路径正确：检查 &lt;code&gt;datasets/&lt;/code&gt; 目录&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 配置文件中的路径与实际目录匹配&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 符号链接正确：使用 &lt;code&gt;ls -la&lt;/code&gt; 检查符号链接&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;调试技巧&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;调试技巧&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b0%83%e8%af%95%e6%8a%80%e5%b7%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 启用详细日志：设置 &lt;code&gt;*_LOG=verbose&lt;/code&gt; 环境变量&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 最小测试：创建简单的测试脚本验证每个组件&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 逐步排查：先测试 EGL 初始化，再测试场景加载，最后测试完整流程&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 日志重定向：&lt;code&gt;&amp;gt; log.txt 2&amp;gt;&amp;amp;1&lt;/code&gt; 保存日志避免终端溢出&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;六、常见错误与解决方案速查&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六常见错误与解决方案速查&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af%e4%b8%8e%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e9%80%9f%e6%9f%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;错误信息&lt;/th&gt;
          &lt;th&gt;原因&lt;/th&gt;
          &lt;th&gt;解决方案&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;GL::Context: cannot retrieve OpenGL version&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;EGL 初始化失败&lt;/td&gt;
          &lt;td&gt;设置 &lt;code&gt;EGL_PLATFORM=surfaceless&lt;/code&gt; 和 &lt;code&gt;LD_PRELOAD&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;malloc_consolidate(): unaligned fastbin chunk detected&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;多进程环境下库加载冲突&lt;/td&gt;
          &lt;td&gt;使用 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 强制加载 NVIDIA 库&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;ImportError: libEGL.so.1&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;EGL 库未安装&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;apt-get install libegl1-mesa&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;FileNotFoundError: Could not find dataset file&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;数据集路径不匹配&lt;/td&gt;
          &lt;td&gt;检查配置文件中的路径，创建符号链接&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;No Stage Attributes exists for requested scene&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;场景文件路径错误&lt;/td&gt;
          &lt;td&gt;检查场景数据集目录结构，创建缺失的符号链接&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>LOVON 相关 Baseline 调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-11/</guid>
      <description>
        
        
        &lt;h1&gt;LOVON 相关 Baseline 调研&lt;/h1&gt;&lt;p&gt;本篇内容主要集中在针对 LOVON 论文中所对比的 paper 工作，他的仿真指标，一方面顺着前人的工作一路做下来思路比较直接也比较连贯，另一方面我还是觉得&lt;code&gt;gym-unrealcv&lt;/code&gt;这个模拟仿真的引擎相对&lt;code&gt;MatterPort3d&lt;/code&gt;还是小众了一点，也没有现成的博客文章去汇总有哪些工作用到了这个。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/lovon-baseline.png&#34; alt=&#34;这个仿真得分已经终结这个领域了&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;DIMP: Learning discriminative model prediction for tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;dimp-learning-discriminative-model-prediction-for-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dimp-learning-discriminative-model-prediction-for-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;SARL: End-to-end active object tracking and its real-world deployment via reinforcement learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;sarl-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#sarl-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;AD-VAT: End-to-end active object tracking and its real-world deployment via reinforcement learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ad-vat-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ad-vat-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;AD-VAT+: An asymmetric dueling mechanism for learning and understanding visual active tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ad-vat-an-asymmetric-dueling-mechanism-for-learning-and-understanding-visual-active-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ad-vat-an-asymmetric-dueling-mechanism-for-learning-and-understanding-visual-active-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;TS: Towards distraction-robust active visual tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ts-towards-distraction-robust-active-visual-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ts-towards-distraction-robust-active-visual-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;RSPT: reconstruct surroundings and predict trajectory for generalizable active object tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;rspt-reconstruct-surroundings-and-predict-trajectory-for-generalizable-active-object-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#rspt-reconstruct-surroundings-and-predict-trajectory-for-generalizable-active-object-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;EVT: Empowering embodied visual tracking with visual foundation models and offline RL&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;evt-empowering-embodied-visual-tracking-with-visual-foundation-models-and-offline-rl&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#evt-empowering-embodied-visual-tracking-with-visual-foundation-models-and-offline-rl&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;TrakVLA: Embodied visual tracking in the wild&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;trakvla-embodied-visual-tracking-in-the-wild&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#trakvla-embodied-visual-tracking-in-the-wild&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;
  &lt;p&gt;PKU在2025年5月的工作，VLA对训练算力和时间的要求堪称恐怖，所以这里单纯参考一下思想&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;Embodied visual tracking enables an agent to follow a specific target in dynamic environments using &lt;strong&gt;only egocentric vision&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of &lt;strong&gt;severe occlusion&lt;/strong&gt; and &lt;strong&gt;high scene dynamics&lt;/strong&gt;, 也就是遮挡和高动态性。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;研究方向&lt;/th&gt;
          &lt;th&gt;现有方法特点&lt;/th&gt;
          &lt;th&gt;局限性&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;具身视觉跟踪&lt;/td&gt;
          &lt;td&gt;分模型基（IBVS）、RL 基（AD-VAT、EVT [6]）、IL 基（Uni-NaVid）&lt;/td&gt;
          &lt;td&gt;误差累积；Uni-NaVid 依赖离散动作空间&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;具身导航&lt;/td&gt;
          &lt;td&gt;聚焦静态室内环境（如视觉 - 语言导航）&lt;/td&gt;
          &lt;td&gt;忽略真实世界动态性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;VLA 模型&lt;/td&gt;
          &lt;td&gt;用于操纵/导航，基于预训练 VLM 扩展动作生成&lt;/td&gt;
          &lt;td&gt;推理效率低，仅在低动态环境验证&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;而现有的相关工作都是将Recognition和Trajectory Planning给decouple出来的，而these methods are limited to category-level tracking in relatively open areas, 看来大家都意识到了这个问题，而作者指出这是因为上面decoupling的两个模块会产生error accumulation——识别错误可能导致规划失效，反之亦然。&lt;/p&gt;
&lt;p&gt;因此要用unified framework统合起来，共用token encoding（deconding的时候再分为两个头，一个language modeling head解码识别任务的文本响应，一个anchor-based diffusion head生成航点轨迹应用于规划任务）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/trackVLA_overview.png&#34; alt=&#34;overview&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Manim 入门与数学动画制作</title>
      <link>http://localhost:1313/blog/2025/2025-12-10-manim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-10-manim/</guid>
      <description>
        
        
        &lt;h1&gt;manim入门&lt;/h1&gt;&lt;p&gt;Manim (Mathematical Animation Engine) 是由 3Blue1Brown (Grant Sanderson) 开发的 Python 库，用于制作高质量的数学动画。&lt;/p&gt;
&lt;p&gt;用它来复现和理解论文中的 Methodology 和数学推导是一个非常棒的想法，不仅能吃透公式背后的逻辑，还能产出极具展示价值的可视化素材。&lt;/p&gt;
&lt;h2&gt;环境配置&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;环境配置&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;conda install -c conda-forge ffmpeg&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;LaTeX环境MikTex我先前已经安好了，这里不再继续
然后&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;pip install manim&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;接下来安装Manim Sideview插件，这样一来就能在IDE的侧边栏预览效果了&lt;/p&gt;
&lt;p&gt;实际体验下来，gemini生成的并不优秀，不能用其作为理解数学公式的方法（也可能是我的提示词工程做的不好），可视化overview、pipeline倒是可以。&lt;/p&gt;
&lt;p&gt;具体我不打算深入语法这些，作为工具，仅把其&lt;a href=&#34;https://manim.foresai.com/tutorials_guides/tutorials/quickstart/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;中文文档&lt;/a&gt;放在这里。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Social Navigation Idea</title>
      <link>http://localhost:1313/blog/2025/2025-12-15-social-navigation-idea/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-15-social-navigation-idea/</guid>
      <description>
        
        
        &lt;h2&gt;自问自答&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;自问自答&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%87%aa%e9%97%ae%e8%87%aa%e7%ad%94&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在接触 LOVON 这类开放词汇物体导航工作时，发现它们对语义的理解停留在 YOLO 层面，追目标时避障效果很差。当时考虑过用边走边建图的 3D 重建方式，但看到 social navigation 后就搁置了。这里有两个问题值得深入思考。&lt;/p&gt;
&lt;h3&gt;问题一：Social Navigation 需要 3D 重建吗？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题一social-navigation-需要-3d-重建吗&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e4%b8%80social-navigation-%e9%9c%80%e8%a6%81-3d-%e9%87%8d%e5%bb%ba%e5%90%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在 Social Navigation 领域，做显式 3D 重建是一条歧路。这个结论需要从 ObjectNav 和 SocialNav 的本质区别说起。&lt;/p&gt;
&lt;p&gt;ObjectNav 的核心难点是记忆。机器人需要记住&amp;quot;我去过厨房没？那个杯子在哪？&amp;ldquo;这类问题。这里用 3D 建图（如语义地图）是有用的，因为它解决的是静态环境的探索与回溯。LOVON 避障差，通常是因为它是模块化的（YOLO 指方向，然后 Planner 走），尽管利用了 LLM 进行 NER 分解，这也算是一种传统 Planner，对近距离动态避障很弱。&lt;/p&gt;
&lt;p&gt;但 SocialNav 的核心难点是动态性。人是会动的。如果做 3D 重建（TSDF Fusion、NeRF-SLAM 等），会面临严重的鬼影问题。一个人从左走到右，3D 地图上会留下一串残影，不仅不能辅助导航，反而会变成一堆不存在的障碍物墙，把路堵死。&lt;/p&gt;
&lt;p&gt;Falcon、Rank 1、Rank 2 的成功证明了：处理动态环境，需要的是第一视角感知（Egocentric Perception）加上时序记忆（RNN/Transformer），而不是全局静态地图。&lt;/p&gt;
&lt;p&gt;关于&amp;quot;避障不行&amp;quot;的问题，之前遇到的 YOLO 检测到了但还是撞上去的情况，正是端到端强化学习（SocialNav）试图解决的。关键是把&amp;quot;识别&amp;quot;和&amp;quot;运动控制&amp;quot;分开。SocialNav 的方法（如 Falcon）是把深度/RGB 直接映射到动作。如果智能体撞了，它会直接受到惩罚。这比&amp;quot;YOLO 告诉 Planner 有人，Planner 计算路径&amp;quot;的链路反应更快，且更能处理复杂交互。&lt;/p&gt;
&lt;h3&gt;问题二：Depth Anything V3 是合适的&amp;quot;新锤子&amp;quot;吗？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题二depth-anything-v3-是合适的新锤子吗&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e4%ba%8cdepth-anything-v3-%e6%98%af%e5%90%88%e9%80%82%e7%9a%84%e6%96%b0%e9%94%a4%e5%ad%90%e5%90%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;用目标驱动研究的标准来评估这个&amp;quot;锤子&amp;rdquo;，会发现两种截然不同的使用方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;工程思维（不推荐）&lt;/strong&gt;：把 Falcon 输入端的深度传感器换成 Depth Anything V3 生成的深度。问题在于，在仿真器（如 Habitat）里，已经有了完美深度（Ground Truth Depth）。Depth Anything V3 生成的深度再好，也不可能比仿真器自带的完美深度更好。结果可能是性能下降（因为引入了推理延迟和误差），且没有任何创新性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;科研思维（推荐）&lt;/strong&gt;：Depth Anything V3 的真正价值在于&amp;quot;仿真到现实的鸿沟&amp;quot;或&amp;quot;语义感知&amp;quot;。&lt;/p&gt;
&lt;p&gt;第一个方向是鲁棒性/仿真到现实。SocialNav 在仿真里跑得好，是因为仿真里的深度是完美的。真实世界的深度传感器（Realsense/LiDAR）有噪声、有盲区（玻璃、强光）。Depth Anything V3 是一个鲁棒感知探针，它是从大量真实图片训练出来的，对真实世界的噪声有极强的鲁棒性。可以提出一个框架，证明在仿真里使用 Depth Anything V3 提取的特征（而非原始深度），能够让智能体在零样本迁移到真实世界时表现得更好，因为它学到的是通用的深度特征，而不是仿真特定的几何特征。&lt;/p&gt;
&lt;p&gt;第二个方向是隐式语义引导（更高级的）。Depth Anything 不仅仅是深度，它其实是基础模型。为了估计深度，它必须&amp;quot;理解&amp;quot;物体是什么（比如理解这块平滑的像素是墙而不是空洞）。利用 Depth Anything V3 的编码器特征作为 SocialNav 的额外输入，它的特征里不仅包含距离，还隐式包含了物体语义。这可能解决 Falcon&amp;quot;把人当圆柱体&amp;quot;的问题，让智能体能区分&amp;quot;人&amp;quot;和&amp;quot;人形雕塑&amp;quot;，或者区分&amp;quot;柔软的窗帘&amp;quot;和&amp;quot;坚硬的墙&amp;quot;。&lt;/p&gt;
&lt;p&gt;Falcon（基线）和 Rank 2 都依赖几何信息（位置、速度、距离）来判断风险。但有些情况仅靠深度和坐标无法区分：一个人站着不动是在玩手机（不会突然动），还是在等人（可能会突然拥抱）？一个人跑过来是冲着我来的（攻击性），还是只是路过（中性）？&lt;/p&gt;
&lt;p&gt;科学的做法是：不要用 Depth Anything V3 做深度，用它（或者 VLM 如 CLIP/SigLIP）做视觉语义特征提取。参考 Rank 2 的架构，加一个辅助模块，但不是预测&amp;quot;风险分数&amp;quot;，而是预测&amp;quot;社会意图&amp;quot;或&amp;quot;语义状态&amp;quot;。&lt;/p&gt;
&lt;p&gt;为什么这样做有效？Rank 1 证明了数据重要，可以利用基础模型里的海量数据。Rank 2 证明了显式预测隐变量重要，可以预测比风险更高级的意图。&lt;/p&gt;
&lt;h2&gt;Literature Review&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;literature-review&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#literature-review&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在 Google Scholar 里检索引用了 Falcon 的工作，整理如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;序号&lt;/th&gt;
          &lt;th&gt;论文标题&lt;/th&gt;
          &lt;th&gt;中稿状态&lt;/th&gt;
          &lt;th&gt;主要功能&lt;/th&gt;
          &lt;th&gt;核心内容&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding&lt;/td&gt;
          &lt;td&gt;已中稿 CVPR 2025&lt;/td&gt;
          &lt;td&gt;解决零样本开放词汇的 3D 视觉定位问题，即根据自然语言描述在 3D 场景中找到特定物体&lt;/td&gt;
          &lt;td&gt;提出 SeeGround 方法，无需针对 3D 数据进行专门训练。核心思想是将 3D 场景转化为 2D VLM 可理解的格式，通过渲染 3D 场景图像并使用视觉提示技术建立 2D 图像与 3D 空间信息的对应关系，利用预训练的 2D VLM 理解场景并定位物体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 技术报告，Top-2&lt;/td&gt;
          &lt;td&gt;解决跨模态无人机导航中的图像检索问题，即根据自然语言描述从大规模数据库中检索对应的无人机视角图像&lt;/td&gt;
          &lt;td&gt;提出 CGRS 两阶段检索增强框架：第一阶段使用基线模型进行粗略排序；第二阶段利用 VLM 为候选图像生成详细描述，计算查询文本与生成描述的相似度进行精细重排序，显著提高检索精度&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;Zero-Shot 3D Visual Grounding from Vision-Language Models&lt;/td&gt;
          &lt;td&gt;arXiv Preprint (arXiv:2505.22429)&lt;/td&gt;
          &lt;td&gt;零样本 3D 视觉定位&lt;/td&gt;
          &lt;td&gt;从作者和题目看，极大概率是 Seeground (CVPR 2025) 的预印本或其前身，内容与 Seeground 一致，探讨如何利用 VLM 实现零样本 3D 视觉定位&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;Learning to Navigate Socially Through Proactive Risk Perception&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告，第 2 名&lt;/td&gt;
          &lt;td&gt;解决社会导航问题，让机器人在人群密集的动态环境中安全、合乎社会规范地导航&lt;/td&gt;
          &lt;td&gt;基于 Falcon 模型改进，增加主动风险感知模块，能够预测周围行人的基于距离的碰撞风险分数，让机器人具备更强的空间感知能力，主动采取避障行为并保持合适的社交距离&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;解决多楼层环境下的零样本物体目标导航&lt;/td&gt;
          &lt;td&gt;提出 ASCENT 框架，结合多楼层空间抽象和基于 LLM 的由粗到细的边界探索，利用 LLM 的常识推理能力（如&amp;quot;瑜伽垫可能在健身房，而健身房在楼下&amp;quot;）指导机器人跨楼层搜索&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;零样本社会导航&lt;/td&gt;
          &lt;td&gt;提出 SocialNav-Map 框架，结合动态人类轨迹预测和占据栅格地图。不需要针对特定环境训练，使用两种互补方法预测人类轨迹（基于历史路径和基于朝向），将预测结果作为动态障碍物整合进地图，使机器人能预见人的移动并提前规划路径&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;7&lt;/td&gt;
          &lt;td&gt;View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;零样本 3D 视觉定位&lt;/td&gt;
          &lt;td&gt;提出 VoG 方法，不同于直接把图像喂给模型，该方法将 3D 场景构建为多模态、多层级的场景图。VLM 被设计为主动代理，在图上进行遍历和推理，逐步搜索并定位目标物体，这种结构化方式降低了推理难度，提高了可解释性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;社会导航，强调符合人类社会规范的舒适度&lt;/td&gt;
          &lt;td&gt;提出 RLSLM 混合强化学习框架，将心理学实验推导出的规则基社会运动模型整合到 RL 的奖励函数中，让 RL 智能体在学习导航策略时天生倾向于遵守人类的社交舒适区，实现规则可解释性与 RL 适应性的结合&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;Comfort-Aware Trajectory Optimization for Immersive Human-Robot Interaction&lt;/td&gt;
          &lt;td&gt;已发表于 IEEE Open Journal on Immersive Displays (2025)&lt;/td&gt;
          &lt;td&gt;针对沉浸式环境（如 VR）中的人机交互，优化机器人运动轨迹&lt;/td&gt;
          &lt;td&gt;提出轨迹预测与优化框架，专门针对舒适度和路径合理性进行优化。在 VR 环境中进行用户研究，证明该方法生成的轨迹比传统方法更自然、更让用户感到舒适&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;Where to Fuse in the VLM Era: A Survey on Integrating Knowledge into Object Goal Navigation&lt;/td&gt;
          &lt;td&gt;Workshop Paper，发表于 HEAI Workshop&lt;/td&gt;
          &lt;td&gt;综述&lt;/td&gt;
          &lt;td&gt;探讨在物体目标导航任务中应该在&amp;quot;哪里&amp;quot;融合 VLM/LLM 的知识。借鉴自动驾驶的感知-预测-规划范式，将现有工作分类为在感知层融合、在预测层融合或在规划层融合，并分析各类优缺点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;Layout-Robust LiDAR 3D Object Detection via Multi-Representation Fusion&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;解决跨不同车辆平台的 LiDAR 3D 目标检测问题&lt;/td&gt;
          &lt;td&gt;针对不同车辆上 LiDAR 传感器布局（位置、数量、角度）不同导致模型泛化能力差的问题，提出统一表示框架，包含多视图融合模块（通过点-体素注意力机制学习统一视图不变表示）和运动引导的时空融合模块&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;Enhancing Multi-View Driving VLMs via Pseudo-Label Pretraining and Long-Tail Balancing&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;提升视觉语言模型在自动驾驶场景中的理解能力（感知、预测、规划）&lt;/td&gt;
          &lt;td&gt;基于 InternVL3-8B 模型提出两阶段优化框架：第一阶段利用伪标签预训练，结合思维链推理，将多视角图像按固定序列拼接；第二阶段针对长尾数据进行平衡处理，结合官方数据与合成数据进行混合微调，通过模型集成提升鲁棒性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;13&lt;/td&gt;
          &lt;td&gt;Robust 3D Object Detection under Sensor Placement Variability&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;增强 3D 目标检测模型对传感器安装位置变化的鲁棒性&lt;/td&gt;
          &lt;td&gt;针对不同车型 LiDAR 安装位置差异大导致模型失效的问题，提出三种策略集成：时序增强（聚合连续 LiDAR 扫描帧以丰富几何信息）、混合位置训练（在训练中模拟多种传感器配置）、推理时增强。在 Track 5 基准测试中表现出色&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;14&lt;/td&gt;
          &lt;td&gt;Enhancing VLMs for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;解决 VLM 在自动驾驶中空间推理弱和多任务干扰的问题&lt;/td&gt;
          &lt;td&gt;提出系统解决方案，核心是任务特定的提示。Prompt Routing：根据问题类型（感知、预测、规划等）将问题路由到专门的 expert prompt。空间推理增强：显式定义多视图坐标系和领域约束（如&amp;quot;后视摄像头的物体一定在车后&amp;quot;），帮助 VLM 理解空间关系。在 Track 1 的 Phase-1 和 Phase-2 中均取得很高准确率（70%+）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;15&lt;/td&gt;
          &lt;td&gt;Towards Socially Compliant Navigation: Hybrid Parameter Optimization for Falcon in Dynamic Environments&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;优化社会导航模型 Falcon 的参数，使其更符合社会规范&lt;/td&gt;
          &lt;td&gt;针对 Falcon 模型在平衡&amp;quot;任务效率&amp;quot;和&amp;quot;遵守社会规范&amp;quot;之间的矛盾，提出混合参数优化策略，结合比例约束的参数耦合和网格搜索，解决奖励函数参数过多导致的维度爆炸问题，更有效地找到让机器人既跑得快又懂礼貌的参数组合&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;16&lt;/td&gt;
          &lt;td&gt;HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;解决无人机跨模态导航中的定位匹配问题&lt;/td&gt;
          &lt;td&gt;针对无人机视角（俯视、广角）与文本描述之间的巨大差异，提出 HCCM 框架，核心在于分层和跨粒度。不仅做整体图像匹配，还将图像和文本分解为不同层级（如全局场景 vs. 局部地标），在不同粒度上进行对比学习和匹配，提高在大范围航拍图像中定位具体目标的准确性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;17&lt;/td&gt;
          &lt;td&gt;Unsupervised Domain Adaptation for 3D Object Detection via Adversarial Learning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;解决跨平台 3D 目标检测的域适应问题&lt;/td&gt;
          &lt;td&gt;针对源域和目标域 LiDAR 配置不同导致的数据分布差异，采用无监督域适应方法，核心引入对抗学习。通过训练域判别器区分特征来自哪个平台，同时强制特征提取器欺骗判别器，提取平台无关特征，使模型能泛化到新车型上&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;18&lt;/td&gt;
          &lt;td&gt;Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 获奖方案&lt;/td&gt;
          &lt;td&gt;高效的跨平台 3D 检测&lt;/td&gt;
          &lt;td&gt;基于强力的 PVRCNN++ 基线模型，使用两项关键技术弥补域差异：强数据增强（在数据层面模拟不同传感器噪声和几何变换）和伪标签（使用模型在未标注目标域数据上生成的置信度高的预测结果作为伪标签进行自我训练，逐步适应新环境）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;19&lt;/td&gt;
          &lt;td&gt;Task Aware Prompt Routing and CoT Augmented Fine Tuning for Driving VQA&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;提升自动驾驶 VLM 处理复杂问答（感知、预测、规划）的能力&lt;/td&gt;
          &lt;td&gt;提出任务感知提示路由：不使用通用提示，先判断问题属于哪类任务（如&amp;quot;前方有车吗？&amp;ldquo;属感知，&amp;ldquo;它会左转吗？&amp;ldquo;属预测），然后路由到专门优化的 Prompt 模板。思维链增强微调：在微调过程中加入推理步骤，强迫模型在给出结论前先生成推理过程，显著提升复杂逻辑问题的回答准确率&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;Driving Robustly through Corruptions: Multi-Source LoRA Fine-Tuning of Driving VLMs for Multi-View Reasoning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;增强 VLM 对图像腐蚀/干扰的鲁棒性&lt;/td&gt;
          &lt;td&gt;针对雨雪雾、传感器噪声等恶劣视觉条件，采用 Multi-Source LoRA 微调策略。在训练时故意引入多种类型的图像腐蚀数据作为多源输入，通过轻量级 LoRA 模块让大模型快速适应这些低质量输入，保证在视觉条件退化时仍能安全推理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;21&lt;/td&gt;
          &lt;td&gt;SegSy3D: Segmentation-Guided Self-Training and Model Synergy for Cross-Platform 3D Detection&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;利用语义信息辅助跨平台 3D 检测&lt;/td&gt;
          &lt;td&gt;分割引导：认为仅仅做检测不够，利用点云的语义分割任务作为辅助，帮助模型更好地理解物体形状和背景，从而提升检测器的特征质量。模型协同：涉及多个模型（如分割模型和检测模型）之间的互助学习或集成，以克服单一模型的偏差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;22&lt;/td&gt;
          &lt;td&gt;Towards Generalizable 3D Object Detection Across Sensor Placements&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;解决 LiDAR 安装位置变化带来的检测失效问题&lt;/td&gt;
          &lt;td&gt;重点研究当 LiDAR 安装高度、俯仰角发生变化时点云分布的改变，提出通用检测框架，可能包含几何校正模块，或在特征空间进行视角对齐，确保无论雷达装在车顶还是车头，提取出的车辆特征是一致的&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;23&lt;/td&gt;
          &lt;td&gt;PlaceRecover: A Transformer-based Point Cloud Recovery Network with Implicit Neural Representations for Robust LiDAR Placement Adaptation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;通过重建/恢复点云来解决传感器位置差异问题&lt;/td&gt;
          &lt;td&gt;独特思路：不同于调整检测器，试图直接调整数据。PlaceRecover 是基于 Transformer 的网络，结合隐式神经表示，目标是将不同位置采集的畸变点云恢复/重构为标准视角下的点云，这样后续检测模型不需要修改，直接在恢复后的标准点云上运行即可&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;24&lt;/td&gt;
          &lt;td&gt;A Parameter-Efficient MoE Framework for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 冠军方案&lt;/td&gt;
          &lt;td&gt;高效、高精度的无人机跨模态检索与导航&lt;/td&gt;
          &lt;td&gt;引入混合专家模型架构，MoE 允许模型拥有巨大参数量但推理计算量很小（每次只激活部分专家）。在无人机导航任务中，不同专家可能分别负责处理文本理解、视觉特征提取或地理空间推理。参数高效通常意味着使用 Adapter 或 LoRA 等技术，使模型在有限算力下快速适应新任务&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;25&lt;/td&gt;
          &lt;td&gt;Robust 3D Object Detection via Physical-Aware Augmentation and Class-Specific Model Ensembling&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;通过物理感知增强和集成学习提升检测鲁棒性&lt;/td&gt;
          &lt;td&gt;物理感知增强：传统复制粘贴增强可能把车放在天上或穿墙，该方法设计符合物理规律的增强策略（如贴地、防碰撞），生成更逼真的训练样本。类别特定模型集成：针对不同类别（如车、人、骑行者）训练专门检测器，最后进行集成，利用不同模型在不同类别上的优势，最大化整体分数&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;除了上述文献，RoboSense 2025 Track 2 的两篇冠亚军方案也值得关注。它们的改进思路截然不同：一个改了模型架构，另一个改了训练策略。这种对比很有意思，展现了同一个问题可以从不同角度切入。&lt;/p&gt;
&lt;p&gt;亚军方案来自小米的工作，核心思路是**&amp;ldquo;预知不够，还需要风险评估&amp;rdquo;**。这个洞察很有意思。&lt;/p&gt;
&lt;p&gt;Falcon 虽然能预测人类未来的轨迹，但它对危险的感知是滞后的。Falcon 主要靠碰撞后的惩罚来学习，这导致智能体知道人要去哪，但不知道**&amp;ldquo;离得近有多危险&amp;rdquo;**。就像一个人能预测另一个人会走到哪里，但不知道保持多远的距离才安全。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，研究者在 Falcon 的架构上增加了一个&lt;strong&gt;主动风险感知模块&lt;/strong&gt;。这是一个轻量级的神经网络，利用共享的隐层状态，显式地预测周围每个人类的碰撞风险分数。它把风险分成了三个等级：&lt;strong&gt;Safe、Warning、Danger&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;这个模块的贡献在于将&lt;strong&gt;隐式的避障逻辑变成了显式的风险监督信号&lt;/strong&gt;。智能体在还没撞上之前，就学会了&amp;quot;这种距离是不舒服的&amp;rdquo;，从而更早地进行微调。这种从隐式到显式的转变，让模型的行为更加可解释，也更容易调试。&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;这个模块的贡献在于将隐式的避障逻辑变成了显式的风险监督信号智能体在还没撞上之前就学会了这种距离是不舒服的从而更早地进行微调这种从隐式到显式的转变让模型的行为更加可解释也更容易调试&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%bf%99%e4%b8%aa%e6%a8%a1%e5%9d%97%e7%9a%84%e8%b4%a1%e7%8c%ae%e5%9c%a8%e4%ba%8e%e5%b0%86%e9%9a%90%e5%bc%8f%e7%9a%84%e9%81%bf%e9%9a%9c%e9%80%bb%e8%be%91%e5%8f%98%e6%88%90%e4%ba%86%e6%98%be%e5%bc%8f%e7%9a%84%e9%a3%8e%e9%99%a9%e7%9b%91%e7%9d%a3%e4%bf%a1%e5%8f%b7%e6%99%ba%e8%83%bd%e4%bd%93%e5%9c%a8%e8%bf%98%e6%b2%a1%e6%92%9e%e4%b8%8a%e4%b9%8b%e5%89%8d%e5%b0%b1%e5%ad%a6%e4%bc%9a%e4%ba%86%e8%bf%99%e7%a7%8d%e8%b7%9d%e7%a6%bb%e6%98%af%e4%b8%8d%e8%88%92%e6%9c%8d%e7%9a%84%e4%bb%8e%e8%80%8c%e6%9b%b4%e6%97%a9%e5%9c%b0%e8%bf%9b%e8%a1%8c%e5%be%ae%e8%b0%83%e8%bf%99%e7%a7%8d%e4%bb%8e%e9%9a%90%e5%bc%8f%e5%88%b0%e6%98%be%e5%bc%8f%e7%9a%84%e8%bd%ac%e5%8f%98%e8%ae%a9%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%a1%8c%e4%b8%ba%e6%9b%b4%e5%8a%a0%e5%8f%af%e8%a7%a3%e9%87%8a%e4%b9%9f%e6%9b%b4%e5%ae%b9%e6%98%93%e8%b0%83%e8%af%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;冠军方案来自 Zhang 等人的 PER-Falcon，核心思路是**&amp;ldquo;模型不需要动，是数据利用效率太低&amp;rdquo;**。这个角度很独特，大多数工作都在改模型，但这个方法选择改训练策略。&lt;/p&gt;
&lt;p&gt;在强化学习训练社会导航时，大部分回合都是失败的，要么撞人，要么超时。成功的回合非常稀缺且珍贵，包含了完美的绕行和避让操作。Falcon 在训练时对所有数据一视同仁，导致智能体学了一堆&amp;quot;怎么死&amp;rdquo;，却没学够&amp;quot;怎么活&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;PER-Falcon 引入了&lt;strong&gt;正样本回放机制&lt;/strong&gt;。这是一种数据为中心的方法：把那些回报大于 10 的回合（即成功到达且避障良好）存到一个专门的缓冲区里。每隔一段时间，把这些&amp;quot;满分作业&amp;quot;拿出来让智能体再复习一遍，通过辅助的 PPO 更新来强化这些好的行为。&lt;/p&gt;
&lt;p&gt;这个方法的贡献在于证明了在社会导航中，&lt;strong&gt;强化好的行为比单纯修补坏的行为更有效&lt;/strong&gt;。这其实是一个训练技巧，但效果极好，提升了 7 个百分点。有时候，简单的方法反而最有效，关键是要找到问题的本质。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;特征&lt;/th&gt;
          &lt;th&gt;Rank 2 (Risk Perception)&lt;/th&gt;
          &lt;th&gt;Rank 1 (PER-Falcon)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;改进维度&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Perception / Loss Design (感知/损失函数)&lt;/td&gt;
          &lt;td&gt;Data Efficiency / Optimization (数据效率/优化)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;把&amp;quot;危险&amp;quot;显式量化，作为辅助监督信号&lt;/td&gt;
          &lt;td&gt;把&amp;quot;成功经验&amp;quot;加权，避免被噪音数据淹没&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;新增参数量&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;极小 (两层 MLP)&lt;/td&gt;
          &lt;td&gt;0 (仅改变训练流程)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;性能&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;SR 0.656, H-Coll 0.33&lt;/td&gt;
          &lt;td&gt;SR 0.660, H-Coll 0.32&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;借鉴点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;引入 Dense Signal 辅助 RL 训练&lt;/td&gt;
          &lt;td&gt;在 World Model 训练中进行数据筛选 (Data Curation)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Paper List&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;paper-list&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#paper-list&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于上述分析，可以梳理出两类值得深入阅读的论文。第一类是直接处理社会导航核心问题的论文，第二类是可以作为方法论迁移的&amp;quot;新锤子&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;直接相关的必读论文&lt;/strong&gt;主要解决如何在人群中导航的核心问题。&lt;strong&gt;SocialNav-Map&lt;/strong&gt; 是一个很好的基线或对比对象，它尝试把&amp;quot;预测&amp;quot;显式地画在地图上（Occupancy Map），而 Falcon 是隐式编码在特征里。对比这两者的优劣是很好的讨论点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RLSLM&lt;/strong&gt; 试图将基于规则的社会力模型的可解释性融合进强化学习。这直接关联到 Falcon 缺乏显式社会规范的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Towards Socially Compliant Navigation&lt;/strong&gt; 是针对 Falcon 的超参数调优。虽然技术含量可能不高，但它揭示了奖励函数设计的敏感性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comfort-Aware Trajectory Optimization&lt;/strong&gt; 关注&amp;quot;舒适度&amp;quot;指标。如果目标是 Level 5（社会智能导航），这篇论文定义的指标可能比单纯的成功率更有用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论迁移的选读论文&lt;/strong&gt;虽然不在 SocialNav 领域，但其中的技术（VLM、Visual Grounding）正是需要的&amp;quot;新锤子&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Seeground / Zero-Shot 3D Visual Grounding&lt;/strong&gt; 是极其重要的&amp;quot;新锤子&amp;quot;来源。如果想做语义社会导航，需要这篇论文的方法：如何把 3D 场景转化为 2D VLM 能理解的 Prompt。可以把它的&amp;quot;物体定位&amp;quot;任务替换为&amp;quot;社会规范定位&amp;quot;任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where to Fuse in the VLM Era&lt;/strong&gt; 是一本&amp;quot;操作手册&amp;quot;。当决定引入 VLM 时，这篇综述告诉应该把它放在感知层（用来理解人）、预测层（用来预测意图）还是规划层（用来写代码）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enhancing VLMs for Autonomous Driving&lt;/strong&gt; 虽然是自动驾驶（室外），但它处理思维链（Chain of Thought）和空间推理的 Prompt Engineering 技巧，完全可以迁移到室内 SocialNav。例如：&amp;ldquo;那个人在看手机 -&amp;gt; 所以他不会让路 -&amp;gt; 我应该从左边绕&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;Top 2 的方法依然在&lt;strong&gt;几何空间（距离、坐标）&lt;strong&gt;和&lt;/strong&gt;RL 优化（Loss、Replay）&lt;strong&gt;里打转。它们都没有解决&lt;/strong&gt;语义理解（Semantic Understanding）&lt;strong&gt;和&lt;/strong&gt;显式博弈（Explicit Negotiation）&lt;/strong&gt;。这正是未来研究的机会所在。&lt;/p&gt;
&lt;h3&gt;After&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;after&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#after&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;读完这 7 篇论文后，可以清晰地看到当前社会导航领域的图景。目前正在见证一场**&amp;ldquo;分裂&amp;rdquo;**：一边在优化几何强化学习范式（从深度传感器和 PPO 中榨取性能），另一边则倡导结构化/混合方法（地图、规则和 VLM）来绕过纯强化学习的低效。&lt;/p&gt;
&lt;p&gt;这个文献图谱的中心节点是 &lt;strong&gt;Falcon&lt;/strong&gt;，它通过将范式从反应式避障转向预测式轨迹规划，建立了一个强基线。&lt;/p&gt;
&lt;p&gt;**Falcon（基础）**提出了&amp;quot;未来感知&amp;quot;框架。它不再仅仅对当前人类位置做出反应，而是使用辅助任务显式预测人类轨迹（未来 $H$ 步），并惩罚机器人阻塞这些未来路径。关键指标是在 Social-HM3D 上达到了 55% 的成功率。但它的弱点是依赖&amp;quot;盲目&amp;quot;的端到端强化学习，需要大量训练（约 2400 GPU 小时），并且将人类视为简单的移动障碍物，缺乏语义上下文。&lt;/p&gt;
&lt;p&gt;有三篇论文接受 Falcon 的架构，但认为其训练方法有缺陷。它们旨在解决样本效率和奖励稀疏性问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据效率（&amp;ldquo;好学生&amp;quot;方法）&lt;/strong&gt;：PER-Falcon（Rank 1）发现 Falcon 通过平等对待所有训练回合而浪费数据。它引入了正样本回放（PER）机制，缓存&amp;quot;高价值&amp;quot;回合（成功导航）并定期回放给策略网络。结果是在成功率上比基线提升了约 12%，证明了课程/数据质量比模型大小更重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;信号密度（&amp;ldquo;直觉&amp;quot;方法）&lt;/strong&gt;：主动风险感知（Rank 2）认为 Falcon 的碰撞惩罚太&amp;quot;稀疏&amp;rdquo;（只有在撞到人时才受到惩罚）。它添加了一个模块来基于距离预测连续的风险分数，让机器人在碰撞发生前很久就能&amp;quot;感知危险&amp;rdquo;。结果获得了第 2 名，证明了密集监督有助于强化学习收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;超参数调优（&amp;ldquo;暴力&amp;quot;方法）&lt;/strong&gt;：混合参数优化认为 Falcon 的奖励权重（平衡效率与社会合规性）不是最优的。它使用网格搜索和耦合调优来找到参数的&amp;quot;帕累托前沿&amp;rdquo;。结果仅通过调参就实现了 15% 的成功率提升，凸显了强化学习基线的脆弱性。&lt;/p&gt;
&lt;p&gt;有两篇论文挑战端到端强化学习的主导地位，认为结构和心理学是比试错更好的老师。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;零样本&amp;quot;反叛&lt;/strong&gt;：SocialNav-Map 是一个关键的挑战者。它认为强化学习&amp;quot;不透明&amp;quot;且&amp;quot;难以泛化&amp;rdquo;。它实时构建动态占据地图，预测人类轨迹（使用历史+朝向）并将其&amp;quot;绘制&amp;quot;到地图上作为临时障碍物，然后使用经典路径规划器（快速行进方法）。洞察是它在没有训练的情况下击败了基于强化学习的 Falcon（后者需要 2396 GPU 小时训练），这表明显式世界建模可能优于隐式强化学习记忆。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;以人为中心&amp;quot;的混合&lt;/strong&gt;：RLSLM 认为机器人不应该只是&amp;quot;猜测&amp;quot;社会规范。它将从心理学实验推导出的社会运动模型（SLM）直接整合到奖励函数中，创建了一个&amp;quot;非对称舒适场&amp;rdquo;（人类讨厌从前方接近，而不是从后方）。验证是通过 VR 让人类评价机器人，证明它比标准基于规则的方法&amp;quot;更有礼貌&amp;quot;。&lt;/p&gt;
&lt;p&gt;最后两篇论文虽然不严格属于&amp;quot;社会导航&amp;quot;论文，但它们提供了解决社会导航中&amp;quot;语义鸿沟&amp;quot;的&amp;quot;新锤子&amp;quot;（技术）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SeeGround&lt;/strong&gt; 解决了 3D 视觉定位（通过文本找到物体）而无需 3D 训练数据。技术是使用&amp;quot;视角适应模块&amp;quot;（模拟相机看向物体）从 3D 场景渲染 2D 图像，并将其输入到 2D VLM。相关性在于它证明了如果正确格式化数据（渲染图像+空间提示），可以使用冻结的 2D VLM 来理解 3D 空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where to Fuse&lt;/strong&gt; 是一篇综述，分类了如何将 VLM/LLM 知识注入导航。框架将融合分为感知（识别物体）、预测（猜测关系）和规划（边界选择）。相关性在于它明确将&amp;quot;社会交互导航&amp;quot;称为未来，指出当前方法&amp;quot;将人类简化为移动障碍物&amp;quot;。&lt;/p&gt;
&lt;p&gt;基于这些论文，可以绘制出研究路线图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Novelty Tree（技术演进）&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Level 1：几何反应（过去）&lt;/strong&gt; → ORCA、Social Force&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 2：几何预测（Falcon 时代）&lt;/strong&gt; → Falcon、PER-Falcon（预测轨迹）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 3：结构化混合（当前 SOTA）&lt;/strong&gt; → SocialNav-Map（显式映射动态风险）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 4：语义社会智能（空白）&lt;/strong&gt; → 理解上下文（例如：&amp;ldquo;那两个人正在交谈，不要从中间走过&amp;rdquo;，或&amp;quot;那个人在赶路，让路&amp;quot;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge-Insight Tree（工具箱）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;社会导航中的挑战&lt;/th&gt;
          &lt;th&gt;当前解决方案（&amp;ldquo;旧&amp;quot;方法）&lt;/th&gt;
          &lt;th&gt;提出的洞察（使用文献）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;数据效率&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;训练 1000 万步（Falcon）&lt;/td&gt;
          &lt;td&gt;回放缓冲区：优先&amp;quot;正样本回合&amp;rdquo;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;奖励稀疏性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;稀疏碰撞惩罚&lt;/td&gt;
          &lt;td&gt;密集风险：预测连续&amp;quot;风险分数&amp;quot;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;泛化性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;在新地图上微调&lt;/td&gt;
          &lt;td&gt;零样本映射：使用动态占据地图&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;社会规范&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;希望强化学习&amp;quot;学会&amp;quot;它们&lt;/td&gt;
          &lt;td&gt;显式建模：注入心理学规则（SLM）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;语义盲区&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;仅深度输入（看不到&amp;quot;活动&amp;quot;）&lt;/td&gt;
          &lt;td&gt;VLM 注入：使用 SeeGround 的&amp;quot;视角渲染&amp;quot;来分类社会情境&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;文献清楚地表明，&lt;strong&gt;几何问题已经解决&lt;/strong&gt;（SocialNav-Map 证明了可以零样本完成）。下一个前沿是&lt;strong&gt;语义&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;机会：&amp;ldquo;语义社会地图&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;空白：Falcon 和 SocialNav-Map 将人类视为动态圆柱体。RLSLM 将它们视为磁场。它们都不知道人类在做什么。&lt;/p&gt;
&lt;p&gt;新锤子：SeeGround 证明了可以使用 VLM 来&amp;quot;看&amp;quot;特定的 3D 坐标并理解它。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;提出的想法：&amp;ldquo;VLM 驱动的社会可供性地图&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;感知&lt;/strong&gt;：使用 VLM（如 SeeGround）分析来自 RGB 相机的人类裁剪图像。分类它们的状态：&amp;ldquo;交互中&amp;rdquo;、&amp;ldquo;等待中&amp;rdquo;、&amp;ldquo;看手机&amp;rdquo;、&amp;ldquo;匆忙&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;映射&lt;/strong&gt;：不仅仅是&amp;quot;占据地图&amp;quot;（如 SocialNav-Map），而是构建&amp;quot;社会规范地图&amp;quot;。
&lt;ul&gt;
&lt;li&gt;示例：如果两个人正在&amp;quot;交互&amp;quot;，在它们之间创建&amp;quot;禁止通行区&amp;quot;。&lt;/li&gt;
&lt;li&gt;示例：如果一个人正在&amp;quot;看手机&amp;quot;，扩大其风险半径（他们分心了）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规划&lt;/strong&gt;：在这个新的语义地图上使用 SocialNav-Map 规划器（FMM）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这结合了 SocialNav-Map 的结构和 SeeGround 的语义能力，解决了 Where to Fuse 中识别的局限性。&lt;/p&gt;
&lt;p&gt;Google Search 验证建议：你可以搜一下 &amp;ldquo;Foundation model for social navigation&amp;rdquo; 或 &amp;ldquo;Language-guided social navigation&amp;rdquo;，看看Level 4/5目前是否已经有人在用VLM/DepthAnything的Feature做SocialNav了。如果没有，这就是Blue Ocean。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>VLN 正交分析法寻找创新点</title>
      <link>http://localhost:1313/blog/2025/2025-11-23-vln-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-23-vln-matrix/</guid>
      <description>
        
        
        &lt;h1&gt;VLN 正交分析法寻找创新点&lt;/h1&gt;&lt;h2&gt;框架一：【表征-推理】矩阵 (Representation-Reasoning Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架一表征-推理矩阵-representation-reasoning-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%b8%80%e8%a1%a8%e5%be%81-%e6%8e%a8%e7%90%86%e7%9f%a9%e9%98%b5-representation-reasoning-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：解决&amp;quot;机器人怎么看世界&amp;quot;和&amp;quot;机器人怎么做决策&amp;quot;的匹配问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：推理范式 (Reasoning) \ &lt;strong&gt;横轴&lt;/strong&gt;：环境表征 (Representation)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;推理范式 \ 环境表征&lt;/th&gt;
          &lt;th&gt;A. 纯视觉流&lt;/th&gt;
          &lt;th&gt;B. 2D 语义地图&lt;/th&gt;
          &lt;th&gt;C. 3D 场景图&lt;/th&gt;
          &lt;th&gt;D. 拓扑/文本图&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. End-to-End RL / IL&lt;/td&gt;
          &lt;td&gt;已拥挤&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;较少&lt;/td&gt;
          &lt;td&gt;较少&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Modular + LLM Prompting&lt;/td&gt;
          &lt;td&gt;难点&lt;/td&gt;
          &lt;td&gt;拥挤&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. System 1 + System 2&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. World Model / Generative&lt;/td&gt;
          &lt;td&gt;前沿&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 1 (A-3)&lt;/strong&gt;: 目前 End-to-End 模型（如 NaVid）反应快但缺乏长程逻辑，而 LLM 反应慢。能否设计一个机制，平时用小模型看视频流走路（System 1），遇到&amp;quot;迷路&amp;quot;或&amp;quot;歧义&amp;quot;时，动态唤醒 LLM 分析当前视频帧（System 2）？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 2 (C-4)&lt;/strong&gt;: 现在的 Scene Graph 都是用来做当前状态的 Prompt。能否基于 Scene Graph 做&amp;quot;世界模型&amp;quot;？ 即：让 LLM 预测&amp;quot;如果我向左走，场景图会变成什么样？&amp;quot;，从而在图空间里做 Model-Based Planning，而不是由 LLM 直接瞎猜。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;框架二：【反馈-修正】矩阵 (Feedback-Correction Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架二反馈-修正矩阵-feedback-correction-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%ba%8c%e5%8f%8d%e9%a6%88-%e4%bf%ae%e6%ad%a3%e7%9f%a9%e9%98%b5-feedback-correction-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：针对 2024 年后的趋势——从&amp;quot;如何走对&amp;quot;转向&amp;quot;走错了如何修正&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：修正机制 (Correction) \ &lt;strong&gt;横轴&lt;/strong&gt;：错误源 (Source of Error)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;修正机制 \ 错误源&lt;/th&gt;
          &lt;th&gt;A. 感知幻觉&lt;/th&gt;
          &lt;th&gt;B. 空间迷失&lt;/th&gt;
          &lt;th&gt;C. 指令歧义&lt;/th&gt;
          &lt;th&gt;D. 动态障碍/变化&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. Passive (被动重规划)&lt;/td&gt;
          &lt;td&gt;传统方法&lt;/td&gt;
          &lt;td&gt;传统方法&lt;/td&gt;
          &lt;td&gt;无解&lt;/td&gt;
          &lt;td&gt;传统 DWA/TEB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Active Perception (主动探索)&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. Dialogue / Interaction&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
          &lt;td&gt;已拥挤&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. Self-Reflexion (自省)&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 3 (B-4)&lt;/strong&gt;: 现在的 Self-Reflexion 大多是在想&amp;quot;我是不是理解错指令了&amp;quot;。很少有工作做&amp;quot;空间自省&amp;quot;——即 LLM 结合历史轨迹图，反思&amp;quot;我现在的视觉观测和我记忆中的地图不一致，我是不是已经走到错误的房间了？&amp;quot;（Spatial Consistency Check via LLM）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 4 (A-3)&lt;/strong&gt;: 当 VLM 觉得前面是&amp;quot;椅子&amp;quot;但又不确定时（Confidence score 低），目前的做法是硬着头皮走。创新点可以是：主动发起一轮对话确认，或者主动移动相机去验证（Active Perception for VLM uncertainty）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;框架三：【多模态融合-时空】矩阵 (Fusion-Spatiotemporal Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架三多模态融合-时空矩阵-fusion-spatiotemporal-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%b8%89%e5%a4%9a%e6%a8%a1%e6%80%81%e8%9e%8d%e5%90%88-%e6%97%b6%e7%a9%ba%e7%9f%a9%e9%98%b5-fusion-spatiotemporal-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：针对 CoRL/ICRA 等机器人会议，关注&amp;quot;具体怎么融合特征&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：融合阶段 (Fusion Stage) \ &lt;strong&gt;横轴&lt;/strong&gt;：时间维处理 (Temporal)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;融合阶段 \ 时间维处理&lt;/th&gt;
          &lt;th&gt;A. Frame-wise (单帧)&lt;/th&gt;
          &lt;th&gt;B. Feature Buffer&lt;/th&gt;
          &lt;th&gt;C. Explicit Map&lt;/th&gt;
          &lt;th&gt;D. Neural Memory&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. Early Fusion&lt;/td&gt;
          &lt;td&gt;基础&lt;/td&gt;
          &lt;td&gt;计算量大&lt;/td&gt;
          &lt;td&gt;VLMaps&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Late Fusion&lt;/td&gt;
          &lt;td&gt;CLIP-Nav&lt;/td&gt;
          &lt;td&gt;NaVid&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;IVLN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. LLM-in-the-loop&lt;/td&gt;
          &lt;td&gt;GPT-4V Nav&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;UniGoal&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. Cross-Attention (Query-based)&lt;/td&gt;
          &lt;td&gt;传统 Transformer&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gap 5 (D-3)&lt;/strong&gt;: 目前的 LLM 导航要么看单张图，要么看 3D 场景图。很少有结合 Mamba 或 SSM (State Space Models) 的工作。 创新点：利用 Mamba 这种长序列处理能力极强的架构，作为 VLN 的&amp;quot;隐式记忆体&amp;quot;，替代显式的地图构建，实现无图但有长记忆的导航（Mamba-VLN）。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>VLN 综述以及后续文献</title>
      <link>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</guid>
      <description>
        
        
        &lt;h1&gt;VLN 系列&lt;/h1&gt;&lt;p&gt;从 Poing Navigation 到 Object Navigation，这也太难了，找Idea真的太难了。&lt;/p&gt;
&lt;p&gt;然后秋冬学期的一半，也就是大四上的一半已经过去了，马上就要寒假了，寒假做什么，实习还是论文？真能憋出论文吗？&lt;/p&gt;
&lt;h2&gt;Survey&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;survey&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#survey&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;研发能够与人类及其周边环境进行交互的&lt;strong&gt;具身智能体&lt;/strong&gt;（embodied agents），是&lt;strong&gt;人工智能&lt;/strong&gt;（AI）领域长期以来的核心目标之一。这类 AI 系统在现实世界中具有巨大的应用潜力，可作为日常生活中的多功能助手，例如&lt;strong&gt;家用机器人&lt;/strong&gt;、&lt;strong&gt;自动驾驶汽车&lt;/strong&gt;以及&lt;strong&gt;个人助手&lt;/strong&gt;。推动这一研究方向的一个正式问题设定是&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（Vision-and-Language Navigation, &lt;strong&gt;VLN&lt;/strong&gt;）—— 这是一项多模态协作任务，要求智能体遵循人类指令、探索&lt;strong&gt;三维&lt;/strong&gt;（3D）环境，并在存在各类歧义的场景下开展情境化通信。多年来，研究者已在&lt;strong&gt;照片级真实感模拟器&lt;/strong&gt;和&lt;strong&gt;真实环境&lt;/strong&gt;中对 VLN 展开探索，由此形成了一系列&lt;strong&gt;基准数据集&lt;/strong&gt;，每个数据集的问题表述略有不同。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/law-Challenge.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类&lt;/strong&gt;（Human）：给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;rdquo;；在智能体询问 &amp;ldquo;左边的房间还是前面的房间？&amp;rdquo; 时回复 &amp;ldquo;左边&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;物理环境&lt;/strong&gt;（Physical Environment）：智能体感知的视觉场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体&lt;/strong&gt;（VLN Agent）：接收指令后进行 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;（Grounding &amp;amp; Reasoning）、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;（Planning）、&lt;strong&gt;&amp;ldquo;对话&amp;rdquo;&lt;/strong&gt;（Dialogue），执行 &lt;strong&gt;&amp;ldquo;导航动作&amp;rdquo;&lt;/strong&gt;（Navigation Execution），并生成&lt;strong&gt;语言响应&lt;/strong&gt;（Language Response）；过程中可能产生疑问，如 &amp;ldquo;…… 进入房间。左边？右边？&amp;ldquo;&amp;ldquo;左边的房间还是前面的房间？&amp;quot;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心模块&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;（World Model）、&lt;strong&gt;人类模型&lt;/strong&gt;（Human Model），分别支撑智能体的环境理解与人类意图解读。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其&lt;a href=&#34;https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;仓库&lt;/a&gt;提到了一些工作内容，但是不全。&lt;/p&gt;
&lt;h3&gt;背景与任务基础&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;背景与任务基础&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%83%8c%e6%99%af%e4%b8%8e%e4%bb%bb%e5%8a%a1%e5%9f%ba%e7%a1%80&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;人类及其他具备导航能力的动物，很早就展现出对环境导航的理解与策略。例如，加利斯特尔（Gallistel）提出了两种基础机制：其一为&lt;strong&gt;引导法&lt;/strong&gt;（piloting），即利用环境地标计算距离与角度；其二为&lt;strong&gt;路径积分&lt;/strong&gt;（path integration），即通过自运动感知计算位移与方向变化。理解空间导航的核心是&lt;strong&gt;认知地图假说&lt;/strong&gt;（cognitive map hypothesis）—— 该假说认为，大脑会形成统一的空间表征，以支持记忆存储并指导导航行为。例如，托尔曼（Tolman）观察到，当熟悉的路径被阻断且地标消失时，大鼠仍能选择正确的新路径。神经科学家还发现了&lt;strong&gt;海马体位置细胞&lt;/strong&gt;（hippocampal place cells），这表明存在一种以&lt;strong&gt;异中心视角&lt;/strong&gt;（allocentrically）编码地标与目标的空间坐标系。&lt;/p&gt;
&lt;p&gt;传统上，&lt;strong&gt;&amp;ldquo;遵循自然语言导航指令&amp;rdquo;&lt;strong&gt;的任务多采用地图等&lt;/strong&gt;符号化世界表征&lt;/strong&gt;（symbolic world representations）进行建模。然而，本综述聚焦于采用视觉环境的模型，重点探讨&lt;strong&gt;多模态理解与接地&lt;/strong&gt;（grounding）的相关挑战。与此相对，关于&lt;strong&gt;视觉导航&lt;/strong&gt;和&lt;strong&gt;移动机器人导航&lt;/strong&gt;的综述文献已十分丰富，这类综述主要关注视觉感知与物理具身性，但若涉及 &lt;strong&gt;&amp;ldquo;语言在导航任务中的作用&amp;rdquo;&lt;/strong&gt;，则讨论较为简略，建议读者参考这些文献以获取相关背景。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;接地（Grounding）指将抽象的语言符号与具体的物理世界或感知数据建立对应关系的过程。在 VLN 中，接地是将自然语言指令映射到视觉场景中的具体位置、物体或动作。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;尽管在讨论 VLN 时，我们难免会将范围拓展到导航之外的领域（如移动操作、对话），但本综述的核心焦点仍是&lt;strong&gt;导航任务&lt;/strong&gt;，并将针对该任务提供详细的文献梳理。此外，以往的 VLN 综述多采用 &lt;strong&gt;&amp;ldquo;自下而上&amp;rdquo;&lt;/strong&gt; 的总结方式，聚焦于基准数据集与建模创新；而本综述则采用 &lt;strong&gt;&amp;ldquo;自上而下&amp;rdquo;&lt;/strong&gt; 的视角，并以&lt;strong&gt;基础模型&lt;/strong&gt;的角色为核心，将现有研究成果从 &lt;strong&gt;&amp;ldquo;世界模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;VLN 智能体&amp;rdquo;&lt;/strong&gt; 三个维度，归类为&lt;strong&gt;三大核心挑战&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;典型的 &lt;strong&gt;VLN 智能体&lt;/strong&gt;会在指定位置接收人类指令者给出的（一系列）&lt;strong&gt;语言指令&lt;/strong&gt;。该智能体以&lt;strong&gt;自我为中心的视觉视角&lt;/strong&gt;（egocentric visual perspective）在环境中导航，其核心任务是遵循指令生成&lt;strong&gt;轨迹&lt;/strong&gt; —— 轨迹可基于一系列离散视角，也可基于低层级动作与控制指令（例如 &amp;ldquo;前进 0.25 米&amp;rdquo;），最终抵达&lt;strong&gt;目标终点&lt;/strong&gt;。若智能体最终位置与目标终点的距离在指定范围内（例如 3 米），则判定为&lt;strong&gt;导航成功&lt;/strong&gt;。此外，智能体在导航过程中可与指令者交互：既可以请求帮助，也可进行自由形式的语言沟通。近年来，研究者对 VLN 智能体的期望进一步提升，要求其在导航的同时整合附加任务，例如&lt;strong&gt;操作任务&lt;/strong&gt;与&lt;strong&gt;目标检测任务&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-benchmark-2024.png&#34; alt=&#34;vln-benchmark-2024&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如上表，现有（2024）VLN &lt;strong&gt;基准数据集&lt;/strong&gt;可分为以下四类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航发生的 &amp;ldquo;世界&amp;rdquo;&lt;/strong&gt;：包括领域（室内或室外）与具体环境（如模拟器或真实场景）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类交互类型&lt;/strong&gt;：包括交互轮次（单轮或多轮）、通信格式（自由对话、受限对话或多指令）、语言粒度（动作导向或目标导向）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体属性&lt;/strong&gt;：包括智能体类型（如家用机器人、自动驾驶车辆、自主无人机）、动作空间（图基、离散或连续）、附加任务（操作与目标检测）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集收集方式&lt;/strong&gt;：包括文本收集（人类生成或模板生成）与路线演示（人类执行或规划器生成）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;研究者主要采用三类指标评估 VLN 智能体的&lt;strong&gt;导航寻路性能&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航误差&lt;/strong&gt;（Navigation Error, &lt;strong&gt;NE&lt;/strong&gt;）：智能体最终位置与目标终点之间最短路径距离的平均值&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成功率&lt;/strong&gt;（Success Rate, &lt;strong&gt;SR&lt;/strong&gt;）：最终位置足够接近目标终点的任务占比&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;路径长度加权成功率&lt;/strong&gt;（Success Rate Weighted Path Length, &lt;strong&gt;SPL&lt;/strong&gt;）：通过轨迹长度对成功率进行归一化，平衡 &lt;strong&gt;&amp;ldquo;抵达正确终点的成功率&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;路径效率&amp;rdquo;&lt;/strong&gt; 两大指标&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，还有一类指标用于衡量 &lt;strong&gt;&amp;ldquo;指令遵循的忠实度&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;预测轨迹和真实轨迹的一致性&amp;rdquo;&lt;/strong&gt;，例如：
4. &lt;strong&gt;长度加权覆盖得分&lt;/strong&gt;（Coverage Weighted by Length Score, &lt;strong&gt;CLS&lt;/strong&gt;）：衡量智能体轨迹与参考路径的贴合程度，通过 &lt;strong&gt;&amp;ldquo;参考路径覆盖范围&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;轨迹长度效率&amp;rdquo;&lt;/strong&gt; 两个维度平衡智能体性能
5. &lt;strong&gt;归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping, &lt;strong&gt;nDTW&lt;/strong&gt;）：对偏离真实轨迹的行为进行惩罚
6. &lt;strong&gt;成功率加权归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping Weighted by Success Rate, &lt;strong&gt;sDTW&lt;/strong&gt;）：在惩罚轨迹偏离的同时，还会结合导航成功率综合评估&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-challenges-and-soluions.png&#34; alt=&#34;vln-challenges-and-soluions&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;该图反映的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心模块关联&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;历史与记忆&amp;rdquo;&lt;/strong&gt;，&lt;strong&gt;人类模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;模糊指令&amp;rdquo;&lt;/strong&gt;，两者均涉及 &lt;strong&gt;&amp;ldquo;泛化能力&amp;rdquo;&lt;/strong&gt;；&lt;strong&gt;VLN 智能体&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;基础模型适配为智能体&amp;rdquo;&lt;/strong&gt; 三大方法&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基础模型角色&lt;/strong&gt;：根据基础模型承担的功能，将方法分为四类 —— &lt;strong&gt;数据与知识处理&lt;/strong&gt;（预处理 / 增强 / 合成数据、利用预训练常识）、&lt;strong&gt;表征学习&lt;/strong&gt;（通用文本 / 视觉表征、历史记忆处理）、&lt;strong&gt;决策制定&lt;/strong&gt;（导航规划器、信息寻求对话管理器、通用决策智能体）、&lt;strong&gt;任务学习&lt;/strong&gt;（具身推理、语言接地、少样本 / 上下文 / 微调学习具身任务）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;交互示例&lt;/strong&gt;：人类给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;quot;&amp;ldquo;去卫生间&amp;rdquo;；智能体通过提问（&amp;ldquo;走廊在哪里？&amp;ldquo;&amp;ldquo;哪个房间？&amp;quot;）寻求信息，人类回复（&amp;ldquo;左边的房间还是前面的房间？&amp;ldquo;&amp;ldquo;左边&amp;rdquo;）后，智能体执行动作（&amp;ldquo;前进&amp;quot;&amp;ldquo;左转&amp;rdquo;）并生成轨迹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战与未来方向&lt;/strong&gt;：&lt;strong&gt;基准数据集&lt;/strong&gt;（数据与任务局限）、&lt;strong&gt;世界模型&lt;/strong&gt;（从 2D 世界到 3D 世界）、&lt;strong&gt;人类模型&lt;/strong&gt;（从指令到对话）、&lt;strong&gt;智能体模型&lt;/strong&gt;（LLM 与 VLM 适配）、&lt;strong&gt;部署&lt;/strong&gt;（从仿真到真实机器人）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;三大解决方案&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三大解决方案&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e5%a4%a7%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;World Model: Learning and Representing the Visual Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-learning-and-representing-the-visual-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-learning-and-representing-the-visual-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;能够帮助 VLN 智能体理解周边环境、预测自身动作对世界状态的改变，并使自身感知与动作与语言指令对齐。现有研究中，学习世界模型主要面临两大挑战：一是将当前任务段内的&lt;strong&gt;视觉观测历史&lt;/strong&gt;编码为&lt;strong&gt;记忆&lt;/strong&gt;，二是实现对未见过环境的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;History and Memory&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;history-and-memory&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#history-and-memory&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;与&lt;strong&gt;视觉问答&lt;/strong&gt;（Visual Question Answering, &lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;视觉蕴含&lt;/strong&gt;（Visual Entailment）等其他视觉-语言任务不同，VLN 智能体需将过去动作与观测的&lt;strong&gt;历史信息&lt;/strong&gt;融入当前步骤的输入中以决策动作，而非仅依赖单一步骤的图像与文本。在 VLN 中应用基础模型之前，研究者通常采用 &lt;strong&gt;LSTM 隐藏状态&lt;/strong&gt;作为支持智能体导航决策的隐式记忆，并进一步设计不同的&lt;strong&gt;注意力机制&lt;/strong&gt;或&lt;strong&gt;辅助任务&lt;/strong&gt;，以提升编码历史与指令的对齐程度。&lt;/p&gt;
&lt;p&gt;目前已有多种基于基础模型的&lt;strong&gt;导航历史编码技术&lt;/strong&gt;，核心可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）基于令牌更新或序列建模的编码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer 初始化&lt;/strong&gt;：以基于域内指令-轨迹数据预训练的模型（如 &lt;strong&gt;Prevalent&lt;/strong&gt;）为基础，构建&lt;strong&gt;多模态 Transformer&lt;/strong&gt;，将编码后的指令与导航历史作为输入以实现决策。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;循环状态令牌编码&lt;/strong&gt;：部分方法通过循环更新的&lt;strong&gt;状态令牌&lt;/strong&gt;编码导航历史。例如，利用上一步的单个 &lt;strong&gt;[CLS] 令牌&lt;/strong&gt;编码历史信息；或设计&lt;strong&gt;变长记忆框架&lt;/strong&gt;，将过去步骤的多个动作激活值存储在记忆库中，作为历史编码。但这类方法需逐步骤更新令牌，难以高效检索导航轨迹中任意步骤的历史编码，限制了预训练的可扩展性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;全景与历史分层编码&lt;/strong&gt;：另一类方法直接通过多模态 Transformer 将导航历史编码为序列。例如，对轨迹中每一步的单视角图像进行编码；或进一步提出 &lt;strong&gt;&amp;ldquo;全景编码器 + 历史编码器&amp;rdquo;&lt;/strong&gt; 的分层设计 —— &lt;strong&gt;全景编码器&lt;/strong&gt;处理每一时间步的全景视觉观测，&lt;strong&gt;历史编码器&lt;/strong&gt;则编码所有过往观测。这种设计可分离全景视图中的空间关系与导航历史中跨全景的时间动态性，且无需依赖循环更新的状态令牌，便于基于指令-路径对进行高效、大规模的预训练。后续研究分别用 &lt;strong&gt;&amp;ldquo;图像均值池化&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;前视图像编码&amp;rdquo;&lt;/strong&gt; 替代全景编码器，均保持了良好的导航性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）基于 LLM 的文本化历史编码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随着基于 &lt;strong&gt;LLM&lt;/strong&gt; 的导航智能体兴起，&lt;strong&gt;&amp;ldquo;将视觉环境转换为文本描述&amp;rdquo;&lt;/strong&gt; 成为主流趋势。此时导航历史被编码为 &lt;strong&gt;&amp;ldquo;图像描述序列 + 相对空间信息&amp;rdquo;&lt;/strong&gt;（如朝向、高度、距离）的组合。例如，&lt;strong&gt;HELPER&lt;/strong&gt; 设计了 &lt;strong&gt;&amp;ldquo;语言-程序对&amp;rdquo;&lt;/strong&gt; 的外部记忆，通过检索增强的 LLM 提示，将人类与机器人的自由形式对话解析为动作程序。&lt;/p&gt;
&lt;p&gt;另一类研究通过融入&lt;strong&gt;图信息&lt;/strong&gt;增强导航历史建模，核心思路是利用&lt;strong&gt;结构化图表征&lt;/strong&gt;环境几何与空间关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拓扑图与结构化编码&lt;/strong&gt;：部分方法采用&lt;strong&gt;结构化 Transformer 编码器&lt;/strong&gt;捕捉环境中的几何线索。除编码中使用的&lt;strong&gt;拓扑图&lt;/strong&gt;外，许多研究还将&lt;strong&gt;俯视图信息&lt;/strong&gt;（如&lt;strong&gt;网格图&lt;/strong&gt;、&lt;strong&gt;语义图&lt;/strong&gt;、&lt;strong&gt;局部度量图&lt;/strong&gt;）与&lt;strong&gt;局部邻域图&lt;/strong&gt;纳入导航过程中的观测历史建模。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM 与图的结合&lt;/strong&gt;：近期基于 LLM 的导航智能体在记忆构建中引入了创新性的图应用。例如，提出一种基于&lt;strong&gt;地图引导的 GPT 智能体&lt;/strong&gt;，利用语言化形式的地图存储和管理拓扑图信息；&lt;strong&gt;MC-GPT&lt;/strong&gt; 则将拓扑图作为记忆结构，记录视角、物体及其空间关系的信息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;跨环境泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;跨环境泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%a8%e7%8e%af%e5%a2%83%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;VLN 的核心挑战之一是：如何从有限的可用环境中学习，并泛化到新的、未见过的环境中。现有研究表明，以下方法可提升智能体对未见过环境的泛化性能：&lt;strong&gt;学习语义分割特征&lt;/strong&gt;、&lt;strong&gt;利用训练过程中环境的 dropout 信息&lt;/strong&gt;、&lt;strong&gt;最大化不同环境中语义对齐图像对的相似度&lt;/strong&gt;。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;方法&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.1 预训练视觉表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;传统视觉编码器&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究采用在 ImageNet 上预训练的 &lt;strong&gt;ResNet&lt;/strong&gt; 提取视觉表征&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基于 VL 基础模型的表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;用 &lt;strong&gt;CLIP 视觉编码器&lt;/strong&gt;替代 ResNet——CLIP 通过图文对的对比损失预训练，可自然实现图像与指令的更好对齐，显著提升 VLN 性能&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;视频预训练表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;探索将从视频数据中学习的视觉表征迁移到 VLN 任务中，证实视频中的&lt;strong&gt;时间信息&lt;/strong&gt;对导航至关重要&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.2 环境增强&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;静态环境修改&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;EnvEdit&lt;/strong&gt;、&lt;strong&gt;EnvMix&lt;/strong&gt;、&lt;strong&gt;KED&lt;/strong&gt; 与 &lt;strong&gt;FDA&lt;/strong&gt; 通过修改 Matterport3D 中的现有环境生成合成数据，具体手段包括混合不同环境的房间、改变环境外观与风格、对环境高频特征进行插值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;动态环境合成&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;Pathdreamer&lt;/strong&gt; 与 &lt;strong&gt;SE3DS&lt;/strong&gt; 进一步实现 &lt;strong&gt;&amp;ldquo;基于当前观测合成未来步骤环境&amp;rdquo;&lt;/strong&gt;，并探索将合成视图作为 VLN 训练的增强数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.3 学习范式的转变&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;前基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究直接用自动收集的新环境增强训练环境，并微调基于 LSTM 的 VLN 智能体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;预训练&lt;/strong&gt;被证实对基础模型至关重要，因此 &lt;strong&gt;&amp;ldquo;在预训练阶段从收集的环境中学习&amp;rdquo;&lt;/strong&gt; 成为 VLN 的标准做法。基于增强域内数据的&lt;strong&gt;大规模预训练&lt;/strong&gt;，已成为缩小智能体与人类性能差距的关键；且域内预训练的多模态 Transformer，被证实比从 VLMs（如 &lt;strong&gt;Oscar&lt;/strong&gt;、&lt;strong&gt;LXMERT&lt;/strong&gt;）初始化的多模态 Transformer 更有效&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Human Model: Interpreting and Communicating with Humans&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;human-model-interpreting-and-communicating-with-humans&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#human-model-interpreting-and-communicating-with-humans&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;除学习和建模世界外，VLN 智能体还需一个 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; —— 该模型能根据具体场景理解人类提供的自然语言指令，从而完成导航任务。这一过程主要面临两大挑战：一是解决&lt;strong&gt;指令的模糊性&lt;/strong&gt;，二是实现 &lt;strong&gt;&amp;ldquo;接地指令&amp;rdquo;&lt;/strong&gt; 在不同视觉环境中的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;模糊指令&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;模糊指令&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a8%a1%e7%b3%8a%e6%8c%87%e4%bb%a4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;模糊指令&lt;/strong&gt;主要出现在&lt;strong&gt;单轮导航场景&lt;/strong&gt;中：智能体仅遵循初始指令执行任务，无法通过进一步人类交互获取澄清。这类指令缺乏灵活性，难以训练智能体根据动态环境调整自身的&lt;strong&gt;语言理解&lt;/strong&gt;与&lt;strong&gt;视觉感知能力&lt;/strong&gt;。例如，指令中可能包含 &lt;strong&gt;&amp;ldquo;当前视角不可见的地标&amp;rdquo;&lt;/strong&gt;，或 &lt;strong&gt;&amp;ldquo;从多个视角观察均难以区分的地标&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在基础模型应用于 VLN 之前，模糊指令问题几乎未得到有效解决。尽管 &lt;strong&gt;LEO 模型&lt;/strong&gt;尝试通过整合 &lt;strong&gt;&amp;ldquo;从不同视角描述同一轨迹的多条指令&amp;rdquo;&lt;/strong&gt; 来缓解该问题，但仍依赖人工标注的指令。而基础模型所具备的 &lt;strong&gt;&amp;ldquo;全面感知上下文&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;常识知识&amp;rdquo;&lt;/strong&gt;，使智能体既能利用外部知识解读模糊指令，也能向其他 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; 寻求协助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CLIP&lt;/strong&gt; 等大规模跨模态预训练模型具备&lt;strong&gt;视觉语义与文本的匹配能力&lt;/strong&gt;，这使得 VLN 智能体可利用 &lt;strong&gt;&amp;ldquo;当前感知到的视觉物体及其状态&amp;rdquo;&lt;/strong&gt; 来解决指令模糊性问题，在单轮导航场景中尤为有效。具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Trans 模型&lt;/strong&gt;：通过 CLIP 提取 &lt;strong&gt;&amp;ldquo;可见且具有辨识度的物体&amp;rdquo;&lt;/strong&gt;，构建易于遵循的子指令；并预训练一个 &lt;strong&gt;&amp;ldquo;转换器&amp;rdquo;&lt;/strong&gt;（Translator），将原始模糊指令转换为易于理解的子指令表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LANA+ 模型&lt;/strong&gt;：利用 CLIP，以视觉全景观测为输入，查询 &lt;strong&gt;&amp;ldquo;地标语义标签文本列表&amp;rdquo;&lt;/strong&gt;，并选取排名靠前的检索文本线索作为 &lt;strong&gt;&amp;ldquo;待跟随显著地标的表征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;KERM 模型&lt;/strong&gt;：提出一种 &lt;strong&gt;&amp;ldquo;知识增强推理模型&amp;rdquo;&lt;/strong&gt;，可检索 &lt;strong&gt;&amp;ldquo;以语言描述形式存储的导航视角相关知识事实&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavHint 方法&lt;/strong&gt;：构建一个提示数据集，提供详细的视觉描述，帮助 VLN 智能体全面理解视觉环境，而非仅聚焦于指令中提及的物体。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另一方面，&lt;strong&gt;LLM&lt;/strong&gt; 的&lt;strong&gt;常识推理能力&lt;/strong&gt;可用于 &lt;strong&gt;&amp;ldquo;澄清或修正指令中的模糊地标&amp;rdquo;&lt;/strong&gt;，并将指令拆解为可执行步骤。例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 LLM 提供 &lt;strong&gt;&amp;ldquo;开放世界中地标共现的常识&amp;rdquo;&lt;/strong&gt;，并结合 CLIP 实现地标探测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayCan 方法&lt;/strong&gt;：将指令拆解为 &lt;strong&gt;&amp;ldquo;预定义可执行动作的排序列表&amp;rdquo;&lt;/strong&gt;，并结合一个 &lt;strong&gt;&amp;ldquo;效用函数&amp;rdquo;&lt;/strong&gt; —— 该函数对当前场景中出现的物体赋予更高权重。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管可通过&lt;strong&gt;视觉感知&lt;/strong&gt;与&lt;strong&gt;场景上下文&lt;/strong&gt;解决模糊指令问题，但更直接的方法是向 &lt;strong&gt;&amp;ldquo;通信伙伴&amp;rdquo;&lt;/strong&gt;（即生成指令的人类）寻求帮助。这类研究主要面临三大核心挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断 &lt;strong&gt;&amp;ldquo;何时请求帮助&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;生成 &lt;strong&gt;&amp;ldquo;信息寻求问题&amp;rdquo;&lt;/strong&gt;（如询问下一步动作、物体位置、方向等）&lt;/li&gt;
&lt;li&gt;设计 &lt;strong&gt;&amp;ldquo;信息提供方&amp;rdquo;&lt;/strong&gt;（oracle）—— 可为真实人类、规则与模板或神经模型&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 与 &lt;strong&gt;VLM&lt;/strong&gt; 在该框架中可承担两种角色：一是 &lt;strong&gt;&amp;ldquo;信息寻求模型&amp;rdquo;&lt;/strong&gt;，二是 &lt;strong&gt;&amp;ldquo;人类助手的代理&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;信息提供模型&amp;rdquo;&lt;/strong&gt;。已有初步研究探索将 LLM 用作信息寻求模型，解决 &lt;strong&gt;&amp;ldquo;何时问&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;问什么&amp;rdquo;&lt;/strong&gt; 的问题 —— 这需借助 &lt;strong&gt;&amp;ldquo;保形预测&amp;rdquo;&lt;/strong&gt;（conformal prediction, &lt;strong&gt;CP&lt;/strong&gt;）或 &lt;strong&gt;&amp;ldquo;上下文学习&amp;rdquo;&lt;/strong&gt;（in-context learning, &lt;strong&gt;ICL&lt;/strong&gt;）等技术实现。&lt;/p&gt;
&lt;p&gt;对于 &lt;strong&gt;&amp;ldquo;信息提供&amp;rdquo;&lt;/strong&gt; 角色，基础模型需扮演 &lt;strong&gt;&amp;ldquo;掌握信息提供方专属信息的助手&amp;rdquo;&lt;/strong&gt; —— 例如知晓目标位置、环境地图等任务执行者无法获取的信息。近期相关研究包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Copilot 方法&lt;/strong&gt;：使智能体在遇到困惑时主动寻求协助，其中 LLM 扮演 &lt;strong&gt;&amp;ldquo;副驾驶&amp;rdquo;&lt;/strong&gt; 角色，为导航提供支持。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;GPT-3&lt;/strong&gt; 可逐步拆解训练数据中的真实响应，这有助于利用预训练的 &lt;strong&gt;SwinBert&lt;/strong&gt; 视频-语言模型训练信息提供方模型；同时，&lt;strong&gt;mPLUG-Owl&lt;/strong&gt; 等大型视觉-语言模型可作为 &lt;strong&gt;&amp;ldquo;现成的强零样本信息提供方&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自驱动通信智能体&lt;/strong&gt;：通过学习 &lt;strong&gt;&amp;ldquo;信息提供方给出肯定答案的置信度&amp;rdquo;&lt;/strong&gt; 实现，可采用 &lt;strong&gt;&amp;ldquo;自我问答&amp;rdquo;&lt;/strong&gt; 模式，在推理阶段无需依赖信息提供方。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;接地指令的泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地指令的泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e6%8c%87%e4%bb%a4%e7%9a%84%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;导航数据在规模与多样性上的局限，是影响 VLN 智能体 &lt;strong&gt;&amp;ldquo;理解多样语言表达、有效遵循指令&amp;rdquo;&lt;/strong&gt; 的另一重要问题 —— 在未见过的导航环境中该问题尤为突出。尽管&lt;strong&gt;语言风格&lt;/strong&gt;本身在 &lt;strong&gt;&amp;ldquo;见过与未见过的环境&amp;rdquo;&lt;/strong&gt; 中具备良好泛化能力，但受限于训练指令的规模，&lt;strong&gt;&amp;ldquo;如何将指令与未见过的环境进行接地&amp;rdquo;&lt;/strong&gt; 仍是一项难题。基础模型通过 &lt;strong&gt;&amp;ldquo;预训练表征&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;指令生成数据增强&amp;rdquo;&lt;/strong&gt; 两种方式，为解决这些问题提供了支持。&lt;/p&gt;
&lt;p&gt;在基础模型出现前，多数研究依赖 &lt;strong&gt;LSTM&lt;/strong&gt; 等文本编码器表征文本指令。而基础模型通过&lt;strong&gt;预训练表征&lt;/strong&gt;，显著提升了 VLN 智能体的&lt;strong&gt;语言泛化能力&lt;/strong&gt;，具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRESS 方法&lt;/strong&gt;：对预训练语言模型 &lt;strong&gt;BERT&lt;/strong&gt; 进行微调，获得对 &lt;strong&gt;&amp;ldquo;未见过指令&amp;rdquo;&lt;/strong&gt; 泛化性更强的文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer&lt;/strong&gt;：为 &lt;strong&gt;VLN-BERT&lt;/strong&gt;、&lt;strong&gt;PREVALENT&lt;/strong&gt; 等方法提供支撑 —— 这些方法通过在 &lt;strong&gt;&amp;ldquo;从网络收集的大规模图文对&amp;rdquo;&lt;/strong&gt; 上预训练，获得更通用的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Airbert 模型&lt;/strong&gt;：训练一个类 &lt;strong&gt;ViLBERT&lt;/strong&gt; 架构，从 &lt;strong&gt;&amp;ldquo;互联网收集的图像-标题对&amp;rdquo;&lt;/strong&gt; 中学习文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLEAR 方法&lt;/strong&gt;：学习 &lt;strong&gt;&amp;ldquo;跨语言语言表征&amp;rdquo;&lt;/strong&gt;，捕捉指令背后的&lt;strong&gt;视觉概念&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProbES 方法&lt;/strong&gt;：通过采样轨迹实现&lt;strong&gt;环境自探索&lt;/strong&gt;，并利用 CLIP 检测到的 &lt;strong&gt;&amp;ldquo;动作与物体短语&amp;rdquo;&lt;/strong&gt; 填充指令模板，自动构建对应指令；同时借助 &lt;strong&gt;&amp;ldquo;基于提示的学习&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;语言嵌入&lt;/strong&gt;的快速适配。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT-2 模型&lt;/strong&gt;：探索利用 &lt;strong&gt;&amp;ldquo;预训练 VLMs&amp;rdquo;&lt;/strong&gt;（如结合 &lt;strong&gt;Flan-T5&lt;/strong&gt; 或 &lt;strong&gt;Vicuna&lt;/strong&gt; 的 &lt;strong&gt;InstructBLIP&lt;/strong&gt;）的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;，提升&lt;strong&gt;导航策略学习&lt;/strong&gt;与&lt;strong&gt;导航推理能力&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;提升智能体泛化能力的另一方法是 &lt;strong&gt;&amp;ldquo;合成更多指令&amp;rdquo;&lt;/strong&gt;。相关研究可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）离线指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;早期研究采用 &lt;strong&gt;&amp;ldquo;说话者-跟随者（Speaker-Follower）框架&amp;rdquo;&lt;/strong&gt;：利用人工标注的 &lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 训练一个 &lt;strong&gt;&amp;ldquo;离线说话者（指令生成器）&amp;rdquo;&lt;/strong&gt;，再让其基于 &lt;strong&gt;&amp;ldquo;给定轨迹上的全景序列&amp;rdquo;&lt;/strong&gt; 生成新指令。但发现这类方法生成的指令质量较低，在人类寻路评估中表现不佳。&lt;/p&gt;
&lt;p&gt;后续改进方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Marky 模型&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;多语言 T5 模型的多模态扩展版本&amp;rdquo;&lt;/strong&gt;，结合 &lt;strong&gt;&amp;ldquo;文本对齐的视觉地标对应关系&amp;rdquo;&lt;/strong&gt;，在未见过环境的 R2R 风格路径上生成 &lt;strong&gt;&amp;ldquo;接近人类质量&amp;rdquo;&lt;/strong&gt; 的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PASTS 模型&lt;/strong&gt;：引入 &lt;strong&gt;&amp;ldquo;进度感知的时空 Transformer 说话者&amp;rdquo;&lt;/strong&gt;，更好地利用 &lt;strong&gt;&amp;ldquo;有序的多视觉与动作特征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SAS 方法&lt;/strong&gt;：利用环境的 &lt;strong&gt;&amp;ldquo;语义与结构线索&amp;rdquo;&lt;/strong&gt;，生成包含丰富&lt;strong&gt;空间信息&lt;/strong&gt;的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SRDF 方法&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;迭代自训练&amp;rdquo;&lt;/strong&gt; 构建一个性能强劲的指令生成器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）导航中实时指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;部分近期研究不再训练离线指令生成器，而是在&lt;strong&gt;导航过程中实时生成指令&lt;/strong&gt;。例如，&lt;strong&gt;LANA 模型&lt;/strong&gt;提出一种 &lt;strong&gt;&amp;ldquo;具备语言能力的导航智能体&amp;rdquo;&lt;/strong&gt; —— 该智能体不仅能执行导航指令，还可生成路线描述。&lt;/p&gt;
&lt;h4&gt;VLN Agent: Learning an Embodied Agent for Reasoning and Planning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管&lt;strong&gt;世界模型&lt;/strong&gt;与&lt;strong&gt;人类模型&lt;/strong&gt;为智能体赋予了&lt;strong&gt;视觉与语言理解能力&lt;/strong&gt;，但 VLN 智能体仍需培养&lt;strong&gt;具身推理&lt;/strong&gt;（embodied reasoning）与&lt;strong&gt;规划能力&lt;/strong&gt;，以支撑自身决策。从这一角度出发，我们将探讨两大挑战：&lt;strong&gt;接地与推理&lt;/strong&gt;、&lt;strong&gt;规划&lt;/strong&gt;；同时还将研究 &lt;strong&gt;&amp;ldquo;直接以基础模型作为 VLN 智能体核心骨干&amp;rdquo;&lt;/strong&gt; 的方法。&lt;/p&gt;
&lt;h5&gt;接地与推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地与推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e4%b8%8e%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;视觉-语言领域的其他任务（如&lt;strong&gt;视觉问答&lt;/strong&gt;（&lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;图像描述生成&lt;/strong&gt;）主要聚焦于 &lt;strong&gt;&amp;ldquo;图像与对应文本描述之间的静态对齐&amp;rdquo;&lt;/strong&gt;，而 VLN 智能体则需基于自身动作，对 &lt;strong&gt;&amp;ldquo;指令与环境中的时空动态信息&amp;rdquo;&lt;/strong&gt; 进行推理。具体而言，智能体需考虑&lt;strong&gt;过往动作&lt;/strong&gt;、识别&lt;strong&gt;待执行的子指令片段&lt;/strong&gt;，并将文本与视觉环境进行&lt;strong&gt;接地&lt;/strong&gt;（grounding），从而执行相应动作。&lt;/p&gt;
&lt;p&gt;传统方法主要通过 &lt;strong&gt;&amp;ldquo;显式语义建模&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;辅助任务设计&amp;rdquo;&lt;/strong&gt; 获取上述能力；但随着基础模型的兴起，&lt;strong&gt;&amp;ldquo;基于特定设计任务的预训练&amp;rdquo;&lt;/strong&gt; 已成为主流方案。&lt;/p&gt;
&lt;p&gt;传统研究通过 &lt;strong&gt;&amp;ldquo;视觉与语言模态的显式语义建模&amp;rdquo;&lt;/strong&gt; 提升智能体的&lt;strong&gt;显式接地能力&lt;/strong&gt;，具体方向包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;建模动作与地标&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;利用指令中的句法信息&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;建模空间关系&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前，&lt;strong&gt;&amp;ldquo;基于基础模型实现 VLN 智能体显式接地&amp;rdquo;&lt;/strong&gt; 的研究仍较少。例如，提出 &lt;strong&gt;&amp;ldquo;动作原子概念学习&amp;rdquo;&lt;/strong&gt;，并将视觉观测映射为&lt;strong&gt;多模态对齐特征&lt;/strong&gt;，以辅助接地。&lt;/p&gt;
&lt;p&gt;除显式语义建模外，传统研究还通过 &lt;strong&gt;&amp;ldquo;辅助推理任务&amp;rdquo;&lt;/strong&gt; 提升智能体的接地能力。但在基于基础模型的 VLN 智能体中，这类方法较少被探索 —— 因为基础模型的预训练过程已使其在导航前就具备了对 &lt;strong&gt;&amp;ldquo;时空语义&amp;rdquo;&lt;/strong&gt; 的通用理解。&lt;/p&gt;
&lt;p&gt;现有研究通过设计&lt;strong&gt;特定预训练任务&lt;/strong&gt;，进一步提升智能体的接地能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;设计专门针对 &lt;strong&gt;&amp;ldquo;场景与物体接地&amp;rdquo;&lt;/strong&gt; 的预训练任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOViS&lt;/strong&gt;：提出两项专项预训练任务，分别增强智能体的 &lt;strong&gt;&amp;ldquo;方向感知&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;视觉信息理解&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HOP&lt;/strong&gt;：提出 &lt;strong&gt;&amp;ldquo;历史与顺序感知预训练范式&amp;rdquo;&lt;/strong&gt;，重点强调&lt;strong&gt;历史信息&lt;/strong&gt;与&lt;strong&gt;轨迹顺序&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;&amp;ldquo;增强智能体的未来视角语义预测能力&amp;rdquo;&lt;/strong&gt; 有助于提升其在&lt;strong&gt;长路径导航&lt;/strong&gt;中的性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设计 &lt;strong&gt;&amp;ldquo;掩码路径建模目标&amp;rdquo;&lt;/strong&gt; —— 给定随机掩码的子路径，重建原始完整路径&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;提出 &lt;strong&gt;&amp;ldquo;实体感知预训练&amp;rdquo;&lt;/strong&gt;，通过预测&lt;strong&gt;接地实体&lt;/strong&gt;并将其与文本对齐实现接地能力提升&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;动态规划&lt;/strong&gt;能让 VLN 智能体实时适应环境变化、优化导航策略。目前，规划方法主要分为两类：一类是 &lt;strong&gt;&amp;ldquo;利用全局图信息增强局部动作空间&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;图基规划器&lt;/strong&gt;；另一类是随基础模型（尤其是 &lt;strong&gt;LLM&lt;/strong&gt;）兴起的 &lt;strong&gt;LLM 基规划器&lt;/strong&gt; —— 这类规划器借助 LLM 的&lt;strong&gt;海量常识&lt;/strong&gt;与&lt;strong&gt;先进推理能力&lt;/strong&gt;，生成动态规划方案，提升决策效果。&lt;/p&gt;
&lt;p&gt;近期 VLN 研究的核心方向之一，是通过 &lt;strong&gt;&amp;ldquo;全局图信息&amp;rdquo;&lt;/strong&gt; 增强导航智能体的规划能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;已访问节点的图边界&amp;rdquo;&lt;/strong&gt; 中的&lt;strong&gt;全局动作步骤&lt;/strong&gt;，增强&lt;strong&gt;局部导航动作空间&lt;/strong&gt;，以实现更优&lt;strong&gt;全局规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过 &lt;strong&gt;&amp;ldquo;高层规划（区域选择）+ 低层规划（节点选择）&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;分层策略&lt;/strong&gt;，进一步优化导航决策&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;&amp;ldquo;基于图边界的全局与局部动作空间&amp;rdquo;&lt;/strong&gt; 中融入 &lt;strong&gt;&amp;ldquo;网格级动作&amp;rdquo;&lt;/strong&gt;，提升&lt;strong&gt;动作预测精度&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在&lt;strong&gt;连续环境&lt;/strong&gt;中，规划方法进一步演进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;采用&lt;strong&gt;分层规划思路&lt;/strong&gt; —— 通过 &lt;strong&gt;&amp;ldquo;从预测的局部可导航性图中选择局部航点&amp;rdquo;&lt;/strong&gt;，用&lt;strong&gt;高层动作空间&lt;/strong&gt;替代&lt;strong&gt;低层动作空间&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CM2&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;在局部地图中实现指令接地&amp;rdquo;&lt;/strong&gt;，辅助&lt;strong&gt;轨迹规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拓展上述策略，构建&lt;strong&gt;全局拓扑图&lt;/strong&gt;或&lt;strong&gt;网格图&lt;/strong&gt;，支持 &lt;strong&gt;&amp;ldquo;基于地图的全局规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;视频预测模型&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;神经辐射表征模型&amp;rdquo;&lt;/strong&gt; 预测多个&lt;strong&gt;未来航点&lt;/strong&gt;，并基于 &lt;strong&gt;&amp;ldquo;预测候选航点的长期影响&amp;rdquo;&lt;/strong&gt; 规划最优动作&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与此同时，部分研究借助 LLM 的&lt;strong&gt;常识知识&lt;/strong&gt;生成 &lt;strong&gt;&amp;ldquo;基于文本的规划方案&amp;rdquo;&lt;/strong&gt;，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-Planner&lt;/strong&gt;：生成由 &lt;strong&gt;&amp;ldquo;子目标&amp;rdquo;&lt;/strong&gt; 构成的详细规划，并根据&lt;strong&gt;预定义程序模式&lt;/strong&gt;整合检测到的物体，实时动态调整规划&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mic&lt;/strong&gt; 与 &lt;strong&gt;A²Nav&lt;/strong&gt;：专注于将导航任务拆解为详细文本指令 —— Mic 从&lt;strong&gt;静态与动态双视角&lt;/strong&gt;生成分步规划，A²Nav 则利用 &lt;strong&gt;GPT-3&lt;/strong&gt; 将指令解析为可执行子任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ThinkBot&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;思维链推理&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought Reasoning），生成 &lt;strong&gt;&amp;ldquo;与交互物体相关的缺失动作&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VL-Map&lt;/strong&gt;：基于 &lt;strong&gt;&amp;ldquo;代码化 LLM&amp;rdquo;&lt;/strong&gt;（遵循 &lt;strong&gt;Code-as-Policy&lt;/strong&gt; 框架），将导航指令拆解为 &lt;strong&gt;&amp;ldquo;代码格式的时序化目标相关函数&amp;rdquo;&lt;/strong&gt;，并利用 &lt;strong&gt;&amp;ldquo;动态构建的可查询地图&amp;rdquo;&lt;/strong&gt; 指导目标执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayNav&lt;/strong&gt;：构建 &lt;strong&gt;&amp;ldquo;已探索环境的 3D 场景图&amp;rdquo;&lt;/strong&gt;，将其作为 LLM 输入，为导航器生成 &lt;strong&gt;&amp;ldquo;可行且符合上下文的高层规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;作为 VLN 智能体的基础模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;作为-vln-智能体的基础模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bd%9c%e4%b8%ba-vln-%e6%99%ba%e8%83%bd%e4%bd%93%e7%9a%84%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;主流方案以 &lt;strong&gt;&amp;ldquo;单流 VL 模型&amp;rdquo;&lt;/strong&gt; 作为 VLN 智能体的核心结构：这类模型在每个时间步同时处理 &lt;strong&gt;&amp;ldquo;语言、视觉、历史令牌（token）&amp;rdquo;&lt;/strong&gt; 输入，通过对&lt;strong&gt;跨模态令牌&lt;/strong&gt;的自注意力运算捕捉 &lt;strong&gt;&amp;ldquo;文本-视觉对应关系&amp;rdquo;&lt;/strong&gt;，进而推断动作概率。&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;零样本 VLN&lt;/strong&gt; 场景中，&lt;strong&gt;CLIP-NAV&lt;/strong&gt; 利用 CLIP 获取 &lt;strong&gt;&amp;ldquo;描述目标物体的自然语言指称表达式&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;序贯导航决策&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;此外，&lt;strong&gt;VLN-CE&lt;/strong&gt;（连续环境 VLN）智能体与 &lt;strong&gt;VLN-DE&lt;/strong&gt;（离散环境 VLN）智能体的核心差异在于&lt;strong&gt;动作空间&lt;/strong&gt;：前者在连续环境中执行&lt;strong&gt;低层控制&lt;/strong&gt;，而非后者 &lt;strong&gt;&amp;ldquo;基于图的高层视角选择动作&amp;rdquo;&lt;/strong&gt;。尽管早期研究采用 &lt;strong&gt;LSTM&lt;/strong&gt; 推断低层动作，但 &lt;strong&gt;&amp;ldquo;航点预测器&amp;rdquo;&lt;/strong&gt;（waypoint predictor）的引入实现了 &lt;strong&gt;&amp;ldquo;从 DE 到 CE 的方法迁移&amp;rdquo;&lt;/strong&gt; —— 所有这些方法均通过航点预测器获取 &lt;strong&gt;&amp;ldquo;局部可导航性图&amp;rdquo;&lt;/strong&gt;，使 DE 场景中的基础模型能适配连续环境。具体而言，航点检测过程主要通过 &lt;strong&gt;&amp;ldquo;视觉观测&amp;rdquo;&lt;/strong&gt;（如全景 RGBD 图像），从智能体当前位置预测 &lt;strong&gt;&amp;ldquo;可导航的相邻候选航点&amp;rdquo;&lt;/strong&gt; 作为潜在目标，再由智能体选择其一作为当前目的地。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 具备强大的&lt;strong&gt;推理能力&lt;/strong&gt;与&lt;strong&gt;世界语义抽象能力&lt;/strong&gt;，且在 &lt;strong&gt;&amp;ldquo;未知大规模环境&amp;rdquo;&lt;/strong&gt; 中表现出优异的&lt;strong&gt;泛化性&lt;/strong&gt; —— 因此，近期 VLN 研究开始直接将 LLM 作为智能体执行导航任务。其核心流程为：将&lt;strong&gt;视觉观测&lt;/strong&gt;转换为&lt;strong&gt;文本描述&lt;/strong&gt;，与指令一同输入 LLM，由 LLM 完成&lt;strong&gt;动作预测&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;代表性创新方案包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT&lt;/strong&gt; 与 &lt;strong&gt;MapGPT&lt;/strong&gt;：验证了&lt;strong&gt;零样本导航&lt;/strong&gt;的可行性 —— NavGPT 利用 &lt;strong&gt;GPT-4&lt;/strong&gt; 自主生成动作，MapGPT 将&lt;strong&gt;拓扑图&lt;/strong&gt;转换为&lt;strong&gt;全局探索提示&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiscussNav&lt;/strong&gt;：拓展上述思路，部署 &lt;strong&gt;&amp;ldquo;多领域专用 VLN 专家&amp;rdquo;&lt;/strong&gt;（包括&lt;strong&gt;指令分析专家&lt;/strong&gt;、&lt;strong&gt;视觉感知专家&lt;/strong&gt;、&lt;strong&gt;完成度估计专家&lt;/strong&gt;、&lt;strong&gt;决策测试专家&lt;/strong&gt;），减少导航任务中的人工参与：通过将任务分配给专用智能体，减轻单一模型负担，实现 &lt;strong&gt;&amp;ldquo;任务专属优化处理&amp;rdquo;&lt;/strong&gt;，并借助&lt;strong&gt;多大型模型的协同优势&lt;/strong&gt;提升&lt;strong&gt;鲁棒性&lt;/strong&gt;、&lt;strong&gt;透明度&lt;/strong&gt;与&lt;strong&gt;整体性能&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MC-GPT&lt;/strong&gt;：利用 &lt;strong&gt;&amp;ldquo;记忆拓扑图&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;人类导航示例&amp;rdquo;&lt;/strong&gt; 丰富&lt;strong&gt;策略多样性&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;InstructNav&lt;/strong&gt;：将导航拆解为&lt;strong&gt;子任务&lt;/strong&gt;，并结合 &lt;strong&gt;&amp;ldquo;多源价值图&amp;rdquo;&lt;/strong&gt; 实现高效执行&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与 &lt;strong&gt;&amp;ldquo;零样本使用&amp;rdquo;&lt;/strong&gt; 不同，部分研究通过 &lt;strong&gt;&amp;ldquo;微调 LLM&amp;rdquo;&lt;/strong&gt;，使其能更有效地处理&lt;strong&gt;具身导航任务&lt;/strong&gt;。另有研究融入 &lt;strong&gt;&amp;ldquo;思维链&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought, &lt;strong&gt;CoT&lt;/strong&gt;）推理机制提升推理过程，例如 &lt;strong&gt;Nav-CoT&lt;/strong&gt; 将 LLM 转化为 &lt;strong&gt;&amp;ldquo;世界模型与导航推理智能体&amp;rdquo;&lt;/strong&gt;，通过模拟未来环境简化决策 —— 这一方案证实了 &lt;strong&gt;&amp;ldquo;微调语言模型&amp;rdquo;&lt;/strong&gt; 在仿真与真实场景中的&lt;strong&gt;灵活性&lt;/strong&gt;与&lt;strong&gt;实用潜力&lt;/strong&gt;，较传统应用实现了显著突破。&lt;/p&gt;
&lt;h3&gt;挑战与未来方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;挑战与未来方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8c%91%e6%88%98%e4%b8%8e%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;尽管基础模型为&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（&lt;strong&gt;VLN&lt;/strong&gt;）提供了创新性解决方案，但仍有若干局限尚未得到充分探索，同时新的挑战也随之出现。在本节中，我们将从&lt;strong&gt;基准数据集&lt;/strong&gt;、&lt;strong&gt;世界模型&lt;/strong&gt;、&lt;strong&gt;人类模型&lt;/strong&gt;、&lt;strong&gt;智能体模型&lt;/strong&gt;及&lt;strong&gt;真实机器人部署&lt;/strong&gt;五个维度，梳理 VLN 领域的挑战与未来研究方向。&lt;/p&gt;
&lt;h4&gt;基准数据集：数据与任务的局限&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基准数据集数据与任务的局限&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e5%87%86%e6%95%b0%e6%8d%ae%e9%9b%86%e6%95%b0%e6%8d%ae%e4%b8%8e%e4%bb%bb%e5%8a%a1%e7%9a%84%e5%b1%80%e9%99%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;当前 &lt;strong&gt;VLN 数据集&lt;/strong&gt;在&lt;strong&gt;质量&lt;/strong&gt;、&lt;strong&gt;多样性&lt;/strong&gt;、&lt;strong&gt;偏差&lt;/strong&gt;及&lt;strong&gt;可扩展性&lt;/strong&gt;方面存在明显局限。例如，在 &lt;strong&gt;R2R&lt;/strong&gt; 数据集中，&lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 偏向于&lt;strong&gt;最短路径&lt;/strong&gt;，无法准确反映现实世界的导航场景。下文将探讨 VLN 基准数据集的改进趋势与建议方向：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;统一且贴近现实的任务与平台&lt;/strong&gt;：构建可靠的基准数据集并确保结果可复现，是评估真实场景下 VLN 性能的关键。现实世界的复杂性要求基准数据集需全面覆盖各类导航挑战，因此需要一个通用的 &lt;strong&gt;&amp;ldquo;仿真到现实&amp;rdquo;&lt;/strong&gt; 评估平台（如 &lt;strong&gt;OVMM&lt;/strong&gt;），以实现仿真与真实场景下的标准化测试。此外，任务与活动设计需贴近现实且源于人类需求，例如 &lt;strong&gt;BEHAVIOR-1K&lt;/strong&gt; 基准数据集，在虚拟、交互式且具生态性的环境中构建&lt;strong&gt;日常家庭活动场景&lt;/strong&gt;，以满足对 &lt;strong&gt;&amp;ldquo;多样性&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;真实性&amp;rdquo;&lt;/strong&gt; 的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;动态环境&lt;/strong&gt;：现实世界环境本质上具有&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;动态性&lt;/strong&gt; —— &lt;strong&gt;移动物体&lt;/strong&gt;、&lt;strong&gt;行人&lt;/strong&gt;，以及&lt;strong&gt;光照&lt;/strong&gt;、&lt;strong&gt;天气&lt;/strong&gt;等环境变化，均可能引发&lt;strong&gt;突发情况&lt;/strong&gt;。这些因素会干扰导航系统的&lt;strong&gt;视觉感知&lt;/strong&gt;，使其难以维持稳定性能。近期部分研究（如 &lt;strong&gt;HAZARD&lt;/strong&gt;、&lt;strong&gt;Habitat 3.0&lt;/strong&gt;、&lt;strong&gt;HA-VLN&lt;/strong&gt;）已开始关注&lt;strong&gt;动态环境&lt;/strong&gt;，为后续研究提供了良好起点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从室内到室外&lt;/strong&gt;：适用于室外环境的 VLN 智能体（如&lt;strong&gt;自动驾驶车辆&lt;/strong&gt;、&lt;strong&gt;无人机&lt;/strong&gt;）正逐渐获得更多关注，相关&lt;strong&gt;语言引导数据集&lt;/strong&gt;也已陆续开发。早期研究尝试将 LLM 融入&lt;strong&gt;室外 VLN 任务&lt;/strong&gt;，具体方式包括&lt;strong&gt;提示工程&lt;/strong&gt;，或通过&lt;strong&gt;微调 LLM&lt;/strong&gt; 实现 &lt;strong&gt;&amp;ldquo;预测下一步动作&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;规划未来轨迹&amp;rdquo;&lt;/strong&gt;。为使现成的 VLN 模型适配&lt;strong&gt;室外导航场景&lt;/strong&gt;，研究者利用&lt;strong&gt;真实驾驶视频&lt;/strong&gt;、&lt;strong&gt;仿真驾驶数据&lt;/strong&gt;或两者结合进行&lt;strong&gt;指令微调&lt;/strong&gt;，使基础模型能够学习预测未来的&lt;strong&gt;油门与转向角度&lt;/strong&gt;。此外，研究者还在基于基础模型的驾驶智能体中集成了额外的&lt;strong&gt;推理与规划模块&lt;/strong&gt;。关于室外 VLN 的详细综述，建议读者参考相关综述文献与立场论文。&lt;/p&gt;
&lt;h4&gt;世界模型：从二维（2D）到三维（3D）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;世界模型从二维2d到三维3d&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%96%e7%95%8c%e6%a8%a1%e5%9e%8b%e4%bb%8e%e4%ba%8c%e7%bb%b42d%e5%88%b0%e4%b8%89%e7%bb%b43d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;构建有效的&lt;strong&gt;世界表征&lt;/strong&gt;是&lt;strong&gt;具身感知&lt;/strong&gt;、&lt;strong&gt;推理&lt;/strong&gt;与&lt;strong&gt;规划&lt;/strong&gt;领域的核心研究主题。VLN 本质上是一项 &lt;strong&gt;3D 任务&lt;/strong&gt; —— 智能体需以 3D 形式感知真实世界环境。尽管当前研究已能通过强大的通用 &lt;strong&gt;2D 表征&lt;/strong&gt;描述世界，但这类表征无法充分支持 3D 场景下的&lt;strong&gt;空间语言理解&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;以往研究已提出多种显式 &lt;strong&gt;3D 表征方式&lt;/strong&gt;，包括各类&lt;strong&gt;语义同步定位与地图构建&lt;/strong&gt;（semantic &lt;strong&gt;SLAM&lt;/strong&gt;）、&lt;strong&gt;体素表征&lt;/strong&gt;、&lt;strong&gt;深度信息&lt;/strong&gt;、&lt;strong&gt;鸟瞰图&lt;/strong&gt;（Bird&amp;rsquo;s-Eye-View）表征（如&lt;strong&gt;网格图&lt;/strong&gt;）及&lt;strong&gt;局部度量图&lt;/strong&gt;。但这些表征存在局限：它们将物体集合限定为 &lt;strong&gt;&amp;ldquo;封闭集合&amp;rdquo;&lt;/strong&gt;，无法适配自然语言对应的 &lt;strong&gt;&amp;ldquo;开放词汇场景&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;部分研究尝试构建 &lt;strong&gt;&amp;ldquo;可查询的地图/场景表征&amp;rdquo;&lt;/strong&gt;，例如将 CLIP 提取的&lt;strong&gt;多视角图像特征&lt;/strong&gt;整合到 &lt;strong&gt;3D 体素网格&lt;/strong&gt;或&lt;strong&gt;俯视特征图&lt;/strong&gt;中，或利用&lt;strong&gt;场景图&lt;/strong&gt;表征&lt;strong&gt;空间关系&lt;/strong&gt;。然而，&lt;strong&gt;&amp;ldquo;如何将大规模数据中学习到的 3D 表征适配于 VLN 智能体，以提升其 3D 环境感知能力&amp;rdquo;&lt;/strong&gt; 仍是待探索的问题。近期兴起的 &lt;strong&gt;3D 基础模型&lt;/strong&gt; —— 包括 &lt;strong&gt;3D 重建模型&lt;/strong&gt; 与 &lt;strong&gt;3D 多模态表征模型&lt;/strong&gt; —— 有望为 VLN 领域提供关键支撑。&lt;/p&gt;
&lt;h4&gt;人类模型：从指令到对话&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;人类模型从指令到对话&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%ba%e7%b1%bb%e6%a8%a1%e5%9e%8b%e4%bb%8e%e6%8c%87%e4%bb%a4%e5%88%b0%e5%af%b9%e8%af%9d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;以往研究多采用 &lt;strong&gt;&amp;ldquo;说话者-倾听者范式&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;受限问答对话&amp;rdquo;&lt;/strong&gt; —— 这类方式仅允许智能体主动请求帮助。近年来，涌现出一批以 &lt;strong&gt;&amp;ldquo;开放式对话指令&amp;rdquo;&lt;/strong&gt; 为核心的新基准数据集，支持智能体在模糊或困惑场景下进行完全&lt;strong&gt;自由形式的通信&lt;/strong&gt;，包括&lt;strong&gt;提问&lt;/strong&gt;、&lt;strong&gt;提议&lt;/strong&gt;、&lt;strong&gt;解释&lt;/strong&gt;、&lt;strong&gt;建议&lt;/strong&gt;、&lt;strong&gt;澄清&lt;/strong&gt;与&lt;strong&gt;协商&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;然而，当前方法仍依赖 &lt;strong&gt;&amp;ldquo;基于规则的对话模板&amp;rdquo;&lt;/strong&gt; 应对上述复杂场景，即便部分方法包含基础模型组件，也未充分发挥其能力。通过 &lt;strong&gt;&amp;ldquo;人类对话数据 + 仿真导航视频&amp;rdquo;&lt;/strong&gt; 对&lt;strong&gt;视频-语言模型&lt;/strong&gt;进行&lt;strong&gt;对话调优&lt;/strong&gt;，使模型在导航过程中具备更强的&lt;strong&gt;对话生成能力&lt;/strong&gt;。未来研究需重点关注两方面：一是将基础模型融入 &lt;strong&gt;&amp;ldquo;情境化任务导向对话管理&amp;rdquo;&lt;/strong&gt;；二是探索现有基础模型在 &lt;strong&gt;&amp;ldquo;任务导向对话&amp;rdquo;&lt;/strong&gt; 中的应用潜力。&lt;/p&gt;
&lt;h4&gt;智能体模型：基础模型在 VLN 中的适配&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;智能体模型基础模型在-vln-中的适配&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%99%ba%e8%83%bd%e4%bd%93%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%9c%a8-vln-%e4%b8%ad%e7%9a%84%e9%80%82%e9%85%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管基础模型具有强大的&lt;strong&gt;泛化能力&lt;/strong&gt;，但将其融入导航任务仍面临挑战：&lt;strong&gt;LLM&lt;/strong&gt; 本质上缺乏对真实环境的&lt;strong&gt;视觉感知能力&lt;/strong&gt;，且易产生 &lt;strong&gt;&amp;ldquo;幻觉&amp;rdquo;&lt;/strong&gt;；下文还将探讨 LLM 在规划与推理方面的能力局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺乏具身经验&lt;/strong&gt;：这一局限可能导致 LLM 在任务规划与推理中仅依赖&lt;strong&gt;预设常识&lt;/strong&gt;，无法满足真实场景的特定需求。部分研究通过 &lt;strong&gt;&amp;ldquo;将视觉观测转换为文本描述，作为 LLM 的提示&amp;rdquo;&lt;/strong&gt; 解决该问题，但这种方式可能丢失关键&lt;strong&gt;视觉语义&lt;/strong&gt;。与 LLM 相比，&lt;strong&gt;VLM&lt;/strong&gt;（视觉-语言模型）智能体虽展现出 &lt;strong&gt;&amp;ldquo;感知视觉世界与规划&amp;rdquo;&lt;/strong&gt; 的潜力，但其训练数据主要源于互联网，缺乏&lt;strong&gt;具身经验&lt;/strong&gt;，需通过&lt;strong&gt;微调&lt;/strong&gt;实现稳健的智能体决策。未来需进一步研究 &lt;strong&gt;&amp;ldquo;如何将基础模型智能体中的常识知识迁移到具身场景中&amp;rdquo;&lt;/strong&gt;。近期提出的 &lt;strong&gt;&amp;ldquo;具身基础模型&amp;rdquo;&lt;/strong&gt;（如 &lt;strong&gt;EmbodieGPT&lt;/strong&gt;、&lt;strong&gt;PaLM-E&lt;/strong&gt;、&lt;strong&gt;Octopus&lt;/strong&gt;）为解决该问题提供了可行方向：这些模型通过在多类具身任务上微调基础模型，缩小智能体在 &lt;strong&gt;&amp;ldquo;视觉-语言-具身动作&amp;rdquo;&lt;/strong&gt; 理解上的差距，提升其基于多模态输入的理解与执行能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;幻觉问题&lt;/strong&gt;：LLM 与 VLM 可能生成 &lt;strong&gt;&amp;ldquo;不存在的物体&amp;rdquo;&lt;/strong&gt;，导致&lt;strong&gt;信息失真&lt;/strong&gt;。例如，LLM 在任务规划时可能生成 &lt;strong&gt;&amp;ldquo;向前走并在沙发处左转&amp;rdquo;&lt;/strong&gt; 的指令，即便房间内并无沙发。这种偏差可能导致智能体执行错误或无法完成的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM 在规划与推理中的能力局限&lt;/strong&gt;：已有文献针对 LLM 的&lt;strong&gt;零样本推理与规划能力&lt;/strong&gt;展开评估（尤其是结合 &lt;strong&gt;PlanBench&lt;/strong&gt; 与 &lt;strong&gt;CogEval&lt;/strong&gt;），结果表明 LLM 在&lt;strong&gt;复杂规划任务&lt;/strong&gt;中存在明显局限。这些研究在 &lt;strong&gt;&amp;ldquo;规划生成、最优性、稳健性、推理&amp;rdquo;&lt;/strong&gt; 等挑战性场景下评估 LLM，发现其不仅易产生幻觉，还可能无法理解复杂规划问题背后的&lt;strong&gt;关系结构&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在 VLN 场景中，由于室内环境固定且导航动作集合有限，&lt;strong&gt;动作空间&lt;/strong&gt;与&lt;strong&gt;规划需求&lt;/strong&gt;相对受限。这种 &lt;strong&gt;&amp;ldquo;有界场景&amp;rdquo;&lt;/strong&gt; 使 LLM 能够生成 &lt;strong&gt;&amp;ldquo;粗粒度方向的分步指令&amp;rdquo;&lt;/strong&gt; —— 已有研究证实该方式的有效性。需强调的是，在 VLN 任务中，LLM 并非主导整个规划过程，而是通过 &lt;strong&gt;&amp;ldquo;结构化拆解指令&amp;rdquo;&lt;/strong&gt; 提供辅助；智能体的实际决策仍主要依赖&lt;strong&gt;感知&lt;/strong&gt;、&lt;strong&gt;运动控制&lt;/strong&gt;等其他组件。因此，LLM 的规划功能更多是 &lt;strong&gt;&amp;ldquo;补充性指导&amp;rdquo;&lt;/strong&gt;，而非唯一决策依据。&lt;/p&gt;
&lt;h4&gt;部署：从仿真到真实机器人&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;部署从仿真到真实机器人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%83%a8%e7%bd%b2%e4%bb%8e%e4%bb%bf%e7%9c%9f%e5%88%b0%e7%9c%9f%e5%ae%9e%e6%9c%ba%e5%99%a8%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;仿真环境往往缺乏真实世界的&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;多样性&lt;/strong&gt;，且低质量渲染图像会进一步加剧这一问题。具体而言，当前部署面临三大瓶颈：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知差距&lt;/strong&gt;：仿真与真实场景的&lt;strong&gt;视觉差异&lt;/strong&gt;导致智能体性能与精度下降，因此需构建更稳健的感知系统。例如，尝试利用&lt;strong&gt;语义地图&lt;/strong&gt;与 &lt;strong&gt;3D 特征场&lt;/strong&gt;为单目机器人提供&lt;strong&gt;全景感知&lt;/strong&gt;，显著提升了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具身差距与数据稀缺&lt;/strong&gt;：仿真环境的&lt;strong&gt;物理规则&lt;/strong&gt;与真实机器人的&lt;strong&gt;具身特性&lt;/strong&gt;不匹配，且真实场景下 VLN 数据收集成本高、规模有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据规模化解决方案&lt;/strong&gt;：&lt;strong&gt;机器人远程操控&lt;/strong&gt;的兴起为解决数据稀缺提供了新思路 —— 通过人类远程控制机器人，可在真实人机交互场景中规模化收集 VLN 数据，为基础模型训练提供支撑。&lt;/p&gt;
&lt;h3&gt;仓库论文链接&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;仓库论文链接&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bb%93%e5%ba%93%e8%ae%ba%e6%96%87%e9%93%be%e6%8e%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：本表格按分类和子分类组织所有 VLN 相关论文，便于浏览和筛选。分类包括：Survey（综述）、World Model（世界模型）、Human Model（人类模型）、VLN Agent（VLN 智能体）、Behavior Analysis（行为分析）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;分类&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;子分类&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Survey&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.12667&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/eric-ai-lab/awesome-vision-language-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s10462-022-10174-9&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual language navigation: A survey and open challenges&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11544&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation: A Survey and Taxonomy&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;World Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.13451&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.03561&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.14158&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Volumetric Environment Representation for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/DefaultRui/VLN-VER&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2023/0204.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision Language Navigation with Knowledge-driven Environmental Dreamer&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;IJCAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/hekj/FDA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.19195&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.02960&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple and Effective Synthesis of Indoor 3D Scenes&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research/se3ds&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Navigational Visual Representations with Semantic Map Supervision&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.11984&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning vision-and-language navigation from youtube videos&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.12907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GridMM: Grid Memory Map for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/GridMM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.04385&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BEVBert: Multimodal Map Pre-training for Language-guided Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/VLN-BEVBert&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15644&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scaling Data Generation in Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/ScaleVLN/tree/main?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.03112&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clin1223/MTVM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.15685&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EnvEdit: Environment Editing for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960375.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.06383&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Much Can CLIP Benefit Vision-and-Language Tasks?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clip-vil/CLIP-ViL&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.11742&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/cshizhe/VLN-DUET&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.13309&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;History Aware Multimodal Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://cshizhe.github.io/projects/vln_hamt.html&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.08756&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pathdreamer: A World Model for Indoor Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Pashevich_Episodic_Transformer_for_Vision-and-Language_Navigation_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Episodic Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.09105&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbert: In-domain Pretraining for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://airbert-vln.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.07876&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation with Random Environmental Mixup&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LCFractal/VLNREM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Human Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene Map-based Prompt Tuning for Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.11142&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/NavRAG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.08467&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/VLN-SRDF&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.15087&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigation Instruction Generation with BEV Perception and Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/FanScy/BEVInstructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.07433&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Controllable Navigation Instruction Generation with Chain of Thought Prompting&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/refkxh/C-Instructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.acl-long.734.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/gmuraleekrishna/SAS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.18721&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Correctable Landmark Discovery via Large Models for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/CONSOLE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.02559&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavHint: Vision and Language Navigation Agent with a Hint Generator&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/NavHint&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10359152&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Follow and Generate Instructions for Language-Capable Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.08409&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lana: A Language-Capable Navigator for Instruction Following and Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wxh1996/LANA-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/xiangyangli-cn/KERM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.11918&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.00852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.09230&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Trans: Translator for the Vision and Language Navigation Agent&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/VLN-trans&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.04006&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/liangcici/Probes-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.14973&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Less is More: Generating Grounded Navigation Instructions from Landmarks&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research-datasets/RxR/tree/main/marky-mT5&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.10504&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Evaluation of Vision-and-Language Navigation Instructions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://say-can.github.io/assets/palm_saycan.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do As I Can, Not As I Say:Grounding Language in Robotic Affordances&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://say-can.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN Agent&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.05552&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/SAME&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.18800&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniVLN: Efficient Vision-and-Language Navigation byProgressive Knowledge Distillation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.06072&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.12587&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CSir1996/VLN-GELA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/chengaopro/AZHP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.04758&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bird&amp;rsquo;s-Eye-View Scene Graph for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14268&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Masked Path Modeling for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.04907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10006384&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2207.11201&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Target-Driven Structured Transformer Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YushengZhao/TD-STP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9880046&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/HOP-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.505.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;COLING&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/LOViS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.12944&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene-Intuitive Agent for Remote Embodied Visual Grounding&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.14143&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YuankaiQi/ORIST&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN BERT: A Recurrent Vision-and-Language BERT for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Recurrent-VLN-BERT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.10638&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/weituo12321/PREVALENT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;VLN-CE&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.02549&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.22548&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://miv-xjtu.github.io/JanusVLN.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23468&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Feliciaxyao/NavMorph&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.05890&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.01943&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/HNR-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03047v2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;PAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/ETPNav?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2408.10388&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Narrowing the Gap between Vision and Action in Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02764&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Discrete-Continuous-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.02857&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jacobkrantz/VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Zero-shot)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10611565&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/DiscussNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.07314&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://chen-judge.github.io/MapGPT/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.10620&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04882&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/InstructNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.16986&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March in Chat: Interactive Prompting for Remote Embodied Referring Expression&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/MiC&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.10822&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision and Language Navigation in the Real World via Online Visual Language Mapping&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://peihaochen.github.io/files/publications/A2Nav.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS Workshop&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.16649&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Fine-tuning)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.01551&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/EvolveNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.findings-naacl.60.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LangNav: Language as a Perceptual Representation for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/pbw-Berwin/LangNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07376&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/NavCoT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.02010&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generalist Model for Embodied Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LaVi-Lab/NaviLLM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2407.12366&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.15852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;RSS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Behavior Analysis&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.16394&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Visual Imaginations Improve Vision-and-Language Navigation Agents?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.17313&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/zehao-wang/navnuances&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Behavioral Analysis of Vision-and-Language Navigation Agents&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Yoark/vln-behave&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.438.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diagnosing Vision-and-Language Navigation: What Really Matters&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/VegB/Diagnose_VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;后续工作&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;后续工作&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%90%8e%e7%bb%ad%e5%b7%a5%e4%bd%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里夸奖一下Gemini3和qwen的deep Research，真的救我狗命。&lt;/p&gt;
&lt;p&gt;重点精读部分就看下面Gemini3提供的一份经过深度调研、严格筛选的 &lt;strong&gt;2023–2025&lt;/strong&gt; 年间顶会（CVPR, ICCV, ECCV, ICLR, NeurIPS, CoRL, RSS, ICRA, IROS）&lt;strong&gt;已接收 (Accepted)&lt;/strong&gt; 且 &lt;strong&gt;已公开代码&lt;/strong&gt; 的 VLN / ObjectNav / Zero-Shot / LLM-assisted Navigation 相关论文列表。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;简介&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;关键词&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CVPR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;UniGoal: Towards Universal Zero-shot Goal-oriented Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了基于场景图（Scene Graph）和 LLM 的通用导航框架，统一了 Object, Image, Text 三种目标导航任务，解决 Zero-Shot 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Zero-Shot, Scene Graph, LLM, Universal Goal&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;专注于提升 VLM 的空间理解能力，通过构建空间感知的指令微调数据集，大幅提升了机器人在 3D 环境中的导航和操作能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/RoboSpatial/RoboSpatial&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Spatial Reasoning, VLM, Robotics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-and-Language Navigation via Causal Learning (VLN-GOAT)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入因果推断（Causal Inference）消除数据偏差，提升 VLN 模型的泛化性&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CrystalSixone/VLN-GOAT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Causal Learning, Deconfounding&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;首个基于视频的大模型（Video-based VLM）端到端导航器，无需构建显式地图，直接从视频流规划动作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jzhzhang/NaVid-VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Video VLM, Mapless, End-to-End&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AIGeN: An Adversarial Approach for Instruction Generation in VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用对抗生成网络生成高质量的导航指令，用于数据增强&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/jialuli-luka/AIGeN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Iterative Vision-and-Language Navigation (IVLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了&amp;quot;迭代式导航&amp;quot;新基准，要求机器人在同一环境中持续执行多条指令，考察记忆能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/JacobKrantz/IVLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Continuous Navigation, Memory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics (VLN-SIG)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过生成未来视角的语义图像来辅助当前决策&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://jialuli-luka.github.io/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;模拟人类认知过程（感知-推理-决策），利用 LLM 进行常识推理和空间推理，解决 ObjectNav 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://yhancao.github.io/CogNav/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page &amp;amp; Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Cognitive Modeling, LLM, ObjectNav&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Learning Vision-and-Language Navigation from YouTube Videos (YouTube-VLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用大规模 YouTube 房屋导览视频进行预训练，学习真实世界先验&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入&amp;quot;心理规划&amp;quot;机制，在执行前在潜在空间预演路径&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/HanqingWangAI/DreamWalker&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ECCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN-Copilot: LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出&amp;quot;副驾驶&amp;quot;概念，当导航智能体困惑时，LLM 提供详细的指导和推理辅助&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zun-Wang/VLN-Copilot&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;LLM Agent, Coarse-grained VLN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过微调适配，激发通用多模态大模型（VLM）的导航推理能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/WZMIAOMIAO/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-Language Navigation with Energy-Based Policy (ENP)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出基于能量的模型（Energy-Based Model）来建模导航策略，更好地模拟专家轨迹分布&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://neurips.cc/virtual/2025/poster/93232&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS Page/GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;构建在线 3D 场景图作为 Prompt，实现无需训练的 Zero-Shot 导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/SG-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;InstructNav: Zero-shot Vision-and-Language Navigation with Instruction Tuning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;这是一个通用的导航大模型框架，统一了 VLN 和 ObjectNav&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;(查看作者 Hao Dong 的 GitHub 或 Project Page)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;PanoGen: Text-Conditioned Panoramic Environment Generation for VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;使用生成式模型根据文本生成全景环境，用于 VLN 的数据增强和训练&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CoRL&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;GC-VLN: Graph-Constrained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;UniGoal&lt;/em&gt; 团队新作，将导航建模为图约束优化问题，无需训练即可在连续环境中导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Training-free, Graph Constraints&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LeLaN: Learning a Language-Conditioned Navigation Policy from In-the-Wild Video&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;直接从野外（In-the-Wild）视频数据中学习语言条件的导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://lelan-video.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OpenVLA: An Open-Source Vision-Language-Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;虽然主要针对操作（Manipulation），但其架构和预训练模型被大量用于导航任务的底座&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/openvla/openvla&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 VLM 进行零样本 3D 视觉定位，是导航的关键前置任务&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/desdemonawang/VLM-Grounder&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OVSG: Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;基于开放词汇 3D 场景图的实体定位与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/ovsg-code/ovsg&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICRA&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation with Open-Source LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;探索使用 Llama 等开源模型替代 GPT-4 进行 Zero-Shot 导航，提出时空思维链&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/Open-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;MonoTransmotion&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;涉及单目视觉下的运动规划与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vita-epfl/MonoTransmotion&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 CLIP 和前沿点（Frontier）地图，指导机器人探索语义目标&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bdaiinstitute/vlfm&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLMaps: Visual Language Maps for Robot Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将 VLM 特征融合进 3D 地图，允许使用自然语言索引地图位置&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vlmaps/vlmaps&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;IROS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LLM3: Large Language Model-based Task and Motion Planning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 LLM 进行任务和运动规划，虽然偏 TAMP，但也包含导航组件&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zju-Robotics-Lab/LLM3&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RSS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Unified Video Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;统一的视频动作模型，涵盖导航和操作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page/Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;雖然偏向操作，但其 Policy 蒸馏方法正被用于加速导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/DLR-RM/Consistency-Policy&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICLR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 Web 教程合成智能体轨迹，辅助导航和任务执行&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/xduan7/AgentTrek&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>世界模型的三种路线</title>
      <link>http://localhost:1313/blog/2025/2025-11-18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-18/</guid>
      <description>
        
        
        &lt;h1&gt;世界模型的三种路线&lt;/h1&gt;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzYzNDE2OTYxMw%3D%3D&amp;amp;mid=2247483737&amp;amp;idx=1&amp;amp;sn=085711a5575f31c0e7ebf14e06a49759&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这则推文&lt;/a&gt;给了我很大的启发，事实上，选择这条道路的一个原因就是 Embodied AI 这条道路并未收敛，而真正落地的成果，以 AI 领域的卷度读个博的4、5年时间以内应该就能吃上红利。而且这个时候恰如上个世纪的物理学界，哪怕一个三流的物理学家在那个年代也能做出一流的发现，我也打算依靠这股浪潮。&lt;/p&gt;
&lt;p&gt;好了，接下来进入正文，引用这篇推文的三个问题：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;什么是世界模型：
是一个供人类观看的东西，一个供智能体训练的场所，还是一个图标内部的黑箱——是系统其他部分需要咨询的实际内部模型？
它的输出是静态资产、实时帧、还是主要驱动和预测和控制的潜在状态？
如果撞倒一个虚拟花瓶，系统中的任何部分是否会记住——并利用该记忆来更新其未来的预期——持续超过一帧？&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;所以得从实际应用出发，从具体的原始论文来熟悉这3类工作。&lt;/p&gt;
&lt;h2&gt;World Model as Interface: Marble&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-interface-marble&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-interface-marble&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;World Model as Simulator: Genie/SIMA 2&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-simulator-geniesima-2&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-simulator-geniesima-2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;World Model as Cognition: Prof. Lecun&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-cognition-prof-lecun&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-cognition-prof-lecun&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;等待更新中&amp;hellip;&amp;hellip;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>从头开始配置 Habitat Autodl 环境</title>
      <link>http://localhost:1313/blog/2025/2025-12-21-habitat-env/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-21-habitat-env/</guid>
      <description>
        
        
        &lt;h2&gt;AutoDL + Habitat + VNC + VGLRun&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;autodl--habitat--vnc--vglrun&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#autodl--habitat--vnc--vglrun&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;真的没招了&amp;hellip;下面的错误层出不穷：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;GL::Context: cannot retrieve OpenGL version: GL::Renderer::Error::InvalidValue&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;因此咨询学长，考虑根据&lt;a href=&#34;https://www.autodl.com/docs/gui/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AUTODL官方远程GUI文档&lt;/a&gt;设置VNC。&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;# 安装基本的依赖包
apt update &amp;amp;&amp;amp; apt install -y libglu1-mesa-dev mesa-utils xterm xauth x11-xkb-utils xfonts-base xkb-data libxtst6 libxv1

# 安装libjpeg-turbo和turbovnc
export TURBOVNC_VERSION=2.2.5
export LIBJPEG_VERSION=2.0.90
wget https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/vnc/libjpeg-turbo-official_${LIBJPEG_VERSION}_amd64.deb
wget https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/vnc/turbovnc_${TURBOVNC_VERSION}_amd64.deb
dpkg -i libjpeg-turbo-official_${LIBJPEG_VERSION}_amd64.deb
dpkg -i turbovnc_${TURBOVNC_VERSION}_amd64.deb
rm -rf *.deb

# 启动VNC服务端，这一步可能涉及vnc密码配置（注意不是实例的账户密码）。另外如果出现报错xauth未找到，那么使用apt install xauth再安装一次&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;在启动前，需要卸载无头版本：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip uninstall habitat-sim -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda remove habitat-sim-mutex headless -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装非无头版本（带渲染支持）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda install habitat-sim withbullet -c conda-forge -c aihabitat -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# mamba! /root/miniconda3/bin/mamba install!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# conda config --set ssl_verify false&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;启动！&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;rm -rf /tmp/.X1*  # 如果再次启动，删除上一次的临时文件，否则无法正常启动
USER=root /opt/TurboVNC/bin/vncserver :1 -desktop X -auth /root/.Xauthority -geometry 1920x1080 -depth 24 -rfbwait 120000 -rfbauth /root/.vnc/passwd -fp /usr/share/fonts/X11/misc/,/usr/share/fonts -rfbport 6006

# 检查是否启动，如果有vncserver的进程，证明已经启动
ps -ef | grep vnc&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;接下来cmd里配置SSH转发：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;ssh -CNg -L 6006:127.0.0.1:6006 root@connect.nmb2.seetacloud.com -p 24413&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;然后设置&lt;code&gt;export DISPLAY=:1&lt;/code&gt;，随便打开一个VNC Client “连接地址” 栏输入：127.0.0.1:6006（本地被 SSH 隧道代理的端口）；
点击连接，前面你设置的 VNC 密码（不是实例 root 密码）&lt;/p&gt;
&lt;p&gt;在这之后我们需要VirtualGL (vglrun)，其拦截了 Habitat 的 OpenGL 渲染请求，重定向到物理 GPU（NVIDIA）上运行，最后再把画面传回 VNC 的虚拟显示器。&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 下载并安装&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://github.com/VirtualGL/virtualgl/releases/download/3.1/virtualgl_3.1_amd64.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dpkg -i virtualgl_3.1_amd64.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;apt install -y libegl1-mesa libgl1-mesa-glx libglu1-mesa&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;这之后就可以用它来运行了：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;vglrun -d :1 python test.py&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;这里会遇到一个非常麻烦的问题：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Platform::WindowlessEglApplication::tryCreateContext&lt;span class=&#34;o&#34;&gt;()&lt;/span&gt;: unable to find CUDA device &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; among &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; EGL devices in total&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;参考&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1959978616382293176&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;知乎经验&lt;/a&gt;，实际上这个问题在Habitat-lab/TROUBLESHOOTING.md 中提到了。&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 设置 EGL vendor library（解决 EGL 设备匹配问题）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__EGL_VENDOR_LIBRARY_FILENAMES&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/usr/share/glvnd/egl_vendor.d/10_nvidia.json&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;接下来因为我是换了一个Autodl实例从头开始跑examples的，这里我们顺便来学习一下&lt;a href=&#34;https://evernorif.github.io/2025/05/07/habitat-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Habitat本身&lt;/a&gt;：&lt;/p&gt;
&lt;h3&gt;autodl-tmp/habitat-lab/examples/shortest_path_follower_example.py&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;autodl-tmphabitat-labexamplesshortest_path_follower_examplepy&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#autodl-tmphabitat-labexamplesshortest_path_follower_examplepy&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;这个里面没有&lt;code&gt;cv2.imshow()&lt;/code&gt;这种弹窗指令，所以逻辑是“静默运行 -&amp;gt; 保存 MP4”。&lt;/p&gt;
&lt;p&gt;可以在autodl-tmp/habitat-lab/examples/images/shortest_path_example看到生成的mp4&lt;/p&gt;
&lt;p&gt;注意其数据：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;# 下载测试场景（存放在 habitat-lab/data 下）
python -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path data/
# 下载导航任务数据
python -m habitat_sim.utils.datasets_download --uids habitat_test_pointnav_dataset --data-path data/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;这个在4090那个容器里也跑起来了。中间报了一个小错误：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;TypeError: write() got an unexpected keyword argument &amp;#39;fps&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;imageio 的 tifffile 插件不支持 fps 参数，问题是 ffmpeg 未安装，导致 imageio 无法使用 ffmpeg 插件写入 MP4，回退到其他插件（如 tifffile）不支持 fps 参数，如果是这样那就很简单了：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;/root/miniconda3/bin/mamba install -n falcon ffmpeg -c conda-forge -y
conda run -n falcon pip install imageio-ffmpeg&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;解决&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>全文献检索工具方案设计</title>
      <link>http://localhost:1313/blog/2025/2025-12-12-paper-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-12-paper-agent/</guid>
      <description>
        
        
        &lt;p&gt;科研paper读着实在是不得劲。独立研究者有着自己的一套&lt;strong&gt;taste&lt;/strong&gt;，正如杨振宁先生所说，这种taste是可以通过&lt;strong&gt;大量的喂养培养出来的&lt;/strong&gt;——但是这有一点&lt;strong&gt;太厚积薄发了&lt;/strong&gt;，看着zotero里面贫瘠的二十多篇论文，我打算做一个全文献检索的工具。&lt;/p&gt;
&lt;p&gt;首先想到的当然是&lt;strong&gt;ArXiv提供的官方库&lt;/strong&gt;，但是养taste的话，&lt;strong&gt;顶会中稿+开源的才有用&lt;/strong&gt;，普通的preprint属于垃圾食品，然而&lt;strong&gt;arXiv API 本身并没有一个专门的 strict 字段来筛选&amp;quot;是否被会议录用&amp;quot;&lt;/strong&gt;，完全依赖于作者手动更新 metadata 中的 comment。&lt;/p&gt;
&lt;h2&gt;方案一：DBLP + Semantic Scholar + ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;方案一dblp--semantic-scholar--arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%96%b9%e6%a1%88%e4%b8%80dblp--semantic-scholar--arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;所以我想到了下面这样的产品流：&lt;/p&gt;
&lt;h3&gt;DBLP（作为&amp;quot;权威判官&amp;quot;）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;dblp作为权威判官&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dblp%e4%bd%9c%e4%b8%ba%e6%9d%83%e5%a8%81%e5%88%a4%e5%ae%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 只负责提供**&amp;ldquo;真·录用名单&amp;rdquo;**&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; DBLP 是计算机领域的**&amp;ldquo;户籍科&amp;rdquo;&lt;strong&gt;，它的数据是&lt;/strong&gt;人工校对的**，只有真正被 CVPR 录用的论文才会出现在 &lt;code&gt;conf/cvpr/2025&lt;/code&gt; 列表里&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决痛点：&lt;/strong&gt; 解决了 arXiv 上作者**&amp;ldquo;自吹自擂&amp;quot;或&amp;quot;撒谎&amp;rdquo;**的问题（比如有的作者被拒稿了也敢写 Accepted）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Semantic Scholar（作为&amp;quot;连接器&amp;quot;）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;semantic-scholar作为连接器&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#semantic-scholar%e4%bd%9c%e4%b8%ba%e8%bf%9e%e6%8e%a5%e5%99%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 负责把 DBLP 的会议论文标题，映射到 &lt;strong&gt;arXiv ID&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; DBLP 的会议条目和 arXiv 条目通常是独立的（两条记录）。Semantic Scholar 构建了&lt;strong&gt;巨大的图谱&lt;/strong&gt;，它能识别出&amp;quot;这篇 CVPR 论文其实就是那篇 arXiv 预印本&amp;quot;，并提供 &lt;code&gt;externalIds&lt;/code&gt; 字段&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决痛点：&lt;/strong&gt; 解决了 &lt;strong&gt;DBLP 不直接提供 PDF 下载链接&lt;/strong&gt;，以及 DBLP 和 arXiv 割裂的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ArXiv（作为&amp;quot;内容仓库&amp;quot;）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;arxiv作为内容仓库&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#arxiv%e4%bd%9c%e4%b8%ba%e5%86%85%e5%ae%b9%e4%bb%93%e5%ba%93&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 负责下载&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; 一旦知道了 arXiv ID，用 Python 的 &lt;strong&gt;arxiv 库下载 PDF 或源码是最稳定、最合规的&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决痛点：&lt;/strong&gt; Semantic Scholar 的 API 有时会&lt;strong&gt;限流&lt;/strong&gt;或不直接提供 PDF 文件流&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;存在的问题&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;存在的问题&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%98%e5%9c%a8%e7%9a%84%e9%97%ae%e9%a2%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;DBLP 的标题有时候会把句号放在最后，或者包含特殊字符。Semantic Scholar 的搜索能力很强，通常能模糊匹配，但偶尔也会因为特殊符号对不上。代码里做一点简单的 &lt;code&gt;strip()&lt;/code&gt; 清洗很有必要，但这不是最大的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Semantic Scholar 和 DBLP 都有频率限制&lt;/strong&gt;。如果名单有几千篇，一定要加 &lt;code&gt;time.sleep(1)&lt;/code&gt; 或者使用 &lt;strong&gt;API Key&lt;/strong&gt;（Semantic Scholar 申请 Key 后并发度更高），但是这导致了一个非常大的问题：&lt;strong&gt;调用Semantic Scholar太久了&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;方案二：直接爬取 CVF&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;方案二直接爬取-cvf&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%96%b9%e6%a1%88%e4%ba%8c%e7%9b%b4%e6%8e%a5%e7%88%ac%e5%8f%96-cvf&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;所以接下来又想了第二条路：直接爬取 &lt;strong&gt;CVF (Computer Vision Foundation)&lt;/strong&gt; 的官方 &lt;strong&gt;Open Access 仓库&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CVPR、ICCV、WACV&lt;/strong&gt; 的所有论文（包括 Supplemental Material）都全量、免费托管在这里，它就是&lt;strong&gt;简单的静态 HTML 页面&lt;/strong&gt;，没有复杂的反爬机制，&lt;strong&gt;没有 API 限流&lt;/strong&gt;，直接 &lt;code&gt;requests&lt;/code&gt; 请求一次就能拿到几千篇论文的列表，通过暴力爬虫直接访问 CVPR 2022 的&amp;quot;所有论文&amp;quot;页面，瞬间解析出所有论文的标题和 PDF 下载链接，并保存为 CSV 文件。&lt;/p&gt;
&lt;p&gt;然而没有投过paper的本科生被这个数量吓晕了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/cvpr.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CVPR 2020: ~&lt;strong&gt;1,467&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2021: ~&lt;strong&gt;1,660&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2022: &lt;strong&gt;2,074&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2023: ~&lt;strong&gt;2,359&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2024: ~&lt;strong&gt;2,719&lt;/strong&gt; 篇&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实在是太疯狂了。如果你一天读 5 篇，读完这一年的会，明年的会都开完了。&lt;/p&gt;
&lt;h2&gt;各顶会的处理方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;各顶会的处理方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%90%84%e9%a1%b6%e4%bc%9a%e7%9a%84%e5%a4%84%e7%90%86%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里还是继续完善离线方法，前面&lt;strong&gt;CVF的方法只适用于3个&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ECCV：&lt;/strong&gt; 虽然不属于 CVF，但 &lt;strong&gt;ECVA 的网站结构和 CVF 惊人的相似&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ICLR 和 CoRL：&lt;/strong&gt; 最优雅的方法不是爬网页，而是使用 &lt;strong&gt;OpenReview 的官方 Python 库&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ICRA, IROS（机器人双雄）：&lt;/strong&gt; 是最麻烦的两个。因为它们版权属于 &lt;strong&gt;IEEE&lt;/strong&gt;；&lt;strong&gt;IEEE Xplore 有很强的反爬&lt;/strong&gt;，且下载 PDF 需要&lt;strong&gt;学校 IP 验证&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NeurIPS &amp;amp; RSS：&lt;/strong&gt; 非常良心，&lt;strong&gt;历史归档都在静态网页上，HTML 结构十年不变&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以对于双雄，只能用&lt;strong&gt;DBLP的方法&lt;/strong&gt;，然后用Arxiv库交叉验证。&lt;/p&gt;
&lt;p&gt;根据经验，机器人领域（Robotics）的 &lt;strong&gt;ArXiv 覆盖率大约在 60% - 80%&lt;/strong&gt;。剩下 &lt;strong&gt;20%&lt;/strong&gt; 的文章，作者可能根本没传 ArXiv，或者传了但是标题改得面目全非（比如从 &amp;ldquo;A Fast Method&amp;hellip;&amp;rdquo; 改成了 &amp;ldquo;FastMethod:&amp;hellip;&amp;quot;）。&lt;/p&gt;
&lt;p&gt;计算机视觉顶会有严格的**“奇偶年份”**规律？！这里修正了我的爬取ICCV和ECCV的方法&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>具身导航调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-14-navigation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-14-navigation/</guid>
      <description>
        
        
        &lt;h1&gt;具身导航调研&lt;/h1&gt;&lt;p&gt;对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。&lt;/p&gt;
&lt;p&gt;本调研主要基于 &lt;strong&gt;&lt;a href=&#34;https://github.com/jiangranlv/embodied-ai-start&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PKU EPIC Lab&lt;/a&gt;&lt;/strong&gt; 、&lt;strong&gt;&lt;a href=&#34;https://github.com/TianxingChen/Embodied-AI-Guide&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lumina具身智能社区&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;一、基础概念 (Basic Concepts)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一基础概念-basic-concepts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1、 什么是具身智能&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-什么是具身智能&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。&lt;/p&gt;
&lt;p&gt;典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心特征：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有多模态感知能力（视觉、触觉、语音等）&lt;/li&gt;
&lt;li&gt;能够执行动作并影响环境&lt;/li&gt;
&lt;li&gt;学习可以通过与环境交互而不仅仅是被动监督完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 具身智能与其他AI的区别&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-具身智能与其他ai的区别&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96ai%e7%9a%84%e5%8c%ba%e5%88%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;如何scale up机器人数据？&lt;/strong&gt; 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？&lt;/strong&gt; 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 研究具身智能的核心原则 (Core Principles)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-研究具身智能的核心原则-core-principles&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；&lt;/p&gt;
&lt;p&gt;但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）&lt;/p&gt;
&lt;p&gt;必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？&lt;/p&gt;
&lt;h2&gt;二、AI and Robotics Basis&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二ai-and-robotics-basis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cai-and-robotics-basis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-Embodied-AI&lt;/strong&gt;
王鹤老师《具身智能导论》，找找类似课程替代&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-CV&lt;/strong&gt;
Stanford CS231N&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning (CS285)&lt;/strong&gt;
Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;三、研究平台与工具&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三研究平台与工具&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Simulation Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-simulation-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-simulation-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;2. Robot Platform&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-platform&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-platform&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;3. Daily ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-daily-arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-daily-arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了：
具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jiangranlv/robotics_arXiv_daily&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jiangranlv/robotics_arXiv_daily&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;四、Research Field on Robots&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四research-field-on-robots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bresearch-field-on-robots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Grasping&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-grasping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-grasping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。&lt;/p&gt;
&lt;p&gt;常见任务有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single object grasping（单物体抓取）&lt;/strong&gt;：抓取一个物体，物体通常放在桌子上。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clutter scene grasping（堆叠场景抓取）&lt;/strong&gt;：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional grasping（带语义抓取）&lt;/strong&gt;：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用机械手末端有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suction cup（吸盘）&lt;/strong&gt;：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel gripper（平行夹抓）&lt;/strong&gt;：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-fingered hand（多指手）&lt;/strong&gt;，又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-loop methods（开环执行）&lt;/strong&gt;：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closed-loop methods（闭环执行）&lt;/strong&gt;：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Manipulation&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-manipulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-manipulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。&lt;/p&gt;
&lt;p&gt;**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）
**Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。
**Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。
**Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。
**Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。
**Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同&lt;/p&gt;
&lt;h3&gt;3. Navigation(NOW)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-navigationnow&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-navigationnow&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。&lt;/p&gt;
&lt;p&gt;常见任务包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Point Goal Navigation (PointNav)&lt;/strong&gt;：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Goal Navigation (ObjectNav)&lt;/strong&gt;：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Navigation (VLN)&lt;/strong&gt;：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embodied Question Answering (EQA)&lt;/strong&gt;：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：机器人持续感知并跟随动态目标（如人或移动物体）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map-based Navigation&lt;/strong&gt;：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompting-Large-Model Navigation&lt;/strong&gt;：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video-based VLM Navigation&lt;/strong&gt;：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unified Embodied Navigation&lt;/strong&gt;：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Locomotion&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-locomotion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-locomotion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。&lt;/p&gt;
&lt;p&gt;技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning from manually designed reward&lt;/strong&gt; (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from human data&lt;/strong&gt; (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】
如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -&amp;gt; whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下一个阶段会涉及到一点除了control之外的东西，就是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引入【视觉】实现户外自主化（perceptive locomotion）&lt;/strong&gt;；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入【物体】实现loco-manipulation&lt;/strong&gt;；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对上述两种task的组合&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调【语义的泛化性】&lt;/strong&gt;，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调一些特殊的capability&lt;/strong&gt;（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;五、Learning based Research Field&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五learning-based-research-field&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94learning-based-research-field&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Few-shot Imitation Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-few-shot-imitation-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-few-shot-imitation-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：Behavior Cloning、DAgger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当前主流方法&lt;/strong&gt;：ACT、Diffusion Policy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。&lt;/p&gt;
&lt;h3&gt;2. Robot Foundation Model&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-foundation-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-foundation-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。
目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenVLA&lt;/strong&gt;：第一个开源且易于follow的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi0 / Pi0.5&lt;/strong&gt;：目前公认最work的VLA，10K+ hours teleop data训练的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraspVLA&lt;/strong&gt;：基于纯仿真数据的抓取任务的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有少量工作没有借助VLM&lt;/strong&gt;，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Sim-to-Real Reinforcement Learning (Distillation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-sim-to-real-reinforcement-learning-distillation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-sim-to-real-reinforcement-learning-distillation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。&lt;/p&gt;
&lt;p&gt;目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。&lt;/p&gt;
&lt;p&gt;核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。&lt;/p&gt;
&lt;h3&gt;4. Real-World Reinforcement Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-real-world-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-real-world-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-world RL 指直接在现实环境中进行探索式学习。&lt;/p&gt;
&lt;p&gt;这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**从零开始的真实世界强化学习：**Hil-Serl&lt;/li&gt;
&lt;li&gt;**基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. World Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-world-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-world-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代表性工作包括 Dreamer 系列&lt;/strong&gt;（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。&lt;/p&gt;
&lt;h2&gt;六、相关领域&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六相关领域&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Graphics&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-graphics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-graphics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。&lt;/p&gt;
&lt;p&gt;**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等
**Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。
**系统性学习图形学推荐课程：**Games 101, 103&lt;/p&gt;
&lt;h3&gt;2. Hardware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tele-operation（遥操作）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。
**主从臂系统：**如 Gello，实现高精度的力控遥操作。
**可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。
&lt;strong&gt;Sensors（传感器）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。&lt;/li&gt;
&lt;li&gt;**Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。&lt;/li&gt;
&lt;li&gt;**Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。&lt;/li&gt;
&lt;li&gt;**Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mainstream Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-mainstream-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-mainstream-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Transformer&lt;/li&gt;
&lt;li&gt;Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Foundation Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-foundation-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-foundation-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。&lt;/li&gt;
&lt;li&gt;CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。&lt;/li&gt;
&lt;li&gt;VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. 3D Vision&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-3d-vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-3d-vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维生成与重建&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。&lt;/li&gt;
&lt;li&gt;**单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。&lt;/li&gt;
&lt;li&gt;**单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。&lt;/li&gt;
&lt;li&gt;**位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维表示&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。&lt;/li&gt;
&lt;li&gt;**点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。&lt;/li&gt;
&lt;li&gt;**Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维理解&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>具身职业规划</title>
      <link>http://localhost:1313/blog/2025/2025-11-27-embodied-jobs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-27-embodied-jobs/</guid>
      <description>
        
        
        &lt;p&gt;灵感主要来源于这篇&lt;a href=&#34;http://xhslink.com/o/3r7R0NWWLxE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;26二硕具身秋招小结&lt;/a&gt;的帖子，因为以前我只知道&lt;strong&gt;互联网开发岗&lt;/strong&gt;的实习和工作，对其他领域实在未知且畏惧。这篇帖子最好的就在于博主是&lt;strong&gt;半路出身&lt;/strong&gt;，和我基本一模一样。将来我无论是去&lt;strong&gt;港科广&lt;/strong&gt;还是&lt;strong&gt;港中文&lt;/strong&gt;，提前规划自己的实习或科研都是必要的。将来的工作，如果具身找不到，也可以往&lt;strong&gt;多模态&lt;/strong&gt;转，像是&lt;strong&gt;LLM应用开发&lt;/strong&gt;之类的岗位。&lt;/p&gt;
&lt;p&gt;还有一些更多的内容，基本上就是按照&lt;strong&gt;具身&lt;/strong&gt;、&lt;strong&gt;秋招&lt;/strong&gt;、&lt;strong&gt;实习&lt;/strong&gt;之类的关键词在社交媒体上检索，下面分别是：&lt;/p&gt;
&lt;h3&gt;信息来源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;信息来源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bf%a1%e6%81%af%e6%9d%a5%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;知乎：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1915789829809108777&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【具身智能】招聘帖：校招、实习、社招&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;小红书：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://xhslink.com/o/3r7R0NWWLxE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;26二硕具身秋招小结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://xhslink.com/o/9qTWqsG2aAD&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;具身秋招小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;其中知乎和小红书的垃圾信息最多，容易陷入其中无意义的刷，这里就暂时贴这些&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Github：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/StarCycle/Awesome-Embodied-AI-Job/tree/main&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;具身智能招贤榜&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;CC98：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6342124&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】offer帮选 具身or互联网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6337094&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】机械-&amp;gt;具身智能入坑指南 (附公司红黑榜)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6334298&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】offer犹豫，二选一，具身人形&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6327793&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【实习兼职】具身智能实习生（模仿学习）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6324539&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】具身菜博offer预选（拼尽全力版）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6296440&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】（10.20更新）具身智能方向还能投什么？一图速览&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6271943&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】具身老博offer二选一（大结局）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6250608&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】本科毕业可以做具身智能吗&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;当前困境与策略&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;当前困境与策略&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%bd%93%e5%89%8d%e5%9b%b0%e5%a2%83%e4%b8%8e%e7%ad%96%e7%95%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;现在的一个问题是，&lt;strong&gt;本科大四空有时间但是无法投递实习&lt;/strong&gt;——这里选择的领域虽然新兴，但是要求也高，很多都是得在读研究生期间才行。因此最初拟定的策略是大四就忙三件事：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;毕设（论文）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;申请季（找硕士读）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;知识准备（面向JD/PhD）&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但是未来其实也有很多路可以走，这里也可以考虑论文之类的，但是&lt;strong&gt;对口实习至少得1份，最好有2份&lt;/strong&gt;。论文这一块至少现在的不确定性比较大，将来如果进入&lt;strong&gt;港科广获得MPhil的机会&lt;/strong&gt;那就可以往这方面努力，只刷1份实习；如果面试失败的话那就老老实实&lt;strong&gt;港中文港深通勤深圳刷2份实习&lt;/strong&gt;。关键的问题是&lt;strong&gt;第一份实习要怎么才能拿到手&lt;/strong&gt;，这里得面向&lt;strong&gt;JD&lt;/strong&gt;努力，或者找一个初创类似性质的地方去，尽可能不要太底层&lt;strong&gt;Robotics&lt;/strong&gt;（但是基础知识又不能不掌握，主要是单纯Robotics的不太好迁移到&lt;strong&gt;LLM/VLM&lt;/strong&gt;之类的工作上去）&lt;/p&gt;
&lt;h2&gt;具身智能岗位三层架构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;具身智能岗位三层架构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e5%b2%97%e4%bd%8d%e4%b8%89%e5%b1%82%e6%9e%b6%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;总结上述的内容，可以总结目前的&lt;strong&gt;具身智能岗位&lt;/strong&gt;不再是浑浑噩噩的一团，而是清晰地分为了三层：&lt;/p&gt;
&lt;h3&gt;顶层（大脑层 - VLA/多模态）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;顶层大脑层---vla多模态&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%a1%b6%e5%b1%82%e5%a4%a7%e8%84%91%e5%b1%82---vla%e5%a4%9a%e6%a8%a1%e6%80%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;代表机构：&lt;/strong&gt; 上海AI Lab、腾讯Robotics X、华为天才少年、智元/银河通用等明星初创的核心组。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征：&lt;/strong&gt; 极其看重**Paper（CVPR/ICRA/CoRL）**或顶尖竞赛。这是&amp;quot;&lt;strong&gt;神仙打架&lt;/strong&gt;&amp;ldquo;的领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BME的策略：&lt;/strong&gt; &lt;strong&gt;不要硬碰硬&lt;/strong&gt;。除非在读研期间发了顶会，否则很难直接作为第一份实习切入。&lt;/p&gt;
&lt;h3&gt;中层（小脑层 - 运控/RL）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;中层小脑层---运控rl&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%ad%e5%b1%82%e5%b0%8f%e8%84%91%e5%b1%82---%e8%bf%90%e6%8e%a7rl&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;代表机构：&lt;/strong&gt; 宇树（Unitree）、小鹏（Robot Center）、以及大多数人形机器人初创。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征：&lt;/strong&gt; 核心是 &lt;strong&gt;Sim2Real（仿真到真机）&lt;/strong&gt;。&lt;strong&gt;PPO算法&lt;/strong&gt;、&lt;strong&gt;Isaac Gym/Lab&lt;/strong&gt;仿真平台是标配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BME的策略：&lt;/strong&gt; &lt;strong&gt;这是主战场&lt;/strong&gt;。有工程背景，理解物理世界（力、摩擦、惯性），这比纯CS背景的人更有优势。&lt;/p&gt;
&lt;h3&gt;基座层（工程层 - 部署/基础软件）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基座层工程层---部署基础软件&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e5%ba%a7%e5%b1%82%e5%b7%a5%e7%a8%8b%e5%b1%82---%e9%83%a8%e7%bd%b2%e5%9f%ba%e7%a1%80%e8%bd%af%e4%bb%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;代表机构：&lt;/strong&gt; 各大机器人公司的&amp;quot;工程落地组&amp;rdquo;、自动驾驶公司的&amp;quot;工具链组&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征：&lt;/strong&gt; &lt;strong&gt;ROS2&lt;/strong&gt;、&lt;strong&gt;C++&lt;/strong&gt;、&lt;strong&gt;Docker&lt;/strong&gt;、通信协议、传感器驱动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BME的策略：&lt;/strong&gt; &lt;strong&gt;这是保底牌和敲门砖&lt;/strong&gt;。很多算法岗其实都需要强工程能力来落地。&lt;/p&gt;
&lt;h2&gt;两条发展路线&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;两条发展路线&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%a4%e6%9d%a1%e5%8f%91%e5%b1%95%e8%b7%af%e7%ba%bf&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;两周后的面试将决定未来两年的主线任务&lt;/strong&gt;。无论结果如何，利用大四做项目是公约数，但在侧重点上需要完全不同的打法。&lt;/p&gt;
&lt;h3&gt;路线 A：港科广 (HKUST-GZ) / MPhil / 走学术科研流&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;路线-a港科广-hkust-gz--mphil--走学术科研流&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%af%e7%ba%bf-a%e6%b8%af%e7%a7%91%e5%b9%bf-hkust-gz--mphil--%e8%b5%b0%e5%ad%a6%e6%9c%af%e7%a7%91%e7%a0%94%e6%b5%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;假设顺利拿到了&lt;strong&gt;MPhil Offer&lt;/strong&gt;，目标是：&lt;strong&gt;两年后进大厂研究院或读博&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; &lt;strong&gt;Novelty (创新性) &amp;gt; Engineering (工程量)&lt;/strong&gt;。简历上必须要有&lt;strong&gt;Paper&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大四空窗期策略（Unitree项目）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;不要满足于&amp;quot;跑通&amp;quot;，要追求&amp;quot;算法改进&amp;quot;&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;课题方向建议：&lt;/strong&gt; 针对Unitree机器狗在极端非结构化环境（如松软沙地、楼梯废墟）下的&lt;strong&gt;RL适应性研究&lt;/strong&gt;。或者，研究如何用更少的算力实现&lt;strong&gt;VLA模型&lt;/strong&gt;在狗身上的&lt;strong&gt;边缘侧推理&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标产出：&lt;/strong&gt; 一篇由自己一作（或共一）的&lt;strong&gt;Workshop论文&lt;/strong&gt;或会议投稿，哪怕没中，&lt;strong&gt;Draft本身也是申请RA实习的硬通货&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实习投递方向：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;首选：&lt;/strong&gt; 腾讯Robotics X（深圳）、IDEA研究院（深圳）、鹏城实验室。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JD匹配：&lt;/strong&gt; 关注JD里写着&amp;quot;探索&amp;hellip;前沿算法&amp;quot;、&amp;ldquo;发表过&lt;strong&gt;ICRA/IROS&lt;/strong&gt;者优先&amp;quot;的岗位。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;路线 B：港中文 (CUHK) / MSc / 走业界就业流&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;路线-b港中文-cuhk--msc--走业界就业流&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%af%e7%ba%bf-b%e6%b8%af%e4%b8%ad%e6%96%87-cuhk--msc--%e8%b5%b0%e4%b8%9a%e7%95%8c%e5%b0%b1%e4%b8%9a%e6%b5%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;假设去了&lt;strong&gt;港中文或港科广的MSc&lt;/strong&gt;，目标是：&lt;strong&gt;两年后秋招拿高薪Offer&lt;/strong&gt;（华为/小鹏/初创独角兽）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; &lt;strong&gt;Complexity (系统复杂度) &amp;gt; Novelty&lt;/strong&gt;。面试官不关心算法多新，只关心&lt;strong&gt;Demo多稳、技术栈多全&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大四空窗期策略（Unitree项目）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;必须做一个&amp;quot;全栈闭环&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;课题方向建议：&lt;/strong&gt; 给Unitree Go2上跑通 &lt;strong&gt;&amp;ldquo;语音指令 -&amp;gt; VLM解析 -&amp;gt; 导航规划 -&amp;gt; RL运动控制&amp;rdquo;&lt;/strong&gt; 的全流程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心技能点：&lt;/strong&gt; 必须熟练掌握 &lt;strong&gt;ROS 2 (Humble)&lt;/strong&gt;，这是工业界通用的语言。代码要写得漂亮（&lt;strong&gt;C++为主，Python为辅&lt;/strong&gt;），要会用&lt;strong&gt;Docker&lt;/strong&gt;打包环境。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实习投递方向：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;策略：&lt;/strong&gt; 大四下/研一入学前，先去一家中小型初创（如&lt;strong&gt;帕西尼&lt;/strong&gt;、&lt;strong&gt;众擎&lt;/strong&gt;、或者深圳无数的机器人Startup）刷简历。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;理由：&lt;/strong&gt; 大厂（如华为、大疆）很难进，但&lt;strong&gt;初创公司急缺能干活的人&lt;/strong&gt;。有了第一份实习，研二再冲大厂。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;值得投递梯队榜&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;值得投递梯队榜&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%80%bc%e5%be%97%e6%8a%95%e9%80%92%e6%a2%af%e9%98%9f%e6%a6%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;基于提供的列表，结合地理位置（深圳/香港），整理了一份&amp;quot;梯队榜&amp;quot;：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;筛选标准：&lt;/strong&gt; Base大湾区优先 + 有AI/具身基因 + 薪资尚可 + 对转行友好。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;梯队&lt;/th&gt;
          &lt;th&gt;公司/机构&lt;/th&gt;
          &lt;th&gt;关键词与评价&lt;/th&gt;
          &lt;th&gt;行动建议&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T0 (灯塔级)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;腾讯 Robotics X&lt;/td&gt;
          &lt;td&gt;极难，技术最强，学术圈认可度高。科研路线必投。如果没有顶会，大概率简历挂，但值得一试。&lt;/td&gt;
          &lt;td&gt;科研路线必投&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T0 (灯塔级)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;IDEA 研究院（粤港澳大湾区数字经济研究院）&lt;/td&gt;
          &lt;td&gt;沈向洋带队，VLA很强。极佳的跳板。这里有很多港科/港中文的学生实习，学术资源好。&lt;/td&gt;
          &lt;td&gt;极佳的跳板&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T0 (灯塔级)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;华为 (2012/制造部)&lt;/td&gt;
          &lt;td&gt;薪资天花板，但分工极细。慎重。制造部可能偏传统，天才少年计划难度太高。可关注其具身智能应用组。&lt;/td&gt;
          &lt;td&gt;可关注其具身智能应用组&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T1 (独角兽)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;小鹏 (Robot Center)&lt;/td&gt;
          &lt;td&gt;深圳具身龙头，做人形，薪资55w+ (红榜)。重点目标。他们非常缺懂&amp;quot;运控+AI&amp;quot;的人。BME背景在这里不减分。&lt;/td&gt;
          &lt;td&gt;重点目标&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T1 (独角兽)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;大疆 (DJI)&lt;/td&gt;
          &lt;td&gt;难度极大，但技术栈极佳。可以投，但大疆更偏向极致的工程控制，对纯End-to-End AI持保留态度。&lt;/td&gt;
          &lt;td&gt;可以投，但需注意匹配度&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T1 (独角兽)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Unitree (宇树)&lt;/td&gt;
          &lt;td&gt;使用的设备厂商，虽然总部在杭州，但行业地位高。利用优势。如果能用他们的狗做出比官方Demo还牛的功能，直接发邮件给CTO。&lt;/td&gt;
          &lt;td&gt;利用优势&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T2 (潜力股)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;众擎机器人&lt;/td&gt;
          &lt;td&gt;人形，初创，势头猛。刷实习首选。初创公司流程快，能接触核心代码。&lt;/td&gt;
          &lt;td&gt;刷实习首选&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T2 (潜力股)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;帕西尼&lt;/td&gt;
          &lt;td&gt;感知触觉传感器+灵巧手。刷实习首选。BME背景懂传感器（Sensor），这里可能是降维打击区。&lt;/td&gt;
          &lt;td&gt;BME背景的降维打击区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T2 (潜力股)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;越疆 / 普渡&lt;/td&gt;
          &lt;td&gt;机械臂 / 移动底盘。保底选择。商业化成熟，适合去学习规范的工程开发流程。&lt;/td&gt;
          &lt;td&gt;保底选择&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;技能黑洞与补课重点&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技能黑洞与补课重点&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e8%83%bd%e9%bb%91%e6%b4%9e%e4%b8%8e%e8%a1%a5%e8%af%be%e9%87%8d%e7%82%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A. 具身算法岗 (The Dream Job)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-具身算法岗-the-dream-job&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-%e5%85%b7%e8%ba%ab%e7%ae%97%e6%b3%95%e5%b2%97-the-dream-job&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;JD高频词：&lt;/strong&gt; &lt;strong&gt;Imitation Learning (模仿学习)&lt;/strong&gt;, &lt;strong&gt;PPO/RL&lt;/strong&gt;, &lt;strong&gt;VLA&lt;/strong&gt;, &lt;strong&gt;Diffusion Policy&lt;/strong&gt;, &lt;strong&gt;Sim2Real&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;现状判定：&lt;/strong&gt; 也许懂深度学习基础，但可能没跑过&lt;strong&gt;Diffusion Policy&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;补课重点：&lt;/strong&gt; 去看 &lt;strong&gt;ACT (Action Chunking with Transformers)&lt;/strong&gt; 和 &lt;strong&gt;Diffusion Policy&lt;/strong&gt; 的开源代码。这是目前最火的两个模仿学习基线。&lt;/p&gt;
&lt;h3&gt;B. 机器人软件岗 (The Engineer Job)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;b-机器人软件岗-the-engineer-job&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#b-%e6%9c%ba%e5%99%a8%e4%ba%ba%e8%bd%af%e4%bb%b6%e5%b2%97-the-engineer-job&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;JD高频词：&lt;/strong&gt; &lt;strong&gt;ROS 2&lt;/strong&gt;, &lt;strong&gt;Linux&lt;/strong&gt;, &lt;strong&gt;C++&lt;/strong&gt;, &lt;strong&gt;Docker&lt;/strong&gt;, &lt;strong&gt;Python&lt;/strong&gt;, &lt;strong&gt;SLAM&lt;/strong&gt;, &lt;strong&gt;Planning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;现状判定：&lt;/strong&gt; 本科做过Demo，但代码规范吗？会用&lt;strong&gt;CMake&lt;/strong&gt;吗？懂进程间通信吗？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;补课重点：&lt;/strong&gt; &lt;strong&gt;ROS 2 Humble&lt;/strong&gt;。不要再学ROS 1了。学会如何在一个&lt;strong&gt;Docker容器&lt;/strong&gt;里配置好所有环境，这是多人协作的基础。&lt;/p&gt;
&lt;h3&gt;C. 仿真与合成数据岗 (The Hidden Gem)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;c-仿真与合成数据岗-the-hidden-gem&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#c-%e4%bb%bf%e7%9c%9f%e4%b8%8e%e5%90%88%e6%88%90%e6%95%b0%e6%8d%ae%e5%b2%97-the-hidden-gem&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;JD高频词：&lt;/strong&gt; &lt;strong&gt;Isaac Gym&lt;/strong&gt;, &lt;strong&gt;Isaac Sim&lt;/strong&gt;, &lt;strong&gt;Isaac Lab&lt;/strong&gt;, &lt;strong&gt;MuJoCo&lt;/strong&gt;, &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;现状判定：&lt;/strong&gt; 这是一个非常适合&lt;strong&gt;BME转行&lt;/strong&gt;的切入点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;补课重点：&lt;/strong&gt; &lt;strong&gt;NVIDIA Isaac Lab&lt;/strong&gt; (Isaac Gym的继任者)。现在的具身智能全是靠仿真数据堆出来的。如果能熟练搭建一个&amp;quot;机器狗在火星表面行走&amp;quot;的仿真环境，就是抢手货。&lt;/p&gt;
&lt;h2&gt;下一步&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;下一步&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8b%e4%b8%80%e6%ad%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;目前处于&amp;quot;信息过载但行动迷茫&amp;quot;的状态。&lt;/p&gt;
&lt;p&gt;现在的核心任务不是&lt;strong&gt;海投&lt;/strong&gt;，而是&amp;quot;备弹&amp;quot;。手中的Unitree机器狗就是核武器。大多数CS专业的学生只能在仿真里跑代码，而有&lt;strong&gt;真机&lt;/strong&gt;，这意味着&lt;strong&gt;Sim2Real经验&lt;/strong&gt;将是真实的，这是&lt;strong&gt;最大的竞争力&lt;/strong&gt;。要做的就是毕设所言的面向JD的&lt;strong&gt;Unitree机器狗全栈项目&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;内容精选&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;内容精选&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%86%85%e5%ae%b9%e7%b2%be%e9%80%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;投递公司速览表&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;投递公司速览表&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%95%e9%80%92%e5%85%ac%e5%8f%b8%e9%80%9f%e8%a7%88%e8%a1%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;根据ZJU学长的投递经验整理（&lt;strong&gt;59投，3offer，2入池&lt;/strong&gt;）：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;智能驾驶&lt;/th&gt;
          &lt;th&gt;制造/硬件&lt;/th&gt;
          &lt;th&gt;互联网大厂&lt;/th&gt;
          &lt;th&gt;具身智能（中厂）&lt;/th&gt;
          &lt;th&gt;研究院&lt;/th&gt;
          &lt;th&gt;具身智能（初创）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;理想&lt;/td&gt;
          &lt;td&gt;宇量晟（入池）?&lt;/td&gt;
          &lt;td&gt;字节&lt;/td&gt;
          &lt;td&gt;三一云&lt;/td&gt;
          &lt;td&gt;上海AI Lab（转正实习 oc/一面）?&lt;/td&gt;
          &lt;td&gt;亦方创新（意向）?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;地平线&lt;/td&gt;
          &lt;td&gt;中兴蓝剑（入池）?&lt;/td&gt;
          &lt;td&gt;腾讯&lt;/td&gt;
          &lt;td&gt;旷世&lt;/td&gt;
          &lt;td&gt;鹏城（博后）?&lt;/td&gt;
          &lt;td&gt;极佳?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MMT&lt;/td&gt;
          &lt;td&gt;美的&lt;/td&gt;
          &lt;td&gt;阿里&lt;/td&gt;
          &lt;td&gt;同花顺&lt;/td&gt;
          &lt;td&gt;北京通用人工智能&lt;/td&gt;
          &lt;td&gt;它石智航（意向）?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;卓驭&lt;/td&gt;
          &lt;td&gt;联想&lt;/td&gt;
          &lt;td&gt;快手&lt;/td&gt;
          &lt;td&gt;宇树&lt;/td&gt;
          &lt;td&gt;北京人形&lt;/td&gt;
          &lt;td&gt;普渡&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;小鹏&lt;/td&gt;
          &lt;td&gt;OPPO&lt;/td&gt;
          &lt;td&gt;网易&lt;/td&gt;
          &lt;td&gt;深信服&lt;/td&gt;
          &lt;td&gt;招商局&lt;/td&gt;
          &lt;td&gt;RoboSense&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;元戎&lt;/td&gt;
          &lt;td&gt;vivo&lt;/td&gt;
          &lt;td&gt;蚂蚁&lt;/td&gt;
          &lt;td&gt;优必选（笔试）?&lt;/td&gt;
          &lt;td&gt;智源（一面挂）?&lt;/td&gt;
          &lt;td&gt;傅里叶&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;文远知行&lt;/td&gt;
          &lt;td&gt;大疆&lt;/td&gt;
          &lt;td&gt;京东&lt;/td&gt;
          &lt;td&gt;讯飞飞星（二面）?&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;众擎&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;算能&lt;/td&gt;
          &lt;td&gt;滴滴&lt;/td&gt;
          &lt;td&gt;安克（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;自变量&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;百度（?）&lt;/td&gt;
          &lt;td&gt;TP&lt;/td&gt;
          &lt;td&gt;米哈游&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;银河通用（笔试）?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;蔚来（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Shopee（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;智元&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;千里（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;美团（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;帕西尼&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;哈啰&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;越疆&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;SharpA（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;标注说明：&lt;/strong&gt; ?一面挂、?简历挂、?还有机会&lt;/p&gt;
&lt;h3&gt;薪资行情与offer选择案例&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;薪资行情与offer选择案例&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%96%aa%e8%b5%84%e8%a1%8c%e6%83%85%e4%b8%8eoffer%e9%80%89%e6%8b%a9%e6%a1%88%e4%be%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;2024年薪资行情参考（非浙地区）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;硕士：&lt;/strong&gt; 有项目或优质实习经历，没顶会的情况下，普遍 &lt;strong&gt;60-70w&lt;/strong&gt;，最高可达 &lt;strong&gt;85w&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;博士：&lt;/strong&gt; 有机会接近 &lt;strong&gt;100w&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;案例：6个offer选择困境&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;讯飞飞星&lt;/strong&gt; - 40-80w（不确定性高），995，二线城市，半国企，VLA算法，浮动大&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;普通初创&lt;/strong&gt; - 70×14，965，一线城市，全栈具身研究员，被裁风险++&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;明星初创&lt;/strong&gt; - 预计可谈到85w左右，9105，一线城市，VLA算法，资金充足&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;央企军工&lt;/strong&gt; - 50w+，965，二线城市，技术管理，AI控制算法，&lt;strong&gt;后续难转具身&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;香港普通初创&lt;/strong&gt; - 90w+（港币），base深圳，HK工签（满7年永居），VLA算法，965+，资金有限&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;等待中&lt;/strong&gt; - AILab、TeleAI、IDEA，进度缓慢&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;选择考虑：&lt;/strong&gt; 对具身短期看空、长期看好，预计3-5年后冷静期，考虑转LLM或进入研究院过渡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;案例：互联网vs机器人方向选择&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;互联网方向：&lt;/strong&gt; 字节多模态算法（搜推组），未开奖&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机器人方向：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;小鹏（已开，总包约55w）&lt;/li&gt;
&lt;li&gt;Sharpa&lt;/li&gt;
&lt;li&gt;中兴（可能给蓝剑，但技术栈可能较落后）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;考虑因素：&lt;/strong&gt; 机器人发展路径不明朗，可能泡沫破裂；互联网存量竞争；薪资差异不大。&lt;/p&gt;
&lt;h3&gt;技术方向建议&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技术方向建议&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e6%9c%af%e6%96%b9%e5%90%91%e5%bb%ba%e8%ae%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;ZJU机械出身学长转具身的推荐路线：&lt;/strong&gt;
以&lt;strong&gt;VLA算法为主&lt;/strong&gt;，主打&lt;strong&gt;综合全栈能力&lt;/strong&gt;，通过错位竞争避免与纯算法同学硬拼。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四个主要技术方向：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLA/WA（决策层）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目前主流demo是&lt;strong&gt;VLA&lt;/strong&gt;，框架分为分层式和一段式&lt;/li&gt;
&lt;li&gt;本质是&lt;strong&gt;VLM的posttrain&lt;/strong&gt;，大规模pretrain学术界资源有限&lt;/li&gt;
&lt;li&gt;掌握&lt;strong&gt;diffusion用于生成轨迹&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;**WA（World Action）**算力需求更高，需生成符合物理规则的动作并对齐视频-action gap，目前较遥远&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RL/IL（运控层）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt;仍是主流，RL发展相对缓慢&lt;/li&gt;
&lt;li&gt;全身运控效果出色，固定任务上成功率有优势（RL100）&lt;/li&gt;
&lt;li&gt;重点掌握：&lt;strong&gt;PPO、SAC、TD3&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLA+RL&lt;/strong&gt;是长远趋势，工厂场景需要few-shot和zero-shot&lt;/li&gt;
&lt;li&gt;纯RL运控竞争激烈，薪资不如以前高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;嵌入式&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;掌握&lt;strong&gt;模型轻量化和部署流程&lt;/strong&gt;（ROS 2节点封装）&lt;/li&gt;
&lt;li&gt;确保算法在边缘设备（如Jetson）实时运行&lt;/li&gt;
&lt;li&gt;掌握CAN和485通信，能写&lt;strong&gt;PID和MPC&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;结构设计&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用SolidWorks，手搓滚珠丝杠、连杆、线驱系统&lt;/li&gt;
&lt;li&gt;液压已过时，现基本为电机驱动&lt;/li&gt;
&lt;li&gt;重视演示能力（&amp;ldquo;No BB, show me your demo&amp;rdquo;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;面试重点：&lt;/strong&gt; 主要考察前两个方向（VLA和RL/IL）。&lt;/p&gt;
&lt;h3&gt;实习机会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;实习机会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ae%9e%e4%b9%a0%e6%9c%ba%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;三一集团耘创新实验室（杭州）- 具身智能算法实习生&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;研究方向：&lt;/strong&gt; 模仿学习、VLA&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;团队背景：&lt;/strong&gt; 顶级会议论文30+篇（NeurIPS、ICML、ICLR、CVPR等）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源：&lt;/strong&gt; 充足尖端显卡&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作内容：&lt;/strong&gt; 智能体策略模型研发、算法研究、系统验证&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;要求：&lt;/strong&gt; 研究生在读，精通Python/C++，熟悉Pytorch/TensorFlow，有模仿学习基础&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;联系方式：&lt;/strong&gt; &lt;a href=&#34;mailto:yunze.pan@irootech.com&#34;&gt;yunze.pan@irootech.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;案例一：转专业二硕求职经验&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;案例一转专业二硕求职经验&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%88%e4%be%8b%e4%b8%80%e8%bd%ac%e4%b8%93%e4%b8%9a%e4%ba%8c%e7%a1%95%e6%b1%82%e8%81%8c%e7%bb%8f%e9%aa%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;背景：&lt;/strong&gt; 9本美硕，本科和一硕均与具身智能无关，24年九月同时修读第二个硕士专业Robotics，一年紧急完成一段对口实习，一篇A会（后面求职发现不太对口），预计12月提前毕业。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;投递策略：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;早投递：&lt;/strong&gt; 因为转专业的弱势背景，选择了在8月中就开始投递，后面证明这其实不一定是一个好的策略&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;只投递初创，不投递大厂：&lt;/strong&gt; 基于对大厂具身部门&amp;quot;规模小，门槛高&amp;quot;的判断。大厂具身部门offer总体上确实呈现一种赢者通吃的状态，冲大厂可能需要非常过硬的Pub/实习&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;除头部初创外只投递上海的公司：&lt;/strong&gt; 个人原因，对象在上海工作&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;投递结果：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;简历挂：&lt;/strong&gt; 智元/银河&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;一面挂：&lt;/strong&gt; Sharpa/光轮/忆生科技&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;二面挂：&lt;/strong&gt; 极佳/维他动力&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offer：&lt;/strong&gt; 穹彻智能/源络科技/小雨智造&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offer后结束推进：&lt;/strong&gt; 矩阵超智/留形科技/帕西尼&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;最终选择：&lt;/strong&gt; 穹彻智能具身算法工程师&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;选择考量：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;穹彻：&lt;/strong&gt; 之前实习过，公司氛围、工作强度、title都很不错，工作内容也很喜欢&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;源络：&lt;/strong&gt; 虽然强度偏大，但给的多，成长快并且leader都比较坦诚开放&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小雨：&lt;/strong&gt; 整体公司非常严谨，从上到下对于公司要干嘛、优先级的划分非常清楚，细分赛道有一套完整的落地预期，非常看好他们未来的商业模式，只可惜因为在北京不得不拒绝&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;个人收获和想法：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;对口实习就是第二学历：&lt;/strong&gt; 对于不追求顶级大包（硕士80+）的同学来说，对口实习非常非常重要，通常比论文在找工作的时候更好用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开奖时机：&lt;/strong&gt; 有时候开奖晚，找得晚一些不是坏事。因为找得比较早所以没有和大部队一起开奖，当时开出来的已经觉得非常高，尤其是源络，但这几天其他初创陆续开奖抢人才发现还是小巫见大巫了&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人才红利期：&lt;/strong&gt; 具身总体处于人才缺口相当大的红利期，今年开始学具身的人在明显变多，但这批人可能还需要一段时间才会进入就业池，过几年人才饱和之后红利期可能不复存在。在红利期结束之前要尽快积累完整项目经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预研vs落地：&lt;/strong&gt; 预研和落地是两条完全不同的路线，目前技术未收敛，企业更愿意为科研支付溢价。技术收敛之后可能会迎来具身初创和大厂的超级混战，落地经验就会更值钱&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;案例二：非科班零论文转行经验&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;案例二非科班零论文转行经验&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%88%e4%be%8b%e4%ba%8c%e9%9d%9e%e7%a7%91%e7%8f%ad%e9%9b%b6%e8%ae%ba%e6%96%87%e8%bd%ac%e8%a1%8c%e7%bb%8f%e9%aa%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;背景：&lt;/strong&gt; 9本德硕非科班，0论文，1个相关实习，2个本科做的水实习，代码能力一般。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;投递情况：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;投递了大概200-300个公司&lt;/li&gt;
&lt;li&gt;初创公司和机器人公司基本都没怎么投递，因为觉得不太稳定&lt;/li&gt;
&lt;li&gt;主要投递的还是制造业&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;关键经验：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对口实习的重要性：&lt;/strong&gt; 秋招期间对口的实习还是非常重要的，在询问的时候基本都只问实习的内容，做的项目也不会问。算法方向还是2段实习最为稳妥，不然背景不够只能海投&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;转行时机：&lt;/strong&gt; 今年3月份的时候有幸找到了一个具身的实习，在面试之前没有任何具身方向的认知，随便问了几个问题就招进去了&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习路径：&lt;/strong&gt; 从0开始入门，完整跑了一遍从运动控制、代码采集系统、数据处理、大模型微调、模型部署的整个pipeline。做完实习之后毕设就开始准备做具身方向，找了个博士生带着做，在秋招期间就读了很多这方面的论文，复现了一下模型，正好就补全了模型框架理论薄弱这方面的问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时间线：&lt;/strong&gt; 从8月底开始投，11月底才开始收到比较好的offer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt; 非常幸运，正好赶在具身这个方向热门之前就入局了。秋招是一个长线的过程，过往的所有经历都会成为你的一部分，不要泄气。&lt;/p&gt;
&lt;h3&gt;上海人工智能实验室招聘&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;上海人工智能实验室招聘&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8a%e6%b5%b7%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%ae%9e%e9%aa%8c%e5%ae%a4%e6%8b%9b%e8%81%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;机构介绍：&lt;/strong&gt; 上海人工智能实验室具身智能中心，面向国家具身智能领域的重大需求，以构建&amp;quot;一体、可泛化的具身人工智能系统&amp;quot;为目标。科研方向涵盖具身生成与数字化、具身智能大模型、人形机器人与运动智能、世界模型、具身多模态感知、物理仿真等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Base：&lt;/strong&gt; 上海-徐汇滨江。校招、实习、社招都招。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub：&lt;/strong&gt; &lt;a href=&#34;https://github.com/Intern-Robotics&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intern Robotics&lt;/a&gt;，建议准备简历的同时先关注相关仓库，欢迎 star 和提出宝贵意见。&lt;/p&gt;
&lt;h4&gt;主要岗位类型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;主要岗位类型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%bb%e8%a6%81%e5%b2%97%e4%bd%8d%e7%b1%bb%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;校招/社招/实习岗位包括：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-AIGC青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：三维重建与生成、几何处理、三维物体/人体生成、运动捕捉&lt;/li&gt;
&lt;li&gt;要求：硕士+，有三维视觉经验，顶级会议论文优先&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-仿真平台青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：具身控制、大模型驱动的智能体、Sim-Real transfer&lt;/li&gt;
&lt;li&gt;要求：硕士+，熟悉IsaacGym/Sim、Gibson、Habitat、Mujoco等仿真平台&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-人体运动策略青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：数字人/人型机器人/灵巧手运动控制&lt;/li&gt;
&lt;li&gt;要求：博士，熟悉生成模型、动捕动画、物理仿真、大模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-具身智能大模型青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：VLA模型框架设计、模型优化、数据生成、持续学习&lt;/li&gt;
&lt;li&gt;要求：硕士+，1年以上经验，熟悉Omniverse等仿真平台&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-足式机器人青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：双足/四足机器人运控算法&lt;/li&gt;
&lt;li&gt;要求：硕士+，有足式机器人项目经验，熟悉运动学动力学&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-强化学习青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：机器人运控与操作、现实世界强化学习&lt;/li&gt;
&lt;li&gt;要求：硕士+，精通PPO、SAC、DDPG等算法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;实习岗位主要类型：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AIGC算法实习生&lt;/strong&gt;：三维重建/生成方向，要求博士/硕士，6个月以上&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机械臂操作算法实习生&lt;/strong&gt;：VLA模型、Sim-to-Real，本硕博均可&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;具身智能大模型算法实习生&lt;/strong&gt;：大模型框架设计、模型优化，本科+&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人体运动策略算法实习生&lt;/strong&gt;：物理仿真、动作生成，博士/硕士，需发表论文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真平台算法实习生&lt;/strong&gt;：本科+，有数据可视化经验优先&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;足式机器人算法实习生&lt;/strong&gt;：双足/四足运控算法，本硕博均可&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习算法实习生&lt;/strong&gt;：RL前沿技术研究，本硕博均可&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真-渲染研发实习生&lt;/strong&gt;：3D重建大模型、物理引擎优化，需并行计算经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真平台系统研发实习生&lt;/strong&gt;：系统架构设计、分布式计算，需开源项目经验&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;社招岗位主要类型：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机器人操作工程师&lt;/strong&gt;：仿真引擎平台搭建，硕士+，有工作经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开放平台产品经理&lt;/strong&gt;：产品设计、需求挖掘，有技术类产品经验优先&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习青年研究员&lt;/strong&gt;：RL前沿研究，硕士+，有论文发表&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习算法工程师&lt;/strong&gt;：大规模并行仿真环境构建，硕士+，有项目经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机械臂操作青年研究员&lt;/strong&gt;：VLA模型、端到端操作，博士，1年以上研究经历&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; 所有岗位均支持校招、实习、社招。具体岗位详情建议关注&lt;a href=&#34;https://github.com/Intern-Robotics&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intern Robotics GitHub&lt;/a&gt;&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h2&gt;滚动招聘信息（按领域分类）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;滚动招聘信息按领域分类&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%bb%9a%e5%8a%a8%e6%8b%9b%e8%81%98%e4%bf%a1%e6%81%af%e6%8c%89%e9%a2%86%e5%9f%9f%e5%88%86%e7%b1%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;学术/研究机构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;学术研究机构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%a6%e6%9c%af%e7%a0%94%e7%a9%b6%e6%9c%ba%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;机构&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;UC San Diego Biwei Huang组&lt;/td&gt;
          &lt;td&gt;因果驱动的世界模型&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;浙江大学机器人与物理智能实验室&lt;/td&gt;
          &lt;td&gt;机器人控制&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;中科院自动化所水下机器人团队&lt;/td&gt;
          &lt;td&gt;水下机器人&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;普林斯顿PRISM实验室&lt;/td&gt;
          &lt;td&gt;Robot Foundation Models/Interaction and Autonomous Improvement/Open-World Systems&lt;/td&gt;
          &lt;td&gt;全奖PostDoc/PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.26&lt;/td&gt;
          &lt;td&gt;灵心巧手(北京大钟寺)&lt;/td&gt;
          &lt;td&gt;学术合作&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;具身手术机器人&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;清华深研院江勇课题组&lt;/td&gt;
          &lt;td&gt;研究型实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;上交ScaleLab&lt;/td&gt;
          &lt;td&gt;具身仿真方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;港中文李钟毓组&lt;/td&gt;
          &lt;td&gt;人形/灵巧手/多智能体&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;北京大学先进制造与机器人学院庞智博组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;浙江大学高飞组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;科研实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;华东师范大学计算机科学与技术学院&lt;/td&gt;
          &lt;td&gt;具身智能方向学者&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.29&lt;/td&gt;
          &lt;td&gt;上海交通大学自动化与感知学院&lt;/td&gt;
          &lt;td&gt;机器人研究和应用方向&lt;/td&gt;
          &lt;td&gt;科研助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.25&lt;/td&gt;
          &lt;td&gt;北京人形机器人创新中心&lt;/td&gt;
          &lt;td&gt;具身仿真链路&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.24&lt;/td&gt;
          &lt;td&gt;MBZUAI 机器人系RCL实验室&lt;/td&gt;
          &lt;td&gt;机器人/混合现实/人工智能&lt;/td&gt;
          &lt;td&gt;博士/硕士/访问学生/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.22&lt;/td&gt;
          &lt;td&gt;长三角国创中心智慧农业机器人研究所&lt;/td&gt;
          &lt;td&gt;具身智能机器人系统工程师/数据工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.9&lt;/td&gt;
          &lt;td&gt;MBZUAI 机器人系RCL实验室&lt;/td&gt;
          &lt;td&gt;机器人/混合现实/人工智能&lt;/td&gt;
          &lt;td&gt;博士/硕士/访问学生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.5&lt;/td&gt;
          &lt;td&gt;南京大学机器人与自动化学院空中机器人课题组&lt;/td&gt;
          &lt;td&gt;空中机器人设计与自主导航&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.2&lt;/td&gt;
          &lt;td&gt;上海交通大学ScaleLab&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;科研实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.2&lt;/td&gt;
          &lt;td&gt;中科院自动化所多模态人工智能系统全国重点实验室&lt;/td&gt;
          &lt;td&gt;具身设计/感知智能系统/灵巧操作/多模态大模型&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.1&lt;/td&gt;
          &lt;td&gt;朗毅机器人&lt;/td&gt;
          &lt;td&gt;SLAM算法/AI视觉算法/结构设计&lt;/td&gt;
          &lt;td&gt;全职/实习/2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.29&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;人形机器人全身控制&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.28&lt;/td&gt;
          &lt;td&gt;西湖大学机器智能实验室（MiLAB）&lt;/td&gt;
          &lt;td&gt;机器人具身大模型/深度强化学习（3-5人）&lt;/td&gt;
          &lt;td&gt;科研助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.27&lt;/td&gt;
          &lt;td&gt;灵巧智能&lt;/td&gt;
          &lt;td&gt;灵巧操作基础模型&lt;/td&gt;
          &lt;td&gt;研究实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.24&lt;/td&gt;
          &lt;td&gt;北京理工大学（珠海）跨域智能无人团队&lt;/td&gt;
          &lt;td&gt;AI机器人与具身智能&lt;/td&gt;
          &lt;td&gt;国家高层次人才/准聘教授/博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.18&lt;/td&gt;
          &lt;td&gt;中豫具身智能实验室&lt;/td&gt;
          &lt;td&gt;科研岗位&lt;/td&gt;
          &lt;td&gt;博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.17&lt;/td&gt;
          &lt;td&gt;北航国新院杨顺昆老师&lt;/td&gt;
          &lt;td&gt;软件工程/具身智能方向&lt;/td&gt;
          &lt;td&gt;博士/硕士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.17&lt;/td&gt;
          &lt;td&gt;香港科技大学（广州）钟秉灼老师&lt;/td&gt;
          &lt;td&gt;具身智能安全/信息物理系统控制和形式化方法&lt;/td&gt;
          &lt;td&gt;博士后/全奖博士生/研究助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;北大信研院应急管理具身智能联合实验室&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;香港大学&lt;/td&gt;
          &lt;td&gt;具身智能方向&lt;/td&gt;
          &lt;td&gt;博士生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.25&lt;/td&gt;
          &lt;td&gt;NUS CLeAR Lab&lt;/td&gt;
          &lt;td&gt;机器人操作方向&lt;/td&gt;
          &lt;td&gt;全职/兼职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;NUS CLeAR 实验室&lt;/td&gt;
          &lt;td&gt;机器人操作&lt;/td&gt;
          &lt;td&gt;全职/兼职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.17&lt;/td&gt;
          &lt;td&gt;浙江大学 FAST LAB 实验室&lt;/td&gt;
          &lt;td&gt;「机器人打羽毛球」和「水上漂机器人」项目招聘科研助理&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.16&lt;/td&gt;
          &lt;td&gt;荷兰特文特大学手术机器人实验室&lt;/td&gt;
          &lt;td&gt;手术机器人&lt;/td&gt;
          &lt;td&gt;博士研究生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.16&lt;/td&gt;
          &lt;td&gt;ELIXIR Lab&lt;/td&gt;
          &lt;td&gt;软体机器人的具身协作操控研究&lt;/td&gt;
          &lt;td&gt;博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.11&lt;/td&gt;
          &lt;td&gt;香港科技大学郭嵩教授&lt;/td&gt;
          &lt;td&gt;大模型/多模态等方向&lt;/td&gt;
          &lt;td&gt;博士/RA/博后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.9&lt;/td&gt;
          &lt;td&gt;北京大学计算机学院张史梁老师课题组&lt;/td&gt;
          &lt;td&gt;具身智能/多模态大模型轻量化/AIGC&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;香港理工大学校长青年学者吴郁杰老师&lt;/td&gt;
          &lt;td&gt;脑启发AI/脑认知科学&lt;/td&gt;
          &lt;td&gt;2-5名全奖博士生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;香港科技大学高揚教授&lt;/td&gt;
          &lt;td&gt;空间机器人中的人工智能/用于空间可持续性的机器人系统/主动太空碎片清除技术（ADR）&lt;/td&gt;
          &lt;td&gt;3名博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;同济大学设计创意学院CDI数字创新中心&lt;/td&gt;
          &lt;td&gt;机器人终端用户编程/大模型驱动/具身机器人交互/Unity开发&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;清华大学设计未来课题组&lt;/td&gt;
          &lt;td&gt;具身智能机器人开发与设计&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;加州大学洛杉矶分校Bolei Zhou老师&lt;/td&gt;
          &lt;td&gt;计算机视觉/机器自主系统&lt;/td&gt;
          &lt;td&gt;博士/visiting students&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;香港中文大学李钟毓组&lt;/td&gt;
          &lt;td&gt;VLA/Humanoid/Control Theory/Design&lt;/td&gt;
          &lt;td&gt;PhD/Postdoc/Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;TU Eindhoven (Netherlands)&lt;/td&gt;
          &lt;td&gt;Dynamic Manipulation in Semi-Structured Industrial Settings&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;KTH (Sweden)&lt;/td&gt;
          &lt;td&gt;Industrial automation and intelligent robotics&lt;/td&gt;
          &lt;td&gt;Assistant Professor&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;北京大学董豪组/UC Berkeley&lt;/td&gt;
          &lt;td&gt;计算机视觉、机器人和具身智能&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;University of Luxembourg&lt;/td&gt;
          &lt;td&gt;SLAM &amp;amp; Situational Awareness for Robotics&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Medical Robotics&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;Harvard University (US)&lt;/td&gt;
          &lt;td&gt;Postdoctoral Fellow in Robotics&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology(Sweden)&lt;/td&gt;
          &lt;td&gt;Field Robotics with a focus on Extreme Environments&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology(Sweden)&lt;/td&gt;
          &lt;td&gt;Data-driven and learning based control&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;King&amp;rsquo;s College of London&lt;/td&gt;
          &lt;td&gt;robotics and neuro-technology&lt;/td&gt;
          &lt;td&gt;Research associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Acoustic Soft Robotics&lt;/td&gt;
          &lt;td&gt;Research Fellow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Fluidic Robotics&lt;/td&gt;
          &lt;td&gt;Research Fellow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;北大-灵初联合实验室&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;南洋理工PineLab王子为组&lt;/td&gt;
          &lt;td&gt;具身基础模型学习/具身策略迁移部署/具身模型高效推理&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;清华深圳研究院王智组&lt;/td&gt;
          &lt;td&gt;空间场景数据标注与构建/VLA空间推理部署与验证&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;上海交大黄涛组&lt;/td&gt;
          &lt;td&gt;跨本体VLA/VLA推理加速/具身世界模型/仿真平台和数据生成&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;港科大郑展鹏组&lt;/td&gt;
          &lt;td&gt;多旋翼无人机的开发和应用/基于水下机器人的水下目标识别和三维重建&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;南洋理工大学Chuanxia Zheng组&lt;/td&gt;
          &lt;td&gt;三维重建与数字孪生&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/Intern/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;南洋理工大学王林组&lt;/td&gt;
          &lt;td&gt;多模态人工智能/基于生物激发的多模态传感融/面向操作与感知的机器人学习&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc/访问博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;意大利技术研究院IIT Arash Ajoudani组&lt;/td&gt;
          &lt;td&gt;VLM+机器人&lt;/td&gt;
          &lt;td&gt;全奖PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;香港大学Zhongliang Jiang组&lt;/td&gt;
          &lt;td&gt;Robotic Learning/Autonomous Surgical Robotics&lt;/td&gt;
          &lt;td&gt;博士/研究助理/访问学者&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;上海交通大学汶川组&lt;/td&gt;
          &lt;td&gt;机器人控制策略模型/硬件设计/机器人系统&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;香港科技大学人工智能机器人与空间可持续性研究中心&lt;/td&gt;
          &lt;td&gt;AI for Space Robotics/Robotic Systems for Space Sustainability/Active Debris Removal&lt;/td&gt;
          &lt;td&gt;PostDoc/研究助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;同济大学齐鹏组&lt;/td&gt;
          &lt;td&gt;具身智能血管介入手术机器人/多模态医学影像人工智能&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;上海交通大学SCALE LAB&lt;/td&gt;
          &lt;td&gt;人形机器人/双臂协作VLA/RoboTwin仿真开发&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Southampton&lt;/td&gt;
          &lt;td&gt;Multimodal Large Language Model in Human-Robot Interaction&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Oxford&lt;/td&gt;
          &lt;td&gt;Multi-collaborative scouting and mapping with a team of highly mobile robots&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Southampton&lt;/td&gt;
          &lt;td&gt;Soft Robots with Integrated Sensing, On-Demand Therapy and AI-assisted Control&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Bristol&lt;/td&gt;
          &lt;td&gt;Dexterous 3D-printed Robot Hands&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Glasgow&lt;/td&gt;
          &lt;td&gt;AI-Powered Resilient Teleoperation for Autonomous Robotics&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Manchester/University of Glasgow/University of Oxford&lt;/td&gt;
          &lt;td&gt;Robotics and AI for Net Zero&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Bath&lt;/td&gt;
          &lt;td&gt;Modelling and analytical framework for developing dexterous soft robotic manipulators&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;MBZUAI(UAE)&lt;/td&gt;
          &lt;td&gt;Robotics and AI&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;The University of Manchester&lt;/td&gt;
          &lt;td&gt;AI Driven Robotics for Intelligent Construction&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;The University of Manchester&lt;/td&gt;
          &lt;td&gt;Cloud robotics/Networked Systems&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.5&lt;/td&gt;
          &lt;td&gt;Heriot Watt University 华威大学 (UK)&lt;/td&gt;
          &lt;td&gt;Chair in Artificial Intelligence&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.30&lt;/td&gt;
          &lt;td&gt;香港科技大学徐英豪组&lt;/td&gt;
          &lt;td&gt;具身智能/3D重建与生成/世界模型&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;卡迪夫大学玛丽居里学者项目(英国)&lt;/td&gt;
          &lt;td&gt;SLAM/强化学习/可微分物理&lt;/td&gt;
          &lt;td&gt;博后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;港科广褚晓文李昊昂组&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;25Fall or 26 Fall PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;复旦大学智能人机交互实验室&lt;/td&gt;
          &lt;td&gt;可穿戴AGI/开源具身智能&lt;/td&gt;
          &lt;td&gt;2026级硕士博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;上交李永露卢策吾组RHOS&lt;/td&gt;
          &lt;td&gt;具身智能/人机协同学习&lt;/td&gt;
          &lt;td&gt;博士/硕士/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;北京大学智能学院钟亦武组&lt;/td&gt;
          &lt;td&gt;多模态推理/具身智能&lt;/td&gt;
          &lt;td&gt;博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.28&lt;/td&gt;
          &lt;td&gt;南科大机械系STAR课题组&lt;/td&gt;
          &lt;td&gt;空中机器人/具身智能&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.27&lt;/td&gt;
          &lt;td&gt;哈工大深圳研究院杨硕组&lt;/td&gt;
          &lt;td&gt;具身智能/多模态模型&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.23&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;robotics with specialization in visual domain adaptation&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.11&lt;/td&gt;
          &lt;td&gt;香港岭南大学陈曦组&lt;/td&gt;
          &lt;td&gt;软体机器人&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Robotics and geometric machine learning&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;香港科技大学（广州）李昊昂组&lt;/td&gt;
          &lt;td&gt;具身智能操纵&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;北京大学Hao Tang组&lt;/td&gt;
          &lt;td&gt;Embodied AI/AIGC&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/Master/Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Machine Learning and Robotics&lt;/td&gt;
          &lt;td&gt;Postdocs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;BMW Group (Munich)&lt;/td&gt;
          &lt;td&gt;Neuromorphic Multimodal Perception and Learning&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology (Sweden)&lt;/td&gt;
          &lt;td&gt;Robotics and Artificial Intelligence WASP&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;Toyota US&lt;/td&gt;
          &lt;td&gt;Robotics &amp;amp; Machine Learning for Human-Robot Interaction and Intelligent Vehicles&lt;/td&gt;
          &lt;td&gt;Research Engineer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;University of Nottingham&lt;/td&gt;
          &lt;td&gt;Soft Robotics and Wearables&lt;/td&gt;
          &lt;td&gt;PhD (UK students only)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;University of Surrey&lt;/td&gt;
          &lt;td&gt;Neuro-muscular state estimation and control for physical human-robot interaction&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;North Dakota State University&lt;/td&gt;
          &lt;td&gt;Precision Agriculture &amp;amp; Robotics&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;Institut de Robòtica i Informàtica Industrial (Spain)&lt;/td&gt;
          &lt;td&gt;Learning robot behaviors in collaborative manufacturing of large components&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.25&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Robotics with spatial understanding and Modelling&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;Imdea Materials (Spain)&lt;/td&gt;
          &lt;td&gt;Collaborative robots and laboratory automation in materials discovery&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;中南大学&lt;/td&gt;
          &lt;td&gt;医疗多模态大模型和具身智能&lt;/td&gt;
          &lt;td&gt;2名博士补录&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;西交利物浦大学/中科院沈阳自动化所&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;联培PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;University of Surrey&lt;/td&gt;
          &lt;td&gt;Evaluating the next generation of warehouse robotics using generative world models&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Horizon Europe MSCA-DN Project AIGreenBots&lt;/td&gt;
          &lt;td&gt;robotics and AI for agricultural robotics&lt;/td&gt;
          &lt;td&gt;11 PhD Positions&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;哈工大深圳&lt;/td&gt;
          &lt;td&gt;微型机器人&lt;/td&gt;
          &lt;td&gt;博士(3月31日前急招)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Multi-Sensor Fusion in Mobile Manipulation&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Singapre Institute of Technology&lt;/td&gt;
          &lt;td&gt;Robotics &amp;amp; Automation&lt;/td&gt;
          &lt;td&gt;Professor&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.22&lt;/td&gt;
          &lt;td&gt;MBZUAI (Abu Dhabi, United Arab Emirates)&lt;/td&gt;
          &lt;td&gt;Robotics&lt;/td&gt;
          &lt;td&gt;Assistant, Associate and Full Professor&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.22&lt;/td&gt;
          &lt;td&gt;南京大学智能科学与技术学院&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;副教授/助理教授&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.21&lt;/td&gt;
          &lt;td&gt;KTH Royal Institute of Technology&lt;/td&gt;
          &lt;td&gt;Research engineers within Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;清华大学深研院王智组&lt;/td&gt;
          &lt;td&gt;三维视觉与具身智能&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;清华大学脑与智能实验室&lt;/td&gt;
          &lt;td&gt;机器人硬件/VLA训练和部署/人机交互与心智理论&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;University of Sheffield&lt;/td&gt;
          &lt;td&gt;Deep Reinforcement Learning with Interactive Human Feedback&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;西交利物浦大学(苏州)&lt;/td&gt;
          &lt;td&gt;多模态脑机接口大模型&lt;/td&gt;
          &lt;td&gt;博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;南京大学单彩峰司晨阳组/南洋理工(NTU)刘子纬组&lt;/td&gt;
          &lt;td&gt;具身视觉理解&lt;/td&gt;
          &lt;td&gt;联培博士生/硕士生/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;南京大学龙霄潇组&lt;/td&gt;
          &lt;td&gt;3D生成/自动驾驶世界模型/3D视觉&lt;/td&gt;
          &lt;td&gt;Master/PhD/Visiting students&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;OpenAI (US)&lt;/td&gt;
          &lt;td&gt;Mechanical Architect/Research Engineer/Systems Integration Electrical Engineer, Robotics&lt;/td&gt;
          &lt;td&gt;FullTime&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;中国人民大学信息学院AIM3实验室&lt;/td&gt;
          &lt;td&gt;具身智能/多模态&lt;/td&gt;
          &lt;td&gt;博士/硕士/访问学生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;Georgia Tech Lunar Lab&lt;/td&gt;
          &lt;td&gt;Robot Learning&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/Master/Undergraduate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;University of Nottingham&lt;/td&gt;
          &lt;td&gt;Soft Robotics and Wearables&lt;/td&gt;
          &lt;td&gt;PhD (UK students only)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;New York University Abu Dhabi (NYUAD)&lt;/td&gt;
          &lt;td&gt;Automation, Robotics, and AI&lt;/td&gt;
          &lt;td&gt;Research Instrumentation Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;西交利物浦大学(苏州)&lt;/td&gt;
          &lt;td&gt;机器人学院&lt;/td&gt;
          &lt;td&gt;R1研究员&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;Cranfield University (UK)&lt;/td&gt;
          &lt;td&gt;Robust Motion Planning for Hopping Robots&lt;/td&gt;
          &lt;td&gt;PhD (self-funded, 3-year)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;Princeton University&lt;/td&gt;
          &lt;td&gt;Construction Robot&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Cambridge University (Rika Antonova team)&lt;/td&gt;
          &lt;td&gt;robot learning / robot hardware design / reinforcement learning&lt;/td&gt;
          &lt;td&gt;Fully-funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Cranfield University (UK)&lt;/td&gt;
          &lt;td&gt;Risk aware planning for multi agent systems&lt;/td&gt;
          &lt;td&gt;PhD (self-funded, 3-year)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;University of Liverpool&lt;/td&gt;
          &lt;td&gt;Adaptive Robotic Chemists for Resilient Pharmaceuticals&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;ETH Artificial Visual Intelligence group (AVI)&lt;/td&gt;
          &lt;td&gt;Computer Vision for Embodied AI&lt;/td&gt;
          &lt;td&gt;PhD（对中国学生有限制）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Chalmers University of Technology (Sweden)&lt;/td&gt;
          &lt;td&gt;Dynamics and Control of Legged Robots&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Technical University of Denmark (DTU)&lt;/td&gt;
          &lt;td&gt;Active Perception and End-to-End AI-driven Intuitive Inspection for Autonomous Aerial Robots&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.12&lt;/td&gt;
          &lt;td&gt;克莱姆森大学(美国)Luyang Zhao组&lt;/td&gt;
          &lt;td&gt;机器人学&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;?rebro University (Sweden)&lt;/td&gt;
          &lt;td&gt;Embodied Learning&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;香港科技大学(广州)许人镜组&lt;/td&gt;
          &lt;td&gt;强化学习/仿生机器人运动控制/具身智能芯片&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;ETH&lt;/td&gt;
          &lt;td&gt;Stretchable Optical Skin&lt;/td&gt;
          &lt;td&gt;Postdoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;Singapore Institute of Technology&lt;/td&gt;
          &lt;td&gt;Assistant Professor / Associate Professor in Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.9&lt;/td&gt;
          &lt;td&gt;NTU Energy Research Institute (南洋理工能源研究所)&lt;/td&gt;
          &lt;td&gt;Computer Science/Robotics/Automation&lt;/td&gt;
          &lt;td&gt;Research Engineer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.9&lt;/td&gt;
          &lt;td&gt;Loughborough University (UK)&lt;/td&gt;
          &lt;td&gt;Agri-Robotics&lt;/td&gt;
          &lt;td&gt;Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.7&lt;/td&gt;
          &lt;td&gt;清华大学协同交互智能研究中心周伯文组&lt;/td&gt;
          &lt;td&gt;人机/多智能体协同&lt;/td&gt;
          &lt;td&gt;PhD / Visiting Student&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.7&lt;/td&gt;
          &lt;td&gt;Arizona State University - Bo Liu&amp;rsquo;s Group&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;Tampere University (Finland)&lt;/td&gt;
          &lt;td&gt;Robotics and AI&lt;/td&gt;
          &lt;td&gt;Postdoctoral Research Fellow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;香港科技大学(广州)李昊昂组&lt;/td&gt;
          &lt;td&gt;具身智能(人形机器人/机械臂)&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;香港大学刘希慧组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.5&lt;/td&gt;
          &lt;td&gt;MIT&lt;/td&gt;
          &lt;td&gt;Soft and Micro Robotics&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;CMU Robotics Institute - Biorobotics Lab&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology (Sweden)&lt;/td&gt;
          &lt;td&gt;Robotics and AI&lt;/td&gt;
          &lt;td&gt;Senior Lecturer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;NTU南洋理工&lt;/td&gt;
          &lt;td&gt;insect-machine hybrid robot&lt;/td&gt;
          &lt;td&gt;Research Assistant&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;NTU南洋理工&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;Robotics Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.28&lt;/td&gt;
          &lt;td&gt;香港中文大学(深圳)&lt;/td&gt;
          &lt;td&gt;医疗具身智能&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.27&lt;/td&gt;
          &lt;td&gt;Eindhoven University of Technology&lt;/td&gt;
          &lt;td&gt;Mechanical Contact Information Processing of Soft and Large-area Robot Skin&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.26&lt;/td&gt;
          &lt;td&gt;宁波东方理工大学&lt;/td&gt;
          &lt;td&gt;机器人与控制&lt;/td&gt;
          &lt;td&gt;博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.22&lt;/td&gt;
          &lt;td&gt;The University of Western Australia&lt;/td&gt;
          &lt;td&gt;Lecturer in Automation and Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.22&lt;/td&gt;
          &lt;td&gt;University of Luxembourg&lt;/td&gt;
          &lt;td&gt;Research Associate in Space Robotics Manipulation&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;香港大学MMLab罗平组 &amp;amp; 深圳星际光年&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习生/联培PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;Washington State University&lt;/td&gt;
          &lt;td&gt;Robotics Planning&lt;/td&gt;
          &lt;td&gt;PhD/Master&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;港科广&lt;/td&gt;
          &lt;td&gt;三维空间感知/运动估计/导航&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;University of Lincoln&lt;/td&gt;
          &lt;td&gt;Robotic Fleet Coordination&lt;/td&gt;
          &lt;td&gt;PostDoc / Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.18&lt;/td&gt;
          &lt;td&gt;Wageningen University (Netherlands)&lt;/td&gt;
          &lt;td&gt;Robotics Sensor Fusion and Robotics Transferable skills&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.18&lt;/td&gt;
          &lt;td&gt;Technical University of Munich&lt;/td&gt;
          &lt;td&gt;Space Robotics or GNC&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc（不推荐中国学生）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.17&lt;/td&gt;
          &lt;td&gt;University of Nottingham&lt;/td&gt;
          &lt;td&gt;Robotics Manipulation Learning&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Research Engineer in Robotics, Perception and Learning&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PhD Social Robotics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.15&lt;/td&gt;
          &lt;td&gt;ETH Zurich &amp;amp; Tethys Robotics&lt;/td&gt;
          &lt;td&gt;Robotics Hardware Engineer&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.14&lt;/td&gt;
          &lt;td&gt;University of Surrey&lt;/td&gt;
          &lt;td&gt;Robotics Lecturer&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;互联网大厂&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;互联网大厂&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%92%e8%81%94%e7%bd%91%e5%a4%a7%e5%8e%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;公司&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;京东物流&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;寒假实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;阿里巴巴达摩院&lt;/td&gt;
          &lt;td&gt;具身智能招聘(杭州/北京)&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;高德视觉技术中心&lt;/td&gt;
          &lt;td&gt;世界模型&lt;/td&gt;
          &lt;td&gt;全职（学历不限）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.29&lt;/td&gt;
          &lt;td&gt;TikTok&lt;/td&gt;
          &lt;td&gt;多模态基础模型&amp;amp;多模态推荐大模型/MLLM合成数据研究型/大模型后训练RL&lt;/td&gt;
          &lt;td&gt;社招/校招/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.13&lt;/td&gt;
          &lt;td&gt;蚂蚁天玑实验室&lt;/td&gt;
          &lt;td&gt;机器人算法专家/具身智能遥操作系统开发工程师&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.28&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;语言模型/大模型系统/具身智能/多模态/大模型基础设施/AI安全攻防&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;阿里达摩院具身智能大模型团队&lt;/td&gt;
          &lt;td&gt;大模型的具身智能大脑/世界模型与VLA方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.3&lt;/td&gt;
          &lt;td&gt;阿里达摩院&lt;/td&gt;
          &lt;td&gt;具身智能招聘大模型、多模态方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.19&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;具身智能方向财务专家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.14&lt;/td&gt;
          &lt;td&gt;腾讯RoboticsX实验室&lt;/td&gt;
          &lt;td&gt;具身智能方向&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.21&lt;/td&gt;
          &lt;td&gt;微软&lt;/td&gt;
          &lt;td&gt;图像/视频/3d生成or pretraining/posttraining相关&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.14&lt;/td&gt;
          &lt;td&gt;阿里星顶尖人才计划&lt;/td&gt;
          &lt;td&gt;基础模型/Al Infra/大模型应用/产业AI/计算架构等方向&lt;/td&gt;
          &lt;td&gt;2025/2026届本硕博毕业生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.10&lt;/td&gt;
          &lt;td&gt;腾讯&lt;/td&gt;
          &lt;td&gt;技术研究-机器学习/机器人/多模态方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;字节跳动&lt;/td&gt;
          &lt;td&gt;机器人研究员/机器人运动控制算法工程师/机器人多模态大模型算法工程师/NLP算法工程师-应用算法&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;Momenta(北京/上海/苏州)&lt;/td&gt;
          &lt;td&gt;端到端大模型算法工程师&lt;/td&gt;
          &lt;td&gt;面向26届博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;蚂蚁集团(上海)&lt;/td&gt;
          &lt;td&gt;整机硬件产品专家 具身智能方向&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;京东探索研究院(北京亦庄)&lt;/td&gt;
          &lt;td&gt;具身智能VLA/自动驾驶VLA&lt;/td&gt;
          &lt;td&gt;社招|校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;三星电子(北京)&lt;/td&gt;
          &lt;td&gt;VLA/RL/点云处理&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;微软亚洲研究院机器学习组工业创新中心&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;蚂蚁集团(上海)&lt;/td&gt;
          &lt;td&gt;具身感知&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;京东TGT顶尖青年技术天才计划&lt;/td&gt;
          &lt;td&gt;包含具身岗位&lt;/td&gt;
          &lt;td&gt;校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.5&lt;/td&gt;
          &lt;td&gt;Figure AI&lt;/td&gt;
          &lt;td&gt;Training Infra/Large Scale Training/Large Scale Model Evals/Reinforcement Learning&lt;/td&gt;
          &lt;td&gt;Full Time&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.29&lt;/td&gt;
          &lt;td&gt;英伟达(北京)&lt;/td&gt;
          &lt;td&gt;大规模数据集/仿真benchmark/算法部署&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.29&lt;/td&gt;
          &lt;td&gt;百度(北京/上海/深圳)&lt;/td&gt;
          &lt;td&gt;多模态算法/自动驾驶感知算法/深度学习决策规划算法&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.25&lt;/td&gt;
          &lt;td&gt;蚂蚁集团PlanA人才专项&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.12&lt;/td&gt;
          &lt;td&gt;三星中国研究院&lt;/td&gt;
          &lt;td&gt;Robot Leading Scientist&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.8&lt;/td&gt;
          &lt;td&gt;字节跳动实习生招聘——筋斗云人才计划&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.8&lt;/td&gt;
          &lt;td&gt;小米集团2026届转正实习招聘&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.7&lt;/td&gt;
          &lt;td&gt;京东探索研究院具身智能团队&lt;/td&gt;
          &lt;td&gt;VLA算法研发&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;Intel(北京)&lt;/td&gt;
          &lt;td&gt;Robotic System Research&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.2&lt;/td&gt;
          &lt;td&gt;美团大模型北斗实习计划2025&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;截止日期7月31日&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;商汤2025春季校招&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;具身算法/运动控制&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;字节跳动&lt;/td&gt;
          &lt;td&gt;SLAM/运动控制/移动规划&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;Vivo&lt;/td&gt;
          &lt;td&gt;机器人首席科学家&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Waymo&lt;/td&gt;
          &lt;td&gt;Deep Learning &amp;amp; Modeling Research&lt;/td&gt;
          &lt;td&gt;Summer Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Waymo&lt;/td&gt;
          &lt;td&gt;World Modeling&lt;/td&gt;
          &lt;td&gt;Research Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;Foundation Model Training Infrastructure&lt;/td&gt;
          &lt;td&gt;Senior Research Engineer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Google DeepMind&lt;/td&gt;
          &lt;td&gt;Robotics&lt;/td&gt;
          &lt;td&gt;Research Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;华为中央研究院类脑计算团队(北京/上海/杭州/南京/合肥/深圳)&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;算法实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;Vivo Lab(上海)&lt;/td&gt;
          &lt;td&gt;技术规划工程师&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;Generalist Embodied Agent Research&lt;/td&gt;
          &lt;td&gt;Research Scientist (New College Grad 2025)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;京东探索研究院(北京亦庄京东总部)&lt;/td&gt;
          &lt;td&gt;具身智能/人形机器人&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Amazon Robotics (Germany)&lt;/td&gt;
          &lt;td&gt;Motion Planning &amp;amp; Control&lt;/td&gt;
          &lt;td&gt;Applied Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Huawei (Munich, Germany)&lt;/td&gt;
          &lt;td&gt;Robot Learning&lt;/td&gt;
          &lt;td&gt;Industrial PhD（中国学生可能因签证问题被拒）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.12&lt;/td&gt;
          &lt;td&gt;Qualcomm Netherlands&lt;/td&gt;
          &lt;td&gt;Reinforcement learning/MPC/differentiable world models&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.12&lt;/td&gt;
          &lt;td&gt;Microsoft UK (Cambridge)&lt;/td&gt;
          &lt;td&gt;Robotics&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.8&lt;/td&gt;
          &lt;td&gt;Microsoft (Redmond, Washington, US)&lt;/td&gt;
          &lt;td&gt;Spatial AI&lt;/td&gt;
          &lt;td&gt;Research Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;三星电子中国研究院(北京)&lt;/td&gt;
          &lt;td&gt;机器人/具身智能算法&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.23&lt;/td&gt;
          &lt;td&gt;微软亚洲研究院&lt;/td&gt;
          &lt;td&gt;具身智能算法实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;人型机器人工程师 (上海浦东/上海黄埔/杭州西湖)&lt;/td&gt;
          &lt;td&gt;请在猎聘搜索&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;Amazon Robotics Germany&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PostDoc Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;Qualcomm&lt;/td&gt;
          &lt;td&gt;Automotive Engineering Internship&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;Toyata US&lt;/td&gt;
          &lt;td&gt;Multimodal Learning&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;Waymo US&lt;/td&gt;
          &lt;td&gt;Planning Selection&lt;/td&gt;
          &lt;td&gt;PhD Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;Samsung US&lt;/td&gt;
          &lt;td&gt;Embodied Intelligence Intern&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.15&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;PhD Research Intern - Robotics and/or Autonomous Vehicles&lt;/td&gt;
          &lt;td&gt;Remote!&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.15&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;Principal Autonomous Vehicles Engineer - Mapping and Localization&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.13&lt;/td&gt;
          &lt;td&gt;Amazon Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;Intern/FullTime&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;字节跳动机器人&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;阿里巴巴达摩院视觉技术实验室&lt;/td&gt;
          &lt;td&gt;VLA方向&lt;/td&gt;
          &lt;td&gt;研究实习生&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;机器人/具身智能公司&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;机器人具身智能公司&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e5%85%ac%e5%8f%b8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;公司&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;小鹏机器人中心(北上广深北美)&lt;/td&gt;
          &lt;td&gt;多模态数据/灵巧操作&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;极佳视界(北上广深杭州武汉)&lt;/td&gt;
          &lt;td&gt;具身智能校园招聘&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;优必选(深圳)&lt;/td&gt;
          &lt;td&gt;感知算法/系统开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;晨昏线(深圳)&lt;/td&gt;
          &lt;td&gt;视觉感知/嵌入式/VLA/SLAM/强化学习/机械结构&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;深圳市大寰机器人&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;越疆机器人(深圳)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;普渡机器人(深圳/成都)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;北京术锐机器人&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;埃夫特机器人(吉林)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.11&lt;/td&gt;
          &lt;td&gt;星际光年(深圳)&lt;/td&gt;
          &lt;td&gt;灵巧操作算法&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.27&lt;/td&gt;
          &lt;td&gt;仁新机器人&lt;/td&gt;
          &lt;td&gt;机器人 AI算法工程师(5名)&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.25&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法量化工程师&lt;/td&gt;
          &lt;td&gt;实习/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.23&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;高级机械结构工程师/导航算法工程师/高级硬件工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.21&lt;/td&gt;
          &lt;td&gt;聆动通用&lt;/td&gt;
          &lt;td&gt;机器人运动控制/机器人视觉感知/机械臂运动规划&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.18&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;强化学习算法/量化工程师&lt;/td&gt;
          &lt;td&gt;实习/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.17&lt;/td&gt;
          &lt;td&gt;人形机器人(上海)有限公司&lt;/td&gt;
          &lt;td&gt;算法工程师/软件工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.15&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;具身大模型算法工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.30&lt;/td&gt;
          &lt;td&gt;首形科技&lt;/td&gt;
          &lt;td&gt;机械工程师/嵌入式软件工程师/算法工程师（大模型多模态交互方向）&lt;/td&gt;
          &lt;td&gt;2026届招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.29&lt;/td&gt;
          &lt;td&gt;上海市人形机器人创新孵化器&lt;/td&gt;
          &lt;td&gt;具身智能算法工程师/机器人装配工程师/机械结构设计等&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.26&lt;/td&gt;
          &lt;td&gt;穹彻智能(上海)&lt;/td&gt;
          &lt;td&gt;视觉/三维重建/位姿估计/模仿学习/视频理解&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.26&lt;/td&gt;
          &lt;td&gt;Infermove&lt;/td&gt;
          &lt;td&gt;规划算法负责人/规划算法工程师/资深全栈开发工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.19&lt;/td&gt;
          &lt;td&gt;智平方&lt;/td&gt;
          &lt;td&gt;算法/工程/产品/硬件/设计/生产岗&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.17&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;多模态强化学习算法工程师/具身智能大模型算法工程师/多模态数据算法工程师&lt;/td&gt;
          &lt;td&gt;社招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.16&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;具身智能大模型算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.16&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;多模态强化学习算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.16&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;多模态数据算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.12&lt;/td&gt;
          &lt;td&gt;智拓科技&lt;/td&gt;
          &lt;td&gt;具身智能工程师&lt;/td&gt;
          &lt;td&gt;全职/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.7&lt;/td&gt;
          &lt;td&gt;聆动通用机器人&lt;/td&gt;
          &lt;td&gt;研究算法类/研发类&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.6&lt;/td&gt;
          &lt;td&gt;云鲸智能具身智能团队&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.4&lt;/td&gt;
          &lt;td&gt;零次方机器人&lt;/td&gt;
          &lt;td&gt;机械工程师/硬件工程师/仿真工程师/具身数据工程师/深度强化学习算法工程师等&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.3&lt;/td&gt;
          &lt;td&gt;智身科技&lt;/td&gt;
          &lt;td&gt;解决方案工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.30&lt;/td&gt;
          &lt;td&gt;诺亦腾机器人Noitom Robotics&lt;/td&gt;
          &lt;td&gt;算法研究员/具身智能系统应用开发工程师/结构工程师&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.30&lt;/td&gt;
          &lt;td&gt;微分智飞&lt;/td&gt;
          &lt;td&gt;强化学习算法工程师/机器人大模型算法工程师/机器人系统工程师&lt;/td&gt;
          &lt;td&gt;2026校招/全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.29&lt;/td&gt;
          &lt;td&gt;非夕科技&lt;/td&gt;
          &lt;td&gt;机器人应用&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.27&lt;/td&gt;
          &lt;td&gt;光轮智能&lt;/td&gt;
          &lt;td&gt;机器人仿真框架用户测试&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.23&lt;/td&gt;
          &lt;td&gt;千寻智能 Spirit Al&lt;/td&gt;
          &lt;td&gt;具身智能算法工程师/机器学习系统工程师/机器学习平台后端工程师&lt;/td&gt;
          &lt;td&gt;2026应届校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.22&lt;/td&gt;
          &lt;td&gt;小鹏机器人&lt;/td&gt;
          &lt;td&gt;Research Scientist/Research Intern&lt;/td&gt;
          &lt;td&gt;实习+校/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.21&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;算法、研发、产品、运营、解决方案、职能/支持&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;忆生科技&lt;/td&gt;
          &lt;td&gt;三维重建与生成算法工程师/多模态算法工程师/机器人遥操作系统开发工程师&lt;/td&gt;
          &lt;td&gt;2026届校招/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;大模型类/算法类/软件系统类/测试类/其它&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;银河通用机器人&lt;/td&gt;
          &lt;td&gt;具身多模态大模型/具身智能操作算法/人形强化学习控制/机器人规划与控制&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.16&lt;/td&gt;
          &lt;td&gt;原力灵机&lt;/td&gt;
          &lt;td&gt;具身智能大模型算法研究员/具身智能强化学习算法研究员/机器人系统算法工程师&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.13&lt;/td&gt;
          &lt;td&gt;Dyna Robotics&lt;/td&gt;
          &lt;td&gt;机械工程师/高级机器人工程师/机器学习工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.13&lt;/td&gt;
          &lt;td&gt;宇树机器人&lt;/td&gt;
          &lt;td&gt;操作员&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.12&lt;/td&gt;
          &lt;td&gt;PaXini&lt;/td&gt;
          &lt;td&gt;具身智能算法/机器人智能系统/运动控制算法&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.12&lt;/td&gt;
          &lt;td&gt;河南具身智能产业发展有限公司&lt;/td&gt;
          &lt;td&gt;招聘10人&lt;/td&gt;
          &lt;td&gt;国企&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.11&lt;/td&gt;
          &lt;td&gt;舞肌科技&lt;/td&gt;
          &lt;td&gt;具身大模型强化学习算法工程师/大模型部署工程师&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.7&lt;/td&gt;
          &lt;td&gt;自变量机器人&lt;/td&gt;
          &lt;td&gt;多模态生成算法工程师/3D生成算法工程师/语音算法工程师/运动控制算法工程师&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.31&lt;/td&gt;
          &lt;td&gt;小鹏机器人中心多模态智能部&lt;/td&gt;
          &lt;td&gt;具身多模态大模型/世界模型方向&lt;/td&gt;
          &lt;td&gt;社招/校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.30&lt;/td&gt;
          &lt;td&gt;原力灵机&lt;/td&gt;
          &lt;td&gt;具身智能大模型算法研究员/机器人系统算法工程师/具身智能传感器工程师&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.29&lt;/td&gt;
          &lt;td&gt;Sharpa&lt;/td&gt;
          &lt;td&gt;机器人机械工程师/机器人电子工程师/机器人系统工程师/触觉应用算法工程师等&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.29&lt;/td&gt;
          &lt;td&gt;智元机器人&amp;quot;优才计划&amp;quot;&lt;/td&gt;
          &lt;td&gt;真机强化学习(具身操作)/运动控制算法/感知/规控/力控算法/多模态大模型(VLA/VLM)&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.28&lt;/td&gt;
          &lt;td&gt;硅基方舟(杭州)&lt;/td&gt;
          &lt;td&gt;运动控制算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.28&lt;/td&gt;
          &lt;td&gt;跨维智能(深圳)&lt;/td&gt;
          &lt;td&gt;数据生成研发工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;汉宇晨星&lt;/td&gt;
          &lt;td&gt;机器人研发专家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;帕西尼感知科技（天津）有限公司&lt;/td&gt;
          &lt;td&gt;数据采集员/机器人助理工程师/视觉算法工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.26&lt;/td&gt;
          &lt;td&gt;厨芯科技&lt;/td&gt;
          &lt;td&gt;具身智能算法工程师（数据方向）&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.23&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;仿真开发工程师/硬件工程师/数据分析与挖掘工程师&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.22&lt;/td&gt;
          &lt;td&gt;珞石机器人&lt;/td&gt;
          &lt;td&gt;软件工程师/机械工程师/测试工程师/控制工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.22&lt;/td&gt;
          &lt;td&gt;灵心巧手/赛那德招聘&lt;/td&gt;
          &lt;td&gt;海外销售经理/具身智能大客户经理/机器人KA销售&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;深圳星际光年&lt;/td&gt;
          &lt;td&gt;灵巧手嵌入式软件工程师/灵巧手设计工程师(机械方向)/灵巧操作学习算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;RoboSense 2026&amp;quot;天才罗伯特&amp;quot;人才计划&lt;/td&gt;
          &lt;td&gt;硬件/算法/产品方向&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.17&lt;/td&gt;
          &lt;td&gt;苏州一星机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法岗/软件开发岗/硬件开放岗/GPU计算集群岗/产品岗&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.17&lt;/td&gt;
          &lt;td&gt;深庭纪&lt;/td&gt;
          &lt;td&gt;机器学习工程师/机器人运动控制工程师/高性能计算工程师/强化学习工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.10&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;机器人解决方案实习生/具身感知算法工程师/大语言模型算法实习生&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.9&lt;/td&gt;
          &lt;td&gt;一星机器人(苏州)&lt;/td&gt;
          &lt;td&gt;硬件开发/集群运维工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;大模型类/算法类/软件系统类/测试类/其它&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;Sharpa灵巧手(上海长宁)&lt;/td&gt;
          &lt;td&gt;机器人软件主架构师/强化学习&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;真友科技(武汉)&lt;/td&gt;
          &lt;td&gt;人形强化学习运动控制&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;加速进化(北京)&lt;/td&gt;
          &lt;td&gt;强化学习运动控制算法&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;自变量(深圳优先)&lt;/td&gt;
          &lt;td&gt;slam/导航/点云/传感器标定/VLN&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;RoboSense速腾聚创&lt;/td&gt;
          &lt;td&gt;研发技术类/产品管理类/制造技术类&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;知象光电&lt;/td&gt;
          &lt;td&gt;具身智能算法方向/产品研发方向/市场营销方向&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;深圳特修斯机器人有限公司新余研究所&lt;/td&gt;
          &lt;td&gt;机械/结构设计工程师、工业设计工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;源络科技&lt;/td&gt;
          &lt;td&gt;多模态算法工程师/机器人学习工程师/感知算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;众擎机器人科技&lt;/td&gt;
          &lt;td&gt;机器人本体研发/具身智能/运动控制/生产制造&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.6&lt;/td&gt;
          &lt;td&gt;星猿哲&lt;/td&gt;
          &lt;td&gt;视觉算法/运动算法/机器人仿真/机械设计研发工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.4&lt;/td&gt;
          &lt;td&gt;眸深智能&lt;/td&gt;
          &lt;td&gt;机器人工程师/机器人算法&lt;/td&gt;
          &lt;td&gt;全职/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.3&lt;/td&gt;
          &lt;td&gt;它石智航&lt;/td&gt;
          &lt;td&gt;机器人算法/机器人多模态感知/VLA/机器人SLAM等算法工程师&lt;/td&gt;
          &lt;td&gt;校招/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.3&lt;/td&gt;
          &lt;td&gt;妙动科技&lt;/td&gt;
          &lt;td&gt;硬件/机械/算法&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;银河通用机器人&lt;/td&gt;
          &lt;td&gt;具身大模型/灵巧手/机器人全身控制/足式强化学习/感知/仿真等算法工程师&lt;/td&gt;
          &lt;td&gt;社招/2026届实习生/暑期实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;干寻智能&lt;/td&gt;
          &lt;td&gt;机器人高级机械工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;干寻智能&lt;/td&gt;
          &lt;td&gt;机器人资深硬件工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;一星机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;苏州园区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;一星机器人&lt;/td&gt;
          &lt;td&gt;硬件开放工程师&lt;/td&gt;
          &lt;td&gt;苏州园区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.28&lt;/td&gt;
          &lt;td&gt;卓驭(大疆车载, 深圳)&lt;/td&gt;
          &lt;td&gt;SLAM/决策规划/大模型&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;银河通用&lt;/td&gt;
          &lt;td&gt;灵巧手操作/机器人全身规划/机器人规划/强化学习控制算法工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;宇树科技&lt;/td&gt;
          &lt;td&gt;机器人运动控制/深度强化学习/激光SLAM/AI算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.26&lt;/td&gt;
          &lt;td&gt;跨维（深圳）智能数字科技有限公司&lt;/td&gt;
          &lt;td&gt;机器人算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.26&lt;/td&gt;
          &lt;td&gt;穹彻智能(上海)&lt;/td&gt;
          &lt;td&gt;机械臂运动规划与控制&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;普渡机器人/零次方(深圳)&lt;/td&gt;
          &lt;td&gt;强化学习/运动控制算法/具身通用操作&lt;/td&gt;
          &lt;td&gt;实习/社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;唯实具身智能(北京)&lt;/td&gt;
          &lt;td&gt;步态算法工程师&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;莱福(北京)&lt;/td&gt;
          &lt;td&gt;机器人运动控制&lt;/td&gt;
          &lt;td&gt;实习/社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;逐际动力(北京/深圳)&lt;/td&gt;
          &lt;td&gt;具身大模型/RLHF/数据/后端/AI软件/Python后端/前端/人形全身控制/产品经理&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;同方鼎欣科技股份有限公司&lt;/td&gt;
          &lt;td&gt;具身机器人系统工程师&lt;/td&gt;
          &lt;td&gt;北京海淀区-全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;自变量机器人(深圳/北京)&lt;/td&gt;
          &lt;td&gt;强化学习算法工程师/多模态理解算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;逐际动力&lt;/td&gt;
          &lt;td&gt;RLHF强化学习/多模态具身大模型/具身仿真Benchmark&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;傅里叶智能&lt;/td&gt;
          &lt;td&gt;视觉感知工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.6&lt;/td&gt;
          &lt;td&gt;无界智慧&lt;/td&gt;
          &lt;td&gt;机器人操作工程师/医疗Agent开发工程师/机器人导航工程师/机器人硬件运动控制工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.5&lt;/td&gt;
          &lt;td&gt;深圳无界智慧&lt;/td&gt;
          &lt;td&gt;机器人导航/医疗Agent开发工程师/具身操作算法&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.4&lt;/td&gt;
          &lt;td&gt;无界智慧&lt;/td&gt;
          &lt;td&gt;操作算法/导航算法/运动控制&lt;/td&gt;
          &lt;td&gt;社招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.4&lt;/td&gt;
          &lt;td&gt;它石智航&lt;/td&gt;
          &lt;td&gt;机器人算法工程师-强化学习/机器人SLAM算法工程师/机器人算法工程师-决策规划&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.2&lt;/td&gt;
          &lt;td&gt;银河通用&lt;/td&gt;
          &lt;td&gt;具身智能课程研究员&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.22&lt;/td&gt;
          &lt;td&gt;安克创新(深圳/北京/上海/杭州)&lt;/td&gt;
          &lt;td&gt;具身智能系列岗位&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;PNDbotics(北京)&lt;/td&gt;
          &lt;td&gt;人形机器人与生成式AI&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;留形科技(深圳/香港)&lt;/td&gt;
          &lt;td&gt;三维重建/SLAM/VLA/定位/结构&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;它石智航(上海)&lt;/td&gt;
          &lt;td&gt;机器人运动控制/计算机视觉/感知算法/端到端/VLA算法/SLAM算法/机器人软件开发/底软开发/自动驾驶开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;智元机器人(北京/上海)&lt;/td&gt;
          &lt;td&gt;人形模仿学习/强化学习&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;小米机器人(北京亦庄)&lt;/td&gt;
          &lt;td&gt;双足人形&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;小雨机器人(北京)&lt;/td&gt;
          &lt;td&gt;RL/运控/VSLAM/软件开发&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;RoboScience(北京)&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;Light Robotics(上海/新加坡)&lt;/td&gt;
          &lt;td&gt;算法/软件/硬件/产品&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.28&lt;/td&gt;
          &lt;td&gt;地瓜机器人(北京)&lt;/td&gt;
          &lt;td&gt;VLA/多模态infra/机器人应用开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.26&lt;/td&gt;
          &lt;td&gt;无限工坊(上海)&lt;/td&gt;
          &lt;td&gt;具身智能算法研发&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.21&lt;/td&gt;
          &lt;td&gt;千觉机器人(上海)&lt;/td&gt;
          &lt;td&gt;触觉传感器/电机/算法/结构/材料/机械/强化学习/仿真/嵌入式软件工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;Industrial Next(苏州)&lt;/td&gt;
          &lt;td&gt;工业机器人多模态感知与自主决策算法/强化学习/模仿学习/VLA/仿真训练/实际落地验证&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;阿加犀智能科技(成都)&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;暑期实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.5&lt;/td&gt;
          &lt;td&gt;浙江人形机器人中心&lt;/td&gt;
          &lt;td&gt;VLA,灵巧手，操作，仿真急招&lt;/td&gt;
          &lt;td&gt;校招/社招/实习皆可&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.19&lt;/td&gt;
          &lt;td&gt;银河通用&lt;/td&gt;
          &lt;td&gt;三维重建&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;星海图&lt;/td&gt;
          &lt;td&gt;VLA/算法/Infra/三维重建/机械&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;DJI大疆&lt;/td&gt;
          &lt;td&gt;图像算法&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;DJI大疆&lt;/td&gt;
          &lt;td&gt;数据闭环工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;银河通用(北京)&lt;/td&gt;
          &lt;td&gt;足式控制&lt;/td&gt;
          &lt;td&gt;全职急招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;傅利叶智能(上海)&lt;/td&gt;
          &lt;td&gt;具身智能VLA多模态大模型&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;灵初智能&lt;/td&gt;
          &lt;td&gt;2025春季招聘&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;逐际动力(北京/深圳)&lt;/td&gt;
          &lt;td&gt;具身算法/强化学习工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;众擎机器人&lt;/td&gt;
          &lt;td&gt;运动控制&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;大模型算法/电机控制&lt;/td&gt;
          &lt;td&gt;校招/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;石头科技&lt;/td&gt;
          &lt;td&gt;运动控制/导航/视觉&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;云鲸智能&lt;/td&gt;
          &lt;td&gt;运筹优化/机械臂操作/3D感知&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;海康机器人&lt;/td&gt;
          &lt;td&gt;运动控制/计算机视觉&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;它石智航&lt;/td&gt;
          &lt;td&gt;运动控制/SLAM/感知&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;2025年春季社招&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;荣耀(北京)&lt;/td&gt;
          &lt;td&gt;机器人软件系统开发工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Neura Robotics (Germany)&lt;/td&gt;
          &lt;td&gt;Embodied AI&lt;/td&gt;
          &lt;td&gt;Intern/Expert&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;小鹏机器人&lt;/td&gt;
          &lt;td&gt;后训练/RL/reasoning/agentic llm/world model/humanoid vla/机器人仿真&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;自变量机器人(深圳/北京)&lt;/td&gt;
          &lt;td&gt;多模态大模型/数据筛选算法/运动控制&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;松应科技(上海/深圳/北京)&lt;/td&gt;
          &lt;td&gt;3D仿真引擎开发/强化学习/数据系统/系统测试和解决方案/产品经理&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;多模态大模型/空间智能算法/具身算法/强化学习算法&lt;/td&gt;
          &lt;td&gt;研究员/工程师/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;新松机器人(沈阳)&lt;/td&gt;
          &lt;td&gt;视觉算法工程师/具身智能专家/控制算法工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;国家地方共建人形机器人创新中心/人形机器人(上海)有限公司&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2025校招/社招集中招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;星尘智能&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2025校招/社招/实习集中招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.4&lt;/td&gt;
          &lt;td&gt;星尘智能&lt;/td&gt;
          &lt;td&gt;机器人交互研究实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;光轮智能&lt;/td&gt;
          &lt;td&gt;模仿学习/强化学习算法实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;梅卡曼德机器人&lt;/td&gt;
          &lt;td&gt;VLA全职工程师/实习&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.18&lt;/td&gt;
          &lt;td&gt;VLAI未来动力&lt;/td&gt;
          &lt;td&gt;强化学习全职工程师/实习/远程&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.17&lt;/td&gt;
          &lt;td&gt;NOETIX Robotics&lt;/td&gt;
          &lt;td&gt;算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.13&lt;/td&gt;
          &lt;td&gt;千寻智能&lt;/td&gt;
          &lt;td&gt;具身业务算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.13&lt;/td&gt;
          &lt;td&gt;艾欧智能&lt;/td&gt;
          &lt;td&gt;具身智能实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;北京启物科技&lt;/td&gt;
          &lt;td&gt;机器人算法实习/校招-操作/导航/仿真&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;银河通用人型机器人&lt;/td&gt;
          &lt;td&gt;强化学习or运动规划实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;星尘智能&lt;/td&gt;
          &lt;td&gt;具身智能算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;极佳科技&lt;/td&gt;
          &lt;td&gt;具身智能机器人算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;速腾聚创&lt;/td&gt;
          &lt;td&gt;多模态大模型算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;中科慧灵&lt;/td&gt;
          &lt;td&gt;VLA方向&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;比亚迪&lt;/td&gt;
          &lt;td&gt;人型机器人算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;国家地方共建人形机器人创新中心/人形机器人(上海)有限公司&lt;/td&gt;
          &lt;td&gt;具身大模型实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;清华大学&amp;amp;地瓜机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;逐际动力&lt;/td&gt;
          &lt;td&gt;具身大模型算法+物理仿真+视频生成+世界模型+运动控制实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;星海图许华哲组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;研究院/实验室&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究院实验室&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e9%99%a2%e5%ae%9e%e9%aa%8c%e5%ae%a4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;机构&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;光明实验室(深圳)&lt;/td&gt;
          &lt;td&gt;具身智能机器人研究人员/工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.28&lt;/td&gt;
          &lt;td&gt;江淮前沿技术协同创新中心&lt;/td&gt;
          &lt;td&gt;博士后一具身智能方向/智能机器人算法专家/多模态大模型研发工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.8&lt;/td&gt;
          &lt;td&gt;中国空间技术研究院CAST+AI·R人工智能专班&lt;/td&gt;
          &lt;td&gt;大模型算法研发岗/大模型数据治理岗/大模型应用系统开发岗/具身智能算法研发岗&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.18&lt;/td&gt;
          &lt;td&gt;华为制造部&lt;/td&gt;
          &lt;td&gt;视觉算法开发与高级应用工程师/具身智能机器人应用高级工程师&lt;/td&gt;
          &lt;td&gt;2026届博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.5&lt;/td&gt;
          &lt;td&gt;智源研究院&lt;/td&gt;
          &lt;td&gt;机器人系统/数据/多模态/具身智能等方向&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.1&lt;/td&gt;
          &lt;td&gt;智源&lt;/td&gt;
          &lt;td&gt;具身大模型研究员&lt;/td&gt;
          &lt;td&gt;社招/校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.31&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室具身智能中心&lt;/td&gt;
          &lt;td&gt;具身智能-AIGC青年研究员/仿真平台青年研究员/人体运动策略青年研究员/具身智能大模型青年研究员/足式机器人青年研究员/强化学习青年研究员/AIGC算法实习生&lt;/td&gt;
          &lt;td&gt;校招/实习/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;中国科学院空间应用工程与技术中心 空间实验技术研究室&lt;/td&gt;
          &lt;td&gt;空间具身智能系统/空间灵巧机构研发岗/传感器与感知算法开发&lt;/td&gt;
          &lt;td&gt;2025年招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;嘉陵江实验室&lt;/td&gt;
          &lt;td&gt;大模型工程师(含LLM 与时空模型)/嵌入式与实时系统工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.25&lt;/td&gt;
          &lt;td&gt;深圳人工智能与机器人研究院&lt;/td&gt;
          &lt;td&gt;大模型算法研究员/工程师、具身智能研究员/工程师、机器人导航算法研究员/工程师&lt;/td&gt;
          &lt;td&gt;2026届招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.24&lt;/td&gt;
          &lt;td&gt;中豫具身智能实验室&lt;/td&gt;
          &lt;td&gt;机械工程/自动化/机器人学等专业相关科研岗&lt;/td&gt;
          &lt;td&gt;博士研究生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;清华+智谱&lt;/td&gt;
          &lt;td&gt;AI基础模型研究/AI机器人技术研究/记忆机理与实时学习算法研究&lt;/td&gt;
          &lt;td&gt;博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;北京智源人工智能研究院&lt;/td&gt;
          &lt;td&gt;机器人系统/多模态/数据/信息检索与知识计算/具身智能&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;上海算法创新研究院/上海交大人工智能学院具身智能团队&lt;/td&gt;
          &lt;td&gt;机器人学习研究工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.4&lt;/td&gt;
          &lt;td&gt;北大银河通用&lt;/td&gt;
          &lt;td&gt;具身智能科研&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.3&lt;/td&gt;
          &lt;td&gt;IDEA机器人感知/VLA 研发招聘&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;中国信通院人工智能研究所&lt;/td&gt;
          &lt;td&gt;具身智能研究岗/具身智能实验室运营岗&lt;/td&gt;
          &lt;td&gt;全职/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;中国信息通信研究院&lt;/td&gt;
          &lt;td&gt;具身智能研究员&lt;/td&gt;
          &lt;td&gt;北京海淀区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.4&lt;/td&gt;
          &lt;td&gt;杭州湾具身智能创新中心&lt;/td&gt;
          &lt;td&gt;机器人数采实习生/机器人数据审核实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;三维重建&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室夏季招聘&lt;/td&gt;
          &lt;td&gt;包含具身岗位&lt;/td&gt;
          &lt;td&gt;领军科学家/青年科学家/PostDoc/工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;北京大学信息技术高等研究院MAII Lab&lt;/td&gt;
          &lt;td&gt;机器学习/具身智能&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.26&lt;/td&gt;
          &lt;td&gt;北京具身智能机器人创新中心&lt;/td&gt;
          &lt;td&gt;具身智能数据实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;上海算法创新研究院&amp;amp;上海交大人工智能学院&lt;/td&gt;
          &lt;td&gt;空间智能/具身智能&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.25&lt;/td&gt;
          &lt;td&gt;招商局先进院(深圳)&lt;/td&gt;
          &lt;td&gt;具身数据/多模态/动作捕捉/VSLAM/人体姿态估计/大模型训练/大模型数据工程/机械臂运动控制&lt;/td&gt;
          &lt;td&gt;全职/实习（可给香港身份）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.17&lt;/td&gt;
          &lt;td&gt;OpenDriveLab&lt;/td&gt;
          &lt;td&gt;RAP/PostDoc/机器人硬件工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;2026联培博士招生简章&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.8&lt;/td&gt;
          &lt;td&gt;深圳河套学院&lt;/td&gt;
          &lt;td&gt;24/25/26级各方向PhD&lt;/td&gt;
          &lt;td&gt;4月10日截止&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;深圳科创学院具身智能团队&lt;/td&gt;
          &lt;td&gt;机器人系统工程师/具身智能算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;上海期智研究院&lt;/td&gt;
          &lt;td&gt;全职研究员&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;智源研究院&lt;/td&gt;
          &lt;td&gt;端到端VLA&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;北京通用人工智能研究院(BIGAI)机器人团队&lt;/td&gt;
          &lt;td&gt;跨本体操作策略/异构多机任务规划&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;西安交大/优艾智合具身智能机器人联合研究院&lt;/td&gt;
          &lt;td&gt;海外优青/青年拔尖人才/博士后&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;复旦大学可信具身智能研究院&lt;/td&gt;
          &lt;td&gt;海外优青&lt;/td&gt;
          &lt;td&gt;博士后/助理教授/副教授/教授&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;17家国企/央企/研究院&lt;/td&gt;
          &lt;td&gt;具身智能招聘岗位汇总&lt;/td&gt;
          &lt;td&gt;具身智能之心(微信: AIDriver002)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.17&lt;/td&gt;
          &lt;td&gt;CVTE中央研究院机器人创新部&lt;/td&gt;
          &lt;td&gt;机器人具身智能算法实习生/全职&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;汽车/自动驾驶&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;汽车自动驾驶&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b1%bd%e8%bd%a6%e8%87%aa%e5%8a%a8%e9%a9%be%e9%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;公司&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;东风汽车研发总院(武汉)&lt;/td&gt;
          &lt;td&gt;模型训练/运控/机械设计&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.6&lt;/td&gt;
          &lt;td&gt;苏州博世XC事业部&lt;/td&gt;
          &lt;td&gt;自动驾驶高精地图算法工程师(两年以上智驾行业经验)&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.25&lt;/td&gt;
          &lt;td&gt;小鹏汽车&lt;/td&gt;
          &lt;td&gt;多模态理解/多模态生成/三维视觉/自动驾驶/大模型/CV/Audio/NLP&lt;/td&gt;
          &lt;td&gt;社招(急)/校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.14&lt;/td&gt;
          &lt;td&gt;华为车BU天才少年&lt;/td&gt;
          &lt;td&gt;自动驾驶世界模型/VLA/强化学习/并行训练/仿真/数据/多模态理解生成&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.11&lt;/td&gt;
          &lt;td&gt;小鹏汽车&lt;/td&gt;
          &lt;td&gt;人形机器人运动控制/VLA/VLN/灵巧手/多模态/大模型&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;自动驾驶与具身智能算法研究员 (VLA/具身方向)&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.1&lt;/td&gt;
          &lt;td&gt;华为&lt;/td&gt;
          &lt;td&gt;研发算法岗&lt;/td&gt;
          &lt;td&gt;应届生/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;橙子运力&lt;/td&gt;
          &lt;td&gt;规划算法工程师&lt;/td&gt;
          &lt;td&gt;正式/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;理想汽车『自动驾驶』&lt;/td&gt;
          &lt;td&gt;大模型/端到端/强化学习算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;机器人操作抓取/足式机器人强化学习/大模型强化学习/多模态/机器学习/感知/机器人算法工程师&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.23&lt;/td&gt;
          &lt;td&gt;蔚来&lt;/td&gt;
          &lt;td&gt;春季招聘&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;科大讯飞&lt;/td&gt;
          &lt;td&gt;自动驾驶工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;文远知行&lt;/td&gt;
          &lt;td&gt;春招&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;酷睿程(地平线大众合资)&lt;/td&gt;
          &lt;td&gt;生成式算法/规控/控制/SLAM&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.20&lt;/td&gt;
          &lt;td&gt;小马智行&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2025校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.20&lt;/td&gt;
          &lt;td&gt;地平线(北京/上海/南京)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2026春季实习生招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Hyundai (US)&lt;/td&gt;
          &lt;td&gt;Autonomous Driving&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;IAV GmbH (Germany)&lt;/td&gt;
          &lt;td&gt;Autonomous Driving: Path Planning in Unstructured Environments&lt;/td&gt;
          &lt;td&gt;Internship&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;其他（教育/投资/内容/医疗等）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;其他教育投资内容医疗等&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%b6%e4%bb%96%e6%95%99%e8%82%b2%e6%8a%95%e8%b5%84%e5%86%85%e5%ae%b9%e5%8c%bb%e7%96%97%e7%ad%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;机构&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;元枢智汇(上海)&lt;/td&gt;
          &lt;td&gt;AI数据开源社区运营&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室具身智能中心&lt;/td&gt;
          &lt;td&gt;具身开源社区运营&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;秦皇岛中秦智能装备有限公司(秦皇岛)&lt;/td&gt;
          &lt;td&gt;机械研发/机器人&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;长智具身智能(海南三亚)&lt;/td&gt;
          &lt;td&gt;销售代表/解决方案工程师/研发工程师/数采员&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;新生纪智能科技有限公司(德国 可远程)&lt;/td&gt;
          &lt;td&gt;FAE工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;西湖机器人(杭州)&lt;/td&gt;
          &lt;td&gt;强化学习/具身智能算法/虚幻引擎/VR/SLAM/机械结构/市场&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;南洋理工大学LinsLab(新加坡)&lt;/td&gt;
          &lt;td&gt;灵巧操作/VLA/世界模型&lt;/td&gt;
          &lt;td&gt;26Fall PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.19&lt;/td&gt;
          &lt;td&gt;深蓝学院&lt;/td&gt;
          &lt;td&gt;机器人算法实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.14&lt;/td&gt;
          &lt;td&gt;深蓝学院&lt;/td&gt;
          &lt;td&gt;人工智能教育产品经理/人工智能教研&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.18&lt;/td&gt;
          &lt;td&gt;Motphys&lt;/td&gt;
          &lt;td&gt;初级引擎开发工程师/具身智能场景美术/具身智能技术应用工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.10&lt;/td&gt;
          &lt;td&gt;Motphys(武汉)&lt;/td&gt;
          &lt;td&gt;仿真解决方案/工具链/视觉感知/仿真资产制作/产品经理/仿真训练/物理引擎开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.30&lt;/td&gt;
          &lt;td&gt;NVIDIA（北京）&lt;/td&gt;
          &lt;td&gt;三维重建/世界模型/VLM/VLA&lt;/td&gt;
          &lt;td&gt;Solution Architect全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.26&lt;/td&gt;
          &lt;td&gt;中金公司&lt;/td&gt;
          &lt;td&gt;AI&amp;amp;机器人产业研究&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.17&lt;/td&gt;
          &lt;td&gt;辉羲智能&lt;/td&gt;
          &lt;td&gt;AI编译器工程师/专家、NPU算子开发工程师/专家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.17&lt;/td&gt;
          &lt;td&gt;Leading Future&lt;/td&gt;
          &lt;td&gt;多模态大模型科学家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.11&lt;/td&gt;
          &lt;td&gt;凤麟核集团&lt;/td&gt;
          &lt;td&gt;具身智能机器人项目经理/AI医疗产品经理/AI医学影像软件工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;具身智能内容运营(深圳)&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;机器人数据工程师(深圳)&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;深度学习平台工程师(深圳)&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;腿足/人形全身控制算法专家(深圳)&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;科研采购工程师(深圳)&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;深度学习推理工程师(深圳)&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.19&lt;/td&gt;
          &lt;td&gt;自动驾驶之心&lt;/td&gt;
          &lt;td&gt;内容运营(自驾/大模型/具身相关研究方向)&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.16&lt;/td&gt;
          &lt;td&gt;Leading Future&lt;/td&gt;
          &lt;td&gt;多模态大模型科学家（Embodied AI / Robotics Foundation Model）&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.15&lt;/td&gt;
          &lt;td&gt;深蓝学院&lt;/td&gt;
          &lt;td&gt;机器人算法实习生/机械臂研发实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.9&lt;/td&gt;
          &lt;td&gt;智子力控(宁波)&lt;/td&gt;
          &lt;td&gt;机器人电气与控制工程师(ROS和柔顺控制)&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.7&lt;/td&gt;
          &lt;td&gt;稳正资产&lt;/td&gt;
          &lt;td&gt;智能硬件投资总监/具身智能投资总监&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.1&lt;/td&gt;
          &lt;td&gt;天奇股份&lt;/td&gt;
          &lt;td&gt;机器人视觉工程师/机器人工程师/机器人销售经理/机器人数据采集工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.28&lt;/td&gt;
          &lt;td&gt;亮源新创(北京/上海/深圳)&lt;/td&gt;
          &lt;td&gt;产品经理&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.26&lt;/td&gt;
          &lt;td&gt;中能坤域科技控股(浙江)有限公司&lt;/td&gt;
          &lt;td&gt;具身智能/产品研发/产品运营等相关岗位&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.16&lt;/td&gt;
          &lt;td&gt;小米(北京)&lt;/td&gt;
          &lt;td&gt;机器人多模态大模型研究专家&lt;/td&gt;
          &lt;td&gt;急招全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;稳正资产&lt;/td&gt;
          &lt;td&gt;智能硬件投资总监/具身智能投资总监&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;机器人算法实习生(具身智能方向)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;小米(北京)&lt;/td&gt;
          &lt;td&gt;具身智能/世界模型&lt;/td&gt;
          &lt;td&gt;暑期实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;具身智能之心&lt;/td&gt;
          &lt;td&gt;课程讲师/硬件开发者&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;具身研习社&lt;/td&gt;
          &lt;td&gt;具身智能机器人深度内容作者&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.21&lt;/td&gt;
          &lt;td&gt;中金研究院(北京)&lt;/td&gt;
          &lt;td&gt;具身智能产业研究&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.20&lt;/td&gt;
          &lt;td&gt;北京中关村学院&lt;/td&gt;
          &lt;td&gt;具身智能方向&lt;/td&gt;
          &lt;td&gt;研究员/工程师/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;北京中关村学院&lt;/td&gt;
          &lt;td&gt;全球招募副院长/助理院长&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;中关村人工智能研究院&lt;/td&gt;
          &lt;td&gt;2025年超能实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;音波迭代Embodied Pioneering&lt;/td&gt;
          &lt;td&gt;一级市场具身智能方向&lt;/td&gt;
          &lt;td&gt;投资实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;传统运动控制/强化学习运动控制/SLAM&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;海尔&lt;/td&gt;
          &lt;td&gt;SLAM/运动控制/抓取/视觉&lt;/td&gt;
          &lt;td&gt;算法工程师/算法总监&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;汇川技术&lt;/td&gt;
          &lt;td&gt;传统运动控制/仿真/电机控制&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;国金证券具身智能组&lt;/td&gt;
          &lt;td&gt;人形机器人板块&lt;/td&gt;
          &lt;td&gt;实习生(可远程可留用)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;17家国企/央企/研究院岗位包括：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上海电气中央研究院 - 具身智能机器人算法/增材仿真/模拟计算/优化算法 - 校招&lt;/li&gt;
&lt;li&gt;中国电信人工智能研究院(TeleAI) - 具身智能大模型/具身仿真/具身硬件/机器人操作控制/具身感知与规划/灵巧操作/嵌入式开发 - 校招&lt;/li&gt;
&lt;li&gt;中兵智能创新研究院 - 机器人算法研究员&lt;/li&gt;
&lt;li&gt;中兴通讯 - 强化学习/视觉大模型/人形机器人机电 - 校招&lt;/li&gt;
&lt;li&gt;中国移动具身智能产业创新中心&lt;/li&gt;
&lt;li&gt;西安航天自动化 - 机器人开发工程师 - 校招&lt;/li&gt;
&lt;li&gt;国核电站运行服务技术有限公司 - 机器人电气工程师&lt;/li&gt;
&lt;li&gt;中科航天人才服务有限公司 - 人形机器人研发总监&lt;/li&gt;
&lt;li&gt;南方海洋科学与工程广东省实验室 - 水下机器人运动控制&lt;/li&gt;
&lt;li&gt;新兴际华北京智研院 - 智能机器人专业总工/总师&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>具身调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-14/</guid>
      <description>
        
        
        &lt;h1&gt;具身调研&lt;/h1&gt;&lt;p&gt;对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。&lt;/p&gt;
&lt;p&gt;本调研主要基于**&lt;a href=&#34;https://github.com/jiangranlv/embodied-ai-start&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PKU EPIC Lab&lt;/a&gt;&lt;strong&gt;、&lt;/strong&gt;&lt;a href=&#34;https://github.com/TianxingChen/Embodied-AI-Guide&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lumina具身智能社区&lt;/a&gt;**&lt;/p&gt;
&lt;h2&gt;一、基础概念 (Basic Concepts)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一基础概念-basic-concepts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1、 什么是具身智能&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-什么是具身智能&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。&lt;/p&gt;
&lt;p&gt;典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心特征：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有多模态感知能力（视觉、触觉、语音等）&lt;/li&gt;
&lt;li&gt;能够执行动作并影响环境&lt;/li&gt;
&lt;li&gt;学习可以通过与环境交互而不仅仅是被动监督完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 具身智能与其他AI的区别&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-具身智能与其他ai的区别&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96ai%e7%9a%84%e5%8c%ba%e5%88%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;如何scale up机器人数据？&lt;/strong&gt; 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？&lt;/strong&gt; 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 研究具身智能的核心原则 (Core Principles)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-研究具身智能的核心原则-core-principles&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；&lt;/p&gt;
&lt;p&gt;但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）&lt;/p&gt;
&lt;p&gt;必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？&lt;/p&gt;
&lt;h2&gt;二、AI and Robotics Basis&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二ai-and-robotics-basis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cai-and-robotics-basis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-Embodied-AI&lt;/strong&gt;
王鹤老师《具身智能导论》，找找类似课程替代&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-CV&lt;/strong&gt;
Stanford CS231N&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning (CS285)&lt;/strong&gt;
Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;三、研究平台与工具&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三研究平台与工具&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Simulation Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-simulation-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-simulation-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;2. Robot Platform&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-platform&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-platform&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;3. Daily ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-daily-arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-daily-arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了：
具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jiangranlv/robotics_arXiv_daily&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jiangranlv/robotics_arXiv_daily&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;四、Research Field on Robots&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四research-field-on-robots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bresearch-field-on-robots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Grasping&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-grasping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-grasping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。&lt;/p&gt;
&lt;p&gt;常见任务有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single object grasping（单物体抓取）&lt;/strong&gt;：抓取一个物体，物体通常放在桌子上。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clutter scene grasping（堆叠场景抓取）&lt;/strong&gt;：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional grasping（带语义抓取）&lt;/strong&gt;：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用机械手末端有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suction cup（吸盘）&lt;/strong&gt;：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel gripper（平行夹抓）&lt;/strong&gt;：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-fingered hand（多指手）&lt;/strong&gt;，又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-loop methods（开环执行）&lt;/strong&gt;：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closed-loop methods（闭环执行）&lt;/strong&gt;：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Manipulation&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-manipulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-manipulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。&lt;/p&gt;
&lt;p&gt;**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）
**Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。
**Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。
**Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。
**Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。
**Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同&lt;/p&gt;
&lt;h3&gt;3. Navigation(NOW)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-navigationnow&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-navigationnow&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。&lt;/p&gt;
&lt;p&gt;常见任务包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Point Goal Navigation (PointNav)&lt;/strong&gt;：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Goal Navigation (ObjectNav)&lt;/strong&gt;：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Navigation (VLN)&lt;/strong&gt;：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embodied Question Answering (EQA)&lt;/strong&gt;：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：机器人持续感知并跟随动态目标（如人或移动物体）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map-based Navigation&lt;/strong&gt;：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompting-Large-Model Navigation&lt;/strong&gt;：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video-based VLM Navigation&lt;/strong&gt;：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unified Embodied Navigation&lt;/strong&gt;：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Locomotion&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-locomotion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-locomotion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。&lt;/p&gt;
&lt;p&gt;技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning from manually designed reward&lt;/strong&gt; (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from human data&lt;/strong&gt; (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】
如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -&amp;gt; whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下一个阶段会涉及到一点除了control之外的东西，就是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引入【视觉】实现户外自主化（perceptive locomotion）&lt;/strong&gt;；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入【物体】实现loco-manipulation&lt;/strong&gt;；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对上述两种task的组合&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调【语义的泛化性】&lt;/strong&gt;，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调一些特殊的capability&lt;/strong&gt;（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;五、Learning based Research Field&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五learning-based-research-field&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94learning-based-research-field&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Few-shot Imitation Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-few-shot-imitation-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-few-shot-imitation-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：Behavior Cloning、DAgger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当前主流方法&lt;/strong&gt;：ACT、Diffusion Policy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。&lt;/p&gt;
&lt;h3&gt;2. Robot Foundation Model&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-foundation-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-foundation-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。
目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenVLA&lt;/strong&gt;：第一个开源且易于follow的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi0 / Pi0.5&lt;/strong&gt;：目前公认最work的VLA，10K+ hours teleop data训练的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraspVLA&lt;/strong&gt;：基于纯仿真数据的抓取任务的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有少量工作没有借助VLM&lt;/strong&gt;，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Sim-to-Real Reinforcement Learning (Distillation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-sim-to-real-reinforcement-learning-distillation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-sim-to-real-reinforcement-learning-distillation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。&lt;/p&gt;
&lt;p&gt;目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。&lt;/p&gt;
&lt;p&gt;核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。&lt;/p&gt;
&lt;h3&gt;4. Real-World Reinforcement Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-real-world-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-real-world-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-world RL 指直接在现实环境中进行探索式学习。&lt;/p&gt;
&lt;p&gt;这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**从零开始的真实世界强化学习：**Hil-Serl&lt;/li&gt;
&lt;li&gt;**基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. World Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-world-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-world-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代表性工作包括 Dreamer 系列&lt;/strong&gt;（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。&lt;/p&gt;
&lt;h2&gt;六、相关领域&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六相关领域&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Graphics&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-graphics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-graphics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。&lt;/p&gt;
&lt;p&gt;**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等
**Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。
**系统性学习图形学推荐课程：**Games 101, 103&lt;/p&gt;
&lt;h3&gt;2. Hardware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tele-operation（遥操作）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。
**主从臂系统：**如 Gello，实现高精度的力控遥操作。
**可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。
&lt;strong&gt;Sensors（传感器）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。&lt;/li&gt;
&lt;li&gt;**Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。&lt;/li&gt;
&lt;li&gt;**Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。&lt;/li&gt;
&lt;li&gt;**Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mainstream Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-mainstream-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-mainstream-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Transformer&lt;/li&gt;
&lt;li&gt;Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Foundation Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-foundation-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-foundation-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。&lt;/li&gt;
&lt;li&gt;CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。&lt;/li&gt;
&lt;li&gt;VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. 3D Vision&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-3d-vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-3d-vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维生成与重建&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。&lt;/li&gt;
&lt;li&gt;**单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。&lt;/li&gt;
&lt;li&gt;**单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。&lt;/li&gt;
&lt;li&gt;**位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维表示&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。&lt;/li&gt;
&lt;li&gt;**点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。&lt;/li&gt;
&lt;li&gt;**Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维理解&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>复现 SocialNav-Map</title>
      <link>http://localhost:1313/blog/2025/2025-12-17-socialnav-map-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-17-socialnav-map-1/</guid>
      <description>
        
        
        &lt;p&gt;接下来复现 &lt;strong&gt;SocialNav-Map&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt; /etc/network_turbo
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda install -n base -c conda-forge mamba
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;~/miniconda3/bin/mamba install habitat-sim&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0.3.1 withbullet headless -c conda-forge -c aihabitat
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; SocialNav-Map/Falcon
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -e habitat-lab -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -e habitat-baselines -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;然后是麻烦的&lt;strong&gt;数据这一块&lt;/strong&gt;，在 &lt;strong&gt;autodl&lt;/strong&gt; 一定要放在 &lt;strong&gt;autodl-fs&lt;/strong&gt; 里&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 在 autodl-fs 里创建一个专门存放这个项目数据的文件夹&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p /root/autodl-fs/SocialNavData
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 建立软链接：把 autodl-fs 的文件夹映射到当前目录的 data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ln -s /root/autodl-fs/SocialNavData data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 验证一下&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ls -l data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 输出应该显示 data -&amp;gt; /root/autodl-fs/SocialNavData&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 现在，往 Falcon/data 里下载的任何东西，实际上都会存进那是 200GB 的 autodl-fs 里，不用担心爆盘&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;然后经典 &lt;strong&gt;libEGL.so.1 缺失&lt;/strong&gt;，habitat-sim 启动前找不到 EGL&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Traceback&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;recent&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;call&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/root/miniconda3/envs/falcon/lib/python3.9/runpy.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;188&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_run_module_as_main&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mod_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mod_spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;code&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_get_module_details&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mod_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_Error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/root/miniconda3/envs/falcon/lib/python3.9/runpy.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;111&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_get_module_details&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;__import__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pkg_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/root/miniconda3/envs/falcon/lib/python3.9/site-packages/habitat_sim-0.3.1-py3.9-linux-x86_64.egg/habitat_sim/__init__.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;module&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;habitat_sim._ext.habitat_sim_bindings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ne&#34;&gt;ImportError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;libEGL&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;so&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cannot&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shared&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;object&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;No&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;such&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;directory&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;虽然 &lt;strong&gt;nvidia-smi&lt;/strong&gt; 显示驱动正常（这是内核层面的），但 Python 环境或系统缺少用户空间的 &lt;strong&gt;EGL 接口库 (libEGL.so.1)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;libEGL.so.1&lt;/strong&gt; 通常是一个分发器（Dispatcher）。它的工作是指挥程序去调用真正的后端驱动（在你的情况下是 NVIDIA 驱动）。如果没有这个文件，程序就不知道如何去&amp;quot;握手&amp;quot;并调用显卡。&lt;/p&gt;
&lt;p&gt;为了避免 fallback 到纯软件渲染，我们可以通过安装与厂商无关的调度库 &lt;strong&gt;(GLVND)&lt;/strong&gt; 来解决。这不是 Mesa 软件渲染，而是让系统能正确识别并调用 NVIDIA 硬件的标准接口。&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;apt update &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y libgl1 libglvnd0 libglx0 libegl1&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;安装 &lt;strong&gt;libglvnd（GL Vendor-Neutral Dispatch）&lt;/strong&gt; 提供 libEGL.so.1，并会自动检测并使用 NVIDIA 驱动，不会强制变为 Mesa 软件渲染。此时我们再全局查找 &lt;code&gt;find / -name &amp;quot;libEGL.so*&amp;quot; 2&amp;gt;/dev/null&lt;/code&gt; 就有了：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/usr/lib/x86_64-linux-gnu/libEGL.so.1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/usr/lib/x86_64-linux-gnu/libEGL.so.1.1.0&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;然后用 &lt;code&gt;python -m habitat_sim.utils.datasets_download --list&lt;/code&gt; 查看数据源：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;No data-path provided, defaults to: ./data. Use &amp;#39;--data-path&amp;#39; to specify another location.
Note, ./data is a symbolic link that points to /autodl-fs/data/SocialNavData.
====================================
Currently available datasources are:
------------------------------------
hssd-hab
hab3-episodes
hssd-raw
hssd-hab_internal
hssd-hab_objectnav_dataset
ai2thor-hab
procthor-hab_objectnav_dataset
habitat_test_scenes
habitat_test_pointnav_dataset
habitat_example_objects
locobot_merged
mp3d_example_scene
coda_scene
webxr_hand_demo
replica_cad_dataset
replica_cad_baked_lighting
ycb
franka_panda
hab_spot_arm
hab_stretch
hab_fetch
habitat_humanoids
rearrange_pick_dataset_v0
rearrange_dataset_v1
hab2_bench_assets
hab3_bench_assets
hm3d_minival_glb_v0.1
hm3d_minival_glb_v0.2
hm3d_minival_habitat_v0.1
hm3d_minival_habitat_v0.2
hm3d_minival_configs_v0.1
hm3d_minival_configs_v0.2
hm3d_train_glb_v0.1
hm3d_train_glb_v0.2
hm3d_train_habitat_v0.1
hm3d_train_habitat_v0.2
hm3d_train_configs_v0.1
hm3d_train_configs_v0.2
hm3d_val_glb_v0.1
hm3d_val_glb_v0.2
hm3d_val_habitat_v0.1
hm3d_val_habitat_v0.2
hm3d_val_configs_v0.1
hm3d_val_configs_v0.2
hm3d_example_glb
hm3d_example_habitat
hm3d_example_configs
hm3d_minival_semantic_annots_v0.1
hm3d_minival_semantic_annots_v0.2
hm3d_minival_semantic_configs_v0.1
hm3d_minival_semantic_configs_v0.2
hm3d_train_semantic_annots_v0.1
hm3d_train_semantic_annots_v0.2
hm3d_train_semantic_configs_v0.1
hm3d_train_semantic_configs_v0.2
hm3d_val_semantic_annots_v0.1
hm3d_val_semantic_annots_v0.2
hm3d_val_semantic_configs_v0.1
hm3d_val_semantic_configs_v0.2
hm3d_example_semantic_annots
hm3d_example_semantic_configs
====================================
Currently available datagroups are:
------------------------------------
(&amp;#39;ci_test_assets&amp;#39;, [&amp;#39;habitat_test_scenes&amp;#39;, &amp;#39;habitat_test_pointnav_dataset&amp;#39;, &amp;#39;habitat_example_objects&amp;#39;, &amp;#39;locobot_merged&amp;#39;, &amp;#39;mp3d_example_scene&amp;#39;, &amp;#39;coda_scene&amp;#39;, &amp;#39;replica_cad_dataset&amp;#39;, &amp;#39;hab_fetch&amp;#39;, &amp;#39;hab_stretch&amp;#39;, &amp;#39;hab_spot_arm&amp;#39;, &amp;#39;hm3d_example&amp;#39;, &amp;#39;hm3d_example_habitat&amp;#39;, &amp;#39;hm3d_example_configs&amp;#39;, &amp;#39;hm3d_example_semantic_annots&amp;#39;, &amp;#39;hm3d_example_semantic_configs&amp;#39;])
(&amp;#39;rearrange_task_assets&amp;#39;, [&amp;#39;replica_cad_dataset&amp;#39;, &amp;#39;hab_fetch&amp;#39;, &amp;#39;ycb&amp;#39;, &amp;#39;rearrange_pick_dataset_v0&amp;#39;, &amp;#39;rearrange_dataset_v1&amp;#39;])
(&amp;#39;hm3d_example&amp;#39;, [&amp;#39;hm3d_example_habitat&amp;#39;, &amp;#39;hm3d_example_configs&amp;#39;, &amp;#39;hm3d_example_semantic_annots&amp;#39;, &amp;#39;hm3d_example_semantic_configs&amp;#39;])
(&amp;#39;hm3d_val_v0.1&amp;#39;, [&amp;#39;hm3d_val_habitat_v0.1&amp;#39;, &amp;#39;hm3d_val_configs_v0.1&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.1&amp;#39;])
(&amp;#39;hm3d_train_v0.1&amp;#39;, [&amp;#39;hm3d_train_habitat_v0.1&amp;#39;, &amp;#39;hm3d_train_configs_v0.1&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.1&amp;#39;])
(&amp;#39;hm3d_minival_v0.1&amp;#39;, [&amp;#39;hm3d_minival_habitat_v0.1&amp;#39;, &amp;#39;hm3d_minival_configs_v0.1&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.1&amp;#39;])
(&amp;#39;hm3d_semantics_v0.1&amp;#39;, [&amp;#39;hm3d_example_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_example_semantic_configs_v0.1&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.1&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.1&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.1&amp;#39;])
(&amp;#39;hm3d_val_v0.2&amp;#39;, [&amp;#39;hm3d_val_habitat_v0.2&amp;#39;, &amp;#39;hm3d_val_configs_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.2&amp;#39;])
(&amp;#39;hm3d_train_v0.2&amp;#39;, [&amp;#39;hm3d_train_habitat_v0.2&amp;#39;, &amp;#39;hm3d_train_configs_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.2&amp;#39;])
(&amp;#39;hm3d_minival_v0.2&amp;#39;, [&amp;#39;hm3d_minival_habitat_v0.2&amp;#39;, &amp;#39;hm3d_minival_configs_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.2&amp;#39;])
(&amp;#39;hm3d_semantics_v0.2&amp;#39;, [&amp;#39;hm3d_example_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_example_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.2&amp;#39;])
(&amp;#39;hm3d_v0.1&amp;#39;, [&amp;#39;hm3d_val_habitat_v0.1&amp;#39;, &amp;#39;hm3d_val_configs_v0.1&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.1&amp;#39;, &amp;#39;hm3d_train_habitat_v0.1&amp;#39;, &amp;#39;hm3d_train_configs_v0.1&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.1&amp;#39;, &amp;#39;hm3d_minival_habitat_v0.1&amp;#39;, &amp;#39;hm3d_minival_configs_v0.1&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.1&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.1&amp;#39;])
(&amp;#39;hm3d_full&amp;#39;, [&amp;#39;hm3d_minival_glb_v0.2&amp;#39;, &amp;#39;hm3d_minival_habitat_v0.2&amp;#39;, &amp;#39;hm3d_minival_configs_v0.2&amp;#39;, &amp;#39;hm3d_train_glb_v0.2&amp;#39;, &amp;#39;hm3d_train_habitat_v0.2&amp;#39;, &amp;#39;hm3d_train_configs_v0.2&amp;#39;, &amp;#39;hm3d_val_glb_v0.2&amp;#39;, &amp;#39;hm3d_val_habitat_v0.2&amp;#39;, &amp;#39;hm3d_val_configs_v0.2&amp;#39;, &amp;#39;hm3d_example_glb&amp;#39;, &amp;#39;hm3d_example_habitat&amp;#39;, &amp;#39;hm3d_example_configs&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_example_semantic_annots&amp;#39;, &amp;#39;hm3d_example_semantic_configs&amp;#39;])
(&amp;#39;hm3d_train_full&amp;#39;, [&amp;#39;hm3d_train_glb_v0.2&amp;#39;, &amp;#39;hm3d_train_habitat_v0.2&amp;#39;, &amp;#39;hm3d_train_configs_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.2&amp;#39;])
(&amp;#39;hm3d_val_full&amp;#39;, [&amp;#39;hm3d_val_glb_v0.2&amp;#39;, &amp;#39;hm3d_val_habitat_v0.2&amp;#39;, &amp;#39;hm3d_val_configs_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.2&amp;#39;])
(&amp;#39;hm3d_minival_full&amp;#39;, [&amp;#39;hm3d_minival_glb_v0.2&amp;#39;, &amp;#39;hm3d_minival_habitat_v0.2&amp;#39;, &amp;#39;hm3d_minival_configs_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.2&amp;#39;])
(&amp;#39;hm3d_example_full&amp;#39;, [&amp;#39;hm3d_example_glb&amp;#39;, &amp;#39;hm3d_example_habitat&amp;#39;, &amp;#39;hm3d_example_configs&amp;#39;, &amp;#39;hm3d_example_semantic_annots&amp;#39;, &amp;#39;hm3d_example_semantic_configs&amp;#39;])
(&amp;#39;hm3d&amp;#39;, [&amp;#39;hm3d_val_habitat_v0.2&amp;#39;, &amp;#39;hm3d_val_configs_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_val_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_train_habitat_v0.2&amp;#39;, &amp;#39;hm3d_train_configs_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_train_semantic_configs_v0.2&amp;#39;, &amp;#39;hm3d_minival_habitat_v0.2&amp;#39;, &amp;#39;hm3d_minival_configs_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_annots_v0.2&amp;#39;, &amp;#39;hm3d_minival_semantic_configs_v0.2&amp;#39;])
====================================&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;列表中其实只有两种东西：&lt;strong&gt;原子包（Parts）&lt;/strong&gt; 和 &lt;strong&gt;组合包（Groups）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当你指定 &lt;code&gt;uid = hm3d_minival_v0.2&lt;/code&gt; 时，脚本会自动帮你下载该组内的 &lt;strong&gt;4 个原子包&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;下载 Multi-agent necessary data&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;下载-multi-agent-necessary-data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8b%e8%bd%bd-multi-agent-necessary-data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt install git-lfs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -m habitat_sim.utils.datasets_download &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  --username XXX &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  --password XXX &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  --uids hab3-episodes habitat_humanoids hab3_bench_assets hab_spot_arm&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;遇到 huggingface.co 443 超时的话就 &lt;code&gt;source /etc/network_turbo&lt;/code&gt; 一下&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h2&gt;下载 Leg animation (腿部动作数据)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;下载-leg-animation-腿部动作数据&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8b%e8%bd%bd-leg-animation-%e8%85%bf%e9%83%a8%e5%8a%a8%e4%bd%9c%e6%95%b0%e6%8d%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p data/robots/spot_data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 都是在~/SocialNav-Map/Falcon路径下&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://github.com/facebookresearch/habitat-lab/files/12502177/spot_walking_trajectory.csv -O data/robots/spot_data/spot_walking_trajectory.csv&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;下载 Scene Datasets (场景数据)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;下载-scene-datasets-场景数据&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8b%e8%bd%bd-scene-datasets-%e5%9c%ba%e6%99%af%e6%95%b0%e6%8d%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Downloading HM3D v0.2&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;downloading-hm3d-v02&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#downloading-hm3d-v02&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;File Name&lt;/th&gt;
          &lt;th&gt;Data Set&lt;/th&gt;
          &lt;th&gt;Format&lt;/th&gt;
          &lt;th&gt;Link&lt;/th&gt;
          &lt;th&gt;Size&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-minival-glb-v0.2.tar&lt;/td&gt;
          &lt;td&gt;minival&lt;/td&gt;
          &lt;td&gt;glb&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-minival-glb-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-minival-glb-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;464M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-minival-habitat-v0.2.tar&lt;/td&gt;
          &lt;td&gt;minival&lt;/td&gt;
          &lt;td&gt;habitat&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-minival-habitat-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-minival-habitat-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;390M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-minival-semantic-annots-v0.2.tar&lt;/td&gt;
          &lt;td&gt;minival&lt;/td&gt;
          &lt;td&gt;semantic-annots&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-minival-semantic-annots-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-minival-semantic-annots-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;240.6M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-minival-semantic-configs-v0.2.tar&lt;/td&gt;
          &lt;td&gt;minival&lt;/td&gt;
          &lt;td&gt;semantic-configs&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-minival-semantic-configs-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-minival-semantic-configs-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;30K&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-train-glb-v0.2.tar&lt;/td&gt;
          &lt;td&gt;train&lt;/td&gt;
          &lt;td&gt;glb&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-train-glb-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-train-glb-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;32G&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-train-habitat-v0.2.tar&lt;/td&gt;
          &lt;td&gt;train&lt;/td&gt;
          &lt;td&gt;habitat&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-train-habitat-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-train-habitat-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;27G&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-train-semantic-annots-v0.2.tar&lt;/td&gt;
          &lt;td&gt;train&lt;/td&gt;
          &lt;td&gt;semantic-annots&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-train-semantic-annots-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-train-semantic-annots-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;8.1G&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-train-semantic-configs-v0.2.tar&lt;/td&gt;
          &lt;td&gt;train&lt;/td&gt;
          &lt;td&gt;semantic-configs&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-train-semantic-configs-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-train-semantic-configs-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;50K&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-val-glb-v0.2.tar&lt;/td&gt;
          &lt;td&gt;val&lt;/td&gt;
          &lt;td&gt;glb&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-val-glb-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-val-glb-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;4G&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-val-habitat-v0.2.tar&lt;/td&gt;
          &lt;td&gt;val&lt;/td&gt;
          &lt;td&gt;habitat&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-val-habitat-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-val-habitat-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;3.3G&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-val-semantic-annots-v0.2.tar&lt;/td&gt;
          &lt;td&gt;val&lt;/td&gt;
          &lt;td&gt;semantic-annots&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-val-semantic-annots-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-val-semantic-annots-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;2.0G&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-val-semantic-configs-v0.2.tar&lt;/td&gt;
          &lt;td&gt;val&lt;/td&gt;
          &lt;td&gt;semantic-configs&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.matterport.com/resources/habitat/hm3d-val-semantic-configs-v0.2.tar&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.matterport.com/resources/habitat/hm3d-val-semantic-configs-v0.2.tar&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;40K&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-example-glb-v0.2.tar&lt;/td&gt;
          &lt;td&gt;example&lt;/td&gt;
          &lt;td&gt;glb&lt;/td&gt;
          &lt;td&gt;hm3d-example-glb-v0.2.tar&lt;/td&gt;
          &lt;td&gt;186M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-example-habitat-v0.2.tar&lt;/td&gt;
          &lt;td&gt;example&lt;/td&gt;
          &lt;td&gt;habitat&lt;/td&gt;
          &lt;td&gt;hm3d-example-habitat-v0.2.tar&lt;/td&gt;
          &lt;td&gt;154M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-example-semantic-annots-v0.2.tar&lt;/td&gt;
          &lt;td&gt;example&lt;/td&gt;
          &lt;td&gt;semantic-annots&lt;/td&gt;
          &lt;td&gt;hm3d-example-semantic-annots-v0.2.tar&lt;/td&gt;
          &lt;td&gt;60M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;hm3d-example-semantic-configs-v0.2.tar&lt;/td&gt;
          &lt;td&gt;example&lt;/td&gt;
          &lt;td&gt;semantic-configs&lt;/td&gt;
          &lt;td&gt;hm3d-example-semantic-configs-v0.2.tar&lt;/td&gt;
          &lt;td&gt;30K&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别 (Split)&lt;/th&gt;
          &lt;th&gt;用途&lt;/th&gt;
          &lt;th&gt;包含场景数&lt;/th&gt;
          &lt;th&gt;总大小 (解压前)&lt;/th&gt;
          &lt;th&gt;详细组成 (核心文件)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Minival&lt;/td&gt;
          &lt;td&gt;代码调试/快速验证&lt;/td&gt;
          &lt;td&gt;20个&lt;/td&gt;
          &lt;td&gt;~1.1 GB&lt;/td&gt;
          &lt;td&gt;已下载 (GLB 464M + Nav 390M + Annots 240M)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Val&lt;/td&gt;
          &lt;td&gt;标准验证/评估模型&lt;/td&gt;
          &lt;td&gt;100个&lt;/td&gt;
          &lt;td&gt;~9.3 GB&lt;/td&gt;
          &lt;td&gt;GLB: 4G&lt;br /&gt;Habitat: 3.3G&lt;br /&gt;Semantics: 2.0G&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Train&lt;/td&gt;
          &lt;td&gt;大规模模型训练&lt;/td&gt;
          &lt;td&gt;800个&lt;/td&gt;
          &lt;td&gt;~67.1 GB&lt;/td&gt;
          &lt;td&gt;GLB: 32G&lt;br /&gt;Habitat: 27G&lt;br /&gt;Semantics: 8.1G&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;解压后加起来应该在 150GB&lt;/strong&gt; 那样子（按文档所说）&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -m habitat_sim.utils.datasets_download &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  --username XXX &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  --password XXX &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  --uids hm3d_minival_v0.2&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;这玩意下太慢了，对于 &lt;strong&gt;val&lt;/strong&gt; 的话 python 单线程我得下 &lt;strong&gt;5天&lt;/strong&gt;。而 &lt;strong&gt;Autodl 的带宽&lt;/strong&gt;就算用了 aria2 也不够，只能先在自己电脑上下（用 Github 给的&lt;a href=&#34;https://github.com/matterport/habitat-matterport-3dresearch&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;直链&lt;/a&gt;），然后传到 Autodl 上（它的&lt;strong&gt;入站带宽不小&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;这里先把 &lt;strong&gt;minival&lt;/strong&gt; 下下来。&lt;/p&gt;
&lt;p&gt;对于 &lt;a href=&#34;https://github.com/niessner/Matterport&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MP3D&lt;/a&gt;，我终于理解了，我向 TUM 申请的那个 &lt;code&gt;download_mp.py&lt;/code&gt;（只能用 &lt;strong&gt;python 2.7&lt;/strong&gt; 跑的）是 &lt;strong&gt;entire Matterport3D dataset&lt;/strong&gt;，那个是 &lt;strong&gt;1.3TB&lt;/strong&gt; 的，而针对 Habitat 仿真器的版本（即 &lt;code&gt;--task habitat&lt;/code&gt;）大小就只有 &lt;strong&gt;15 GB&lt;/strong&gt;，因为它剔除了巨大的原始 RGB-D 视频帧，只保留了重建好的 3D 模型。那个 1.3TB 的是做 &lt;strong&gt;3D 重建、超分、深度估计&lt;/strong&gt;等底层视觉任务用的原始数据。&lt;/p&gt;
&lt;p&gt;这里得给 &lt;strong&gt;TUM 发邮件&lt;/strong&gt;得到他们那个 download_mp.py 才可以，这里我因为早就发过了所以就跳过了，同样也是本地下好（&lt;strong&gt;3MB/s 左右&lt;/strong&gt;）然后再传上去。&lt;/p&gt;
&lt;h2&gt;下载 Episode Datasets (SocialNav 任务数据)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;下载-episode-datasets-socialnav-任务数据&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8b%e8%bd%bd-episode-datasets-socialnav-%e4%bb%bb%e5%8a%a1%e6%95%b0%e6%8d%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;作者给我们的&lt;a href=&#34;https://drive.google.com/drive/folders/1V0a8PYeMZimFcHgoJGMMTkvscLhZeKzD&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;链接&lt;/a&gt;有一个 &lt;strong&gt;2.24GB&lt;/strong&gt; 的 &lt;code&gt;social-hm3d.zip&lt;/code&gt;，一个 &lt;strong&gt;1.96GB&lt;/strong&gt; 的 &lt;code&gt;social-mp3d.zip&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 在 AutoDL 上下载 Google Drive 文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install gdown
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gdown --folder https://drive.google.com/drive/folders/1V0a8PYeMZimFcHgoJGMMTkvscLhZeKzD
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 这个因为 Google Drive 的文件夹抓取受到很多反爬虫限制，而且当文件夹权限设置为“仅查看”时，gdown 经常无法列出目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 这里以 social-mp3d.zip 为例，先拿到它的File ID：https://drive.google.com/file/d/16KGr9cae1z3ypfgeeDSd5wDnJTfu2HhK/view?usp=drive_link，即‘16KGr9cae1z3ypfgeeDSd5wDnJTfu2HhK’&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gdown 16KGr9cae1z3ypfgeeDSd5wDnJTfu2HhK -O social-mp3d.zip&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;gdown 两种方法都下不下来&lt;/strong&gt;，主要原因还是在于 &lt;strong&gt;Permission&lt;/strong&gt; 本身，建议在本地电脑下载好，然后通过 &lt;strong&gt;AutoDL 的 JupyterLab 网页端&lt;/strong&gt;或者 &lt;strong&gt;FileZilla&lt;/strong&gt; 上传到 &lt;code&gt;data/&lt;/code&gt; 目录。&lt;/p&gt;
&lt;p&gt;接着按照 &lt;strong&gt;README&lt;/strong&gt; 的结构解压：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 确保目录存在&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p datasets/pointnav
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 解压 social-hm3d.zip 到目标目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;unzip social-hm3d.zip -d datasets/pointnav/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 解压 social-mp3d.zip 到目标目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;unzip social-mp3d.zip -d datasets/pointnav/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 验证目录结构（可选）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ls -la datasets/pointnav/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 应该看到 social-hm3d/ 和 social-mp3d/ 两个目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 清理 zip 文件（可选，节省空间）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# rm social-hm3d.zip social-mp3d.zip&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;最后检查一下路径，惊觉自己居然把代码放在了&lt;strong&gt;系统盘&lt;/strong&gt;，于是赶紧迁移到&lt;strong&gt;数据盘&lt;/strong&gt;去：&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mv ~/SocialNav-Map /root/autodl-tmp/&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;好在之前的软链接用的是&lt;strong&gt;绝对路径&lt;/strong&gt;，移动 SocialNav-Map 文件夹时，里面的 &lt;code&gt;data&lt;/code&gt; 这个&amp;quot;快捷方式&amp;quot;文件被一起移动了，但它指向的目标地址（绝对路径）是写死在它里面的不会变。&lt;/p&gt;
&lt;p&gt;然而以&lt;strong&gt;可编辑模式安装的包&lt;/strong&gt;仍指向旧路径，导致导入失败，所以需要重新安装：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;habitat-lab&lt;/strong&gt; 已重新安装到新路径：&lt;code&gt;/root/autodl-tmp/SocialNav-Map/Falcon/habitat-lab&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;habitat-baselines&lt;/strong&gt; 已重新安装到新路径：&lt;code&gt;/root/autodl-tmp/SocialNav-Map/Falcon/habitat-baselines&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;code&gt;ModuleNotFoundError: No module named &#39;habitat_baselines.il.data.nav_data&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决&lt;/strong&gt;：手动把&lt;a href=&#34;https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/data/nav_data.py&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方库&lt;/a&gt;里的 &lt;code&gt;nav_data.py&lt;/code&gt; 创建过来。&lt;/p&gt;
&lt;p&gt;这之后又来一个 &lt;code&gt;ModuleNotFoundError: No module named &#39;habitat_baselines.il.data.data&#39;&lt;/code&gt;，同样把&lt;a href=&#34;https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/data/data.py&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;的 &lt;code&gt;data.py&lt;/code&gt; 挪过来。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>深度研究工作流（DRW）构建</title>
      <link>http://localhost:1313/blog/2025/2025-11-13-deep-research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-13-deep-research/</guid>
      <description>
        
        
        &lt;h1&gt;深度研究工作流（DRW）构建&lt;/h1&gt;&lt;h2&gt;I. 自主研究智能体的基础架构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;i-自主研究智能体的基础架构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#i-%e8%87%aa%e4%b8%bb%e7%a0%94%e7%a9%b6%e6%99%ba%e8%83%bd%e4%bd%93%e7%9a%84%e5%9f%ba%e7%a1%80%e6%9e%b6%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A. 深度研究工作流（DRW）范式的界定&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-深度研究工作流drw范式的界定&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-%e6%b7%b1%e5%ba%a6%e7%a0%94%e7%a9%b6%e5%b7%a5%e4%bd%9c%e6%b5%81drw%e8%8c%83%e5%bc%8f%e7%9a%84%e7%95%8c%e5%ae%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;传统的单体式大型语言模型（LLM）在执行多步骤、高复杂度且需长期记忆的学术研究任务时存在明显局限。单个模型同时承担规划、执行、检索和综合，往往缺乏维持状态、处理复杂决策以及精确纠错的机制。&lt;/p&gt;
&lt;p&gt;完整的文献综述任务包含“识别论文—逐篇阅读—交叉引用—合成结论”等数十个顺序与条件步骤。要让 LLM 从文本生成器转型为可执行复杂任务的自主系统，必须赋予其智能体（Agentic）能力，使其能够适应输入、调用外部工具并自主执行预设或自适应的工作流。&lt;/p&gt;
&lt;p&gt;在深度研究场景中，LLM 需要具备实时信息检索、任务进度管理以及失败后的自我恢复能力。因此，分层多智能体架构是 DRW 的必要条件，它模拟组织化的管理体系，将复杂任务拆解并交给专业化的子智能体，从而保证系统的专业性与鲁棒性。&lt;/p&gt;
&lt;p&gt;分层架构的关键优势包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;专业化与解耦&lt;/strong&gt;：避免单一智能体承担全部工作导致的效率低下与脆弱性，通过专职智能体（如 RAG 执行者）实现功能隔离。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;控制流与维护性&lt;/strong&gt;：提供清晰的委托规则、退避重试（backoff retries）与故障转移逻辑。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当外部工具调用失败时，总任务智能体可以决定重试、更换工具或标记论文失败后继续下一项，从而保障系统稳定与可维护性。&lt;/p&gt;
&lt;h3&gt;B. 本地 LLM：隐私、控制与成本效益&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;b-本地-llm隐私控制与成本效益&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#b-%e6%9c%ac%e5%9c%b0-llm%e9%9a%90%e7%a7%81%e6%8e%a7%e5%88%b6%e4%b8%8e%e6%88%90%e6%9c%ac%e6%95%88%e7%9b%8a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;“本地搭建”是架构设计的关键约束。采用 Ollama 等本地 LLM 服务平台能够增强数据隐私，减少对第三方云服务的依赖，并完全掌控模型版本与参数，从而满足成本与安全性的双重需求。但是这里我们暂且按下不表，选择调用 LLM API。&lt;/p&gt;
&lt;h2&gt;II. 核心编排框架的选择与实施&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ii-核心编排框架的选择与实施&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ii-%e6%a0%b8%e5%bf%83%e7%bc%96%e6%8e%92%e6%a1%86%e6%9e%b6%e7%9a%84%e9%80%89%e6%8b%a9%e4%b8%8e%e5%ae%9e%e6%96%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A. LangGraph&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-langgraph&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-langgraph&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;智能体框架大致分为两类：一类追求配置驱动的简单性（如 CrewAI），另一类提供图驱动的编排控制能力（如 LangGraph）。CrewAI 在角色分工明确的简单任务中表现良好，但面对复杂的条件执行或流程分支时缺乏灵活性。AutoGen 擅长对话式协作，却难以满足科研任务所需的高确定性。&lt;/p&gt;
&lt;p&gt;LangGraph 基于 LangChain 原语构建运行时，其核心即状态机。对于复杂且长周期的 DRW，图结构是刚性需求，因为它能够：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;确保确定性工作流&lt;/strong&gt;：明确定义节点（规划、执行、合成）与状态转换。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现条件执行&lt;/strong&gt;：强制执行 if/then/else 逻辑，例如“RAG 失败则重试，成功则验证”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持持久状态&lt;/strong&gt;：原生提供状态管理与检查点，对耗时数小时甚至数天的流程至关重要。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;B. 主管—子团队分层结构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;b-主管子团队分层结构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#b-%e4%b8%bb%e7%ae%a1%e5%ad%90%e5%9b%a2%e9%98%9f%e5%88%86%e5%b1%82%e7%bb%93%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;总任务智能体（Total Task Agent, TTA）/ 主管&lt;/strong&gt;：负责高层规划、任务拆解、&lt;code&gt;paper_list&lt;/code&gt; 进度跟踪、状态条件路由以及最终合成。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;委托机制&lt;/strong&gt;：根据用户查询与子智能体描述决定任务路由，必要时将论文检索任务委托给执行智能体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;研究执行智能体（Research Executor Agent, STA）&lt;/strong&gt;：与外部环境（Model Context Protocol, MCP 服务器）交互，负责文档检索、RAG 调用与结构化摘要生成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;C. 共享状态架构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;c-共享状态架构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#c-%e5%85%b1%e4%ba%ab%e7%8a%b6%e6%80%81%e6%9e%b6%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;共享状态对象是整个工作流的“单一事实来源”，用于记录流程状态、中间结果与进度。所有智能体必须通过标准化接口读写该状态，以实现模块化解耦：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TTA 无需了解 STA 执行 RAG 的细节，只需读取写回的结果。&lt;/li&gt;
&lt;li&gt;即便替换 RAG 流程或底层工具链，也不会影响 TTA 的高层逻辑，实现可扩展的软件架构原则。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;示例字段如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;字段&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
          &lt;th&gt;说明&lt;/th&gt;
          &lt;th&gt;使用方&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;query_topic&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;字符串&lt;/td&gt;
          &lt;td&gt;初始研究查询&lt;/td&gt;
          &lt;td&gt;TTA（规划）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;paper_list&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;列表（字典）&lt;/td&gt;
          &lt;td&gt;论文主列表：URL、标题、状态&lt;/td&gt;
          &lt;td&gt;TTA（分配、追踪）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;current_paper_id&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;字符串&lt;/td&gt;
          &lt;td&gt;当前 STA 处理的文档 ID&lt;/td&gt;
          &lt;td&gt;TTA / STA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;next_task_route&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;字符串&lt;/td&gt;
          &lt;td&gt;下一节点条件字段&lt;/td&gt;
          &lt;td&gt;TTA（路由）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;task_output&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;字符串 / 字典&lt;/td&gt;
          &lt;td&gt;STA 产出的摘要或错误信息&lt;/td&gt;
          &lt;td&gt;STA / TTA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;verified_summaries&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;字典&lt;/td&gt;
          &lt;td&gt;经核查的摘要索引存储&lt;/td&gt;
          &lt;td&gt;TTA / 合成智能体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;error_log&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;字符串列表&lt;/td&gt;
          &lt;td&gt;失败调用与幻觉警告&lt;/td&gt;
          &lt;td&gt;TTA（回退）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;synthesis_draft&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;字符串&lt;/td&gt;
          &lt;td&gt;文献综述草稿&lt;/td&gt;
          &lt;td&gt;TTA / 合成智能体&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;III. 通过模型上下文协议（MCP）实现工具访问&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;iii-通过模型上下文协议mcp实现工具访问&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#iii-%e9%80%9a%e8%bf%87%e6%a8%a1%e5%9e%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%8d%8f%e8%ae%aemcp%e5%ae%9e%e7%8e%b0%e5%b7%a5%e5%85%b7%e8%ae%bf%e9%97%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A. MCP 在学术研究中的作用&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-mcp-在学术研究中的作用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-mcp-%e5%9c%a8%e5%ad%a6%e6%9c%af%e7%a0%94%e7%a9%b6%e4%b8%ad%e7%9a%84%e4%bd%9c%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;深度研究需要可靠、标准化的数据接口。MCP（Model Context Protocol）通过开放协议，定义了应用如何向 LLM 提供工具与上下文，确保数据摄取的一致性与可验证性。&lt;/p&gt;
&lt;p&gt;MCP 服务器可同时暴露：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;工具（Tools）&lt;/strong&gt;：执行特定任务，如网络搜索、文件解析。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提示（Prompts）&lt;/strong&gt;：针对任务的提示模板，例如“系统综述大纲提示”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源（Resources）&lt;/strong&gt;：提供学术论文 PDF 或文本片段作为上下文。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;借助 MCP，DRW 能以统一方式连接 GitHub、Slack、Google Drive 等资源。若未来接入授权学术数据库，只需替换 MCP 服务器实现，高层编排无需变动。&lt;/p&gt;
&lt;h3&gt;B. 构建论文检索用 MCP 服务器&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;b-构建论文检索用-mcp-服务器&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#b-%e6%9e%84%e5%bb%ba%e8%ae%ba%e6%96%87%e6%a3%80%e7%b4%a2%e7%94%a8-mcp-%e6%9c%8d%e5%8a%a1%e5%99%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;STA 必须完成“通过 MCP 联网读取指定论文并总结”的职责，因此需要搭建封装 RAG 前置流程的 MCP 服务器，核心能力包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fetch_and_prepare_resource(url)&lt;/code&gt;：下载 PDF 并转换成标准资源对象。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;perform_rag_query(resource_id, question)&lt;/code&gt;：对摄取后的论文执行检索增强生成。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;传输方案&lt;/strong&gt;：本地可采用 &lt;code&gt;stdio&lt;/code&gt;；若需并行或远程访问，可切换到支持流式的 HTTP 传输。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;C. 在 STA 中集成 MCP&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;c-在-sta-中集成-mcp&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#c-%e5%9c%a8-sta-%e4%b8%ad%e9%9b%86%e6%88%90-mcp&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;STA 作为 MCP 客户端，可使用 &lt;code&gt;MultiServerMCPClient&lt;/code&gt; 安全调用服务器工具。论文分析通常需要顺序推理（先“方法”，再“实验”，最后“总结”），因此 STA 需要通过 &lt;code&gt;ClientSession&lt;/code&gt; 维持跨调用状态，实现类似研究员的深度迭代分析。&lt;/p&gt;
&lt;h2&gt;IV. 子任务智能体：论文 RAG 管道&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;iv-子任务智能体论文-rag-管道&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#iv-%e5%ad%90%e4%bb%bb%e5%8a%a1%e6%99%ba%e8%83%bd%e4%bd%93%e8%ae%ba%e6%96%87-rag-%e7%ae%a1%e9%81%93&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A. 数据摄取与准备&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-数据摄取与准备&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-%e6%95%b0%e6%8d%ae%e6%91%84%e5%8f%96%e4%b8%8e%e5%87%86%e5%a4%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;学术论文结构复杂，RAG 的数据处理质量直接决定结果准确性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;复杂文档处理&lt;/strong&gt;：必须使用能识别章节、段落、图表的文本分割器，保障语义连贯。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;向量存储与嵌入模型&lt;/strong&gt;：可采用 Pinecone 或本地 Chroma，嵌入模型由本地 Ollama 提供，构建混合式 RAG，结合生成与检索优势，降低幻觉率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;B. 智能体式 RAG 与迭代摘要&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;b-智能体式-rag-与迭代摘要&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#b-%e6%99%ba%e8%83%bd%e4%bd%93%e5%bc%8f-rag-%e4%b8%8e%e8%bf%ad%e4%bb%a3%e6%91%98%e8%a6%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;STA 在 LangGraph 的指导下进行智能决策，仅在需要外部上下文时调用 RAG。它通过多轮定向查询（如“提取架构细节”“总结消融实验”）收集事实，再综合输出高质量摘要。&lt;/p&gt;
&lt;h3&gt;C. 高级上下文工程与长期记忆（VCM）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;c-高级上下文工程与长期记忆vcm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#c-%e9%ab%98%e7%ba%a7%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b7%a5%e7%a8%8b%e4%b8%8e%e9%95%bf%e6%9c%9f%e8%ae%b0%e5%bf%86vcm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;LLM 的上下文窗口限制是深度研究的主要挑战。可引入受 MemGPT 启发的虚拟上下文管理（Virtual Context Management, VCM）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心记忆（Core Memory）&lt;/strong&gt;：相当于 RAM，存储当前指令与摘要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;归档上下文（Archival Context）&lt;/strong&gt;：相当于磁盘，存储所有已验证摘要（如向量数据库）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;VCM 在固定窗口内模拟“无限上下文”，支持跨论文事实汇总。合成智能体具备自定向检索能力，可调用工具按主题调取历史摘要，例如“检索所有提及‘非视觉里程计’的摘要”。&lt;/p&gt;
&lt;h2&gt;V. 工作流执行、质量保障与稳健合成&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;v-工作流执行质量保障与稳健合成&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#v-%e5%b7%a5%e4%bd%9c%e6%b5%81%e6%89%a7%e8%a1%8c%e8%b4%a8%e9%87%8f%e4%bf%9d%e9%9a%9c%e4%b8%8e%e7%a8%b3%e5%81%a5%e5%90%88%e6%88%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A. 委托循环与条件执行&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-委托循环与条件执行&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-%e5%a7%94%e6%89%98%e5%be%aa%e7%8e%af%e4%b8%8e%e6%9d%a1%e4%bb%b6%e6%89%a7%e8%a1%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;TTA 持续监控共享状态，依次选择 &lt;code&gt;paper_list&lt;/code&gt; 中的待处理论文，更新 &lt;code&gt;current_paper_id&lt;/code&gt; 并路由给 STA。凭借 LangGraph 的条件路由能力，可构建稳健的错误处理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;首次失败&lt;/strong&gt;：立即重试 MCP 调用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;二次失败&lt;/strong&gt;：记录错误并返回 TTA 重新评估，例如改用网络摘要工具。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最终失败&lt;/strong&gt;：将论文标记为 &lt;code&gt;Failed&lt;/code&gt;，继续下一项，避免流程阻断。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;B. 数据质量：事实基础与引文校验&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;b-数据质量事实基础与引文校验&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#b-%e6%95%b0%e6%8d%ae%e8%b4%a8%e9%87%8f%e4%ba%8b%e5%ae%9e%e5%9f%ba%e7%a1%80%e4%b8%8e%e5%bc%95%e6%96%87%e6%a0%a1%e9%aa%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;为最大程度降低幻觉风险，需要验证智能体对 STA 生成的摘要进行原文核查。关键指标如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;定义&lt;/th&gt;
          &lt;th&gt;对 DRW 的重要性&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;正确性（Correctness）&lt;/td&gt;
          &lt;td&gt;事实点可在引用文档中核实的比例&lt;/td&gt;
          &lt;td&gt;学术诚信的底线&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;完整性（Completeness）&lt;/td&gt;
          &lt;td&gt;查询或文档关键要点的覆盖程度&lt;/td&gt;
          &lt;td&gt;确保分析全面&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;关联性（Relevancy）&lt;/td&gt;
          &lt;td&gt;引用资源与生成内容的相关度&lt;/td&gt;
          &lt;td&gt;验证任务匹配性&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;C. 合成智能体：整合最终输出&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;c-合成智能体整合最终输出&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#c-%e5%90%88%e6%88%90%e6%99%ba%e8%83%bd%e4%bd%93%e6%95%b4%e5%90%88%e6%9c%80%e7%bb%88%e8%be%93%e5%87%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;当所有论文分析完成或达到失败阈值，TTA 将流程路由至合成阶段。合成智能体通过 VCM 检索已验证摘要，按照标准综述结构（如非 SLAM 3D 方法分类、传感器融合对比、挑战识别）生成报告。LangGraph 的流式输出能力保证用户实时查看长文档生成过程。&lt;/p&gt;
&lt;h3&gt;D. 人在回路（HILT）检查点&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;d-人在回路hilt检查点&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#d-%e4%ba%ba%e5%9c%a8%e5%9b%9e%e8%b7%afhilt%e6%a3%80%e6%9f%a5%e7%82%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在高度自主系统中引入 HILT 至关重要。LangGraph 支持在工作流中暂停、等待用户输入并从相同状态继续。建议的人工介入点包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大纲审查&lt;/strong&gt;：TTA 制定初始论文列表与研究计划后，人工确认与调整。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合成审查&lt;/strong&gt;：最终报告提交前，由人工审阅草稿，确保学术质量与方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结合评估指标与可观察性工具（如 LangSmith），系统不仅能执行任务，还能自我改进。&lt;code&gt;error_log&lt;/code&gt; 与验证得分帮助持续优化提示工程与 RAG 管道，将智能体系统视为可度量、迭代的软件产品。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>港硕申请回顾</title>
      <link>http://localhost:1313/blog/2025/2025-11-28-hk-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-28-hk-master/</guid>
      <description>
        
        
        &lt;h1&gt;港硕申请回顾&lt;/h1&gt;&lt;h2&gt;推免失败&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;推免失败&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a8%e5%85%8d%e5%a4%b1%e8%b4%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本科大三时经历了一段痛苦的PTSD，导致本科规划出现了巨大偏差。理论上应该是以&lt;strong&gt;二作CCF文章&lt;/strong&gt;加上刚刚过推免线的绩点保研，但后来拼尽全力把3年GPA稳到了&lt;strong&gt;88分&lt;/strong&gt;，也算刚刚好拿到推免资格（22/25）。由于对直接读博产生了巨大阴影，且不太喜欢生仪研究院的4个方向，因而选择无论如何都要读一个&lt;strong&gt;CS/AI相关&lt;/strong&gt;的硕士。&lt;/p&gt;
&lt;p&gt;然而硕士申请需要面临committee的拷打，因此推免&lt;strong&gt;浙计&lt;/strong&gt;失败。面试时被问到的5个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;排序算法&lt;/li&gt;
&lt;li&gt;联邦学习&lt;/li&gt;
&lt;li&gt;数组排序&lt;/li&gt;
&lt;li&gt;职业规划&lt;/li&gt;
&lt;li&gt;还有一个忘记了（记忆自动删除了）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;选择浙计确实有点冲动，主要是有老师愿意给我他们组里专硕的名额，但是老师又不能在委员会里直接捞人（这和博士不一样，但出于上述的原因，我又不愿直博）。在&lt;strong&gt;9月22日&lt;/strong&gt;左右结束之后，就立刻开始捡起托福复习英语了。&lt;/p&gt;
&lt;h2&gt;方向确定与选校&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;方向确定与选校&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%96%b9%e5%90%91%e7%a1%ae%e5%ae%9a%e4%b8%8e%e9%80%89%e6%a0%a1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;大三暑假通过对计院/信电老师（比如计院院长那个组，真是年少轻狂）的套磁，排除了AI4Sci等选项后，确定了可能会感兴趣的方向——&lt;strong&gt;空间智能&lt;/strong&gt;这一块，目前国内也比较火的所谓&lt;strong&gt;具身智能&lt;/strong&gt;。但是机器人这一块我确实不太喜欢，本科的时候一直在避免接触Robotics这个概念，当然也和认识一个比较讨厌的控制学院的同学有关系。不过既然决定了方向，下一步就是选校了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI、CS领域&lt;/strong&gt;，想要获得长足的发展，在我看来只有两个国家可以选择：&lt;strong&gt;中与美&lt;/strong&gt;。美国在我大三的时候特朗普刚开始第二任任期，在签证与大学经费（后果是削减招生名额）以及工作机会（美国本地企业实习）都出现了重大利空，因此不做考虑，当然现在可能又回暖了。但是托福是路径依赖的语言，可以和雅思互相替代。因此选择中国这一块。而在本院预推免结束之后，其他（不被视为下保的）学校的也都差不多来不及报名了，唯一一个中科院大学还留有一点点，思来想去还是放掉选择港硕。&lt;/p&gt;
&lt;p&gt;工科信息这一块，&lt;strong&gt;对口实习甚至比论文一作还重要&lt;/strong&gt;，而新加坡限制中国学生数量并且控制工作签证对外国人的发放，因此不考虑。选择离中国大陆更近的香港。至少因为我还有一个&lt;strong&gt;88的GPA&lt;/strong&gt;，看不太上港五那两所，只选&lt;strong&gt;港三&lt;/strong&gt;申请：&lt;/p&gt;
&lt;h3&gt;第一批申请（9月30日）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第一批申请9月30日&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%80%e6%89%b9%e7%94%b3%e8%af%b79%e6%9c%8830%e6%97%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;港科广&lt;/strong&gt; 红鸟&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;港中深&lt;/strong&gt; MAIR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;港中深&lt;/strong&gt; CS（怕不稳补申）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;第二批申请（10月25日之后，考出托福成绩后补申）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第二批申请10月25日之后考出托福成绩后补申&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%ba%8c%e6%89%b9%e7%94%b3%e8%af%b710%e6%9c%8825%e6%97%a5%e4%b9%8b%e5%90%8e%e8%80%83%e5%87%ba%e6%89%98%e7%a6%8f%e6%88%90%e7%bb%a9%e5%90%8e%e8%a1%a5%e7%94%b3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CUHK&lt;/strong&gt; AI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUHK&lt;/strong&gt; CS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUHK&lt;/strong&gt; IE&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HKUST&lt;/strong&gt; CS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HKUST&lt;/strong&gt; IE&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;没有申请HKU的原因是，HKU CS属于超级大班，感觉含金量基本上靠的是【香港大学】的综排牌子；第二个原因就是这是唯一一个需要笔面的，虽然笔试难度没有咱ZJU的期末考试难度高，但就很恶心。与之形成鲜明对比的就是CUHK，没有笔面，直接花600RMB去抽奖。
BTW，这里面申请页面的前端UI做的，我没有想到会这么拉，看下来港科广做的是最好看的，港中深其次但是它的密码不支持特殊字符（@#￥这些），关键是还不会专门提醒报错是这个原因，我试了好几次换了几个邮箱都输密码错误，最后才被这个问题的原因猜出来气消了。港中港科两所港校本部的申请界面丑的颇有上世纪网站的美感，值得吐槽的是HKUST的Country那一栏不像其他的学校那样是China(Mainland)，它是The Mainland of China，首字母是T还是M来着，导致第一眼根本看不到，只找到一个Chili&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;申请建议不要请中介，如果你是想要找港新的选校的话，欧美啥的另说，但是这几个地区靠ZJU的同学完全可以自己做到。中介的最大作用就是你去加他跟他聊的第一次，它会给你一些关于流程上的信息，比如我就找了一个中介来帮我介绍港科广的申请流程、面试之类的常识信息，后面直接忽略掉他的消息就行。&lt;/p&gt;
&lt;h2&gt;推荐信与申请进度&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;推荐信与申请进度&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a8%e8%8d%90%e4%bf%a1%e4%b8%8e%e7%94%b3%e8%af%b7%e8%bf%9b%e5%ba%a6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;推荐信这一块还是拜本科期间干了点事，拿到了那位计院老师以及本院书记的&lt;strong&gt;两封强推&lt;/strong&gt;（硕士居然要推荐信），所以基本上刚申请就交了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;11月3日&lt;/strong&gt;，CUHK IE发邮件过来催补材料；&lt;strong&gt;11月5日&lt;/strong&gt;回复我说我的申请会被更深入处理。结果最先发offer的居然是&lt;strong&gt;CUHK的AI&lt;/strong&gt;，在&lt;strong&gt;11月20日&lt;/strong&gt;前。&lt;/p&gt;
&lt;p&gt;因为申请比较晚，&lt;strong&gt;港科广红鸟MPhil&lt;/strong&gt;属于第二批，在&lt;strong&gt;11月26日&lt;/strong&gt;才发面邀，时间为线下的&lt;strong&gt;12月10-11日&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;最终选择与规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;最终选择与规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%80%e7%bb%88%e9%80%89%e6%8b%a9%e4%b8%8e%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;比较感兴趣的&lt;strong&gt;CUHKSZ&lt;/strong&gt;居然还不发笔面通知，这样就只能选择&lt;strong&gt;CUHK本部&lt;/strong&gt;了。交了这个留位费之后，剩下像占尽地理劣势的HKUST其实都可以不用考虑了，甚至CUHKSZ都可以不用考虑，专心等港科广的面试结果。如果成功了，每月发的&lt;strong&gt;10000RMB&lt;/strong&gt;凑起来去掉学费完全能把这&lt;strong&gt;11万留位费&lt;/strong&gt;赚回来。&lt;/p&gt;
&lt;p&gt;最坏的情况是港科广面试不过，那样就需要好好进行后面的规划：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;留服认证问题&lt;/strong&gt;：不同于那两所内地校区的港校，CUHK本部算留学要进行留服认证，但是留学生进央国企的话，很多国央企都要求&lt;strong&gt;本硕一致&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学制与实习规划&lt;/strong&gt;：CUHK的CS是&lt;strong&gt;一年制&lt;/strong&gt;，AI是&lt;strong&gt;1.5年制&lt;/strong&gt;。两者都可以延期毕业从而多次参与秋招，从而争取攒出&lt;strong&gt;两段3个月以上的实习&lt;/strong&gt;出来进入这个还未收敛的具身行业。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信息获取渠道&lt;/strong&gt;：像是毕业去向、具体的规划等还是得上&lt;strong&gt;cc98&lt;/strong&gt;问问校友学长之类的，小红书上面中介之类的杂鱼信息太多了。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;最终结果&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;最终结果&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%80%e7%bb%88%e7%bb%93%e6%9e%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;CUHK MscAI&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;cuhk-mscai&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#cuhk-mscai&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;CUHK无需笔面&lt;/strong&gt;，直接根据申请材料下offer。这里聊一聊一般网上不会详细说明的拿到&lt;strong&gt;conditional offer&lt;/strong&gt;之后的流程：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/cuhk-acceptance-of-admission-offer.png&#34; alt=&#34;CUHK录取通知书接受页面&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里分为两步，首先是&lt;strong&gt;交留位费&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/cuhk-deposit-payment.png&#34; alt=&#34;CUHK留位费支付页面&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这样一来这个offer就会为你保留，你不去的话就会亏掉这一笔钱。留位费理论上应该算是学费的一部分（将在第一学期部分学生费用中抵消），而我感觉&lt;strong&gt;CUHK AI是比较新也比较贵的那一批&lt;/strong&gt;。据称其深圳校区的留位费可以退还&lt;strong&gt;90%&lt;/strong&gt;，不过我在写这篇博客的同时还没有给我下面试通知所以无从验证。&lt;/p&gt;
&lt;p&gt;上图所描述的可选支付方式（&lt;strong&gt;选一个即可&lt;/strong&gt;）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;信用卡支付&lt;/strong&gt;（VISA / 万事达 / 银联）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选对应的选项，点 &amp;ldquo;Confirm&amp;rdquo; 按钮，会跳转到在线信用卡支付页面完成付款。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;电子钱包类&lt;/strong&gt;（微信支付 / 支付宝 / AlipayHK/BoC Pay / 银联 App）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后点 &amp;ldquo;Confirm&amp;rdquo;，会跳转到第三方支付平台 &lt;strong&gt;SwiftPass&lt;/strong&gt; 完成付款。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速支付系统（FPS）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后点 &amp;ldquo;Confirm&amp;rdquo;，跳转到 CUHK 的 FPS 支付系统操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;本地支付方式&lt;/strong&gt;（仅香港地区可用：PPS/ATM/ 网上银行 / 现金 / 支票）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后，先下载 &amp;ldquo;Payment Advice&amp;rdquo;，按照页面里 &amp;ldquo;See more&amp;rdquo; 的指引操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;电汇&lt;/strong&gt;（仅非本地支付）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后，点 &amp;ldquo;See more&amp;rdquo; 查看 CUHK 的银行账户详情，直接向该账户电汇（注意电汇金额是 &lt;strong&gt;HK$126,916&lt;/strong&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;注意事项&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;付款时要填对参考号：&lt;strong&gt;88103073875&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;非本地支付（如电汇）可能需要 &lt;strong&gt;7 个工作日&lt;/strong&gt;到账，建议提前操作避免逾期&lt;/li&gt;
&lt;li&gt;付款后确认可能需要 &lt;strong&gt;1-3 个工作日&lt;/strong&gt;（本地）或 &lt;strong&gt;7 个工作日&lt;/strong&gt;（非本地）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;小红书上据称是&lt;strong&gt;微信支付的手续费最少&lt;/strong&gt;，不过既然都读港校了也不差这点钱。&lt;/p&gt;
&lt;h4&gt;注册流程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;注册流程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b3%a8%e5%86%8c%e6%b5%81%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;下一步是完成&lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/registration/how-to-register&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;注册&lt;/a&gt;。在接受录取通知书并缴纳留位费后，要按照以下步骤完成注册，以便在开学前完成所有手续。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有在完成注册程序后，才能领取&lt;strong&gt;CU Link（学生证）&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;所有CUHK学生都需要在校园入口出示&lt;strong&gt;CU Link Card&lt;/strong&gt;。有关迎新活动的详情，请访问 &lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/orientation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Orientation Website 2025&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;自2023年8月1日起，CUHK Mobile Pass App 可供新生使用，可生成二维码用于校园通行和学习空间访问。详情请访问 &lt;a href=&#34;https://www.itsc.cuhk.edu.hk/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ITSC网站&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注册步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 1：提交在线注册&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你必须在指定的付款截止日期前缴纳留位费。&lt;/li&gt;
&lt;li&gt;注册链接将在付款完成后生效：本地学生为&lt;strong&gt;第3个工作日&lt;/strong&gt;，非本地学生为&lt;strong&gt;第14个工作日&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;完成在线注册后，你可以&lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/registration/registration-status&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在此处&lt;/a&gt;查看注册状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 2：提交学生签证申请&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建议学生通过快递将原始签证申请表及&lt;strong&gt;所有&lt;/strong&gt;所需支持文件寄送至CUHK，时间安排如下：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;研究型课程学生&lt;/strong&gt;：&lt;strong&gt;4月中旬&lt;/strong&gt;前&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;授课型课程学生&lt;/strong&gt;：&lt;strong&gt;5月中旬&lt;/strong&gt;前（有预科课程的学生应在预科开始日期前至少&lt;strong&gt;8周&lt;/strong&gt;提交签证申请）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 3：提交研究生宿舍申请&lt;/strong&gt;（如适用）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;修读全日制研究型课程或全日制UGC资助授课型课程的学生可以申请宿舍。请从&lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/postgraduate-halls&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;研究生宿舍网站&lt;/a&gt;查看申请截止日期，并相应地向研究生宿舍办公室提交申请。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 4：提交文件以完成录取条件&lt;/strong&gt;（如有）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在条件完成截止日期前，通过邮寄或快递将录取通知中指定的所需文件寄送：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;授课型课程学生&lt;/strong&gt;：寄送至课程办公室&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;研究型课程学生&lt;/strong&gt;（MPhil &amp;amp; PhD）：寄送至研究生院&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果你未能在规定截止日期前完成录取条件，你的录取通知书将失效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 5：执行签证文件提交第一步&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 6：激活 MyCUHK 账户&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完成上述步骤后：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地学生&lt;/strong&gt;：将从&lt;strong&gt;7月中旬&lt;/strong&gt;开始通过ITSC的电子邮件和/或短信（仅限香港手机）收到v-code&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非本地学生&lt;/strong&gt;：将从&lt;strong&gt;7月中旬&lt;/strong&gt;开始通过ITSC的电子邮件收到v-code&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用v-code从学生计算账户收集系统检索OnePass密码，以便通过MyCUHK进行选课和其他服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 7：执行签证文件提交第二步&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 8：领取 CU Link（学生证）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地学生&lt;/strong&gt;：可在录取日期或之后领取CU Link。请联系&lt;a href=&#34;https://www.cuhk.edu.hk/cu-link/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CU Link Card Centre&lt;/a&gt;查看CU Link是否已准备好并确认领取安排。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非本地学生&lt;/strong&gt;：在完成上述步骤7后&lt;strong&gt;7-10个工作日&lt;/strong&gt;，请联系CU Link Card Centre查看CU Link是否已准备好并确认领取安排。&lt;/li&gt;
&lt;li&gt;部分课程会为学生统一领取CU Link。如有疑问，请联系课程办公室。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 9：更新 MyCUHK 信息&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在MyCUHK更新邮寄地址和联系方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 10：提交研究生奖学金支付指示表&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;填写并返回&amp;quot;研究生奖学金支付指示表&amp;quot;至财务处会计运营与系统组（邵逸夫楼1楼）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 11：选课&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选课将在&lt;strong&gt;8月初&lt;/strong&gt;进行。请根据你课程的学习计划和研究生院的建议选课。详情请参考&lt;a href=&#34;https://www.gs.cuhk.edu.hk/student/course-selection-and-add-drop&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;研究生院网站&lt;/a&gt;（学生 &amp;ndash;&amp;gt; 选课和加退选 &amp;ndash;&amp;gt; 选择相应的学年）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意事项&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果未完成在线注册，将&lt;strong&gt;无法&lt;/strong&gt;进行选课和领取CU Link。&lt;/li&gt;
&lt;li&gt;请&lt;strong&gt;不要&lt;/strong&gt;通过电子邮件将完成录取条件的证明文件发送至任何研究生院邮箱账户，这可能会延迟处理提交。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;到时候看看要不要&lt;a href=&#34;https://www.pgh.cuhk.edu.hk/sc/hall-residence-application/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;申请宿舍&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;HKUST-GZ MPhil RB&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;hkust-gz-mphil-rb&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hkust-gz-mphil-rb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;面邀到线下面试之间有&lt;strong&gt;2周时间&lt;/strong&gt;左右，足够你去准备&lt;strong&gt;英文PPT与5min讲稿&lt;/strong&gt;、看面经、准备机票与衣物等工作，&lt;strong&gt;别忘了带身份证等&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-invitation.png&#34; alt=&#34;港科广面试邀请函&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通勤方面&lt;/strong&gt;，直接选择&lt;strong&gt;萧山机场→白云机场&lt;/strong&gt;，从ZJU到前者和从HKUST-GZ到后者的时间差不多都是&lt;strong&gt;1h左右&lt;/strong&gt;，飞机飞&lt;strong&gt;2h多一点&lt;/strong&gt;，差不多当天直接注册入住，第二天开面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上午群面，下午个面&lt;/strong&gt;。这里的信息来源来自港科广官方微信公众号以及小红书经验贴，当然你也可以在面邀邮件里找到对应的&lt;a href=&#34;https://cft.hkust-gz.edu.cn/2025/07/04/%e9%a6%99%e6%b8%af%e7%a7%91%e6%8a%80%e5%a4%a7%e5%ad%a6%ef%bc%88%e5%b9%bf%e5%b7%9e%ef%bc%89%e6%9c%aa%e6%9d%a5%e6%8a%80%e6%9c%af%e5%ad%a6%e9%99%a2%e7%ba%a2%e9%b8%9f%e7%a1%95%e5%a3%ab%e9%a1%b9%e7%9b%ae-2/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;流程&lt;/a&gt;：&lt;/p&gt;
&lt;h4&gt;未来城市小组项目活动&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;未来城市小组项目活动&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%aa%e6%9d%a5%e5%9f%8e%e5%b8%82%e5%b0%8f%e7%bb%84%e9%a1%b9%e7%9b%ae%e6%b4%bb%e5%8a%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 头脑风暴阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4-6名申请人&lt;/strong&gt;，抽取一个**A+B（AI随机形容词生成）**的主题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未来健康保健技术&lt;/li&gt;
&lt;li&gt;可持续生活&lt;/li&gt;
&lt;li&gt;智能工业化&lt;/li&gt;
&lt;li&gt;低空经济&lt;/li&gt;
&lt;li&gt;海洋科技与经济&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;示例&lt;/strong&gt;：寒冷的 + 智能工业化&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;30+10分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（创意与沟通类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;创意贡献&lt;/td&gt;
          &lt;td&gt;申请人在头脑风暴中是否提出了至少一个具体的想法？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;积极倾听与反馈&lt;/td&gt;
          &lt;td&gt;申请人是否提出了至少一个澄清性问题或重述了队友的想法？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;创造力与独创性&lt;/td&gt;
          &lt;td&gt;申请人是否提出了至少一个在方法或内容上不同于其他人的想法？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;接受反馈的态度&lt;/td&gt;
          &lt;td&gt;申请人是否口头承认了反馈，或根据反馈提出了调整建议？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;2. 制作阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用基础材料（积木、纸、绳子、竹木、乐高等），在一个&lt;strong&gt;长、宽、高分别为 75.5cm、51.5cm、&amp;gt;43cm&lt;/strong&gt; 的立体空间内创作一个主题关键词的未来城市模型，包含&lt;strong&gt;不少于 5 个模块&lt;/strong&gt;（模块功能或内容由小组自行定义）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例&lt;/strong&gt;：具备高度智能工业化生产能力，且能够在严寒环境中提供可持续的生活条件和高效管理的未来城市，包含了智能工业中心、严寒能源供应与管理系统、智能交通网络、常温智能居住区、抗寒垂直农业与生态等模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2名评委&lt;/strong&gt;全过程观察、记录、评分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;110分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（执行与协作类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;制作贡献&lt;/td&gt;
          &lt;td&gt;申请人是否在项目的某个方面进行了实际操作（例如，放置材料、调整设计等）？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;任务责任感&lt;/td&gt;
          &lt;td&gt;申请人是否负责完成了至少一项分配的任务，并且无需提醒？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;协作能力&lt;/td&gt;
          &lt;td&gt;申请人是否与另一名队员在至少一项具体任务上合作（如规划或构建）？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;主动性与解决问题的能力&lt;/td&gt;
          &lt;td&gt;申请人是否在构建阶段遇到问题时提出了至少一个解决方案？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;领导能力&lt;/td&gt;
          &lt;td&gt;申请人是否至少一次分配或委派任务给队友？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;3. 交易与适应阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随机抽取另一小组作为观察对象进行观察，并结合自己小组的作品情况，有针对性地为被观察的小组提出一个挑战任务（例如：地震、海啸、核污染等人类社会可能面临的重大灾难或者挑战，具体内容由各小组自行定义），同时要求自己小组的作品必须能够满足自己所提给对方的挑战任务要求。此期间允许各小组之间使用&lt;strong&gt;金币&lt;/strong&gt;进行模块的自由交易，但最终需标注出所交换的模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;10分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个小组必须在&lt;strong&gt;20分钟&lt;/strong&gt;内完成挑战任务的应对方案，并在白板上写好小组作品介绍，要求包含应对方案，形式不限。此期间，允许各小组调整各自的作品以及进行组间的模块自由交易。建议各小组妥善使用金币，争取使投资产生最大价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;20分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（交易与适应类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;战略思维&lt;/td&gt;
          &lt;td&gt;申请人是否在交易阶段提出了至少一次交易或模块交换的建议？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;谈判技巧&lt;/td&gt;
          &lt;td&gt;申请人是否至少与其他团队进行了一次谈判，无论是提供还是接受条件？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;设计调整的灵活性&lt;/td&gt;
          &lt;td&gt;申请人是否根据挑战任务提出了至少一个设计调整建议或进行调整？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;压力下的适应性&lt;/td&gt;
          &lt;td&gt;申请人在挑战任务引入时是否保持参与而没有退缩或失去参与感？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;4. 挑战与最终展示&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个小组面向各自评委进行团队陈述，结束后按照现场指引完成组内互评。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;30分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（展示与团队互动类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;演讲清晰度&lt;/td&gt;
          &lt;td&gt;申请人是否清晰地展示了自己在项目中的角色，无需队友提示？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;挑战任务中的领导力&lt;/td&gt;
          &lt;td&gt;申请人是否提出了至少一项团队应对挑战任务的行动建议？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;回应挑战的创造力&lt;/td&gt;
          &lt;td&gt;申请人是否提出了一个直接应对挑战的解决方案（例如，功能改动，增加新特性等）？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;团队动态&lt;/td&gt;
          &lt;td&gt;申请人是否鼓励了至少一位队友参与或征求了他们的意见？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;与评委的互动&lt;/td&gt;
          &lt;td&gt;申请人在展示过程中是否至少回答了评委的一个问题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;活动地点&lt;/strong&gt;：香港科技大学（广州）校园的 &lt;strong&gt;Highbay&lt;/strong&gt;，全部制作环节必须在指定区域内完成；各小组在活动结束后须快速整理物资，清理现场后，方可离场。&lt;/p&gt;
&lt;h4&gt;个人面试&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;个人面试&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%aa%e4%ba%ba%e9%9d%a2%e8%af%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;总时长&lt;/strong&gt;：&lt;strong&gt;15分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 口头演讲&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：申请者的口头演讲使用中文，问答环节必将使用英文；如果口头演讲使用英文，问答环节将可使用中文或英文。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;因此，&lt;strong&gt;这部分必须使用英文&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5分钟内&lt;/strong&gt;阐述你攻读该领域硕士学位的原因，从以下五个主题中选择一个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未来健康保健技术&lt;/li&gt;
&lt;li&gt;可持续生活&lt;/li&gt;
&lt;li&gt;智能工业化&lt;/li&gt;
&lt;li&gt;低空经济&lt;/li&gt;
&lt;li&gt;海洋科技与经济&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;评分指标（演讲与主题契合类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;主题相关性&lt;/td&gt;
          &lt;td&gt;演讲是否聚焦于申请人选择的五个主题之一？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;视角与愿景&lt;/td&gt;
          &lt;td&gt;申请人是否展示了多角度的思考，并解决了现实问题或未来挑战？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;组织与结构&lt;/td&gt;
          &lt;td&gt;演讲是否结构合理，有明确的开头、主体和结论？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;内容熟悉度&lt;/td&gt;
          &lt;td&gt;申请人是否展示了对内容的深入理解，并简化了复杂的主题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;幻灯片设计&lt;/td&gt;
          &lt;td&gt;幻灯片是否无误，与内容一致，并有效支持演讲？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;语言表达&lt;/td&gt;
          &lt;td&gt;申请人表达是否流畅清晰，避免了多余叹词和模糊不清的表达？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;与评委的互动&lt;/td&gt;
          &lt;td&gt;演讲过程中，申请人是否有效地与评委进行眼神接触并互动？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;创造性思维&lt;/td&gt;
          &lt;td&gt;申请人在演讲中是否提供了创造性或独特的观点？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;动机与契合度&lt;/td&gt;
          &lt;td&gt;申请人是否清晰阐述了自己为何适合该项目，将个人目标与项目目标对齐？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;2. 问答环节&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（问答与综合能力类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;理解能力&lt;/td&gt;
          &lt;td&gt;申请人是否理解并准确回应了评委的问题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;回答深度&lt;/td&gt;
          &lt;td&gt;回答是否经过深思熟虑，并显示了对主题的深入理解？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;情绪控制&lt;/td&gt;
          &lt;td&gt;面对压力时，申请人是否保持冷静镇定，沉着应对难题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;危机处理能力&lt;/td&gt;
          &lt;td&gt;申请人是否能够很好地应对挑战性或意外问题，并提供相关的解决方案或观点？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;批判性思维&lt;/td&gt;
          &lt;td&gt;申请人是否展示了逻辑推理能力，分析问题并提供有见地的回答？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;团队合作的关注&lt;/td&gt;
          &lt;td&gt;申请人是否提供了团队合作的具体例子，并表达了对协作的强烈兴趣？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;说服力&lt;/td&gt;
          &lt;td&gt;申请人的观点是否具有说服力，并能够有效支持其立场？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;自我意识与成长&lt;/td&gt;
          &lt;td&gt;申请人是否展示了自我意识，讨论了自身的优势、劣势和需要改进的地方？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;共情与参与&lt;/td&gt;
          &lt;td&gt;申请人是否根据评委的反馈调整了自己的回答，并与他们进行有意义的互动？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;详细的面试流程可以参考面邀邮件里的文件：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-agenda1.png&#34; alt=&#34;港科广面试流程1&#34;  loading=&#34;lazy&#34; /&gt;
&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-agenda2.png&#34; alt=&#34;港科广面试流程2&#34;  loading=&#34;lazy&#34; /&gt;
&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-agenda3.png&#34; alt=&#34;港科广面试流程3&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;注意事项&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;注意事项&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;证件准备&lt;/strong&gt;：请准备申请者有效身份证件原件（身份证、护照或其他有效证件）入校及签到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;住宿安排&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;所有申请者在报到当晚将安排住宿（&lt;strong&gt;8人公寓，135元/晚，自费&lt;/strong&gt;）&lt;/li&gt;
&lt;li&gt;如果申请在面试后延长一晚住宿（续住房型：&lt;strong&gt;3人公寓，214元/晚，自费&lt;/strong&gt;），学校将为你安排三人间住宿&lt;/li&gt;
&lt;li&gt;如因住房紧缺，将由校方另作安排&lt;/li&gt;
&lt;li&gt;未申请续住的申请者应在面试当天&lt;strong&gt;19:00之前&lt;/strong&gt;离开学校&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：因此通勤非常紧凑，当天去注册，第二天晚上就要连夜回来，需要注意买票这一块。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;纪律要求&lt;/strong&gt;：迟到、缺席或不遵守规定，甚至对面试现场造成破坏或不良影响者，将被&lt;strong&gt;直接取消面试资格&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;录音录像&lt;/strong&gt;：主办方将根据实际工作需求，在活动现场安排相关录音、录像或拍摄等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;差旅费用&lt;/strong&gt;：申请者参加线下面试的差旅自理，校方将不提供订票服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;面经分享&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;面经分享&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%9d%a2%e7%bb%8f%e5%88%86%e4%ba%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;因为&lt;strong&gt;红鸟的特殊机制&lt;/strong&gt;，面试的时候尽量伪装自己，或者说释放天性，选一个相对冷门的Hub尽情阐述，毕竟进去之后有&lt;strong&gt;半年才正式选导&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;小红书链接汇总&lt;/strong&gt;：
&lt;strong&gt;1. &lt;a href=&#34;https://www.xiaohongshu.com/explore/6751368b0000000007027b9b?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBK4SfTQTgQ_lcCGKMtNdUBCgtSonOqP_4awBVwt_GPdc=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760345377&amp;amp;share_id=021e983567954ebfb12a0db5555fd31a&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广第二批面经（推研版）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前一天&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下午到达科广签到，&lt;strong&gt;五点老师带队参观校园&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;面试当天&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;离开宿舍时要收房卡退房&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;8:30开始签到&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;团队面试&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;题目：&lt;strong&gt;未来健康保健技术 + 宁静的&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;个人整体感受：team2队员都很友好，过程愉快。评委老师似乎在&lt;strong&gt;最后的陈述部分才动笔记录&lt;/strong&gt;（似乎在打分？所以这个part要重视）。最好开始前上好厕所，带杯水进来场地，全程站着特别累，&lt;strong&gt;12:30才结束&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;个人面试&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中午要抓紧吃饭，时间很紧&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;13:00签到，14:00开始&lt;/strong&gt;，我是最后一个等到&lt;strong&gt;17:00面完&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;项目经历中的合作与困难&lt;/li&gt;
&lt;li&gt;你未来想研究这个方向（老龄化）要达到什么目的&lt;/li&gt;
&lt;li&gt;研究方法？&lt;/li&gt;
&lt;li&gt;假设要研究这个项目，你想要什么队友（两个老师分别问了这个问题，答了两次）&lt;/li&gt;
&lt;li&gt;如果你来到这个学校，我要你换个研究方向你愿意吗？你要换什么方向，为什么&lt;/li&gt;
&lt;li&gt;什么情况下你会换研究方向？（逐渐抽象）&lt;/li&gt;
&lt;li&gt;你对你现在的方向感兴趣，还是你想换的另一个方向？&lt;/li&gt;
&lt;li&gt;假如以后没人生小孩，全是老人，没钱发养老金，你是决策者你要怎么办&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt;：我和评委老师专业不对口，几乎没有专业知识的提问，反而&lt;strong&gt;future plan&lt;/strong&gt;引起了他们的兴趣所以一直在抽象地拷打（maybe我是最后一个，老师们都开始放松了随便乱问）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;2. &lt;a href=&#34;https://www.xiaohongshu.com/explore/67458e09000000000202a288?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBCrT4tjGLAuhlyZIu0hu8LFBIEyOeKqPwRHkO-jf_Q2A=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760345606&amp;amp;share_id=f02f9d8111414513a5b91099332f6ea8&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广第二批MPhil面经&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上午团队面试&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;围绕&lt;strong&gt;未来健康技术、宁静&lt;/strong&gt;的两个关键词进行团队合作搭积木，目标是未来城市的设计&lt;/li&gt;
&lt;li&gt;我负责的是老本行，搭建的&lt;strong&gt;能源电力部分&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;感觉这个团队合作需要&lt;strong&gt;多张嘴多动手，自圆其说即可&lt;/strong&gt;，队友都特别给力，也非常nice&lt;/li&gt;
&lt;li&gt;对于我这个i人其实面完精疲力竭，幸亏抽到了比较e的队友，被带动着也就慢慢热络了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;下午个人面试&lt;/strong&gt;（&lt;strong&gt;10分钟&lt;/strong&gt;的提问）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解释一下CLLLC，通俗一点给我们讲一下&lt;/li&gt;
&lt;li&gt;你为什么不保研&lt;/li&gt;
&lt;li&gt;你还申请了哪些学校（或单位）&lt;/li&gt;
&lt;li&gt;通俗解释一下High power与high power density是什么，怎么实现，作用是什么&lt;/li&gt;
&lt;li&gt;你的变换器怎么实现的low ripple低纹波&lt;/li&gt;
&lt;li&gt;你的参数是在仿真里枚举就行了吗，设计逻辑是什么&lt;/li&gt;
&lt;li&gt;你的意向导师是谁&lt;/li&gt;
&lt;li&gt;（英文提问）你觉得你的变换器还有什么地方可以改进，这个你能靠自己完成吗还是需要teamwork&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：存在英文提问但是可以中文回答，老师们都很好，果然是一所氛围特别好的学校，两天体验下来非常非常好。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;3. &lt;a href=&#34;https://www.xiaohongshu.com/explore/680d20c6000000000f032c93?note_flow_source=wechat&amp;amp;xsec_token=CBIMhMvpQJWY392bghHEfAwharwY8DrwQagLQtKp4Hcdk=&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广红鸟MPhil线下面试复盘&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;申请时间线&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.29春节准备材料 → 1.31完成投递 → 3.31收到面邀 → 4.17面试 → 4.25收到通知&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;线下面试复盘&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;小组群面项目为搭建未来城市，随机抽取的关键词为&lt;strong&gt;奇思妙想的 + 可持续生活&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;我主要关注的点为两个：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 快速满足客观要求&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我把规则分为了客观要求和主观要求，我的第一任务是快速满足最低的客观要求。搭建规则只对模块数量和尺寸做了要求，尺寸中相对最难满足的是最低高度。问题就转化为了如何在材料有限的情况下，用最简单的方法达到指定高度。&lt;/p&gt;
&lt;p&gt;最直接的是用手边的置物箱，但空间太小施展不开手脚。经过测量，椅子的长和宽刚好满足所给的尺寸要求。我的策略是将椅子直接放在桌子上作为城市的地表部分，椅子下面则作为地下部分。这样在满足要求的前提下，空间也更宽敞，搭建的时候也可以施展手脚。在快速把客观要求全部满足的前提后，就可以放开手脚对各模块进行自由发挥了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 主观要求的思考&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我的策略只有三个：&lt;strong&gt;做局部区域最优解，命运共同体，以人为本&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;第一&lt;/strong&gt;，面试整体的玩家数量是固定的，所以在资源和时间有限的情况下，我认为关键不是闭门造车，尽善尽美追求极致，而是只需要在某些方面或某个模块做得比其他组好就可以，做到&lt;strong&gt;局部最优解&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第二&lt;/strong&gt;，小的来说，每个人所在的小组是一个独立的城市，小组之间是竞争关系；但从大局来看，所有的组又都是一个&lt;strong&gt;命运共同体&lt;/strong&gt;，如果把城市与城市连接起来，所有的城市会构成一个更大的城市，连接起来后形成的效益将远远大于自身建设的效益。这一点最难的地方是在于如何把所有的组连通在一起，但最好玩的点也在于当把其中一部分组连接在一起之后，其余没有连接的组反而会失去越来越多的东西，从而迫使其加入连接。其实当把这场面试的设计看作如今世界局势的缩影时，也会很有趣。攒局搭建这个连接平台的团队当然会失去一部分精力和时间来做客户的开拓和运营，在自己的城市搭建上会吃亏，但这是长期可以一本万利的事情，所以值得去做。另外，在连接其他组的过程中，也可以提前了解各个组的进度和搭建模块特色，可以有效的为第一点中的局部最优解策略提供信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第三&lt;/strong&gt;，城市是由人构成的。城市的设计是以用户为中心来展开的，所以搭建策略从一开始就从人的&lt;strong&gt;衣食住行&lt;/strong&gt;四方面来进行规划的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4. &lt;a href=&#34;https://www.xiaohongshu.com/explore/67b8b99f000000002a002475?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBbZWt-5GzBBpEiSiIJjr6x4l6y793ZJCsrwK4xdVma0I=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760345905&amp;amp;share_id=cc0c1848bc4a4aa5a0ebfbd7e41dcea8&amp;amp;wechatWid=a38e28d828ad4076b345e342a9c6fefe&amp;amp;wechatOrigin=menu&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广第三批个人面面经（已推研）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下午个人面试要点&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;首先，能脱稿一定要脱稿脱稿！！！&lt;/strong&gt; 我就抱有侥幸心理没脱稿，老师上来第一个问题就是：你手里拿着的是小抄吗？（真的很压力）还好本人机智，笑着解释了一下。最后答辩完评委老师还特地叮嘱&amp;quot;下次就不要带着稿子了&amp;quot;（万幸是过了，当时面完差点以为自己凉了）。而且听说隔壁组一个没脱稿的uu被狠狠压力了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;不同老师问的问题风格有很大差异&lt;/strong&gt;，有些老师可能会问很专业的问题，有些老师的问题就比较general，遇到什么样的老师是不可控的，所以我们能做的就是准备好每个可能的问题。PPT上所有涉及到自己以前的论文、项目一定要再看看弄清楚每个细节，老师提的问题不管多不可控都是基于PPT的。我也遇到比较厉害的uu，有意在pre的时候挖一些坑，刚好评委老师就问到了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;下午面试包含一定压力面的成分&lt;/strong&gt;，所以面对评委老师的刨根究底一定要思路清晰，保持镇定。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;我被提问的问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：为什么转专业？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：因为巴拉巴拉巴拉巴拉（提前准备了顺利完成）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：你的研究计划好像港科广没有这方面的老师啊？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：详细阐述研究计划，有哪个老师是做这方面的…巴拉巴拉…&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：研究计划里某某某是什么意思呀？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：巴拉巴拉…解释一下&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：联系意向导师了吗？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：还没有（确实是本人拖延症又犯了）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：你手里拿着的是小抄吗？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：是我的讲稿，我想把所有细节都注意到，所以带了讲稿&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：你能具体讲讲xxx项目的结果吗？PPT上图是什么意思？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：巴拉巴拉巴拉巴拉（意料之中准备了）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;总之，以不变应万变&lt;/strong&gt;
&lt;strong&gt;5. &lt;a href=&#34;https://www.xiaohongshu.com/explore/6849c12c000000002101ab14?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CB20Qv6po2dCpk24KPlHkXecKzPI-j2H5gSdStHzSKUSE=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760346115&amp;amp;share_id=0c2a5c894c4246869a1bb65ec74f053d&amp;amp;wechatWid=a38e28d828ad4076b345e342a9c6fefe&amp;amp;wechatOrigin=menu&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广红鸟线下面试-面经分享最后一批随缘&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;报道前一天&lt;/strong&gt;：晚八点落地白云机场，坐地铁两个小时到学校附近，找了家宾馆住下开始做PPT，当时PPT完成度只有大概一半（换模版+稿子没写）。&lt;/p&gt;
&lt;p&gt;我PPT内容比较多，精简了好几次之后还是有大概&lt;strong&gt;20页&lt;/strong&gt;。其实也不是东西有多少，主要是之前东西都放到一页，导致图片和文字都很小很密，几次模拟面试反馈都不是很好（加上模版不太学术有点幼稚）。我索性直接重写了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二天&lt;/strong&gt;：报道之后五点下楼有老师领着逛学校，走了一下第二天面试地点路线。晚上吃饭之后就回宿舍做PPT写讲稿，整个Q&amp;amp;A和群面环节几乎没准备（之前忙毕设，就大致扫了几眼面经）。一直做到&lt;strong&gt;凌晨四点&lt;/strong&gt;，上床五点睡到七点半。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上午群面&lt;/strong&gt;：关键词是&lt;strong&gt;未来保健技术和sophisticated&lt;/strong&gt;，我们组氛围还ok，但是大家思路其实比较乱。搭积木还是有点费手的，而且我们的模块很容易倒塌，因为这个原因差一点没搭建完。老师提问的时候一定要听清，当时一个问题：每个人为了应对别的组提出的挑战都做了什么。我看当时没人说话就想打破冷场但是急匆匆的没有回答到点上，感觉是个失分项。群面在这里不详细讲，可以看其他帖子，推荐看我们同组队员的帖子&amp;quot;港科广红鸟硕士面经(被恨版)&amp;ldquo;里的描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下午个面&lt;/strong&gt;：我是最后一个，前边的人反应被拷打了或者提问时间没用完。我问答没有准备特别多，就准备了一些例如为什么选择项目这种比较general的问题，最后都没用到……一直在熟悉稿子，从一开始&lt;strong&gt;800词删到400词&lt;/strong&gt;还是背不下来，索性带平板进去念了，大概念两句背一句看一眼老师。&lt;/p&gt;
&lt;p&gt;主题我选择的是&lt;strong&gt;低空经济&lt;/strong&gt;，项目都是实体机器人和上面的算法，三个小项目一个大项目。中间穿插一些学工和比赛之类的经历。时间分配大概是个人介绍和动机一分钟，项目和经历三分钟，最后原因总结致谢一分钟。没有讲技术细节，图片视频比较多但是每页一两张。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q&amp;amp;A&lt;/strong&gt;：我的感觉是要多说，另外，最好表现的自信一点，本人平时比较e，面试的时候也是语速很快，思路也比较清晰。一开始几个老师是想压力的，但是每个问题问一个方面，我一般会多说好几个点综合性的讲，都超出了他们的预期，后面整个氛围都非常好了。面试结束的时候我说期望再见到你们，几个老师都笑了，有一个老师还让我去官网看一下他的实验室和研究方向（暗示我？）最后关门出去的时候听见几个老师笑着说：年轻人还是热情啊。感觉整体上是认可的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. &lt;a href=&#34;https://www.xiaohongshu.com/explore/6718bf950000000016022927?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBwAOpeg7jwqIUyMksgf-Il0Dw3KznZjyX2AV1cOq66AU=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760346158&amp;amp;share_id=1e75d75593334459ab3f046cd41bed68&amp;amp;wechatWid=a38e28d828ad4076b345e342a9c6fefe&amp;amp;wechatOrigin=menu&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广RBM第一批面经&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;申请时间线&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;9.1提交网申&lt;/li&gt;
&lt;li&gt;9.13通知补充材料&lt;/li&gt;
&lt;li&gt;10.14面邀&lt;/li&gt;
&lt;li&gt;10.23线下面试&lt;/li&gt;
&lt;li&gt;11.4推研&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;上午小组活动&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;感觉总体很玄学，注意要&lt;strong&gt;全程保持参与&lt;/strong&gt;，然后在&lt;strong&gt;交易阶段做出贡献&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;下午个人面&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;感觉也很玄学，准备了很多专业和研究项目的问题，完全没问到，都是很general的问题。&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;RP的应用场景，为什么工业4.0一直没有得到全面的推广&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;是否准备创业&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;如果让你选择，你是会深入学习本领域的知识，还是会去学习其他领域的知识&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;本科竞赛，自己做了什么&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;本科的TA工作经历&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;RP能给计划的研究领域（智能工业化）带来什么提升&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;RP的具体创新点&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;感觉面试老师应该都不是CS/AI/CE相关专业的，没问到专业问题。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;7.&lt;a href=&#34;https://www.xiaohongshu.com/explore/6904cfea000000000301966f?xsec_token=ABWEBsV8CjOxkXWOGeEWymXY6O04guA8mAXsqoSQqwfSo=&amp;amp;xsec_source=pc_search&amp;amp;source=unknown&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广红鸟26fall正式批面经&lt;/a&gt;
📌tl
10.22 报道、刷身份证、签名、付住宿费，有车接送到宿舍楼下，找到宿舍收拾东西/认识新舍友；会领到一张纸，上面有地图以及第二天面试时间
10.22 17:00，步行、宿舍楼下集合逛校园，可以看到第二天上午群面的场地
10.22 ～18:00之后，自行吃饭（可以体验一下学校食堂），回宿舍休息，想晚起的话记得准备明天的早饭
10.23 上午，先退房，然后把行李箱搬到面试的地方，拿身份证签到，现场抽签分组，抽主题，开始面试，有电子钟，注意把握时间
10.24 下午，拿身份证签到，现场抽签面试教室，拷贝ppt，到等候室等待面试，面试完可以直接走人&lt;/p&gt;
&lt;p&gt;📌面经：
1⃣上午小组面：
我们当天抽到的是“可持续生活”+“ 独自漫游的”，算是关联性比较直接的两个主题，当然这会导致大家能想到的东西差不多。总体的流程就是构思讨论、搭乐高、看其他小组的模型并提出挑战、交易以解决自己收到的挑战、向面试老师汇报。
建议：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;务必仔细看官网的评分表&lt;/li&gt;
&lt;li&gt;可以自己找AI模拟一下提示词，想想怎么布局、切合题目&lt;/li&gt;
&lt;li&gt;搭积木以及城市规划不是本质的考核内容，基本切题即可，搭的结构也不需要特别复杂，能说得通就行，抽象表述也ok&lt;/li&gt;
&lt;li&gt;但是，老师也有可能在大家都介绍完之后询问城市的功能区和内容，正常回答即可，没考虑到就及时承认&lt;/li&gt;
&lt;li&gt;感觉自己群面表现一般，在无领导小组中，不是发言很多的或者经常帮助组员的角色（没遇到机会，玄学），但是在交易环节比较活跃，老师应该也关注到了。所以不必太焦虑和纠结，正常表现就可以&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2⃣下午个面：
我选的是英文汇报+中文回答。
PPT的思路是讲故事，把自己的经历融入到故事里面去：为什么未来想做这个主题的工作，我有什么专业基础（这里可以展现标化的绩点、获奖、论文）来支撑未来的研究，然后也可以讲讲科研探索小故事，比如做了第一个项目之后思考到了什么问题，从而想做主题方向的研究，为此又做了哪些努力（论文、项目、实践），最后再往你的主题和构想上面靠一靠，讲讲为什么选择红鸟。
提问环节应该有压力面，正常回答即可。可能会问你的项目需要哪些学域的老师指导、需要哪些专业的同学参与、不同专业的合作者分别负责项目的哪些部分、有无创业经历、（有经历）为什么没有创业成果等等。
建议：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;准备充分，汇报只有五分钟，内容多的话语速要快，但是亲测现场语速更快（甚至会漏了一些点忘记讲），自己准备大概5.30，现场只有4分钟&lt;/li&gt;
&lt;li&gt;一些很难的问题一般都是压力面，所以如实回答，稳住心态，保持微笑 ：）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;8.&lt;a href=&#34;https://www.xiaohongshu.com/explore/69088033000000000701686b?xsec_token=AB0g3k8Fm6ofVBT6UbVgQwO5T9_cBsxgizX-oE_h7ZzWk=&amp;amp;xsec_source=pc_search&amp;amp;source=unknown&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;红鸟计划第一批推研！&lt;/a&gt;
10.1面邀-10.24面试-10.31下推 有始有终的十月！&lt;/p&gt;
&lt;p&gt;群面：智能工业化+Aurora
一开始担心和四个工科生一组会不会一句话说不出来？但很快联想到极光粒子能收集和有色金属后完美的发挥了商科生+e人的诡辩能力，个人觉得群面真的超超常发挥！&lt;/p&gt;
&lt;p&gt;个面：
①陈述：极限准备了一周，但我真的觉得我把个人陈述环节做的非常非常好（需要参考可以私信），紧扣“可持续生活”主题的同时，用了个word play搭了两条很巧妙的主线和暗线联系自身经历贯彻全部，三个导师真的一直在点头！（所以哪怕科研和项目背景较弱也不要怕！！商科生有自己的扬长避短的打法）&lt;/p&gt;
&lt;p&gt;②盘问：详细说了我CV里的一个项目经历和创业经历，前期自认为得心应手，直到有个老师突然灵魂发问，质疑我能否达到红鸟的学术要求？于是乱了阵脚…后面一顿胡说，时间到了之后，这个老师又给了我个追问，潦草收场。（好在结果满意）
总结：
庆幸自己作为极少数商科生脱颖而出，群面五进一（如果组员刷到这里，认出了我，我想说我也真心的感谢你们没有push我，和你们度过了非常快乐两小时）个面也是水来土掩。&lt;/p&gt;
&lt;p&gt;9.&lt;a href=&#34;https://www.xiaohongshu.com/explore/692c68fb000000001e00d2b0?xsec_token=ABilGWB6Une0uN2WfqWRRyo1xgvKUqzq_3lWlzw_Dn-OU=&amp;amp;xsec_source=pc_search&amp;amp;source=unknown&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;拿下港科广红鸟面试，只因我做对了这几件事&lt;/a&gt;
主播是25届mphil的最后一批面试，凭借一些面试的小技巧成功通过！
红鸟的面试分为团面与个面，这篇就先讲讲团面的部分。&lt;/p&gt;
&lt;p&gt;团面主要考察的是团队协作能力与创新思维，内容是用积木搭建未来城市。
主办方会在未来健康保健技术、可持续生活、智能工业化等题材中抽取一个范围，并用AI随机生成一个关键词。
我记得当时我们抽中的是海洋科技与经济，生成的关键词是空灵的。&lt;/p&gt;
&lt;p&gt;在团面环节，我认为有一些会对红鸟面试很加分的点：
第一，做你自己，不必强行角色扮演。
在无领导小组讨论过程中，你只要完成好自己负责的板块，在小组讨论中提出建设性意见，负责好自己里面的任务。
我认为团面的本质是看你是否具有与人合作的潜力，能否利用好团队优势进行分工，不必为了展现自己的领导能力而“过度表演”。&lt;/p&gt;
&lt;p&gt;第二，善用身边工具，带好便签纸。
在团面中，面试官并不纠结你搭建的是否能实现，重视你对想法的表达。这时候运用好便签纸就很重要，可以通过便签纸记下自己搭建部件想要达成的功能是什么，在便签纸中记下一条从What到How的路径，帮助自己与团队更好地记忆与表达设计理念。&lt;/p&gt;
&lt;p&gt;第三，天马行空不一定是贬义词
在积木环节，你可以提一些富有想像力的点，不必太拘泥于技术能否实现，只要能解释通，就可能成为设计的闪光点。
当时我们小组有位同学提出设计“泡泡”作为交通运输工具，灵感源于小时候在公园玩的浮力球。我们并没有详细阐述这个泡泡是怎么驱动，但提出了这一个新奇想法，显得我们有独特的创造力。&lt;/p&gt;
&lt;p&gt;第四，模块化思考是设计的重要能力
模块化建设城市我是很好的思路，我们小组把城市分为了多个不同功能模块，通过模块组合的方式进行城市规划。在团面开始前与小组进行头脑风暴，提出这个想法，可以在早期向面试官展现你们的建设思维，为后续的高效率建设提供方向。&lt;/p&gt;
&lt;p&gt;第五，言简意赅总结自己的贡献
在个人陈述部分，可以把表达重点放在这几个问题，尽可能展现自己的价值：
在头脑风暴环节中，你在团队中扮演的角色是什么？
你在头脑风暴中提出了什么建设性意见？
在制作过程，你和谁进行了合作？合作过程中如何分工的？你们的合作达到了什么效果？
在交易环节，你如何跟其他小组谈判？谈判发挥了什么作用？&lt;/p&gt;
&lt;h4&gt;PI调研&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;pi调研&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#pi%e8%b0%83%e7%a0%94&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;以免个人面试的时候被问到，所以这里得先准备一下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;陈昶昊&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;陈昶昊博士（Dr. Changhao Chen）是香港科技大学（广州）&lt;strong&gt;系统枢纽智能交通学域、信息枢纽人工智能学域联聘助理教授&lt;/strong&gt;，副研究员，博士生导师。获&lt;strong&gt;英国牛津大学计算机科学博士学位&lt;/strong&gt;，并在英国工程和自然科学研究委员会（EPSRC）资助下从事博士后研究。入选&lt;strong&gt;全球前2%顶尖科学家榜单（2024）&lt;/strong&gt;、**中国科协青年人才托举工程（2022）**和国际机器人科学与系统大会（Robotics: Science and Systems）&lt;strong&gt;先锋者（2020）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;组建港科大（广州）&lt;strong&gt;PEAK-Lab (Perception, Embodiment, Autonomy and Kinematics)&lt;strong&gt;课题组，现有博士生及科研助理&lt;/strong&gt;10人&lt;/strong&gt;。课题组聚焦&lt;strong&gt;具身智能和无人系统研究&lt;/strong&gt;，致力于构建在动态开放环境交互的具身智能体，服务&lt;strong&gt;低空经济、智能交通和智慧城市&lt;/strong&gt;等应用领域。主持&lt;strong&gt;国家自然科学基金&lt;/strong&gt;等纵向项目&lt;strong&gt;5项&lt;/strong&gt;，先后三次获/提名权威学术会议优秀论文奖。在人工智能、机器人和智能交通领域已发表高水平论文&lt;strong&gt;50多篇&lt;/strong&gt;，包括TITS、TNNLS、TMC、TIP、TIV等领域权威期刊以及CVPR、ICCV、AAAI、ECCV、ICRA、IROS、WWW、MobiCom、MobiSys、SenSys等国际顶级会议，&lt;strong&gt;谷歌学术引用超过2900次&lt;/strong&gt;。长期担任NeurIPS、ICML、ICLR、CVPR、TRO、IJRR、IJCV、TAC等&lt;strong&gt;30多个&lt;/strong&gt;国际会议、期刊的程序委员会委员/元审稿人/审稿人，中国科协会刊《科技导报》首届青年编委，受邀担任国际机器人与自动化会议（ICRA-2024）以及中俄&amp;quot;导航与运动控制&amp;quot;青年学者论坛的分会场主席。已授权国家发明专利、国际PCT专利、美国和欧洲专利共&lt;strong&gt;14项&lt;/strong&gt;，包含1项在英国成功成果转化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主页&lt;/strong&gt;：https://changhao-chen.github.io/&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;研究方向&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;人工智能在计算机视觉和自然语言处理领域取得显著进展，为**通用人工智能（AGI）**奠定基础。尽管大语言模型在虚拟环境中展现巨大潜力，将人工智能融入现实世界仍面临诸多挑战。未来十年，&lt;strong&gt;具身智能将推动下一次技术革命&lt;/strong&gt;。实现机器智能从虚拟、受限环境延伸到物理、开放世界，需要深入理解三维场景、本体运动、具身智能以及高效计算。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;三维空间感知&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;三维空间感知将多视图几何与深度神经网络结合，提供从低层级特征感知、中间层级几何表达到高层级语义理解。课题组已取得三方面突破：构建自监督学习的空间感知框架，实现二维到三维数据的直接特征提取匹配、基于特征元学习的视觉定位与建图、自监督位姿与深度估计等；构建城市级别的视觉定位与三维重建框架，包括视觉定位Transformer大模型以及多神经渲染网络的合并渲染、匹配与定位；针对烟、雾等视觉受限环境，实现红外相机和毫米波雷达的稀疏数据信号处理，构建超越人类视界的感知系统。未来将进一步面向动态变化的实际场景，开展新场景自适应的空间感知研究，实现感知系统的长效终身学习以及学习模型的可信度分析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;运动状态估计&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;三维空间运动的智能体需进行速度、姿态、位置等系统状态的估计，以支撑规划和决策。课题组开展以本体感知为核心的运动估计研究，在世界范围内首次提出基于深度学习的惯性导航算法并公开相关数据集，得到全球&lt;strong&gt;40多个国家&lt;/strong&gt;的研究人员关注和使用，被后续研究者拓展为数十种算法模型，用于解决四足机器人、无人机、水下机器人、车辆、智能穿戴设备等载体的本体运动状态估计难题。未来将进一步研究结合物理模型和机器学习的动态建模，通过可学习的状态空间模型和神经常微分方程等方法，实现理解物理世界的运动估计、预测以及状态分析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身导航决策&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;实现三维空间导航与交互是通往通用智能的重要目标。课题组面向高效可靠自主导航的需求，提出基于特征选择的多传感器融合方法，从数据中学习到传感器间的融合策略，实现强鲁棒多源导航。课题组构建融合深度神经网络的卡尔曼滤波模型，实现具备李雅普诺夫稳定的导航动力学模型自动构建。课题组探索了融合传统控制策略与强化学习的决策控制模型，实现导航策略更高效学习。未来研究将借鉴人类大脑与小脑的功能分工，提出层次化的具身导航与操纵决策框架，通过大语言模型理解语言指令用于全局规划与决策，通过具身基础模型实现局部运动规划和控制，最终实现智能体自主导航、探索、操纵与协作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;高效神经网络计算&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未来社会将依赖数十亿台具身智能系统，传统通用计算架构难以满足低成本、低能耗和低延迟的需求，需探索面向具身智能的专用计算框架。课题组已突破轻量化神经网络知识蒸馏和量化、视觉感知FPGA硬件加速单元、硬件加速的视觉定位与建图系统等关键技术。未来将进一步研究通用计算与神经网络计算融合的新型智能计算框架，同时考虑计算特性和约束条件进行软硬件协同设计，优化系统并行计算需求，加速矩阵乘法等热点函数，高效实现系统整体迭代优化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;汪军&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3DGS World Model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;马骏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LOVON (Legged Open-Vocabulary Object Navigator)&lt;/strong&gt;：足式机器人、开放世界导航、大语言模型任务规划、机器人学习&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;分层任务规划&lt;/strong&gt;：利用大语言模型（LLMs）将复杂的自然语言指令（如&amp;quot;先跑向椅子，再靠近行人&amp;rdquo;）拆解为一系列可执行的子任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开放词汇视觉检测&lt;/strong&gt;：让机器人能够识别超出预定义类别的各种物体，大大增强了在陌生环境中的适应能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语言-运动模型&lt;/strong&gt;：将文字指令和视觉信息直接转化为控制机器人运动的向量，实现精准的导航控制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;抗干扰与鲁棒性&lt;/strong&gt;：项目专门设计了应对视觉画面抖动、目标临时丢失等现实问题的策略，提升了系统在真实场景下的可靠性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;梁俊卫&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维场景理解、具身智能&lt;/strong&gt;；为零样本3D视觉定位方法&lt;strong&gt;SeeGround&lt;/strong&gt;的博士生导师&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SeeGround方法&lt;/strong&gt;，创新性地利用2D视觉语言模型来完成零样本的3D物体定位，无需昂贵的3D标注数据，在复杂场景中表现出色。这项技术对于机器人在未知环境中进行自主操作至关重要。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>港科广红鸟面试准备</title>
      <link>http://localhost:1313/blog/2025/2025-12-8-hkustgz-preparation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-8-hkustgz-preparation/</guid>
      <description>
        
        
        &lt;h1&gt;红鸟面试准备&lt;/h1&gt;&lt;p&gt;上面那则hk-master的文档有点太满了，所以放在这里。&lt;/p&gt;
&lt;h2&gt;面经&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;面经&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%9d%a2%e7%bb%8f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;
  &lt;p&gt;12/12 Update&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;群面&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;群面&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%be%a4%e9%9d%a2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;群面很糟糕，因为我是&lt;strong&gt;第一个发言的&lt;/strong&gt;，住宅区不太好体现&lt;strong&gt;quintessencial 低空经济&lt;/strong&gt;，只能更多地去说区位优势，并且&lt;strong&gt;没有cue队友&lt;/strong&gt;（得分点的&lt;strong&gt;teamwork我觉得废了&lt;/strong&gt;）。老师提问两个问题，第二个老师的问题是我们的作品是如何体现&lt;strong&gt;精髓典范&lt;/strong&gt;这个关键词的（在我们已经在第一个问题回答了的前提下），通过这个可以判断&lt;strong&gt;前期他们根本没有关注&lt;/strong&gt;（或者说他们自己也不理解这个关键词），自然也没法关注到&lt;strong&gt;我确实做了一些teamwork&lt;/strong&gt;，而且我发言没有框架，串联头脑风暴和交易适应的也不好。&lt;/p&gt;
&lt;p&gt;我还是挺委屈的，交易前的头脑风暴阶段我搭了&lt;strong&gt;三个冗余模块&lt;/strong&gt;（&lt;strong&gt;工作量相对单个积木那种来说非常大&lt;/strong&gt;，模块大小超过掌心），其中一个被&lt;strong&gt;两名队友拿去当救生舱用成为他们的模块功劳了&lt;/strong&gt;（&lt;strong&gt;我不好意思指出&lt;/strong&gt;），一个被别组的舍友&lt;strong&gt;入室抢劫般以一个金条的价格交易出去了&lt;/strong&gt;，一个被另一个队友交易出去了，&lt;strong&gt;没有办法成为我的工作成果&lt;/strong&gt;（演示时&lt;strong&gt;没法拿起来给老师们看&lt;/strong&gt;），而多出去Social拿到交易模块的人就可以举起来给老师看看其他组所做的。&lt;/p&gt;
&lt;p&gt;这有点让我回忆起&lt;strong&gt;ITP面试&lt;/strong&gt;时候的场景了，貌似大差不差，最后&lt;strong&gt;倒在了二面校友面&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;个面&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;个面&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%aa%e9%9d%a2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;0、&lt;/strong&gt; 很多学科都跟AI相关，有必要吗？像是你&lt;strong&gt;BME本身不太需要&lt;/strong&gt;？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、&lt;/strong&gt; 拷打&lt;strong&gt;BME为什么做这么多CS相关&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、&lt;/strong&gt; 必须&lt;strong&gt;放弃推免才能申请吗&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、&lt;/strong&gt; 如果你是项目制，你打算怎样的项目和怎样的队友，你的缺陷是什么&lt;/p&gt;
&lt;p&gt;总的来说，面试总共在5个房间进行，不同房间的老师提问风格不一样，我所在的&lt;strong&gt;421B就属于非常友善的那一组&lt;/strong&gt;，只有最左边那个老师（可能是化工相关专业）提的问题不算general。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;同学A：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上了红鸟之后如何提升自己&lt;/li&gt;
&lt;li&gt;排名够吗？为什么不去保研&lt;/li&gt;
&lt;li&gt;有没有offer（Mphil新南威尔士）&lt;/li&gt;
&lt;li&gt;化工吸收（专业问题）&lt;/li&gt;
&lt;li&gt;对机器学习代码有多少了解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;同学B：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;郑大为什么不接北理推免offer？&lt;/li&gt;
&lt;li&gt;（回答不接之后问）你是不是想创业？为什么想创业？&lt;/li&gt;
&lt;li&gt;质疑项目经历：工程化的东西，没有很难&lt;/li&gt;
&lt;li&gt;有没有什么小组合作项目经历&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同学B就是421A的老师问的，而同学A和我一样是421B，可以看出&lt;strong&gt;421B确实相当温柔了&lt;/strong&gt;，下面还有其他同学被压力的吐槽：
&lt;img src=&#34;http://localhost:1313/blog/2025/rb-question.jpg&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;不拉我通过的话也就算了吧。毕竟这样复盘下来，失分点主要在这个群面上，因为他们自己的筛选机制错过了我这样优秀的学生（迫真），那是他们招生组的损失，更何况他们上一学年貌似也出现了一些&lt;a href=&#34;https://www.xiaohongshu.com/explore/67d941ad000000000900e0d5?xsec_token=ABMi9Wyxxdx1vWShI9d3ugsSjE5ySJDTFpTcSq_T0rl5Y=&amp;amp;xsec_source=pc_search&amp;amp;source=unknown&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;状况&lt;/a&gt;，广州南沙也非常郊区看起来不方便找实习。而且第二批面试分为3天，我隶属的第2天就有66名学生参与，整个第二批可想而知池子有多大。&lt;/p&gt;
&lt;h2&gt;群面&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;群面-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%be%a4%e9%9d%a2-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在流程方面，上午10:00开始：头脑风暴，A+B，B是五大主题，A由AI随机生成一个形容词。&lt;/p&gt;
&lt;h3&gt;1. 低空经济 (Low-altitude Economy)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-低空经济-low-altitude-economy&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bd%8e%e7%a9%ba%e7%bb%8f%e6%b5%8e-low-altitude-economy&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;eVTOL (电动垂直起降飞行器)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;evtol-电动垂直起降飞行器&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#evtol-%e7%94%b5%e5%8a%a8%e5%9e%82%e7%9b%b4%e8%b5%b7%e9%99%8d%e9%a3%9e%e8%a1%8c%e5%99%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;空中的士&amp;quot;或&amp;quot;放大版的大疆无人机（能坐人）&amp;quot;。不需要跑道，电动的，声音小，适合在城市楼顶起降。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;无人机物流&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;无人机物流&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%97%a0%e4%ba%ba%e6%9c%ba%e7%89%a9%e6%b5%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;美团/顺丰空投&amp;rdquo;。外卖不走马路，走窗户。关键技术是航路规划（不撞楼）和末端投放（不砸人）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;低空旅游&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;低空旅游&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bd%8e%e7%a9%ba%e6%97%85%e6%b8%b8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：坐直升机/eVTOL看风景。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;农业植保无人机&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;农业植保无人机&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%86%9c%e4%b8%9a%e6%a4%8d%e4%bf%9d%e6%97%a0%e4%ba%ba%e6%9c%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;农田洒水机&amp;rdquo;。大疆在这个领域很强，用无人机喷农药、播种，效率是人的几十倍。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;2. 未来健康 (Future Health)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-未来健康-future-health&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e6%9c%aa%e6%9d%a5%e5%81%a5%e5%ba%b7-future-health&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;脑机接口 (BCI)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;脑机接口-bci&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%84%91%e6%9c%ba%e6%8e%a5%e5%8f%a3-bci&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;意念控制&amp;rdquo;。在大脑植入芯片或戴个头盔，捕获脑电波，让瘫痪的人能控制鼠标或机械臂（像马斯克的Neuralink）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;AI辅助诊断&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ai辅助诊断&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ai%e8%be%85%e5%8a%a9%e8%af%8a%e6%96%ad&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;AI看片&amp;rdquo;。医生看CT要10分钟，AI看只要1秒，还能发现肉眼看不到的小结节。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;基因编辑 (CRISPR)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基因编辑-crispr&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e5%9b%a0%e7%bc%96%e8%be%91-crispr&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;上帝的手术刀&amp;rdquo;。像编辑Word文档一样修改DNA，把致病的片段剪掉，换上好的。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;远程手术&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;远程手术&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%bf%9c%e7%a8%8b%e6%89%8b%e6%9c%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;5G隔空开刀&amp;rdquo;。医生在北京操作机械臂，给新疆的病人做手术，延迟极低。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;可穿戴监测&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;可穿戴监测&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8f%af%e7%a9%bf%e6%88%b4%e7%9b%91%e6%b5%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：Apple Watch的医疗级进阶版。贴在皮肤上的柔性电路，能测汗液成分、血糖等。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;3. 可持续生活 (Sustainable Living)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-可持续生活-sustainable-living&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%8f%af%e6%8c%81%e7%bb%ad%e7%94%9f%e6%b4%bb-sustainable-living&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;零碳建筑&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;零碳建筑&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%9b%b6%e7%a2%b3%e5%bb%ba%e7%ad%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;自给自足的房子&amp;rdquo;。房子本身发电（光伏）= 房子消耗的电。不给地球排二氧化碳。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;垂直农业&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;垂直农业&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9e%82%e7%9b%b4%e5%86%9c%e4%b8%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;摩天大楼种菜&amp;rdquo;。在市中心的写字楼里，一层层架子上用水培技术种菜，省地、省水、不需要运输。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;水循环系统&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;水循环系统&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b0%b4%e5%be%aa%e7%8e%af%e7%b3%bb%e7%bb%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;中水回用&amp;rdquo;。洗澡水过滤后用来冲厕所，雨水收集起来浇花。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;可降解材料&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;可降解材料&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8f%af%e9%99%8d%e8%a7%a3%e6%9d%90%e6%96%99&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;吃土塑料&amp;rdquo;。袋子扔土里，几个月就被细菌吃没了，变回泥土。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;共享经济&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;共享经济&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%b1%e4%ba%ab%e7%bb%8f%e6%b5%8e&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;只用不买&amp;rdquo;。共享单车、共享充电宝、共享雨伞。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;4. 智能工业化 (Smart Industry)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-智能工业化-smart-industry&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e6%99%ba%e8%83%bd%e5%b7%a5%e4%b8%9a%e5%8c%96-smart-industry&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;数字孪生 (Digital Twin)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;数字孪生-digital-twin&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%95%b0%e5%ad%97%e5%ad%aa%e7%94%9f-digital-twin&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;虚拟克隆体&amp;rdquo;。工厂里有一台机器，电脑里有一个一模一样的3D模型。机器动，模型也动；模型预测机器明天会坏，机器明天真就坏了。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;柔性制造&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;柔性制造&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9f%94%e6%80%a7%e5%88%b6%e9%80%a0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;个性化定制流水线&amp;rdquo;。以前一条线只能产黑色T型车，现在这条线上一秒产红色SUV，下一秒产蓝色跑车。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;工业物联网 (IIoT)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;工业物联网-iiot&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%b7%a5%e4%b8%9a%e7%89%a9%e8%81%94%e7%bd%91-iiot&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;机器说话&amp;rdquo;。螺丝刀、机械臂、传送带都连网，互相发数据，协同工作。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;人机协作机器人 (Cobots)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;人机协作机器人-cobots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%ba%e6%9c%ba%e5%8d%8f%e4%bd%9c%e6%9c%ba%e5%99%a8%e4%ba%ba-cobots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;不伤人的机械臂&amp;rdquo;。以前机械臂要把人围起来怕打死人，现在的Cobot碰到人会自动停，可以跟人肩并肩干活。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;5. 海洋科技 (Ocean)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-海洋科技-ocean&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e6%b5%b7%e6%b4%8b%e7%a7%91%e6%8a%80-ocean&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;海水淡化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;海水淡化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b5%b7%e6%b0%b4%e6%b7%a1%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：把咸水变纯净水。中东国家常用。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;海上风电&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;海上风电&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b5%b7%e4%b8%8a%e9%a3%8e%e7%94%b5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：把大风车插在海里。海风比陆地风大且稳。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;海底数据中心&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;海底数据中心&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b5%b7%e5%ba%95%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：&amp;ldquo;把服务器扔海里&amp;rdquo;。微软干过。因为海底冷，省了空调电费；而且海底没氧气，机器不容易氧化。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;深海采矿&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;深海采矿&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b7%b1%e6%b5%b7%e9%87%87%e7%9f%bf&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：去海底捡&amp;quot;土豆&amp;rdquo;（多金属结核），里面全是稀有金属。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;海上漂浮城市&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;海上漂浮城市&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b5%b7%e4%b8%8a%e6%bc%82%e6%b5%ae%e5%9f%8e%e5%b8%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;人话&lt;/strong&gt;：像巨型航母或积木一样的城市，浮在水面上，应对海平面上升。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：我不知道可不可以带平板，可以带平板的话能不能问AI，根据面经下午的个面有人带平板过的（当稿子）&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;城市基础五大系统&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;城市基础五大系统&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%8e%e5%b8%82%e5%9f%ba%e7%a1%80%e4%ba%94%e5%a4%a7%e7%b3%bb%e7%bb%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;城市类型分类&lt;/strong&gt;：根据产业定位，城市可分为&lt;strong&gt;农业城&lt;/strong&gt;（第一产业）、&lt;strong&gt;工业城&lt;/strong&gt;（第二产业）、&lt;strong&gt;旅游城&lt;/strong&gt;（第三产业）等。不同城市类型会突出不同的系统模块。例如：&lt;strong&gt;寒冷的智能工业化城市&lt;/strong&gt; = 具备高度智能工业化生产能力，且能够在严寒环境中提供可持续的生活条件和高效管理的未来城市，包含智能工业中心、严寒能源供应与管理系统、智能交通网络、常温智能居住区、抗寒垂直农业与生态等模块。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模块序号&lt;/th&gt;
          &lt;th&gt;模块名称&lt;/th&gt;
          &lt;th&gt;功能定义&lt;/th&gt;
          &lt;th&gt;ZJU仪器切入点&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Module 1&lt;/td&gt;
          &lt;td&gt;核心产业区 (The Core)&lt;/td&gt;
          &lt;td&gt;这是题眼。根据抽到的5大主题变身（如工厂、医院、机场）。&lt;/td&gt;
          &lt;td&gt;部署自动化流水线 / 手术机器人 / 植保无人机。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Module 2&lt;/td&gt;
          &lt;td&gt;能源动力区 (Energy Hub)&lt;/td&gt;
          &lt;td&gt;给城市供电。风/光/核/地热。&lt;/td&gt;
          &lt;td&gt;能源管理系统(EMS)：部署传感器监控能耗，进行PID调节。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Module 3&lt;/td&gt;
          &lt;td&gt;立体交通网 (Transport)&lt;/td&gt;
          &lt;td&gt;人流物流。地面/地下/空中。&lt;/td&gt;
          &lt;td&gt;轨迹规划算法：无人驾驶调度，防碰撞系统。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Module 4&lt;/td&gt;
          &lt;td&gt;生活与生态区 (Living &amp;amp; Eco)&lt;/td&gt;
          &lt;td&gt;给人住的 + 处理垃圾/水的。&lt;/td&gt;
          &lt;td&gt;非侵入式健康监测：智能家居里藏着传感器，养老不用穿戴设备。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Module 5&lt;/td&gt;
          &lt;td&gt;中央指挥大脑 (The Brain)&lt;/td&gt;
          &lt;td&gt;你的主场。控制中心/数据中心。&lt;/td&gt;
          &lt;td&gt;数字孪生/IoT平台：所有传感器数据汇聚于此，大屏幕实时报警。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;针对五大主题的应用&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;针对五大主题的应用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%92%88%e5%af%b9%e4%ba%94%e5%a4%a7%e4%b8%bb%e9%a2%98%e7%9a%84%e5%ba%94%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;1. 低空经济 (Low-altitude Economy)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-低空经济-low-altitude-economy-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bd%8e%e7%a9%ba%e7%bb%8f%e6%b5%8e-low-altitude-economy-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;M1 核心产业&lt;/strong&gt;：eVTOL垂直起降枢纽（像蜂巢一样的停机坪）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M2 能源&lt;/strong&gt;：分布式换电站（无人机飞累了直接换电池，不用充）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M3 交通&lt;/strong&gt;：3D空域航道（用绳子拉出空中轨道，分层飞行）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M4 生活&lt;/strong&gt;：空投接收阳台（每家每户窗外有个伸缩平台接外卖）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M5 大脑&lt;/strong&gt;：空域流量监管塔（雷达+视觉识别，防止撞机）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. 未来健康 (Future Health)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-未来健康-future-health-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e6%9c%aa%e6%9d%a5%e5%81%a5%e5%ba%b7-future-health-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;M1 核心产业&lt;/strong&gt;：个性化基因治疗中心 或 脑机接口康复仓&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M2 能源&lt;/strong&gt;：生物质能发电厂（利用医疗废弃物发电）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M3 交通&lt;/strong&gt;：急救绿色通道（地下真空管道，胶囊列车运送器官/病人）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M4 生活&lt;/strong&gt;：全适老化社区（防摔倒地板+情绪监测墙壁）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M5 大脑&lt;/strong&gt;：全民健康云平台（实时处理所有市民的心跳血压数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. 可持续生活 (Sustainable Living)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-可持续生活-sustainable-living-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%8f%af%e6%8c%81%e7%bb%ad%e7%94%9f%e6%b4%bb-sustainable-living-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;M1 核心产业&lt;/strong&gt;：垂直农业塔（每一层都种菜，像DNA螺旋结构）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M2 能源&lt;/strong&gt;：光伏玻璃幕墙 + 雨水收集净化罐&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M3 交通&lt;/strong&gt;：共享单车/步行绿道网络（禁止机动车）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M4 生活&lt;/strong&gt;：模块化可降解住宅（房子是积木拼的，不想住了拆了换地方）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M5 大脑&lt;/strong&gt;：碳足迹追踪中心（计算每个人排了多少碳，以此发货币）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4. 智能工业化 (Smart Industry)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-智能工业化-smart-industry-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e6%99%ba%e8%83%bd%e5%b7%a5%e4%b8%9a%e5%8c%96-smart-industry-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;M1 核心产业&lt;/strong&gt;：黑灯工厂（无人工厂，机械臂自动作业）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M2 能源&lt;/strong&gt;：微型核聚变反应堆（工业耗电大，需要强能源）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M3 交通&lt;/strong&gt;：AGV自动物流小车轨道（地面全是二维码，小车跑来跑去）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M4 生活&lt;/strong&gt;：职住一体胶囊公寓（工人住在工厂楼上，下楼上班）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M5 大脑&lt;/strong&gt;：工业数字孪生中心（你的测控强项：预测性维护，机器坏之前先报警）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;5. 海洋科技 (Ocean)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-海洋科技-ocean-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e6%b5%b7%e6%b4%8b%e7%a7%91%e6%8a%80-ocean-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;M1 核心产业&lt;/strong&gt;：深海矿产采集站 或 海底数据中心&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M2 能源&lt;/strong&gt;：潮汐能/波浪能发电机（利用海浪晃动发电）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M3 交通&lt;/strong&gt;：潜水艇驳接港口&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M4 生活&lt;/strong&gt;：海上漂浮居住岛（像荷叶一样漂在水面）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M5 大脑&lt;/strong&gt;：水下声呐监测网（监控鱼群和海啸）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;用&amp;quot;形容词&amp;quot;进行针对性调整&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;用形容词进行针对性调整&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%94%a8%e5%bd%a2%e5%ae%b9%e8%af%8d%e8%bf%9b%e8%a1%8c%e9%92%88%e5%af%b9%e6%80%a7%e8%b0%83%e6%95%b4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;1. 负面/困难类形容词（如：寒冷的、焦虑的、破碎的、危险的）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-负面困难类形容词如寒冷的焦虑的破碎的危险的&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e8%b4%9f%e9%9d%a2%e5%9b%b0%e9%9a%be%e7%b1%bb%e5%bd%a2%e5%ae%b9%e8%af%8d%e5%a6%82%e5%af%92%e5%86%b7%e7%9a%84%e7%84%a6%e8%99%91%e7%9a%84%e7%a0%b4%e7%a2%8e%e7%9a%84%e5%8d%b1%e9%99%a9%e7%9a%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;策略&lt;/strong&gt;：加&amp;quot;防御&amp;quot;和&amp;quot;冗余&amp;quot;&lt;/p&gt;

&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;寒冷的&lt;/strong&gt; → 给M2能源加&amp;quot;供热管道&amp;quot;，给M4住宅加&amp;quot;保温层&amp;quot;，M1工厂变成&amp;quot;全封闭式&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;焦虑的&lt;/strong&gt; → 给M4生活区加&amp;quot;心理疗愈花园&amp;quot;，M5大脑加强&amp;quot;隐私加密算法&amp;quot;，M3交通强调&amp;quot;绝对安全防撞&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;危险的&lt;/strong&gt; → 给整个城市加&amp;quot;防护罩&amp;quot;，M5大脑变成&amp;quot;灾难预警中心&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. 正面/抽象类形容词（如：快乐的、甚至、漫游的、无形的）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-正面抽象类形容词如快乐的甚至漫游的无形的&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e6%ad%a3%e9%9d%a2%e6%8a%bd%e8%b1%a1%e7%b1%bb%e5%bd%a2%e5%ae%b9%e8%af%8d%e5%a6%82%e5%bf%ab%e4%b9%90%e7%9a%84%e7%94%9a%e8%87%b3%e6%bc%ab%e6%b8%b8%e7%9a%84%e6%97%a0%e5%bd%a2%e7%9a%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;策略&lt;/strong&gt;：加&amp;quot;体验&amp;quot;和&amp;quot;连接&amp;quot;&lt;/p&gt;

&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;快乐的&lt;/strong&gt; → M1产业里增加&amp;quot;多巴胺制造&amp;quot;，M4生活区增加&amp;quot;游戏化设施&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;独自漫游的&lt;/strong&gt; → M3交通变成&amp;quot;单人飞行器&amp;quot;，M4住宅变成&amp;quot;移动房车&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无形的&lt;/strong&gt; → 强调M5大脑（看不见的网），实体建筑都做成透明的或者地下的&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;评分标准&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;评分标准&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%af%84%e5%88%86%e6%a0%87%e5%87%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：这一块有点冷冰冰（形式主义）了，有点像卡戴珊那种莫名其妙的东西必须得做。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h4&gt;头脑风暴阶段&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;头脑风暴阶段&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%a4%b4%e8%84%91%e9%a3%8e%e6%9a%b4%e9%98%b6%e6%ae%b5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;提出了至少一个具体的想法&lt;/li&gt;
&lt;li&gt;提出了至少一个澄清性问题或重述了队友的想法&lt;/li&gt;
&lt;li&gt;提出了至少一个在方法或内容上不同于其他人的想法&lt;/li&gt;
&lt;li&gt;口头承认了队友反馈，或根据反馈提出了调整建议&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;制作城市阶段&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;制作城市阶段&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%88%b6%e4%bd%9c%e5%9f%8e%e5%b8%82%e9%98%b6%e6%ae%b5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;尺寸要求：长、宽、高分别为 75.5cm、51.5cm、&amp;gt;43cm&lt;/li&gt;
&lt;li&gt;在项目的某个方面进行了实际操作（例如，放置材料、调整设计等）&lt;/li&gt;
&lt;li&gt;负责完成了至少一项分配的任务，并且无需提醒&lt;/li&gt;
&lt;li&gt;与另一名队员在至少一项具体任务上合作（如规划或构建）&lt;/li&gt;
&lt;li&gt;在构建阶段遇到问题时提出了至少一个解决方案&lt;/li&gt;
&lt;li&gt;至少一次分配或委派任务给队友体现领导能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;交易/适应阶段&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;交易适应阶段&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%a4%e6%98%93%e9%80%82%e5%ba%94%e9%98%b6%e6%ae%b5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;在交易阶段提出了至少一次交易或模块交换的建议&lt;/li&gt;
&lt;li&gt;至少与其他团队进行了一次谈判，无论是提供还是接受条件&lt;/li&gt;
&lt;li&gt;根据挑战任务提出了至少一个设计调整建议或进行调整&lt;/li&gt;
&lt;li&gt;在挑战任务引入时保持参与而没有退缩或失去参与感&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;最终展示阶段&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;最终展示阶段&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%80%e7%bb%88%e5%b1%95%e7%a4%ba%e9%98%b6%e6%ae%b5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;清晰地演讲自己在项目中的角色，无需队友提示&lt;/li&gt;
&lt;li&gt;提出了至少一项团队应对挑战任务的行动建议体现领导力&lt;/li&gt;
&lt;li&gt;提出了一个直接应对挑战的解决方案（例如，功能改动，增加新特性等）&lt;/li&gt;
&lt;li&gt;鼓励了至少一位队友参与或征求了他们的意见&lt;/li&gt;
&lt;li&gt;在展示过程中至少回答了评委的一个问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;模版&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;模版&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a8%a1%e7%89%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;“各位评委老师好，我是包博文。在这个项目中，我主要担任的是&lt;strong&gt;技术落地与系统整合（Technical Integrator）&lt;strong&gt;的角色。”
“在头脑风暴阶段，面对【题目形容词 + 主题】这个命题，大家一开始主要关注在概念发散上。 我的贡献是引入了&lt;/strong&gt;系统工程/测控专业&lt;/strong&gt;的视角，提出了【你的核心Idea】的概念。 具体来说，我建议不仅要关注建筑外观，更要通过【具体技术手段，如&lt;strong&gt;传感器/IoT/闭环控制&lt;/strong&gt;】来解决【题目形容词】带来的挑战。 这个建议确立了我们后续**‘技术驱动’**的搭建基调。”&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;(示例：面对‘脆弱的海洋’，我建议不仅要建岛屿，更要引入**‘分布式传感器网络’&lt;strong&gt;，通过&lt;/strong&gt;实时监测环境数据&lt;strong&gt;来预警灾害，从而化解&lt;/strong&gt;‘脆弱’&lt;strong&gt;这一难题。)
“在制作阶段，我主要负责【你的模块】的搭建，期间我与负责【队友模块】的【队友名】进行了&lt;/strong&gt;深度合作**。 分工上，他负责【外观/结构/上层建筑】，而我负责【&lt;strong&gt;底层逻辑/能源/传感器/内部连接&lt;/strong&gt;】。 合作中，为了解决【具体困难，如&lt;strong&gt;高度/稳定性&lt;/strong&gt;】的问题，我利用【&lt;strong&gt;物理原理/材料特性&lt;/strong&gt;】辅助他进行了加固。 最终效果是，我们的模型不仅满足了【&lt;strong&gt;硬性指标&lt;/strong&gt;】，更实现了【&lt;strong&gt;功能上的互联互通&lt;/strong&gt;】。”
(示例：我与负责主体结构的B同学合作。他负责搭建高塔，我负责底部的&lt;strong&gt;海上风力发电底座&lt;/strong&gt;。为了解决&lt;strong&gt;高度不够&lt;/strong&gt;的问题，我利用&lt;strong&gt;风机叶片的长度优势&lt;/strong&gt;，帮助团队轻松突破了&lt;strong&gt;43cm的限制&lt;/strong&gt;，同时为他的建筑提供了&lt;strong&gt;稳定的能源概念支撑&lt;/strong&gt;。)
“在交易环节，面对【挑战名称】的突发状况，我主动与【对手组名】进行了谈判。 我的策略是**‘价值交换’&lt;strong&gt;，我用我们多余的【本组资源】换取了他们的【急需模块】。 这一谈判的关键作用在于，它让我们在不推翻原有设计的前提下，以&lt;/strong&gt;最低成本&lt;strong&gt;引入了【&lt;strong&gt;外部技术&lt;/strong&gt;】，成功解决了【挑战难题】，体现了红鸟倡导的&lt;/strong&gt;跨学科融合精神**。”
(示例：面对**‘噪音污染’&lt;strong&gt;挑战，我用我们的&lt;/strong&gt;深海鱼油&lt;strong&gt;换取了隔壁组的&lt;/strong&gt;‘多孔吸音材料’&lt;strong&gt;。这让我们引入了&lt;/strong&gt;物理降噪层**，配合我的&lt;strong&gt;主动降噪算法&lt;/strong&gt;，成功化解了&lt;strong&gt;生态危机&lt;/strong&gt;。)
“总的来说，我认为我们组不仅建成了一个模型，更构建了一个&lt;strong&gt;有生命力的系统&lt;/strong&gt;。当然，这个系统的美学设计离不开【队友名】的贡献，下面请他来补充。”&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h2&gt;个面&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;个面-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%aa%e9%9d%a2-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;稿子&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;稿子&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a8%bf%e5%ad%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;演讲评分标准&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;演讲评分标准&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%bc%94%e8%ae%b2%e8%af%84%e5%88%86%e6%a0%87%e5%87%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;演讲要聚焦于申请人选择的五个主题之一——智能工业化&lt;/li&gt;
&lt;li&gt;要展示多角度的思考，并解决了现实问题或未来挑战&lt;/li&gt;
&lt;li&gt;演讲应该结构合理，有明确的开头、主体和结论&lt;/li&gt;
&lt;li&gt;展示对内容的深入理解，并简化复杂的主题&lt;/li&gt;
&lt;li&gt;幻灯片无误，与内容一致，并有效支持演讲&lt;/li&gt;
&lt;li&gt;表达要流畅清晰，避免多余叹词和模糊不清的表达&lt;/li&gt;
&lt;li&gt;演讲过程中，申请人要有效地与评委进行眼神接触并互动&lt;/li&gt;
&lt;li&gt;在演讲中要提供创造性或独特的观点&lt;/li&gt;
&lt;li&gt;要清晰阐述自己为何适合RB项目，将个人目标与项目目标对齐&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Slide 1-2: Title&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;slide-1-2-title&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#slide-1-2-title&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Good afternoon, professors. I am Bao Bowen from Zhejiang University. Today, I present my proposal for the Redbird program&lt;/p&gt;
&lt;p&gt;I major in Biomedical Engineering with a GPA of 3.97. During my undergraduate studies, I didn&amp;rsquo;t limit myself to coursework. I actively participated in interdisciplinary research and innovation competitions, winning the University Scholarship. This rigorous engineering training provided me with a solid foundation in Artificial Intelligence, preparing me for complex system design.&lt;/p&gt;
&lt;p&gt;So, why Smart Industrialization? Please look at this diagram from the World Economic Forum&amp;rsquo;s latest report. It shows a &amp;lsquo;Lighthouse Factory&amp;rsquo; achieving a 67% increase in productivity using AMR. This is impressive, but&amp;hellip; it is mostly limited to isolated, caged zones. To me, Smart Industrialization means moving from Rigid Automation to Embodied Collaboration. It is not just about robots working faster; it is about robots working safely alongside humans in unstructured environments. This is where Embodied AI creates real value&lt;/p&gt;
&lt;p&gt;To achieve this, I first built my foundation in data processing. In my research on Wavelet Convolutions, I optimized time-series analysis for medical data. While this was for healthcare, the core capability is universal: I learned to design lightweight, efficient algorithms that extract precise features from noisy data. This intuition is critical for any real-time robotic system&lt;/p&gt;
&lt;p&gt;In the Innovation Competition, where we won the National Bronze Award,  we identified a real pain point: the labor-intensive digitization of archaeological reports. I led the team to build a product that fused YOLO detection with layout analysis to automate this process. This experience taught me how to transform technical solutions into viable products, which aligns perfectly with Redbird&amp;rsquo;s maker spirit&lt;/p&gt;
&lt;p&gt;Moving forward, I have identified two key research directions for my Master&amp;rsquo;s. First is End-to-End Control, similar to XPeng&amp;rsquo;s recent work on VLA models, which cuts out the middle language translation step, enabling direct mapping from visual signals to action commands. Second is World Models—giving robots an internal simulator to predict future consequences before acting.&lt;/p&gt;
&lt;p&gt;Currently, I am using my Bachelor Thesis as an entry point into this field. I chose Social Navigation because it allows me to focus on Reinforcement Learning logic without getting bogged down by complex kinematics yet. I am optimizing the Falcon baseline by adding a Risk Preperception Module. The goal is to train an agent that doesn&amp;rsquo;t just reach the goal, but knows how to be &amp;lsquo;polite&amp;rsquo; and aware of environmental obstacles&lt;/p&gt;
&lt;p&gt;Beyond simulation, I am also diving into hardware deployment. I am porting the LOVON algorithm to Unitree Go2. It’s a learning process. For example, currently, the robot lacks occlusion awareness—if a person hides behind a wall, the dog might crash into the wall trying to track them. Debugging these real-world failures is exactly where I am gaining my Sim-to-Real experience.&lt;/p&gt;
&lt;p&gt;Looking at my roadmap for Redbird: Short-term: I will finish my thesis and continue solving those hardware navigation issues.&lt;/p&gt;
&lt;p&gt;Mid-term: At HKUST(GZ), I plan to dive deeper into my research interest, exploring how to integrate better environmental representations.&lt;/p&gt;
&lt;p&gt;Long-term: I hope to leverage the GBA scenarios to realize &amp;ldquo;Smart Industrialization, Become a leading EAI Researcher&lt;/p&gt;
&lt;p&gt;Finally, why Redbird?  It has provided access to over 90 enterprises and 10 more labs in the GBA. I specifiy 3 things here:&lt;/p&gt;
&lt;p&gt;The Interdisciplinary Synergy that allows me to collaborate across hubs;&lt;/p&gt;
&lt;p&gt;The opportunities to leverage GBA Supply Chain &amp;amp; Industry Giants;&lt;/p&gt;
&lt;p&gt;And the university&amp;rsquo;s proven track record of incubation,&lt;/p&gt;
&lt;p&gt;That’s my presentation. Thank you&lt;/p&gt;
&lt;h3&gt;提问&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;提问&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8f%90%e9%97%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;提问环节评分标准&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;提问环节评分标准&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8f%90%e9%97%ae%e7%8e%af%e8%8a%82%e8%af%84%e5%88%86%e6%a0%87%e5%87%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;理解并准确回应了评委的问题&lt;/li&gt;
&lt;li&gt;回答经过深思熟虑，并显示了对主题的深入理解&lt;/li&gt;
&lt;li&gt;面对压力时保持冷静镇定，沉着应对难题&lt;/li&gt;
&lt;li&gt;很好地应对挑战性或意外问题，并提供相关的解决方案或观点&lt;/li&gt;
&lt;li&gt;展示逻辑推理能力，分析问题并提供有见地的回答&lt;/li&gt;
&lt;li&gt;提供团队合作的具体例子，并表达对协作的强烈兴趣&lt;/li&gt;
&lt;li&gt;观点具有说服力，并能够有效支持其立场&lt;/li&gt;
&lt;li&gt;展示自我意识，讨论了自身的优势、劣势和需要改进的地方&lt;/li&gt;
&lt;li&gt;根据评委的反馈调整自己的回答，并与他们进行有意义的互动&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;面经总结&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;面经总结&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%9d%a2%e7%bb%8f%e6%80%bb%e7%bb%93&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;第一类：研究计划与学术愿景 (Research Proposal &amp;amp; Vision)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第一类研究计划与学术愿景-research-proposal--vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%80%e7%b1%bb%e7%a0%94%e7%a9%b6%e8%ae%a1%e5%88%92%e4%b8%8e%e5%ad%a6%e6%9c%af%e6%84%bf%e6%99%af-research-proposal--vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; 考察你的 RP 是否经过深思熟虑，是否有现实意义，以及你的思维深度（Critical Thinking）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;宏观视角类：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你未来想研究这个方向要达到什么目的？&lt;/li&gt;
&lt;li&gt;RP 的应用场景是什么？&lt;/li&gt;
&lt;li&gt;为什么工业 4.0 一直没有得到全面的推广？（考察对行业痛点的理解）&lt;/li&gt;
&lt;li&gt;RP 能给计划的研究领域（智能工业化）带来什么提升？&lt;/li&gt;
&lt;li&gt;RP 的具体创新点在哪里？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;抽象思维/假设类（考察应变）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假如以后没人生小孩，全是老人，没钱发养老金，你是决策者你要怎么办？&lt;/li&gt;
&lt;li&gt;研究方法是什么？&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第二类：技术细节与硬实力 (Technical Deep Dive)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第二类技术细节与硬实力-technical-deep-dive&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%ba%8c%e7%b1%bb%e6%8a%80%e6%9c%af%e7%bb%86%e8%8a%82%e4%b8%8e%e7%a1%ac%e5%ae%9e%e5%8a%9b-technical-deep-dive&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; 如果碰巧遇到懂行的老师，或者老师对你 PPT 里的某张图感兴趣，会抓住细节&amp;quot;拷打&amp;rdquo;，确认项目是你自己做的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;原理阐释类：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通俗解释一下 CLLLC（或你的核心算法，如 PER/LOVON）&lt;/li&gt;
&lt;li&gt;High power 与 high power density 的区别，怎么实现？&lt;/li&gt;
&lt;li&gt;你的变换器怎么实现的 low ripple（低纹波）？&lt;/li&gt;
&lt;li&gt;PPT 上这张图是什么意思？能具体讲讲结果吗？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;工程逻辑类：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你的参数是在仿真里枚举就行了吗？设计逻辑是什么？&lt;/li&gt;
&lt;li&gt;你觉得你的项目还有什么地方可以改进？能靠自己完成还是需要团队？&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第三类：个人动机与红鸟契合度 (Motivation &amp;amp; Fit)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第三类个人动机与红鸟契合度-motivation--fit&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%89%e7%b1%bb%e4%b8%aa%e4%ba%ba%e5%8a%a8%e6%9c%ba%e4%b8%8e%e7%ba%a2%e9%b8%9f%e5%a5%91%e5%90%88%e5%ba%a6-motivation--fit&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; 考察你&amp;quot;为什么来这里&amp;quot;以及&amp;quot;能不能适应红鸟的模式（Project-based Learning）&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;转专业/择校类：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[高频]&lt;/strong&gt; 为什么转专业？（BME -&amp;gt; Robotics）&lt;/li&gt;
&lt;li&gt;为什么不保研（Baoyan）？&lt;/li&gt;
&lt;li&gt;你还申请了哪些学校？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;红鸟模式类：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[高频]&lt;/strong&gt; 假设要研究这个项目，你想要什么样的队友？（考察对跨学科的理解）&lt;/li&gt;
&lt;li&gt;是否准备创业？（考察 Maker 精神）&lt;/li&gt;
&lt;li&gt;如果你来到这个学校，我要你换个研究方向你愿意吗？为什么？&lt;/li&gt;
&lt;li&gt;你对现在的方向感兴趣，还是你想换的另一个方向？&lt;/li&gt;
&lt;li&gt;如果让你选择，你会深入学习本领域知识，还是去学习其他领域知识？（Deep vs. Broad）&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第四类：项目经历与软实力 (Experience &amp;amp; Soft Skills)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第四类项目经历与软实力-experience--soft-skills&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e5%9b%9b%e7%b1%bb%e9%a1%b9%e7%9b%ae%e7%bb%8f%e5%8e%86%e4%b8%8e%e8%bd%af%e5%ae%9e%e5%8a%9b-experience--soft-skills&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; 考察你的过往经历是否真实，以及你在团队中的角色。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;项目经历中的合作与困难是什么？&lt;/li&gt;
&lt;li&gt;本科竞赛，你自己做了什么？&lt;/li&gt;
&lt;li&gt;本科的 TA（助教）工作经历？&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第五类：压力测试与行政问题 (Pressure &amp;amp; Admin)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第五类压力测试与行政问题-pressure--admin&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%ba%94%e7%b1%bb%e5%8e%8b%e5%8a%9b%e6%b5%8b%e8%af%95%e4%b8%8e%e8%a1%8c%e6%94%bf%e9%97%ae%e9%a2%98-pressure--admin&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; 考察抗压能力，或者单纯是老师没话找话/核实信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;压力面：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你的研究计划好像港科广没有这方面的老师啊？&lt;/li&gt;
&lt;li&gt;你手里拿着的是小抄吗？&lt;/li&gt;
&lt;li&gt;（英文提问）这个问题你能靠自己完成吗？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;行政类：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;联系意向导师了吗？意向导师是谁？&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;示例问题&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;示例问题&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a4%ba%e4%be%8b%e9%97%ae%e9%a2%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;第一维度：个人背景与动机 (The &amp;ldquo;Pivot&amp;rdquo; &amp;amp; Motivation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第一维度个人背景与动机-the-pivot--motivation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%80%e7%bb%b4%e5%ba%a6%e4%b8%aa%e4%ba%ba%e8%83%8c%e6%99%af%e4%b8%8e%e5%8a%a8%e6%9c%ba-the-pivot--motivation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心考察：&lt;/strong&gt; 你为什么&amp;quot;弃医从工&amp;quot;？你的逻辑能否自洽？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[必问]&lt;/strong&gt; Why did you switch from Biomedical Engineering to Robotics? Isn&amp;rsquo;t it a waste of your 4 years in BME? (你为什么转行？不觉得浪费吗？)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[进阶]&lt;/strong&gt; Why didn&amp;rsquo;t you choose Medical Robotics (e.g., Da Vinci)? That seems like a more natural fit for your background. (为什么不选医疗机器人？那不是更顺理成章吗？)&lt;/li&gt;
&lt;li&gt;Why HKUST(GZ) Redbird? Why not a traditional CS Master&amp;rsquo;s at ZJU or HKUST Clear Water Bay? (为什么是红鸟，不是浙大或清水湾的传统 CS？)&lt;/li&gt;
&lt;li&gt;You have a high GPA (3.97). Why apply for an MPhil instead of a PhD directly? (绩点这么高，为什么不直接申博？)&lt;/li&gt;
&lt;li&gt;What is the biggest &amp;ldquo;gap&amp;rdquo; you feel you have compared to a pure CS student, and how do you plan to fill it? (你觉得自己比纯 CS 学生缺什么？怎么补？)&lt;/li&gt;
&lt;li&gt;How did your &amp;ldquo;Internet+&amp;rdquo; competition experience influence your decision to join Redbird? (那次&amp;quot;互联网+&amp;ldquo;比赛怎么影响了你来红鸟的决定？)&lt;/li&gt;
&lt;li&gt;If I force you to change your topic from &amp;ldquo;Smart Industrialization&amp;rdquo; to &amp;ldquo;Sustainable Living&amp;rdquo;, what would you research? (参考面经：如果我强迫你换个主题，比如可持续生活，你会做什么？)&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第二维度：PPT 核心叙事 (Smart Industrialization)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第二维度ppt-核心叙事-smart-industrialization&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%ba%8c%e7%bb%b4%e5%ba%a6ppt-%e6%a0%b8%e5%bf%83%e5%8f%99%e4%ba%8b-smart-industrialization&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心考察：&lt;/strong&gt; 你对你选的主题（Topic）是否有真正的洞察？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why do you think &amp;ldquo;Smart Industrialization&amp;rdquo; is the best entry point for Embodied AI, rather than Home Service? (为什么觉得工业是具身智能最好的切入点，而不是家庭服务？)&lt;/li&gt;
&lt;li&gt;Factories are structured. Why do we need &amp;ldquo;Social Navigation&amp;rdquo; in a factory? Workers wear uniforms and follow lanes. (工厂是很结构化的，工人穿制服走通道，真的需要&amp;quot;社交导航&amp;quot;这么复杂的东西吗？)&lt;/li&gt;
&lt;li&gt;You mentioned the &amp;ldquo;Lighthouse Factory&amp;rdquo; productivity boost. Do you think robots will replace human workers? Is that ethical? (机器人替代工人，这符合伦理吗？)&lt;/li&gt;
&lt;li&gt;What is the biggest bottleneck for &amp;ldquo;Industry 4.0&amp;rdquo; deployment right now? (工业 4.0 现在最大的落地瓶颈是什么？不要只说技术，也要谈谈成本/信任。)&lt;/li&gt;
&lt;li&gt;Your solution seems expensive (Unitree Go2 + LiDAR + Chips). How can small factories afford this? (你的方案成本太高，小工厂怎么用？)&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第三维度：本科毕设深挖 (Social Navigation &amp;amp; Thesis)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第三维度本科毕设深挖-social-navigation--thesis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%89%e7%bb%b4%e5%ba%a6%e6%9c%ac%e7%a7%91%e6%af%95%e8%ae%be%e6%b7%b1%e6%8c%96-social-navigation--thesis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心考察：&lt;/strong&gt; 技术细节，验证你是否亲自做了项目&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[必问]&lt;/strong&gt; Explain &amp;ldquo;Prioritized Experience Replay (PER)&amp;rdquo; in one sentence to a non-expert. (用一句话给外行解释 PER。)&lt;/li&gt;
&lt;li&gt;How exactly do you define &amp;ldquo;Risk&amp;rdquo; in your Risk Awareness Module? Is it a hard threshold or a gradient? (风险怎么定义的？是硬阈值还是梯度？)&lt;/li&gt;
&lt;li&gt;Why did you choose Falcon as your baseline? Are there newer SOTA methods in 2024? (为什么选 Falcon 做基线？没有更新的方法了吗？)&lt;/li&gt;
&lt;li&gt;You mentioned the &amp;ldquo;Brain-Cerebellum&amp;rdquo; architecture. How do you handle the latency difference between the LLM (Brain) and the Motor Control (Cerebellum)? (大脑和小脑的频率不同，延迟怎么解决？)&lt;/li&gt;
&lt;li&gt;In your simulation (Habitat), how did you model human behavior? Are they static or dynamic? (仿真里的人是静止的还是动态的？用的什么模型？)&lt;/li&gt;
&lt;li&gt;What is the &amp;ldquo;Action Space&amp;rdquo; of your robot in the RL training? Continuous or Discrete? (动作空间是连续的还是离散的？)&lt;/li&gt;
&lt;li&gt;How do you balance the trade-off between &amp;ldquo;Safety&amp;rdquo; (Risk Module) and &amp;ldquo;Efficiency&amp;rdquo; (getting to the goal fast)? (安全和效率冲突了怎么办？)&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第四维度：硬件部署与 Sim-to-Real (The &amp;ldquo;Crash&amp;rdquo; &amp;amp; Reality)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第四维度硬件部署与-sim-to-real-the-crash--reality&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e5%9b%9b%e7%bb%b4%e5%ba%a6%e7%a1%ac%e4%bb%b6%e9%83%a8%e7%bd%b2%e4%b8%8e-sim-to-real-the-crash--reality&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心考察：&lt;/strong&gt; 动手能力，以及面对失败的态度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[必问]&lt;/strong&gt; You said the robot crashes into walls due to occlusion. Why didn&amp;rsquo;t you use a map (SLAM)? (你提到撞墙，为什么不用 SLAM 建图？)&lt;/li&gt;
&lt;li&gt;What sensors are on the Unitree Go2? Which one is the most critical for your algorithm? (Go2 上有哪些传感器？哪个对你最重要？)&lt;/li&gt;
&lt;li&gt;Have you encountered any &amp;ldquo;Reality Gap&amp;rdquo; issues other than occlusion? (除了遮挡，Sim-to-Real 还有什么坑？比如摩擦力？光照？)&lt;/li&gt;
&lt;li&gt;LOVON is an open-source project. What specific modifications did YOU make to it? (LOVON 是开源的，你具体改了哪里？)&lt;/li&gt;
&lt;li&gt;If the robot fails in a real factory, how do you ensure it doesn&amp;rsquo;t hurt anyone? (如果真在工厂里失控了，怎么保证不伤人？Fail-safe 机制是什么？)&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第五维度：过往项目 (Archaeology &amp;amp; Signal)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第五维度过往项目-archaeology--signal&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%ba%94%e7%bb%b4%e5%ba%a6%e8%bf%87%e5%be%80%e9%a1%b9%e7%9b%ae-archaeology--signal&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心考察：&lt;/strong&gt; 技术栈的广度与迁移能力&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In your Archaeology project, how did you align the visual data (YOLO) with the text data (LLM)? (考古项目里，视觉和文本怎么对齐的？)&lt;/li&gt;
&lt;li&gt;Why use Wavelet Convolutions instead of Transformers for signal processing? (信号处理为什么用小波不用 Transformer？)&lt;/li&gt;
&lt;li&gt;You mentioned &amp;ldquo;deploying efficient models&amp;rdquo;. Have you ever quantized a model? (你说部署高效模型，你做过量化吗？)&lt;/li&gt;
&lt;li&gt;Did you publish any papers from these projects? If not, why? (这些项目发论文了吗？没发是为什么？)&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;第六维度：红鸟契合度与抽象压力面 (The &amp;ldquo;Redbird Style&amp;rdquo;)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第六维度红鸟契合度与抽象压力面-the-redbird-style&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e5%85%ad%e7%bb%b4%e5%ba%a6%e7%ba%a2%e9%b8%9f%e5%a5%91%e5%90%88%e5%ba%a6%e4%b8%8e%e6%8a%bd%e8%b1%a1%e5%8e%8b%e5%8a%9b%e9%9d%a2-the-redbird-style&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;核心考察：&lt;/strong&gt; 软实力、团队合作、以及应对&amp;quot;抽象&amp;quot;问题的能力&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[面经原题]&lt;/strong&gt; If you want to build this project at Redbird, what kind of teammates do you need? (你想找什么样的队友？)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[面经原题]&lt;/strong&gt; If we have no professors specializing in &amp;ldquo;Social Navigation&amp;rdquo; here, what will you do? (如果这里没有做社交导航的老师，你怎么办？)&lt;/li&gt;
&lt;li&gt;Do you plan to start a startup? If yes, what is your product? If no, why did you mention the &amp;ldquo;Internet+&amp;rdquo; award? (你想创业吗？产品是什么？不想的话为什么提创赛？)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[抽象题]&lt;/strong&gt; Explain &amp;ldquo;Embodied AI&amp;rdquo; to an elderly person. (给老奶奶解释什么是具身智能。)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[抽象题]&lt;/strong&gt; If robots take over all logistics, what is the value of humans in the loop? (如果机器人接管了物流，人的价值在哪？)&lt;/li&gt;
&lt;li&gt;What is your &amp;ldquo;Plan B&amp;rdquo; if your Sim-to-Real transfer fails completely in the first year? (如果第一年 Sim-to-Real 彻底失败，你的 B 计划是什么？)&lt;/li&gt;
&lt;li&gt;How do you handle conflict in a team? Give an example from your &amp;ldquo;Internet+&amp;rdquo; competition. (举个你在创赛中处理团队冲突的例子。)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[压力面]&lt;/strong&gt; Your research plan sounds very ambitious for a 2-year MPhil. How can you finish all this (World Model + VLA + Hardware)? (两年读完这些是不是太贪心了？怎么可能做完？)&lt;/li&gt;
&lt;li&gt;What is the most creative thing you have ever done? (你做过最创造性的事是什么？)&lt;/li&gt;
&lt;li&gt;Which &amp;ldquo;Hub&amp;rdquo; (学域) do you think you belong to? (你觉得自己属于哪个域？)&lt;/li&gt;
&lt;li&gt;Describe a time you failed and what you learned. (描述一次失败经历。)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[最后]&lt;/strong&gt; In one sentence, why should we choose you over a student with a pure Robotics background? (用一句话概括，为什么选你而不选纯机器人背景的学生？)&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>社会意识的导航模型</title>
      <link>http://localhost:1313/blog/2025/2025-11-29-social-nav/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-29-social-nav/</guid>
      <description>
        
        
        &lt;p&gt;起因是在小红书上刷到了这一篇2025年11月的新文章&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/social-nav.jpg&#34; alt=&#34;Social Navigation&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;结果却搜到了 &lt;strong&gt;[ICRA 2025] From Cognition to Precognition: A Future-Aware Framework for Social Navigation&lt;/strong&gt;，于是误闯天家到了 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Robot Social Navigation&lt;/a&gt; 的领域。&lt;/p&gt;
&lt;h2&gt;什么是 Social Navigation？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;什么是-social-navigation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bb%80%e4%b9%88%e6%98%af-social-navigation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Social Navigation（社会导航）&lt;/strong&gt; 的核心思想是 &lt;strong&gt;&amp;ldquo;以人为本&amp;rdquo;&lt;/strong&gt;。它要求机器人不仅仅把人类当作需要避开的障碍物，而是能够理解并尊重人类的社会规范与个人空间，最终实现&lt;strong&gt;自然、和谐、无感知压迫&lt;/strong&gt;的共同空间使用。例如，在走廊中与人迎面相遇时，机器人会像人一样靠右行驶；当需要穿过一群人时，它会寻找合适的时机和路径，而不是生硬地&amp;quot;切开&amp;quot;人群。&lt;/p&gt;
&lt;h2&gt;技术对比：Social Navigation vs LOVON vs VLN&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技术对比social-navigation-vs-lovon-vs-vln&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e6%9c%af%e5%af%b9%e6%af%94social-navigation-vs-lovon-vs-vln&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;特性维度&lt;/th&gt;
          &lt;th&gt;Social Navigation (社会导航)&lt;/th&gt;
          &lt;th&gt;LOVON (腿部开放词汇物体导航)&lt;/th&gt;
          &lt;th&gt;VLN (视觉语言导航)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心目标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;安全、舒适、符合社会规范地在人类共享空间中导航&lt;/td&gt;
          &lt;td&gt;在开放世界中，根据物体名称，自主搜索并导航到指定物体&lt;/td&gt;
          &lt;td&gt;根据自然语言指令，在环境中执行导航任务 (如&amp;quot;去厨房拿杯水&amp;quot;)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;环境特点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;动态、拥挤的人类环境，充满不确定性&lt;/td&gt;
          &lt;td&gt;非结构化的开放环境，地形复杂，目标物体可能被遮挡或距离遥远&lt;/td&gt;
          &lt;td&gt;通常基于仿真器（如Habitat, AI2-THOR），环境可以是静态的，也引入动态人类&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;关键输入&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;人类的位置、运动轨迹、群体行为、社会规范&lt;/td&gt;
          &lt;td&gt;目标物体的文本名称 (如 &amp;ldquo;chair&amp;rdquo;)、机器人视觉传感器数据&lt;/td&gt;
          &lt;td&gt;详尽的自然语言指令、机器人视觉传感器数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;技术侧重点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;行人轨迹预测、社交力模型、强化学习策略、舒适度与安全性评估&lt;/td&gt;
          &lt;td&gt;开放词汇目标检测、大语言模型任务分解、腿部机器人运动控制、抗运动模糊&lt;/td&gt;
          &lt;td&gt;视觉-语言对齐、指令理解、跨模态推理、路径规划&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;典型输出/动作&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;避让、保持社交距离、绕行、调整速度、非语言沟通&lt;/td&gt;
          &lt;td&gt;朝向目标物体的运动控制命令 (如速度、方向)，处理复杂地形&lt;/td&gt;
          &lt;td&gt;导航动作 (如&amp;quot;左转&amp;quot;、&amp;ldquo;前进1米&amp;rdquo;、&amp;ldquo;停止&amp;rdquo;)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心挑战&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;对人类意图的预测、复杂社会规则的建模与量化、安全性、舒适感&lt;/td&gt;
          &lt;td&gt;长时序任务规划、动态模糊下的稳定感知、复杂地形下的稳定移动、开放词汇识别泛化能力&lt;/td&gt;
          &lt;td&gt;指令与环境的关联、未知环境泛化、长指令理解、跨模态表示学习&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;学术社区与行业洞察&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;学术社区与行业洞察&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%a6%e6%9c%af%e7%a4%be%e5%8c%ba%e4%b8%8e%e8%a1%8c%e4%b8%9a%e6%b4%9e%e5%af%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;然后去&lt;a href=&#34;http://xhslink.com/o/6M94ZS8vHHm&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;学术社区（迫真）&lt;/a&gt;上搜索了一下，这里 &lt;strong&gt;seven17&lt;/strong&gt; 这位大佬也在2025年11月16-17给出了自己作为人形公司 &lt;strong&gt;SLAM 面试官&lt;/strong&gt;对业界人形机器人在研究的算法的一些经验，非常有参考意义。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;有一说一小红书真的比很多像是CSDN之类的更好的学术交流平台&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;我就很赞同这里在小红书的某个 Ask Me Anything 上看到的&lt;strong&gt;港科广的梁老师&lt;/strong&gt;的话：&lt;/p&gt;
&lt;div style={{display: &#39;flex&#39;, justifyContent: &#39;space-between&#39;, gap: &#39;10px&#39;}}&gt;
  &lt;div style={{flex: 1}}&gt;
    &lt;img src=&#34;http://localhost:1313/blog/2025/gkg-liang1.jpg&#34; alt=&#34;港科广梁老师观点1&#34; style={{width: &#39;100%&#39;}} /&gt;
  &lt;/div&gt;
  &lt;div style={{flex: 1}}&gt;
    &lt;img src=&#34;http://localhost:1313/blog/2025/gkg-liang2.jpg&#34; alt=&#34;港科广梁老师观点2&#34; style={{width: &#39;100%&#39;}} /&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;相关竞赛与研讨会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关竞赛与研讨会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e7%ab%9e%e8%b5%9b%e4%b8%8e%e7%a0%94%e8%ae%a8%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;活动名称&lt;/th&gt;
          &lt;th&gt;主要关联会议&lt;/th&gt;
          &lt;th&gt;活动形式&lt;/th&gt;
          &lt;th&gt;核心侧重点&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RoboSense机器感知挑战赛&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;IROS 2025 (官方认证竞赛)&lt;/td&gt;
          &lt;td&gt;竞赛&lt;/td&gt;
          &lt;td&gt;在动态人群环境中，使机器人的导航行为符合人类的社会规范。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Advances in Social Robot Navigation研讨会&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;ICRA 2025&lt;/td&gt;
          &lt;td&gt;研讨会&lt;/td&gt;
          &lt;td&gt;探讨社交机器人导航在规划、人机交互等领域的最新进展，并包含基准测试挑战。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;RoboSense 挑战赛&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;robosense-挑战赛&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#robosense-%e6%8c%91%e6%88%98%e8%b5%9b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;RoboSense挑战赛&lt;/strong&gt; 是 &lt;strong&gt;IROS 2025&lt;/strong&gt; 的官方认证竞赛，它设置了专门的&lt;strong&gt;社交导航赛道&lt;/strong&gt;，旨在解决机器人在真实动态环境中的导航问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;任务目标：&lt;/strong&gt; 参赛者需要开发一个基于 &lt;strong&gt;RGB-D 输入&lt;/strong&gt;的移动机器人导航模型。该模型的核心任务是让机器人在&lt;strong&gt;不影响周围人类行为&lt;/strong&gt;的前提下完成导航，并使其行为符合人类的社会规范，例如主动避让、保持合适的社交距离等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战与评测：&lt;/strong&gt; 除了衡量导航成功率和路径效率，比赛还特别引入了&lt;strong&gt;个人空间合规性（PSC）&lt;strong&gt;和&lt;/strong&gt;人机碰撞次数（H-Coll）&lt;strong&gt;等指标，专门用于量化机器人行为的&lt;/strong&gt;&amp;ldquo;社交友好度&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;前沿技术：&lt;/strong&gt; 该赛道推荐的基线模型（Baseline）是 &lt;strong&gt;Falcon&lt;/strong&gt;，这是一个由&lt;strong&gt;港科广和港科大联合提出&lt;/strong&gt;的新算法，它通过将&lt;strong&gt;轨迹预测算法融入强化学习框架&lt;/strong&gt;，让机器人能够预测行人未来的移动路径，从而实现&lt;strong&gt;更超前、更安全的规划&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ICRA 2025 研讨会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;icra-2025-研讨会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#icra-2025-%e7%a0%94%e8%ae%a8%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;除了竞争激烈的比赛，&lt;strong&gt;ICRA 的&amp;quot;Advances in Social Robot Navigation&amp;quot;研讨会&lt;/strong&gt;则是深入了解该领域学术研究和前沿发展的绝佳平台。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;活动形式：&lt;/strong&gt; 这是一个学术研讨会，会邀请领域内的专家进行讲座和专题讨论。同时，它也主办 &lt;strong&gt;Arena 4.0 挑战赛&lt;/strong&gt;，旨在为不同的社交导航策略建立基准和评测体系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心议题：&lt;/strong&gt; 研讨会关注如何使机器人的导航行为&lt;strong&gt;更易于理解、更符合社交场景&lt;/strong&gt;。探讨的技术方向包括运动任务规划、&lt;strong&gt;基础模型的应用&lt;/strong&gt;、人机交互策略等。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;我的研究计划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;我的研究计划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%88%91%e7%9a%84%e7%a0%94%e7%a9%b6%e8%ae%a1%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我打算接下来的核心往 &lt;strong&gt;Social Navigation&lt;/strong&gt; 上面靠，这里很符合&lt;strong&gt;以人为本的设计特点&lt;/strong&gt;，而 &lt;strong&gt;LOVON&lt;/strong&gt; 也确实面临这一困境。也如梁老师所言，这是个&lt;strong&gt;容易入门具身的领域&lt;/strong&gt;。可惜这个比赛在这个时候已经结束了，下面计划的第一步是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;研读 Falcon 这个 baseline&lt;/strong&gt;（也就是上面提到的 ICRA 2025 中稿文章）&lt;/li&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://robosense2025.github.io/track2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robosense&lt;/a&gt; 提供的 GitHub 代码和数据集去&lt;strong&gt;复现基线&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;参考排行榜的改进去思考参赛者解决的问题集中在哪里，又是如何进行的&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;相关资源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关资源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%b5%84%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Resource&lt;/th&gt;
          &lt;th&gt;Link&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;GitHub Repository&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/robosense2025/track2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/robosense2025/track2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Baseline code and setup instructions&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Dataset&lt;/td&gt;
          &lt;td&gt;HuggingFace Dataset&lt;/td&gt;
          &lt;td&gt;Dataset with training and test splits&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Baseline Model&lt;/td&gt;
          &lt;td&gt;Pre-Trained Model&lt;/td&gt;
          &lt;td&gt;Weights of the baseline model&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Registration&lt;/td&gt;
          &lt;td&gt;Google Form (Closed on August 15th)&lt;/td&gt;
          &lt;td&gt;Team registration for the challenge&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Evaluation Server&lt;/td&gt;
          &lt;td&gt;EvalAI Platform&lt;/td&gt;
          &lt;td&gt;Online evaluation platform&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;HuggingFace 上的热门研究&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;huggingface-上的热门研究&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#huggingface-%e4%b8%8a%e7%9a%84%e7%83%ad%e9%97%a8%e7%a0%94%e7%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在 huggingface 上按 trending 搜索 social navigation 的&lt;a href=&#34;https://huggingface.co/papers/trending?q=social&amp;#43;navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;结果&lt;/a&gt;如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;论文标题&lt;/th&gt;
          &lt;th&gt;核心工作摘要&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SACSoN&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过最小化机器人对行人行为的**&amp;ldquo;反事实扰动&amp;rdquo;&lt;strong&gt;，学习一种&lt;/strong&gt;不打扰人类的导航策略**。其关键在于使用大量真实人机交互数据进行训练。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Exploiting Proximity-Aware Tasks&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出&lt;strong&gt;邻近感知任务&lt;/strong&gt;，通过让策略理解即时和未来的碰撞危险，为强化学习导航策略注入&lt;strong&gt;常识性社交行为&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SELFI&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出一种&lt;strong&gt;在线自学习方法&lt;/strong&gt;，在预训练策略的基础上，利用在线模型无关的强化学习进行快速微调，使机器人能根据实际经验持续改进社交导航行为。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SocialNav-SUB&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;引入了首个用于评估&lt;strong&gt;视觉语言模型（VLM）&lt;strong&gt;在社交导航场景中理解能力的基准，发现当前 VLM 在&lt;/strong&gt;空间、时空和社交推理&lt;/strong&gt;方面仍有明显不足。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;OLiVia-Nav&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;将&lt;strong&gt;视觉语言模型与在线终身学习框架&lt;/strong&gt;结合，通过独特的蒸馏方法让轻量级 VLM 直接理解社交和环境上下文，并规划符合社交规范的轨迹。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Habitat 3.0&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;推出了一个支持&lt;strong&gt;人、虚拟化身和机器人协同&lt;/strong&gt;的模拟平台，用于研究社交导航等协作任务，并提供了&lt;strong&gt;人类在环的基础设施&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;GOAT&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;通用导航系统&lt;/strong&gt;，能够处理多模态目标，并通过持续构建实例感知的语义记忆，实现&lt;strong&gt;终身学习和跨平台部署&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;GRUtopia&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;构建了一个&lt;strong&gt;大规模的模拟交互式3D社会&lt;/strong&gt;，包含多样化的场景和由 &lt;strong&gt;LLM 驱动的虚拟角色&lt;/strong&gt;，用于支持社交移动导航等具身AI任务的训练与评估。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RoboSense&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;大规模的以自我为中心的多模态数据集&lt;/strong&gt;，专注于拥挤和非结构化环境中的感知与导航，为近场场景理解提供丰富标注。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Social NCE&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过&lt;strong&gt;对比学习&lt;/strong&gt;来提升运动表示的社交感知能力，显式地建模危险负样本，以此降低轨迹预测和行为克隆中的碰撞率。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;DriVLMe&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;探索了基于&lt;strong&gt;视频语言模型的自动驾驶智能体&lt;/strong&gt;，通过模拟环境和真实人类对话进行训练，旨在实现与人类的自然有效沟通。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;EPO&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出&lt;strong&gt;显式策略优化方法&lt;/strong&gt;，利用多轮强化学习和自我博弈来提升大语言模型在社交对话等任务中的战略推理能力。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;EmbodiedEval&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;统一的、交互式的基准&lt;/strong&gt;，用于全面评估多模态大模型在具身任务（如导航、社交交互）中的能力。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SocialEval&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个评估&lt;strong&gt;大语言模型社交智能的双语基准&lt;/strong&gt;，通过叙事脚本从结果和过程两个维度评估模型的人际交往能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;研究趋势分析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究趋势分析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e8%b6%8b%e5%8a%bf%e5%88%86%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;从这些论文可以看出，社交导航领域的研究呈现出一些明显的趋势和重点方向：&lt;/p&gt;
&lt;h3&gt;1. 从&amp;quot;避障&amp;quot;到&amp;quot;避人&amp;quot;，再到&amp;quot;不扰人&amp;quot;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-从避障到避人再到不扰人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%8e%e9%81%bf%e9%9a%9c%e5%88%b0%e9%81%bf%e4%ba%ba%e5%86%8d%e5%88%b0%e4%b8%8d%e6%89%b0%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;像 &lt;strong&gt;SACSoN&lt;/strong&gt; 这样的工作，其目标已经超越了基础的安全避障，而是追求&lt;strong&gt;更高级的社交合规性&lt;/strong&gt;，希望机器人的存在和行为&lt;strong&gt;尽可能不改变人类的自然行为&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;2. 学习与规划的关键：预测与上下文理解&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-学习与规划的关键预测与上下文理解&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%ad%a6%e4%b9%a0%e4%b8%8e%e8%a7%84%e5%88%92%e7%9a%84%e5%85%b3%e9%94%ae%e9%a2%84%e6%b5%8b%e4%b8%8e%e4%b8%8a%e4%b8%8b%e6%96%87%e7%90%86%e8%a7%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;许多研究致力于让机器人更好地&lt;strong&gt;预测未来&lt;/strong&gt;（如行人轨迹）和&lt;strong&gt;理解环境上下文&lt;/strong&gt;（如社交规则）。&lt;strong&gt;Exploiting Proximity-Aware Tasks&lt;/strong&gt; 和 &lt;strong&gt;Social NCE&lt;/strong&gt; 都是通过不同的方式让模型内化对潜在危险和社交规范的理解。&lt;/p&gt;
&lt;h3&gt;3. 基础模型与终身学习成为新风向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-基础模型与终身学习成为新风向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e7%bb%88%e8%ba%ab%e5%ad%a6%e4%b9%a0%e6%88%90%e4%b8%ba%e6%96%b0%e9%a3%8e%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;OLiVia-Nav&lt;/strong&gt; 和 &lt;strong&gt;GOAT&lt;/strong&gt; 等论文清晰地展示了如何利用&lt;strong&gt;视觉语言模型（VLM）的先验知识&lt;/strong&gt;进行社交推理，并强调通过&lt;strong&gt;终身学习&lt;/strong&gt;使机器人能够适应不断变化的环境和新遇到的社交场景。&lt;/p&gt;
&lt;h3&gt;4. 对仿真、数据与评估的持续投入&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-对仿真数据与评估的持续投入&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e5%af%b9%e4%bb%bf%e7%9c%9f%e6%95%b0%e6%8d%ae%e4%b8%8e%e8%af%84%e4%bc%b0%e7%9a%84%e6%8c%81%e7%bb%ad%e6%8a%95%e5%85%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;高质量的仿真平台（&lt;strong&gt;Habitat 3.0&lt;/strong&gt;, &lt;strong&gt;GRUtopia&lt;/strong&gt;）、大规模数据集（&lt;strong&gt;RoboSense&lt;/strong&gt;）和专门的评估基准（&lt;strong&gt;SocialNav-SUB&lt;/strong&gt;, &lt;strong&gt;EmbodiedEval&lt;/strong&gt;, &lt;strong&gt;SocialEval&lt;/strong&gt;）是推动领域发展的&lt;strong&gt;关键基础设施&lt;/strong&gt;，这些工作为训练、测试和公平比较不同算法提供了坚实基础。&lt;/p&gt;
&lt;h2&gt;核心挑战与思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;核心挑战与思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a0%b8%e5%bf%83%e6%8c%91%e6%88%98%e4%b8%8e%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;社会导航的终极目标&lt;/strong&gt;是实现&lt;strong&gt;安全、舒适、符合社会规范的人机共存与协作&lt;/strong&gt;。它关注的是导航行为的**&amp;ldquo;社交智能&amp;quot;和&amp;quot;礼仪&amp;rdquo;&lt;strong&gt;。相比之下，许多&lt;/strong&gt;视觉语言导航（VLN）&lt;strong&gt;或其变体（如 &lt;strong&gt;LOVON&lt;/strong&gt;）更侧重于理解指令、识别物体或地点，并完成具身的导航任务，其核心是&lt;/strong&gt;&amp;ldquo;完成任务&amp;quot;的准确性**。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大的难点在于&lt;/strong&gt;，它需要让机器人理解并量化人类社会中那些&lt;strong&gt;不言自明、动态变化的社交潜规则&lt;/strong&gt;。例如，如何定义并计算**&amp;ldquo;个人空间&amp;rdquo;&lt;strong&gt;？如何判断什么样的路径是&lt;/strong&gt;&amp;ldquo;优雅&amp;quot;而非&amp;quot;冒犯&amp;quot;的**？这与开放词汇任务中要求模型识别未曾见过的物体类别（如 &lt;strong&gt;LOVON&lt;/strong&gt;）相比，是不同类型和层次的挑战。&lt;strong&gt;开放词汇扩展了机器人的&amp;quot;知识面&amp;rdquo;，而社会导航则是在塑造机器人的&amp;quot;情商&amp;quot;和行为方式&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如说 &lt;strong&gt;Track2&lt;/strong&gt; 的工作，核心任务是让机器人学会在充满动态行人的室内环境中（如办公楼、商场），实现&lt;strong&gt;安全、高效且符合社会规范的导航&lt;/strong&gt;。不仅要求机器人成功到达目的地（成功率 &lt;strong&gt;SR&lt;/strong&gt;），还要求其行为**&amp;ldquo;像个有礼貌的人&amp;rdquo;&lt;strong&gt;，比如主动保持舒适的社交距离（个人空间合规性 &lt;strong&gt;PSC&lt;/strong&gt;）、避免碰撞（人类碰撞率 &lt;strong&gt;H-Coll&lt;/strong&gt;），并规划出高效的路径（路径长度加权成功率 &lt;strong&gt;SPL&lt;/strong&gt;）。赛事提供的基线模型是基于 &lt;strong&gt;Falcon 框架&lt;/strong&gt;，它通过融入对&lt;/strong&gt;行人未来轨迹的预测**，来让机器人实现更具前瞻性的导航决策。&lt;/p&gt;
&lt;h2&gt;未来方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;未来方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;而在 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;awesome系列&lt;/a&gt; 里，我们可以看到以下几个重要方向：&lt;/p&gt;
&lt;h3&gt;1. 融合基础模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-融合基础模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e8%9e%8d%e5%90%88%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;这是一个明显的趋势。探索如何利用&lt;strong&gt;大型语言模型（LLM）&lt;strong&gt;和&lt;/strong&gt;视觉语言模型（VLM）&lt;/strong&gt;，让机器人能够理解和遵从复杂、抽象的社会规则（例如，**&amp;ldquo;在拥挤处耐心跟随&amp;rdquo;**而不仅仅是&amp;quot;避开人群&amp;rdquo;），或者更好地解读人类的行为意图。&lt;/p&gt;
&lt;h3&gt;2. 提升仿真环境的真实性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-提升仿真环境的真实性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e6%8f%90%e5%8d%87%e4%bb%bf%e7%9c%9f%e7%8e%af%e5%a2%83%e7%9a%84%e7%9c%9f%e5%ae%9e%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;开发更先进的仿真平台（如持续更新的 &lt;strong&gt;Arena 系列&lt;/strong&gt;），模拟更复杂的人类行为（如&lt;strong&gt;突然驻足、群体交谈、协作避让&lt;/strong&gt;），这对于在低成本前提下验证算法的鲁棒性至关重要。&lt;/p&gt;
&lt;h3&gt;3. 增强算法的可解释性与信任度&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-增强算法的可解释性与信任度&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%a2%9e%e5%bc%ba%e7%ae%97%e6%b3%95%e7%9a%84%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e4%b8%8e%e4%bf%a1%e4%bb%bb%e5%ba%a6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;研究如何让机器人的导航决策过程对人类而言&lt;strong&gt;更透明、更容易理解&lt;/strong&gt;。例如，生成机器人为何选择某条路径的**&amp;ldquo;因果解释&amp;rdquo;**，这能极大地增强人类对机器人的信任，促进人机共处。&lt;/p&gt;
&lt;h3&gt;4. 深化人机交互研究&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-深化人机交互研究&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e6%b7%b1%e5%8c%96%e4%ba%ba%e6%9c%ba%e4%ba%a4%e4%ba%92%e7%a0%94%e7%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;关注机器人的导航行为如何影响人类的感受和效率。通过用户研究，量化什么是让人感到**&amp;ldquo;舒适&amp;rdquo;、&amp;ldquo;自然&amp;rdquo;**的机器人行为，并将这些发现转化为算法设计的指导原则。&lt;/p&gt;
&lt;h3&gt;5. 应对极端与复杂场景&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-应对极端与复杂场景&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e5%ba%94%e5%af%b9%e6%9e%81%e7%ab%af%e4%b8%8e%e5%a4%8d%e6%9d%82%e5%9c%ba%e6%99%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;专注于解决更具挑战性的场景，例如&lt;strong&gt;重度遮挡&lt;/strong&gt;（在人群中&amp;quot;看不见&amp;quot;部分行人）、对**&amp;ldquo;不可预测&amp;quot;行人的识别与避让**，以及在密集人群中如何寻找安全路径。&lt;/p&gt;
&lt;h3&gt;研究方向与未来工作规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究方向与未来工作规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91%e4%b8%8e%e6%9c%aa%e6%9d%a5%e5%b7%a5%e4%bd%9c%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Robot Social Navigation&lt;/a&gt; 的梳理，当前研究主要集中在以下几个方向：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;研究方向&lt;/th&gt;
          &lt;th&gt;具体未来工作规划&lt;/th&gt;
          &lt;th&gt;来源论文&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;模型泛化与适应性&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt;
          &lt;td&gt;开发轻量化VLM便于机器人部署；探索&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;多模态融合（视觉、语言、传感器）&lt;/strong&gt;&lt;/span&gt;；研究&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;在线/终身学习框架以适应新场景&lt;/strong&gt;&lt;/span&gt;；提升对&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;动态场景和长时序任务的理解与规划能力&lt;/strong&gt;&lt;/span&gt;。&lt;/td&gt;
          &lt;td&gt;VLM-Social-Nav, OLiVia-Nav, Following the Human Thread&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;场景理解与交互&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt;
          &lt;td&gt;研究人类轨迹预测与社交动态的&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;实时、精准推断&lt;/strong&gt;&lt;/span&gt;；探索&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;多智能体协同与群体行为建模&lt;/strong&gt;&lt;/span&gt;；开发更强大的&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;场景表征与上下文理解能力&lt;/strong&gt;&lt;/span&gt;，以处理复杂的社会规则。&lt;/td&gt;
          &lt;td&gt;Following the Human Thread, DiPCAN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;评估体系与伦理&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;建立更全面的评估指标（如引入&amp;quot;人类赋权&amp;quot;概念）；设计标准化基准测试与仿真环境；关注算法的公平性、透明度、隐私保护及人类舒适度等社会伦理影响。&lt;/td&gt;
          &lt;td&gt;In Search of a Lost Metric, Frontiers Research Topic&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; benchmark一般貌似都要自己提出一个，这样能增大工作量说是，像TrackVLA就是这样提出了一个EVTbench开源使用&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;基于Awesome系列的具体研究方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基于awesome系列的具体研究方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e4%ba%8eawesome%e7%b3%bb%e5%88%97%e7%9a%84%e5%85%b7%e4%bd%93%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;根据&lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome系列&lt;/a&gt;的详细梳理，以下是从&lt;strong&gt;方法、数据集、评估&lt;/strong&gt;等多个维度总结的具体研究方向：&lt;/p&gt;
&lt;h4&gt;1. 基础模型在社交导航中的应用&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-基础模型在社交导航中的应用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%9c%a8%e7%a4%be%e4%ba%a4%e5%af%bc%e8%88%aa%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; VLM-Social-Nav, OLiVia-Nav, Social-LLaVA, CoNVOI, BehAV&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;轻量化部署：&lt;/strong&gt; 研究如何将大型**视觉语言模型（VLM）&lt;strong&gt;和&lt;/strong&gt;大语言模型（LLM）**蒸馏或微调到适合机器人实时部署的规模&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态融合：&lt;/strong&gt; 探索视觉、语言、传感器数据的深度融合，提升对复杂社交场景的理解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在线终身学习：&lt;/strong&gt; 开发能够持续适应新场景和人类行为的在线学习框架&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. 轨迹预测与场景理解&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-轨迹预测与场景理解&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e8%bd%a8%e8%bf%b9%e9%a2%84%e6%b5%8b%e4%b8%8e%e5%9c%ba%e6%99%af%e7%90%86%e8%a7%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Social LSTM, STGAT, From Cognition to Precognition, Following the Human Thread&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;实时轨迹预测：&lt;/strong&gt; 提升对人类未来移动路径的预测精度和实时性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;群体行为建模：&lt;/strong&gt; 研究多智能体协同、群体动态（如群体分裂与合并）的建模方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上下文理解：&lt;/strong&gt; 开发更强大的场景表征能力，理解复杂的社会规则和社交动态&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. 强化学习与混合方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-强化学习与混合方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%b7%b7%e5%90%88%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; SACSoN, SELFI, DR-MPC, Hybrid Approaches&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;奖励函数设计：&lt;/strong&gt; 探索如何将社交规范、舒适度等抽象概念量化为强化学习的奖励信号&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;混合方法：&lt;/strong&gt; 结合模型预测控制（MPC）、采样规划等传统方法与深度强化学习&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;样本效率：&lt;/strong&gt; 提升强化学习在社交导航任务中的样本效率，减少真实世界训练成本&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4. 可解释性与信任&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-可解释性与信任&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e4%b8%8e%e4%bf%a1%e4%bb%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Generating Causal Explanations, Explainability and Trust&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;因果解释：&lt;/strong&gt; 生成机器人导航决策的因果解释，增强人类对机器人的信任&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;透明度：&lt;/strong&gt; 研究如何让机器人的决策过程对人类更透明、更容易理解&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;5. 数据集与评估基准&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-数据集与评估基准&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e6%95%b0%e6%8d%ae%e9%9b%86%e4%b8%8e%e8%af%84%e4%bc%b0%e5%9f%ba%e5%87%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; SCAND, MuSoHu, SocNavBench, Arena系列, SocialNav-SUB&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;真实世界数据集：&lt;/strong&gt; 构建大规模、多模态的真实人机交互数据集&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真平台：&lt;/strong&gt; 开发更真实的仿真环境（如Arena 4.0, Habitat 3.0），支持复杂人类行为模拟&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标：&lt;/strong&gt; 设计更全面的评估体系，包括&lt;strong&gt;人类赋权&lt;/strong&gt;、个人空间合规性（PSC）、碰撞率（H-Coll）等&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;6. 用户研究与伦理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;6-用户研究与伦理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#6-%e7%94%a8%e6%88%b7%e7%a0%94%e7%a9%b6%e4%b8%8e%e4%bc%a6%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Social Momentum, How Do Robot Experts Measure Success, Overlapping Social Navigation Principles&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;用户研究：&lt;/strong&gt; 通过用户研究量化什么是&amp;quot;舒适&amp;rdquo;、&amp;ldquo;自然&amp;quot;的机器人行为&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;伦理考量：&lt;/strong&gt; 关注算法的公平性、透明度、隐私保护及对人类舒适度的影响&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;研究计划建议&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究计划建议&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e8%ae%a1%e5%88%92%e5%bb%ba%e8%ae%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于以上分析，可以从以下几个方面思考研究计划：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;关注新兴的评估范式：&lt;/strong&gt; 像&lt;a href=&#34;http://export.arxiv.org/abs/2501.01539&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;人类赋权&amp;rdquo;&lt;/a&gt;这类新指标方兴未艾，如何量化、验证并将其有效融入强化学习奖励函数或模型预测控制的代价函数中，是一个很有潜力的方向。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;探索基础模型的高效应用：&lt;/strong&gt; 研究如何蒸馏或微调大型VLM/LMM，在保持其社交推理能力的同时，满足机器人平台对低延迟和低功耗的严苛要求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;致力于弥合仿真与现实差距：&lt;/strong&gt; 开发更好的**领域自适应（Domain Adaptation）&lt;strong&gt;技术或&lt;/strong&gt;元学习（Meta-Learning）**策略，让模型在离开仿真环境后能快速适应真实世界的复杂性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战更复杂的社交场景：&lt;/strong&gt; 可以专注于研究机器人在密集人群、群组交互（如穿越一个正在交谈的群体）或长程、多目标导航任务中的表现，这些场景对现有技术提出了更高要求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;构建自己的评估基准：&lt;/strong&gt; 参考TrackVLA提出EVTbench的做法，开发针对特定场景或问题的标准化评估基准，这不仅能增加研究工作量，还能为领域提供有价值的工具。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>通过图像推理 VLM</title>
      <link>http://localhost:1313/blog/2025/2025-12-18-vlm-think-with-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-18-vlm-think-with-images/</guid>
      <description>
        
        
        &lt;h2&gt;VLM-Think with images 必读论文&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vlm-think-with-images-必读论文&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vlm-think-with-images-%e5%bf%85%e8%af%bb%e8%ae%ba%e6%96%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&#34;https://www.xiaohongshu.com/explore/693f9ea3000000001e032e66?note_flow_source=wechat&amp;amp;xsec_token=CBE05j1SOO06inQfnWwTQXQDMo4ZSS_m0ussgoq0HO25M=&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLM-Think with images必读论文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VLM&lt;/strong&gt; 尽管具备多模态输入的能力，但在推理过程中完全依赖&lt;strong&gt;纯文本的形式&lt;/strong&gt;进行思考，无论是对视觉内容进行描述，还是输出语言化的推理依据，其内部推理路径始终局限于文本上。然而仅通过文本进行多模态推理，并不总是最有效的策略，尤其对于那些&lt;strong&gt;高度依赖视觉信息的任务&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;O3 范式 Think with images&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;o3-范式-think-with-images&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#o3-%e8%8c%83%e5%bc%8f-think-with-images&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;一种方法是 &lt;strong&gt;OpenAI-O3 范式的 Think with images&lt;/strong&gt;，是指在推理过程中通过&lt;strong&gt;视觉工具&lt;/strong&gt;（如放大、裁剪、旋转、绘制辅助线、草图）来进行辅助思考，从而将视觉操作和操作后图像融入思维链，目的是为了让模型可以更深入地理解图像内容。&lt;/p&gt;
&lt;p&gt;但从这样的角度，&lt;strong&gt;本质并没有变&lt;/strong&gt;，思维链还是&lt;strong&gt;纯文本驱动&lt;/strong&gt;，也就是得到视觉信息不是模型生成的，只是借助工具得到的。&lt;strong&gt;模态鸿沟仍然存在&lt;/strong&gt;，即将视觉信息落地为文本后再进行推理，阻碍了模型对视觉特征的精准捕捉。&lt;/p&gt;
&lt;h4&gt;总结使用工具的方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;总结使用工具的方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%bb%e7%bb%93%e4%bd%bf%e7%94%a8%e5%b7%a5%e5%85%b7%e7%9a%84%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;设置工作流固定使用工具&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;固定的一步，调用固定工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型自主决定使用什么工具&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在哪一步，是直接输出答案，还是调用工具。如果使用工具，自主决定使用什么工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;固定的工具代码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码是设置好的，模型只需要预测输入参数。所支持的操作空间相对受限，也依赖精确的参数输入。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型生成工具代码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码由模型生成，支持广泛的视觉操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;相关论文&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关论文&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%ae%ba%e6%96%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Visual SKETCHPAD: Sketching as a Visual Chain of Thought for Multimodal Language Models&lt;/li&gt;
&lt;li&gt;V-Thinker: Interactive Thinking with Images&lt;/li&gt;
&lt;li&gt;DeepEyes: Incentivizing &amp;ldquo;Thinking with Images&amp;rdquo; via Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Thyme: Think Beyond Images&lt;/li&gt;
&lt;li&gt;Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization&lt;/li&gt;
&lt;li&gt;Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning&lt;/li&gt;
&lt;li&gt;From Illusion to Intention- Visual Rationale Learning for Vision-Language Reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;潜在视觉推理（Latent Visual CoT）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;潜在视觉推理latent-visual-cot&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%bd%9c%e5%9c%a8%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86latent-visual-cot&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;所以另一种方法是&lt;strong&gt;潜在视觉推理&lt;/strong&gt;，模型不再调用外部工具得到视觉信息，而是将视觉信息&lt;strong&gt;内化、表征化&lt;/strong&gt;，也就是模型直接生成&lt;strong&gt;视觉 token&lt;/strong&gt;。模型需要学会运用视觉 token 进行推理，而这些视觉 token 通常包括与&lt;strong&gt;分割、深度、边缘、特征&lt;/strong&gt;等视觉线索。&lt;/p&gt;
&lt;p&gt;训练模型生成视觉 token，就需要加入&lt;strong&gt;视觉重建任务&lt;/strong&gt;。&lt;/p&gt;
&lt;h4&gt;视觉 token 重建的 label 有以下几种&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;视觉-token-重建的-label-有以下几种&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%86%e8%a7%89-token-%e9%87%8d%e5%bb%ba%e7%9a%84-label-%e6%9c%89%e4%bb%a5%e4%b8%8b%e5%87%a0%e7%a7%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;引入辅助模型&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入辅助图像&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入原图 ROI 边界框&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;学习的视觉 token 表示有以下几种&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;学习的视觉-token-表示有以下几种&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%a6%e4%b9%a0%e7%9a%84%e8%a7%86%e8%a7%89-token-%e8%a1%a8%e7%a4%ba%e6%9c%89%e4%bb%a5%e4%b8%8b%e5%87%a0%e7%a7%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;VIT 特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VIT 投影特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型中间特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VQVAE 中间离散 token&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;相关论文&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关论文-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%ae%ba%e6%96%87-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens&lt;/li&gt;
&lt;li&gt;Perception tokens enhance visual reasoning in multimodal language models&lt;/li&gt;
&lt;li&gt;DeepSketcher- Internalizing Visual Manipulation for Multimodal Reasoning&lt;/li&gt;
&lt;li&gt;Machine mental imagery: Empower multimodal reasoning with latent visual tokens&lt;/li&gt;
&lt;li&gt;Latent Visual Reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Thinking with Images&lt;/h1&gt;&lt;p&gt;**多模态大语言模型（Multimodal Large Language Models, MLLMs）&lt;strong&gt;正处于从&lt;/strong&gt;&amp;ldquo;感知智能&amp;quot;向&amp;quot;推理智能&amp;rdquo;&lt;strong&gt;跃迁的关键转折点。尽管早期的视觉-语言模型（VLMs）如 &lt;strong&gt;CLIP&lt;/strong&gt; 或 &lt;strong&gt;LLaVA&lt;/strong&gt; 成功实现了图像与文本的语义对齐，但它们在本质上仍遵循&lt;/strong&gt;&amp;ldquo;Thinking about Images&amp;rdquo;&lt;strong&gt;的范式——即迅速将视觉信号转化为文本特征，随后完全依赖语言模型的&lt;/strong&gt;文本思维链（Text-based Chain-of-Thought, CoT）**进行推理。&lt;/p&gt;
&lt;p&gt;这种**&amp;ldquo;模态早融合&amp;rdquo;&lt;strong&gt;与&lt;/strong&gt;&amp;ldquo;推理纯文本化&amp;rdquo;&lt;strong&gt;的架构，导致了严重的&lt;/strong&gt;模态鸿沟（Modality Gap）**：在处理需要空间几何感知、细粒度视觉验证或多步视觉逻辑推演的任务时，模型往往因丢失视觉细节而产生幻觉。&lt;/p&gt;
&lt;p&gt;为了克服这一局限，学术界与工业界正在积极探索**&amp;ldquo;Thinking with Images&amp;rdquo;**的新范式，即让视觉模态深度参与推理的中间过程，甚至主导推理链条。本报告旨在打破现有的二元分类局限（即简单的&amp;quot;工具调用&amp;quot;与&amp;quot;潜在推理&amp;quot;之分），基于对 &lt;strong&gt;2023 年至 2025 年间&lt;/strong&gt;发表于 &lt;strong&gt;CVPR、ICCV、NeurIPS、ICLR、ICML&lt;/strong&gt; 等顶级会议的 &lt;strong&gt;50 余篇里程碑文献&lt;/strong&gt;的详尽调研，构建了一个包含五大范式的全新分类体系：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;工具中介与程序化视觉推理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;显式生成意象与心智模拟&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;潜在空间视觉推理与连续思维&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主动感知与强化视觉搜索&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结构化与组合式视觉推理&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本报告将深入剖析每一范式的核心机制、技术演进路径及代表性工作，揭示 MLLMs 如何通过引入&lt;strong&gt;视觉中间态（Visual Intermediates）&lt;/strong&gt;——无论是代码、像素、潜在向量、动作序列还是结构化图谱——来模拟人类的**&amp;ldquo;系统 2&amp;quot;慢思考能力**，从而实现真正的视觉通用智能。&lt;/p&gt;
&lt;h2&gt;1. 绪论：模态鸿沟与&amp;quot;系统 2&amp;quot;视觉推理的崛起&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-绪论模态鸿沟与系统-2视觉推理的崛起&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e7%bb%aa%e8%ae%ba%e6%a8%a1%e6%80%81%e9%b8%bf%e6%b2%9f%e4%b8%8e%e7%b3%bb%e7%bb%9f-2%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86%e7%9a%84%e5%b4%9b%e8%b5%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1.1 纯文本思维链在多模态语境下的局限性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;11-纯文本思维链在多模态语境下的局限性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#11-%e7%ba%af%e6%96%87%e6%9c%ac%e6%80%9d%e7%bb%b4%e9%93%be%e5%9c%a8%e5%a4%9a%e6%a8%a1%e6%80%81%e8%af%ad%e5%a2%83%e4%b8%8b%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;传统的视觉语言模型（如 &lt;strong&gt;LLaVA 系列&lt;/strong&gt;、&lt;strong&gt;GPT-4V&lt;/strong&gt;）主要采用**&amp;ldquo;编码器-解码器&amp;quot;架构**：视觉编码器（如 &lt;strong&gt;ViT&lt;/strong&gt;）将图像压缩为特征向量，随后投影到大语言模型（&lt;strong&gt;LLM&lt;/strong&gt;）的词嵌入空间。一旦进入 LLM，视觉信息便被视为某种&amp;quot;外语&amp;rdquo;，推理过程完全由预训练的语言概率分布主导。&lt;/p&gt;
&lt;p&gt;这种架构在图像描述（&lt;strong&gt;Captioning&lt;/strong&gt;）等**&amp;ldquo;系统 1&amp;quot;直觉任务**上表现出色，但在需要多步逻辑的复杂任务中面临显著瓶颈：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;信息有损压缩（Information Bottleneck）&lt;/strong&gt;：视觉编码器通常将高分辨率图像压缩为有限数量的 token（例如 &lt;strong&gt;256 或 576 个&lt;/strong&gt;），导致高频细节（如微小文字、物体精确坐标）在推理开始前即丢失。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;视觉-语言非同构性（Isomorphism Deficit）&lt;/strong&gt;：语言是离散、符号化且高度抽象的，而视觉是连续、密集且具象的。强行用文本 CoT 描述复杂的空间拓扑（如&amp;quot;左边第三个红球稍微偏上一点&amp;rdquo;）会导致语义精度的急剧下降，模型往往因此退化为依赖语言先验而非视觉事实进行猜想，即产生**&amp;ldquo;幻觉&amp;rdquo;**。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;缺乏回溯机制（Lack of Retracing）&lt;/strong&gt;：人类在解决视觉难题时会反复观察图像（&lt;strong&gt;Visual Re-scanning&lt;/strong&gt;），而标准 VLM 往往是**&amp;ldquo;看一眼，然后闭眼推理&amp;rdquo;**，缺乏在推理中途重新审视视觉输入的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;1.2 &amp;ldquo;Thinking with Images&amp;rdquo; 的定义与新分类体系&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;12-thinking-with-images-的定义与新分类体系&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#12-thinking-with-images-%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%8e%e6%96%b0%e5%88%86%e7%b1%bb%e4%bd%93%e7%b3%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;Thinking with Images&amp;rdquo;&lt;/strong&gt; 指的是模型在推理过程中，不仅生成文本，还显式或隐式地操作视觉信息，将其作为推理链条中不可或缺的一环。这对应于认知科学中的**&amp;ldquo;心智意象&amp;rdquo;（Mental Imagery）**理论，即人类在思考空间问题时，会在大脑中构建视觉模拟。&lt;/p&gt;
&lt;p&gt;基于对现有文献的系统梳理，我们将这一领域的解决方案扩展为以下五大范式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;范式 I：工具中介与程序化视觉推理（Tool-Mediated &amp;amp; Programmatic Reasoning）&lt;/strong&gt; —— 借用外部引擎的&amp;quot;手&amp;quot;来操作视觉。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 II：显式生成意象与心智模拟（Explicit Generative Imagery &amp;amp; Mental Simulation）&lt;/strong&gt; —— 利用生成模型的&amp;quot;想象力&amp;quot;进行预演。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 III：潜在空间视觉推理（Latent Visual Reasoning）&lt;/strong&gt; —— 在高维特征空间进行高效的&amp;quot;内隐视觉思考&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 IV：主动感知与强化视觉搜索（Active Perception &amp;amp; Agentic Grounding）&lt;/strong&gt; —— 像人类眼动一样主动&amp;quot;寻找&amp;quot;视觉证据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 V：结构化与组合式视觉推理（Compositional &amp;amp; Structured Grounding）&lt;/strong&gt; —— 将图像解构为图谱或掩码进行逻辑运算。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2. 范式 I：工具中介与程序化视觉推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-范式-i工具中介与程序化视觉推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e8%8c%83%e5%bc%8f-i%e5%b7%a5%e5%85%b7%e4%b8%ad%e4%bb%8b%e4%b8%8e%e7%a8%8b%e5%ba%8f%e5%8c%96%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式代表了**&amp;ldquo;符号主义&amp;quot;与&amp;quot;联结主义&amp;rdquo;&lt;strong&gt;的结合。其核心假设是：神经网络在精确计算（如计数、几何测量）和逻辑执行上存在先天不足，应当将这些任务&lt;/strong&gt;&amp;ldquo;外包&amp;rdquo;&lt;strong&gt;给擅长此道的外部工具（如 &lt;strong&gt;Python 解释器&lt;/strong&gt;、&lt;strong&gt;OpenCV 库&lt;/strong&gt;或&lt;/strong&gt;绘图 API**）。VLM 在此扮演**&amp;ldquo;控制器&amp;rdquo;&lt;strong&gt;的角色，负责理解意图、编写程序并解析执行结果。这也就是用户查询中提到的&lt;/strong&gt;&amp;ldquo;OpenAI-O3 范式&amp;rdquo;**的典型体现。&lt;/p&gt;
&lt;h3&gt;2.1 核心机制：思维的外化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;21-核心机制思维的外化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#21-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e6%80%9d%e7%bb%b4%e7%9a%84%e5%a4%96%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在该范式中，中间推理步骤被显式地转化为可执行的代码或可视化操作。这种**&amp;ldquo;外化&amp;rdquo;**不仅提高了推理的准确性，还赋予了模型极强的可解释性。&lt;/p&gt;
&lt;h3&gt;2.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;22-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#22-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.2.1 Visual Sketchpad (NeurIPS 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;221-visual-sketchpad-neurips-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#221-visual-sketchpad-neurips-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：传统的 CoT 仅在文本层面分解问题，但对于几何题或地图导航，纯文本描述极其低效且易错。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：作者提出了一种**&amp;ldquo;视觉草稿本&amp;quot;机制**。模型在推理过程中，可以生成代码来调用绘图 API（如 Matplotlib），在原图上绘制辅助线、标记框或圈出关键区域。这些绘制了标记的新图像被重新输入模型，作为下一步推理的视觉上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：首次将**&amp;ldquo;草图绘制&amp;rdquo;（Sketching）**引入 VLM 推理链。这模拟了人类做几何题时画辅助线的行为。实验表明，这种视觉辅助能显著提升数学几何和视觉逻辑任务的准确率，证明了视觉符号不仅是输出，更是推理的支架。&lt;/p&gt;
&lt;h4&gt;2.2.2 ViperGPT (ICCV 2023)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;222-vipergpt-iccv-2023&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#222-vipergpt-iccv-2023&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：ViperGPT: Visual Inference via Python Execution for Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：**端到端（End-to-End）**模型不仅是黑盒，而且经常在简单的逻辑组合上失败（例如&amp;quot;红帽子的人左边是不是有一辆车&amp;rdquo;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：ViperGPT 彻底摒弃了端到端的视觉问答模式。它利用专门针对代码微调的 LLM（如 &lt;strong&gt;Codex&lt;/strong&gt;）将自然语言问题转化为 &lt;strong&gt;Python 程序&lt;/strong&gt;。该程序调用一系列视觉 API（如对象检测、深度估计模型）。程序执行的结果即为最终答案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：提出了**&amp;ldquo;以代码为策略&amp;rdquo;（Code as Policy）&lt;strong&gt;的视觉推理极致形态。它不需要训练多模态模型，而是通过组合现有的&lt;/strong&gt;视觉专家模型（Vision Experts）**来解决问题，实现了极高的可解释性和组合泛化能力。&lt;/p&gt;
&lt;h4&gt;2.2.3 Visual Program Distillation (VPD) (CVPR 2024 Oral)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;223-visual-program-distillation-vpd-cvpr-2024-oral&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#223-visual-program-distillation-vpd-cvpr-2024-oral&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：像 ViperGPT 这样的工具调用方法虽然准确，但推理延迟高、计算开销大，且依赖外部 API 的稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：VPD 提出了一种**&amp;ldquo;蒸馏&amp;quot;策略**。它首先利用大型模型生成成千上万条高质量的&amp;quot;视觉程序&amp;quot;轨迹（即问题-程序-答案三元组），然后用这些数据微调一个较小的端到端 VLM（如 &lt;strong&gt;PaLM-E&lt;/strong&gt; 或 &lt;strong&gt;LLaVA&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：成功将工具调用的逻辑推理能力**&amp;ldquo;内化&amp;rdquo;**到模型权重中。微调后的模型不再需要外部 API，而是直接预测出类似于程序执行步骤的推理结果，兼顾了工具方法的逻辑严密性和端到端模型的高效性。&lt;/p&gt;
&lt;h4&gt;2.2.4 Task Navigator (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;224-task-navigator-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#224-task-navigator-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Task Navigator: Decomposing Complex Tasks for Multimodal Large Language Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：面对极其复杂的视觉任务（如&amp;quot;分析这张监控截图中所有异常行为&amp;rdquo;），模型往往不知从何下手，因为它的注意力机制无法一次性处理所有信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：引入了一个**导航器（Navigator）**模块，负责将宏观任务分解为一系列子查询（&lt;strong&gt;Sub-queries&lt;/strong&gt;）。系统根据子查询的需要，动态选择调用特定的视觉工具（如 &lt;strong&gt;OCR&lt;/strong&gt;、检测器、知识库检索），并根据工具返回的结果规划下一步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：强调了**&amp;ldquo;规划&amp;rdquo;（Planning）&lt;strong&gt;在视觉推理中的核心地位。证明了&lt;/strong&gt;系统 2 推理**的关键在于将复杂问题降维，并通过工具迭代式地获取信息。&lt;/p&gt;
&lt;h4&gt;2.2.5 DeepSketcher (ArXiv 2025 / ICLR Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;225-deepsketcher-arxiv-2025--iclr-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#225-deepsketcher-arxiv-2025--iclr-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：外部绘图工具（如 Visual Sketchpad）的操作是离散且不可微的，阻断了梯度回传，限制了模型的学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：DeepSketcher 设计了一个**&amp;ldquo;图像嵌入编辑模块&amp;rdquo;（Image Embedding Editing Module）**。它虽然受到代码渲染图像的监督，但在推理时，模型是直接在图像的特征空间（&lt;strong&gt;Embedding Space&lt;/strong&gt;）中进行&amp;quot;涂抹&amp;quot;和&amp;quot;高亮&amp;quot;，模拟绘图操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了工具操作的**&amp;ldquo;软化&amp;quot;和&amp;quot;可微化&amp;rdquo;**。它不仅保留了绘图辅助推理的直观优势，还允许模型通过梯度下降端到端地学习&amp;quot;该在哪里画线&amp;quot;，是范式 I 向范式 III（潜在推理）演进的过渡形态。&lt;/p&gt;
&lt;h2&gt;3. 范式 II：显式生成意象与心智模拟&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-范式-ii显式生成意象与心智模拟&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e8%8c%83%e5%bc%8f-ii%e6%98%be%e5%bc%8f%e7%94%9f%e6%88%90%e6%84%8f%e8%b1%a1%e4%b8%8e%e5%bf%83%e6%99%ba%e6%a8%a1%e6%8b%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式深受&lt;strong&gt;认知心理学&lt;/strong&gt;启发。人类在回答&amp;quot;大象能不能装进冰箱&amp;quot;这个问题时，会在脑海中生成大象和冰箱的视觉意象并进行比对。同样，该范式赋予 VLM 调用生成模型（如 &lt;strong&gt;Stable Diffusion&lt;/strong&gt;）的能力，通过生成&lt;strong&gt;像素级的图像&lt;/strong&gt;来辅助推理，特别是针对空间预测、反事实推理和未来预测任务。&lt;/p&gt;
&lt;h3&gt;3.1 核心机制：视觉想象循环&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;31-核心机制视觉想象循环&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#31-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e8%a7%86%e8%a7%89%e6%83%b3%e8%b1%a1%e5%be%aa%e7%8e%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;推理过程不再是单向的（图像 -&amp;gt; 文本），而是&lt;strong&gt;闭环的&lt;/strong&gt;：文本/图像 -&amp;gt; 生成新图像 -&amp;gt; 视觉感知 -&amp;gt; 文本结论。生成的图像充当了推理的**&amp;ldquo;视觉草稿&amp;rdquo;**。&lt;/p&gt;
&lt;h3&gt;3.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;32-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#32-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;3.2.1 Multimodal Visualization-of-Thought (MVoT) (ICML 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;321-multimodal-visualization-of-thought-mvot-icml-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#321-multimodal-visualization-of-thought-mvot-icml-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Imagine While Reasoning in Space: Multimodal Visualization-of-Thought&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：现有的多模态思维链（&lt;strong&gt;Multimodal CoT&lt;/strong&gt;）大多只是文本 CoT 加上静态图像输入，缺乏真正的&amp;quot;视觉思考&amp;quot;过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：MVoT 提出了一种&lt;strong&gt;交错式的推理模式&lt;/strong&gt;，模型可以像输出词语一样输出图像。为了实现这一点，作者设计了**&amp;ldquo;Token Discrepancy Loss&amp;rdquo;**，解决了 LLM 文本 Token 与图像生成器（如 &lt;strong&gt;VQ-VAE&lt;/strong&gt;）离散 Codebook 之间的分布差异问题。模型在推理过程中会生成一系列中间图像（&lt;strong&gt;Visual Thoughts&lt;/strong&gt;），展示其空间变换的构思过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将图像生成内化为 LLM 的原生能力，实现了真正的**&amp;ldquo;图文交错思维流&amp;rdquo;**。实验证明，这种显式的视觉化过程显著提升了空间旋转、物体拼接等任务的性能。&lt;/p&gt;
&lt;h4&gt;3.2.2 ImagineNav (ICLR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;322-imaginenav-iclr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#322-imaginenav-iclr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：在&lt;strong&gt;具身智能导航&lt;/strong&gt;中，代理（Agent）受限于视野，无法看到墙后或拐角处的物体，导致规划短视。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：ImagineNav 利用 VLM 结合&lt;strong&gt;新视角合成技术（Novel View Synthesis）&lt;/strong&gt;，根据当前观测&amp;quot;想象&amp;quot;出未知区域的景象。模型基于这些生成的幻觉图像来评估路径的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将生成式视觉 CoT 应用于决策规划。证明了**&amp;ldquo;合理的幻觉&amp;rdquo;**（基于先验的预测）是智能体在非结构化环境中生存的关键能力。&lt;/p&gt;
&lt;h4&gt;3.2.3 Perspective-Aware Reasoning (APC) (ICCV 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;323-perspective-aware-reasoning-apc-iccv-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#323-perspective-aware-reasoning-apc-iccv-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：VLM 存在严重的**&amp;ldquo;自我中心偏差&amp;rdquo;（Egocentric Bias）**，难以理解&amp;quot;如果我们换个角度看这个物体会怎样&amp;quot;这类问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：APC 框架模拟了人类的**心智旋转（Mental Rotation）**能力。它首先利用视觉基础模型从输入图像中重建一个粗糙的 &lt;strong&gt;3D 抽象场景&lt;/strong&gt;，然后将该 3D 场景旋转到目标视角，并重新投影为 2D 图像输入 VLM 进行回答。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：引入了 &lt;strong&gt;3D 抽象&lt;/strong&gt;作为中间推理模态。它表明，解决复杂的视觉关系问题需要超越 2D 像素，进入 &lt;strong&gt;3D 语义空间&lt;/strong&gt;的模拟。&lt;/p&gt;
&lt;h4&gt;3.2.4 SpatialDreamer (ArXiv 2025 / Top Venue Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;324-spatialdreamer-arxiv-2025--top-venue-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#324-spatialdreamer-arxiv-2025--top-venue-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Incentivizing Spatial Reasoning via Active Mental Imagery&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：仅仅生成图像是不够的，模型需要知道&amp;quot;生成什么图像&amp;quot;对解题最有帮助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：SpatialDreamer 引入了&lt;strong&gt;强化学习（RL）&lt;strong&gt;来优化生成策略。它训练模型主动进行&lt;/strong&gt;&amp;ldquo;视觉做梦&amp;rdquo;（Dreaming）&lt;/strong&gt;，并通过**世界模型（World Model）**验证这些梦境的物理一致性。奖励机制鼓励模型生成那些能最大程度减少不确定性的视角或状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将生成式 CoT 与强化学习结合，使视觉想象具有了&lt;strong&gt;目的性（Goal-oriented Imagination）&lt;/strong&gt;，这是向自主智能迈进的重要一步。&lt;/p&gt;
&lt;h4&gt;3.2.5 Self-Imagine (ArXiv 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;325-self-imagine-arxiv-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#325-self-imagine-arxiv-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：即使是纯文本的逻辑题，人类也往往需要画图辅助理解，而 LLM 缺乏这种能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：该方法无需训练，通过**提示工程（Prompting）**让 VLM 将抽象的文本问题转化为结构化的 &lt;strong&gt;HTML 或 SVG 代码&lt;/strong&gt;，然后渲染成图像。模型再次读取这张自己生成的图表来回答问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：揭示了**&amp;ldquo;跨模态转换&amp;rdquo;**本身就是一种强大的推理增强手段。将文本转化为视觉结构，能够利用 VLM 强大的视觉模式识别能力来破解复杂的逻辑谜题。&lt;/p&gt;
&lt;h2&gt;4. 范式 III：潜在空间视觉推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-范式-iii潜在空间视觉推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e8%8c%83%e5%bc%8f-iii%e6%bd%9c%e5%9c%a8%e7%a9%ba%e9%97%b4%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式是目前&lt;strong&gt;效率最高、最具理论深度&lt;/strong&gt;的方向。它认为，显式地生成像素图像（如范式 II）虽然直观但计算极其昂贵，且容易受到生成伪影的干扰。真正的&amp;quot;视觉思维&amp;quot;应当发生在紧凑、高维的**潜在空间（Latent Space）**中，类似于人类大脑处理视觉信号时并不需要在视网膜上重新成像。&lt;/p&gt;
&lt;h3&gt;4.1 核心机制：连续视觉思维链&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;41-核心机制连续视觉思维链&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#41-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e8%bf%9e%e7%bb%ad%e8%a7%86%e8%a7%89%e6%80%9d%e7%bb%b4%e9%93%be&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;模型在推理过程中生成特殊的**&amp;ldquo;视觉 Token&amp;rdquo;&lt;strong&gt;或&lt;/strong&gt;&amp;ldquo;思维向量&amp;rdquo;&lt;strong&gt;。这些向量不对应任何具体的单词，也不必解码为像素，而是保留了&lt;/strong&gt;连续的梯度信息**，能够承载比离散文本丰富得多的感知细节（如纹理、深度、精确坐标）。&lt;/p&gt;
&lt;h3&gt;4.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;42-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#42-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;4.2.1 Chain-of-Visual-Thought (CoVT / VChain) (ICCV 2025 / ArXiv 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;421-chain-of-visual-thought-covt--vchain-iccv-2025--arxiv-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#421-chain-of-visual-thought-covt--vchain-iccv-2025--arxiv-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：语言是高度压缩的符号系统，用语言描述视觉细节（如&amp;quot;这个不规则物体的边缘&amp;quot;）会造成巨大的信息丢失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：作者提出了一组**&amp;ldquo;连续视觉 Token&amp;rdquo;（Continuous Visual Tokens）**。模型经过训练，可以在推理步骤中输出这些连续向量。这些向量在训练时通过重构任务（如重构深度图、分割掩码）进行监督，确保其包含物理意义，但在推理时直接作为后续层的输入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：重新定义了 CoT 的载体。思维链不再局限于离散的文本，而是变成了**&amp;ldquo;文本-视觉向量&amp;quot;混合流**。实验表明，这种方法在细粒度感知任务上大幅超越了纯文本 CoT。&lt;/p&gt;
&lt;h4&gt;4.2.2 Mirage (ArXiv 2025 / CVPR Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;422-mirage-arxiv-2025--cvpr-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#422-mirage-arxiv-2025--cvpr-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：如何在不引入沉重的图像生成解码器的情况下，赋予模型**&amp;ldquo;心智意象&amp;rdquo;**能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Mirage 提出了**&amp;ldquo;潜在意象&amp;rdquo;（Latent Imagery）**。它利用轻量级的投影层将视觉编码器的特征映射到 LLM 的嵌入空间。通过两阶段训练（先对齐感知，再强化推理），模型学会了在遇到视觉难题时&amp;quot;调用&amp;quot;潜在视觉记忆，并在多轮对话中保持这些视觉状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了高效的**&amp;ldquo;长上下文视觉推理&amp;rdquo;**。由于潜在 Token 占用的显存远小于生成图像，Mirage 能够支持更长的推理步骤和更复杂的视觉逻辑操作。&lt;/p&gt;
&lt;h4&gt;4.2.3 Latent Visual Reasoning (LVR) (CVPR/ECCV 2025 Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;423-latent-visual-reasoning-lvr-cvpreccv-2025-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#423-latent-visual-reasoning-lvr-cvpreccv-2025-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Latent Visual Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：现有的 VLM 往往在特征提取阶段就丢失了与问题相关的细微特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：LVR 引入了**&amp;ldquo;潜在重放&amp;rdquo;（Latent Replay）&lt;strong&gt;机制。利用强化学习（&lt;strong&gt;GRPO&lt;/strong&gt;），模型在推理过程中学会&amp;quot;回溯&amp;quot;并重新激活与当前推理步骤最相关的视觉潜在特征。它实际上是在潜在空间中进行了一种&lt;/strong&gt;&amp;ldquo;注意力重分配&amp;rdquo;**。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：提出了 **VLPO（Visual-Latent Policy Optimization）**算法，证明了可以通过强化学习直接优化潜在空间的推理路径，而无需显式的监督信号。&lt;/p&gt;
&lt;h4&gt;4.2.4 Slot-VLM (NeurIPS 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;424-slot-vlm-neurips-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#424-slot-vlm-neurips-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Slot-VLM: Object-Centric Learning with Slot Attention&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：标准 Transformer 的注意力机制是全局的、纠缠的，难以分离独立的物体概念，导致数数或关系判断出错。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：引入了**&amp;ldquo;Slot Attention&amp;quot;机制**。图像特征被强制分解为一组独立的**&amp;ldquo;Slot&amp;quot;向量**，每个 Slot 代表一个物体或实体。推理过程变成了对这些 Slot 的操作（如比较 Slot A 和 Slot B 的属性）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将**&amp;ldquo;以物体为中心&amp;rdquo;（Object-Centric）**的归纳偏置引入 VLM。这使得模型在物理推理、物体计数等任务上具有了类似于符号系统的鲁棒性，同时保留了神经网络的灵活性。&lt;/p&gt;
&lt;h4&gt;4.2.5 Coconut (COLM 2025 / ArXiv 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;425-coconut-colm-2025--arxiv-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#425-coconut-colm-2025--arxiv-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Training Large Language Models to Reason in a Continuous Latent Space&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：语言空间的推理往往会过早塌缩（&lt;strong&gt;Collapse&lt;/strong&gt;）到一个确定的路径，限制了**广度优先搜索（BFS）**的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Coconut 提出将 LLM 的最后一个**隐藏状态（Hidden State）&lt;strong&gt;直接作为下一个时间步的输入，而不是解码为离散的词。这种&lt;/strong&gt;&amp;ldquo;连续思维&amp;rdquo;**允许模型在潜在空间中同时探索多个推理分支（&lt;strong&gt;Superposition of thoughts&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：虽然最初针对 LLM 提出，但其理论框架是 2025 年多个视觉潜在推理工作（如 VChain）的基石。它从理论上证明了&lt;strong&gt;连续空间推理比离散空间推理具有更高的表达能力上限&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;5. 范式 IV：主动感知与强化视觉搜索&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-范式-iv主动感知与强化视觉搜索&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e8%8c%83%e5%bc%8f-iv%e4%b8%bb%e5%8a%a8%e6%84%9f%e7%9f%a5%e4%b8%8e%e5%bc%ba%e5%8c%96%e8%a7%86%e8%a7%89%e6%90%9c%e7%b4%a2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式将 VLM 从被动的观察者转变为主动的&lt;strong&gt;视觉智能体（Visual Agent）&lt;/strong&gt;。人类在观察复杂场景时，眼球会不断进行&lt;strong&gt;扫视（Saccade）&lt;strong&gt;和&lt;/strong&gt;注视（Fixation）&lt;/strong&gt;，通过主动改变关注点来获取信息。该范式试图在 VLM 中复现这一机制，通过&amp;quot;动作&amp;quot;来弥补&amp;quot;分辨率&amp;quot;和&amp;quot;注意力&amp;quot;的不足。&lt;/p&gt;
&lt;h3&gt;5.1 核心机制：感知即行动&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;51-核心机制感知即行动&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#51-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e6%84%9f%e7%9f%a5%e5%8d%b3%e8%a1%8c%e5%8a%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;推理过程被建模为一个&lt;strong&gt;马尔可夫决策过程（MDP）&lt;/strong&gt;。模型输出的不仅仅是答案，还有一系列感知动作指令：&lt;code&gt;&amp;lt;Zoom [x,y,w,h]&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;Crop&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;Look_Back&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;Scroll&amp;gt;&lt;/code&gt;。这些动作改变了模型的输入，从而形成了&lt;strong&gt;动态的推理链&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;5.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;52-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#52-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;5.2.1 DeepEyes (ArXiv 2025 / Likely CVPR/NeurIPS 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;521-deepeyes-arxiv-2025--likely-cvprneurips-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#521-deepeyes-arxiv-2025--likely-cvprneurips-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：DeepEyes: Incentivizing &amp;ldquo;Thinking with Images&amp;rdquo; via Reinforcement Learning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：现有的主动感知模型往往需要大量标注数据（如&amp;quot;先看这里，再看那里&amp;rdquo;），不仅昂贵且难以覆盖所有场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：DeepEyes 采用了端到端的&lt;strong&gt;强化学习（RL）&lt;/strong&gt;，具体使用了 &lt;strong&gt;GRPO 算法&lt;/strong&gt;。模型没有被教导如何看，而是仅在最终答案正确时获得奖励。在数万次训练迭代中，模型**自发涌现（Emerged）**出了类似人类的视觉策略：面对小物体会主动&amp;quot;放大&amp;rdquo;，面对大场景会&amp;quot;扫视&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：它是视觉领域的**&amp;ldquo;DeepSeek-R1 时刻&amp;rdquo;**。证明了复杂的视觉推理策略（&lt;strong&gt;System 2&lt;/strong&gt;）可以通过简单的结果奖励从零训练出来，而不需要模仿人类的中间步骤。&lt;/p&gt;
&lt;h4&gt;5.2.2 Visual CoT (NeurIPS 2024 Spotlight)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;522-visual-cot-neurips-2024-spotlight&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#522-visual-cot-neurips-2024-spotlight&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：缺乏一个标准化的基准来衡量模型&amp;quot;看图说话&amp;quot;过程中的中间对齐能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：作者构建了一个大规模数据集，其中的推理链不仅包含文本，还包含&lt;strong&gt;边界框（Bounding Boxes）&lt;/strong&gt;。模型被训练执行**&amp;ldquo;多轮聚焦&amp;quot;策略**：Step 1 预测感兴趣区域（&lt;strong&gt;RoI&lt;/strong&gt;）-&amp;gt; Step 2 裁剪该区域 -&amp;gt; Step 3 基于裁剪图回答。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：提出了**&amp;ldquo;可追踪的视觉推理&amp;rdquo;（Traceable Visual Reasoning）**。通过强制模型每一步都输出坐标，极大地减少了幻觉，并为错误分析提供了精确的依据。&lt;/p&gt;
&lt;h4&gt;5.2.3 Ferret (ICLR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;523-ferret-iclr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#523-ferret-iclr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Ferret: Refer and Ground Anything Anywhere at Any Granularity&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：传统的 &lt;strong&gt;Bounding Box&lt;/strong&gt; 过于粗糙，无法处理不规则形状（如蛇、线缆）或极细小的物体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Ferret 引入了**&amp;ldquo;混合区域编码器&amp;rdquo;（Hybrid Region Encoder）&lt;strong&gt;。它允许模型接受点、框、不规则形状（&lt;strong&gt;Sketch&lt;/strong&gt;）作为输入，并输出精细的区域掩码。这赋予了模型&lt;/strong&gt;&amp;ldquo;指哪打哪&amp;rdquo;**的精细感知能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将**&amp;ldquo;视觉定位&amp;rdquo;（Grounding）**的分辨率提升到了一个新的层级，是主动感知范式中处理细节信息的基石技术。&lt;/p&gt;
&lt;h4&gt;5.2.4 Shikra (ICCV 2023 / ArXiv 2023)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;524-shikra-iccv-2023--arxiv-2023&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#524-shikra-iccv-2023--arxiv-2023&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Shikra: Unleashing Multimodal LLM&amp;rsquo;s Referential Dialogue Magic&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：在 2023 年之前，VLM 的定位能力和对话能力往往是分离的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Shikra 是最早将空间坐标 &lt;strong&gt;[x, y]&lt;/strong&gt; 离散化为自然语言 Token 并参与自回归生成的模型之一。它证明了模型可以像学习外语一样学习**&amp;ldquo;位置语言&amp;rdquo;**，从而在对话中流畅地进行指代（&lt;strong&gt;Referential Dialogue&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：奠定了**&amp;ldquo;统一建模&amp;rdquo;**的基础。后来的 Visual CoT 和 DeepEyes 的坐标输出机制很大程度上继承了 Shikra 的设计哲学。&lt;/p&gt;
&lt;h4&gt;5.2.5 Look Twice Before You Answer (CVPR 2024 Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;525-look-twice-before-you-answer-cvpr-2024-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#525-look-twice-before-you-answer-cvpr-2024-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：VLM 的**&amp;ldquo;遗忘&amp;quot;现象**——随着文本生成越来越长，模型逐渐忘记了最初看到的图像，开始胡编乱造。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：提出了一种**&amp;ldquo;视觉回溯&amp;rdquo;（Visual Retracing）&lt;strong&gt;机制。在生成文本的过程中，模型会动态计算当前文本 Token 与原始图像特征的注意力权重。如果发现注意力发散，模型会强制&lt;/strong&gt;&amp;ldquo;回头看&amp;rdquo;**，重新加权图像特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将主动感知应用到了**&amp;ldquo;时间/记忆&amp;quot;维度**。它不是空间上的移动，而是注意力在时间轴上的回溯，是解决长文本幻觉的关键技术。&lt;/p&gt;
&lt;h2&gt;6. 范式 V：结构化与组合式视觉推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;6-范式-v结构化与组合式视觉推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#6-%e8%8c%83%e5%bc%8f-v%e7%bb%93%e6%9e%84%e5%8c%96%e4%b8%8e%e7%bb%84%e5%90%88%e5%bc%8f%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式认为，视觉推理的本质是&lt;strong&gt;结构化（Structure）&lt;strong&gt;和&lt;/strong&gt;组合性（Compositionality）&lt;/strong&gt;。图像不应被视为一堆像素或 Token，而应被解析为对象、属性和关系的集合。该范式致力于在推理过程中显式地构建或利用这种结构化表征（如&lt;strong&gt;场景图&lt;/strong&gt;、&lt;strong&gt;布局树&lt;/strong&gt;）。&lt;/p&gt;
&lt;h3&gt;6.1 核心机制：从像素到图谱&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;61-核心机制从像素到图谱&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#61-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e4%bb%8e%e5%83%8f%e7%b4%a0%e5%88%b0%e5%9b%be%e8%b0%b1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;推理过程包含显式的结构化步骤：&lt;code&gt;Image -&amp;gt; Scene Graph Generation -&amp;gt; Symbolic Reasoning -&amp;gt; Answer&lt;/code&gt;。这种方法将**感知（Perception）&lt;strong&gt;与&lt;/strong&gt;推理（Reasoning）**解耦，使得推理过程更加鲁棒和可控。&lt;/p&gt;
&lt;h3&gt;6.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;62-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#62-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;6.2.1 Compositional Chain-of-Thought (CCoT) (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;621-compositional-chain-of-thought-ccot-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#621-compositional-chain-of-thought-ccot-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Compositional Chain-of-Thought Prompting for Large Multimodal Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：VLM 经常犯**&amp;ldquo;属性绑定错误&amp;rdquo;（Attribute Binding Error）**，例如把穿红衣服的人看成穿蓝衣服，或者混淆两个物体的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：CCoT 强制模型分两步走：首先生成一个&lt;strong&gt;场景图（Scene Graph）&lt;/strong&gt;，明确列出所有对象节点（&lt;strong&gt;Nodes&lt;/strong&gt;）及其属性和关系边（&lt;strong&gt;Edges&lt;/strong&gt;）；然后基于这个场景图生成答案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将结构化数据作为 CoT 的中间模态。实验表明，显式的结构化描述迫使模型理清对象关系，大幅减少了组合性错误。&lt;/p&gt;
&lt;h4&gt;6.2.2 PixelLM (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;622-pixellm-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#622-pixellm-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：PixelLM: Pixel Reasoning with Large Multimodal Model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：许多推理任务需要&lt;strong&gt;像素级的理解&lt;/strong&gt;（例如&amp;quot;这两个重叠的物体谁在上面？&amp;quot;），传统的 Box 无法表达这种遮挡关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：PixelLM 在 LLM 输出端挂载了一个轻量级的&lt;strong&gt;像素解码器（Pixel Decoder）&lt;/strong&gt;。LLM 能够输出特定的**&amp;ldquo;分割 Token&amp;rdquo;**，这些 Token 解码后形成精细的物体掩码（&lt;strong&gt;Masks&lt;/strong&gt;）。推理过程基于这些掩码的拓扑关系进行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了**&amp;ldquo;像素级思维&amp;rdquo;**。它证明了 VLM 的推理粒度可以下沉到像素级别，为处理复杂的物理接触和遮挡关系提供了可能。&lt;/p&gt;
&lt;h4&gt;6.2.3 Osprey (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;623-osprey-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#623-osprey-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Osprey: Pixel Understanding with Visual Instruction Tuning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：如何让用户对图像中任意不规则区域进行提问？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Osprey 提出了一种**&amp;ldquo;掩码感知视觉提取器&amp;rdquo;（Mask-Aware Visual Extractor）**。它不仅接受图像，还接受一个掩码作为输入，能够提取该掩码覆盖区域的精细视觉特征。这使得推理可以针对图像的任何局部细节进行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了细粒度的**&amp;ldquo;交互式推理&amp;rdquo;**。它不仅是模型看图，更是用户通过 &lt;strong&gt;Point/Mask&lt;/strong&gt; 与模型进行精准的视觉对话。&lt;/p&gt;
&lt;h4&gt;6.2.4 Sphinx (NeurIPS 2024 Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;624-sphinx-neurips-2024-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#624-sphinx-neurips-2024-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Sphinx / ReasonBench Context&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：&lt;strong&gt;抽象视觉推理&lt;/strong&gt;（如瑞文智商测试、图表逻辑）是 VLM 的短板。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Sphinx 通过混合**高分辨率主动缩放（Active Scaling）**和多样化的视觉任务训练，增强了模型对抽象几何结构的感知能力。配合 &lt;strong&gt;ReasonBench 基准&lt;/strong&gt;，它展示了结构化数据训练对提升逻辑推理的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：探索了 VLM 在纯抽象视觉逻辑上的边界，证明了通过丰富的数据结构（如合成图表、几何题）可以提升模型的&lt;strong&gt;通用推理智商&lt;/strong&gt;。&lt;/p&gt;
&lt;h4&gt;6.2.5 Argus (CVPR 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;625-argus-cvpr-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#625-argus-cvpr-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：**视觉定位（Grounding）&lt;strong&gt;与&lt;/strong&gt;文本推理（Reasoning）**往往是割裂的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Argus 提出了一种&lt;strong&gt;以视觉为中心的推理框架&lt;/strong&gt;，强制模型在生成每一个推理步骤时，都要同步输出对应的视觉证据区域。这不仅仅是 Visual CoT 的应用，更是一种由于架构设计带来的&lt;strong&gt;强对齐约束&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：强调了**&amp;ldquo;对齐即推理&amp;rdquo;**。只有当模型能够准确指出&amp;quot;我为什么这么说&amp;quot;的视觉依据时，我们才能认为它真正具备了视觉推理能力。&lt;/p&gt;
&lt;h2&gt;7. 综合对比与未来展望&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;7-综合对比与未来展望&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#7-%e7%bb%bc%e5%90%88%e5%af%b9%e6%af%94%e4%b8%8e%e6%9c%aa%e6%9d%a5%e5%b1%95%e6%9c%9b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;7.1 五大范式横向对比表&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;71-五大范式横向对比表&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#71-%e4%ba%94%e5%a4%a7%e8%8c%83%e5%bc%8f%e6%a8%aa%e5%90%91%e5%af%b9%e6%af%94%e8%a1%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;范式维度&lt;/th&gt;
          &lt;th&gt;范式 I：工具中介&lt;/th&gt;
          &lt;th&gt;范式 II：显式生成&lt;/th&gt;
          &lt;th&gt;范式 III：潜在推理&lt;/th&gt;
          &lt;th&gt;范式 IV：主动感知&lt;/th&gt;
          &lt;th&gt;范式 V：结构化组合&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心隐喻&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;外包给计算器&lt;/td&gt;
          &lt;td&gt;脑海中的草稿纸&lt;/td&gt;
          &lt;td&gt;直觉与内隐思考&lt;/td&gt;
          &lt;td&gt;眼动与探索&lt;/td&gt;
          &lt;td&gt;逻辑大纲与图谱&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;中间模态&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;代码 (Python)、API&lt;/td&gt;
          &lt;td&gt;像素图像 (Images)&lt;/td&gt;
          &lt;td&gt;连续向量 (Vectors)&lt;/td&gt;
          &lt;td&gt;动作指令 (Actions)&lt;/td&gt;
          &lt;td&gt;场景图、掩码 (Graphs)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;优势&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;逻辑严密，可验证，计算精准&lt;/td&gt;
          &lt;td&gt;处理空间变换、反事实推理极强&lt;/td&gt;
          &lt;td&gt;信息密度最高，推理效率高&lt;/td&gt;
          &lt;td&gt;模拟人类行为，适应大图/视频&lt;/td&gt;
          &lt;td&gt;解决复杂关系，鲁棒性高&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;劣势&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;依赖工具库，非端到端，慢&lt;/td&gt;
          &lt;td&gt;计算开销极大，生成误差累积&lt;/td&gt;
          &lt;td&gt;黑盒不可解释，调试困难&lt;/td&gt;
          &lt;td&gt;训练难度大 (RL)，收敛慢&lt;/td&gt;
          &lt;td&gt;依赖解析器，灵活性受限&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;代表作&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Visual Sketchpad, ViperGPT&lt;/td&gt;
          &lt;td&gt;MVoT, ImagineNav&lt;/td&gt;
          &lt;td&gt;CoVT, DeepSketcher&lt;/td&gt;
          &lt;td&gt;DeepEyes, Ferret&lt;/td&gt;
          &lt;td&gt;CCoT, PixelLM&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;数学几何、精确计数、测量&lt;/td&gt;
          &lt;td&gt;导航规划、物理预测、拼图&lt;/td&gt;
          &lt;td&gt;通用视觉问答、细粒度感知&lt;/td&gt;
          &lt;td&gt;极高分辨率图像、监控视频&lt;/td&gt;
          &lt;td&gt;复杂场景理解、关系推理&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;7.2 技术融合的趋势：迈向多模态 AGI&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;72-技术融合的趋势迈向多模态-agi&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#72-%e6%8a%80%e6%9c%af%e8%9e%8d%e5%90%88%e7%9a%84%e8%b6%8b%e5%8a%bf%e8%bf%88%e5%90%91%e5%a4%9a%e6%a8%a1%e6%80%81-agi&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过对 &lt;strong&gt;2025 年最新文献&lt;/strong&gt;的分析，我们可以清晰地看到不同范式正在发生融合：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RL 的全面渗透&lt;/strong&gt;：&lt;strong&gt;DeepEyes&lt;/strong&gt; 和 &lt;strong&gt;SpatialDreamer&lt;/strong&gt; 的成功表明，&lt;strong&gt;强化学习&lt;/strong&gt;正在成为训练 &lt;strong&gt;System 2 视觉推理&lt;/strong&gt;的标准范式。未来的 VLM 将不再仅仅依靠监督微调（&lt;strong&gt;SFT&lt;/strong&gt;），而是通过 RL 自我探索出最优的&amp;quot;看图策略&amp;rdquo;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;潜在与显式的结合&lt;/strong&gt;：&lt;strong&gt;DeepSketcher&lt;/strong&gt; 展示了可以在潜在空间中进行&amp;quot;显式&amp;quot;的操作（如编辑 Embedding）。未来的模型可能会在潜在空间中进行高效推理，仅在需要验证时才&amp;quot;解码&amp;quot;为像素图像。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;从感知到行动&lt;/strong&gt;：随着**具身智能（Embodied AI）**的兴起，Paradigm IV（主动感知）将变得越来越重要。视觉推理将不再局限于静态图像，而是延伸到对环境的主动探索和交互中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;8. 结语&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;8-结语&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#8-%e7%bb%93%e8%af%ad&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;从**&amp;ldquo;Thinking about Images&amp;rdquo;&lt;strong&gt;到&lt;/strong&gt;&amp;ldquo;Thinking with Images&amp;rdquo;**的转变，标志着多模态大模型正在跨越感知的门槛，迈向认知的殿堂。无论是通过代码外化思维、通过生成模拟未来、通过向量内隐推演，还是通过动作主动探索，这些新兴范式都在试图弥合语言与视觉之间的鸿沟。&lt;/p&gt;
&lt;p&gt;本次调研表明，&lt;strong&gt;单纯的文本 CoT 已不足以支撑下一代 VLM 的发展&lt;/strong&gt;。未来的多模态模型必将是&lt;strong&gt;混合架构&lt;/strong&gt;的——它拥有类似 &lt;strong&gt;System 1&lt;/strong&gt; 的快速感知编码器，同时也拥有由 &lt;strong&gt;RL 训练而成的 System 2 推理引擎&lt;/strong&gt;，能够灵活地调用工具、生成意象、在潜在空间深思熟虑，并像人类一样主动去&amp;quot;看&amp;quot;清这个世界。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>重新思考三维空间感知与具身导航决策在毕设中的研究点</title>
      <link>http://localhost:1313/blog/2025/2025-11-12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-12/</guid>
      <description>
        
        
        &lt;p&gt;首先，我们的Baseline —— LOVON (Legged Open-Vocabulary Object Navigator, 2025) 是一个在 Gym-Unreal（即 Gym-UnrealCV 风格的仿真 benchmark）上做了大规模仿真实验来验证其开阔词表目标搜索与导航能力；文中强调用虚幻环境来做长航时、动态目标搜索的系统验证（包括视觉抖动、目标短暂消失等问题）并在仿真里验证 Laplacian Variance Filtering、语言→运动模型等模块。也进行了真实腿式机器人（Unitree 系列）上的跨域验证以检验 sim→real。&lt;/p&gt;
&lt;!-- truncate --&gt;
&lt;h2&gt;思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们第一步要做的就是 Define Problem。若能有清晰的问题定位 + 合理指标 +实证结果 +对比分析，就有很大机会产出成果。一个很好的方法就是自问自答：&lt;/p&gt;
&lt;h3&gt;一、现状定位：用 LOVON 的方法在真机上效果很差──最关键的失败点是什么？“机器狗撞门框”？那是什么原因？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一现状定位用-lovon-的方法在真机上效果很差最关键的失败点是什么机器狗撞门框那是什么原因&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e7%8e%b0%e7%8a%b6%e5%ae%9a%e4%bd%8d%e7%94%a8-lovon-%e7%9a%84%e6%96%b9%e6%b3%95%e5%9c%a8%e7%9c%9f%e6%9c%ba%e4%b8%8a%e6%95%88%e6%9e%9c%e5%be%88%e5%b7%ae%e6%9c%80%e5%85%b3%e9%94%ae%e7%9a%84%e5%a4%b1%e8%b4%a5%e7%82%b9%e6%98%af%e4%bb%80%e4%b9%88%e6%9c%ba%e5%99%a8%e7%8b%97%e6%92%9e%e9%97%a8%e6%a1%86%e9%82%a3%e6%98%af%e4%bb%80%e4%b9%88%e5%8e%9f%e5%9b%a0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;是因为纯2D检测＋动作映射，缺乏深度／3D理解？&lt;/li&gt;
&lt;li&gt;是因为没有障碍物避障规划？&lt;/li&gt;
&lt;li&gt;是因为导航规划缺失，仅“向目标走”而不考虑路径？&lt;/li&gt;
&lt;li&gt;还是别的问题（如机器狗控制延迟、检测误差大、目标消失后无追踪策略）？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LOVON的原理，也就是视觉追踪的原理在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;目标提取与筛选（&lt;code&gt;_yolo_image_post_process&lt;/code&gt; 方法）&lt;/p&gt;
&lt;p&gt;先通过 &lt;code&gt;object_extractor&lt;/code&gt; 从任务指令（默认任务是 run to the person at speed of 0.36 m/s，提取目标为 “person”）中提取目标类别。YOLO 模型输出所有检测框后，只保留类别与提取目标一致的框，过滤无关目标。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;滑动窗口历史管理&lt;/p&gt;
&lt;p&gt;初始化 5 个历史缓存列表，分别存储目标类别、置信度、归一化坐标（xyn）、归一化宽高（whn）、像素坐标（xyxy）。每帧仅保留置信度最高的检测框，加入缓存列表；当列表长度超过 &lt;code&gt;lengthen_filter&lt;/code&gt; 时，删除最早的帧，维持窗口大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;追踪结果计算&lt;/p&gt;
&lt;p&gt;对缓存列表中的数据取平均值，得到平滑后的置信度、坐标和宽高。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;object_xyn[0]&lt;/code&gt; 是目标中心的水平归一化坐标（0~1，0 为左边界、0.5 为图像中心、1 为右边界）。&lt;/li&gt;
&lt;li&gt;若目标在图像中心（&lt;code&gt;xyn[0] ≈ 0.5&lt;/code&gt;）：机器狗沿前后方向运动（&lt;code&gt;v_x&lt;/code&gt; 按任务指令速度，如 0.36m/s，&lt;code&gt;v_y = 0&lt;/code&gt;，&lt;code&gt;w_z = 0&lt;/code&gt;），即 “往前走”。&lt;/li&gt;
&lt;li&gt;若目标偏左（&lt;code&gt;xyn[0] &amp;lt; 0.5&lt;/code&gt;）：&lt;code&gt;w_z&lt;/code&gt; 为正（顺时针旋转），同时 &lt;code&gt;v_x&lt;/code&gt; 降低，直到目标回到中心；偏右则相反。&lt;/li&gt;
&lt;li&gt;任务指令中的 “speed” 仅限制 &lt;code&gt;v_x&lt;/code&gt; 的最大值，而非强制固定 &lt;code&gt;v_x&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;统计列表中出现次数最多的目标类别，作为当前追踪目标（避免单帧误检影响）。若目标类别为 “NULL”（无有效检测），则重置追踪结果为默认值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;跟丢的判定标准&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单帧无检测：YOLO 未检测到与 &lt;code&gt;extracted_object&lt;/code&gt; 匹配的框 → 往历史缓存中添加 “NULL” 和 0 置信度。&lt;/li&gt;
&lt;li&gt;连续跟丢：当历史缓存（长度由 &lt;code&gt;lengthen_filter&lt;/code&gt; 控制）中 “NULL” 出现次数最多 → &lt;code&gt;most_common_object&lt;/code&gt; 变为 “NULL”，&lt;code&gt;avg_confidence&lt;/code&gt; 设为 0 → 判定为 “跟丢”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;motion_predictor&lt;/code&gt; 接收 “跟丢状态” 后，生成搜索型 &lt;code&gt;motion_vector&lt;/code&gt;：&lt;/p&gt;
&lt;p&gt;通常是「旋转搜索」：&lt;code&gt;v_x = 0&lt;/code&gt;（不前后动）、&lt;code&gt;v_y = 0&lt;/code&gt;（不左右动）、&lt;code&gt;w_z ≠ 0&lt;/code&gt;（缓慢旋转，扫描周围环境）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;机器狗撞门框的原因在于，这里现实环境的部署代码通过 YOLO 只识别到了目标但是没有理解环境与障碍物，而当人消失在门后时，最后一帧这个目标是在画面中心的，因此机器狗会往前走直到撞到门框，又或者笨笨的在门框那个位置旋转搜索。因为没有开源其仿真智能体的代码所以不知道模拟环境是怎么规避这个问题的&lt;/p&gt;
&lt;h3&gt;二、Gap 与定位：基于你上面的回答，问题在哪儿？用一句话描述这里的 gap（研究空白）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二gap-与定位基于你上面的回答问题在哪儿用一句话描述这里的-gap研究空白&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cgap-%e4%b8%8e%e5%ae%9a%e4%bd%8d%e5%9f%ba%e4%ba%8e%e4%bd%a0%e4%b8%8a%e9%9d%a2%e7%9a%84%e5%9b%9e%e7%ad%94%e9%97%ae%e9%a2%98%e5%9c%a8%e5%93%aa%e5%84%bf%e7%94%a8%e4%b8%80%e5%8f%a5%e8%af%9d%e6%8f%8f%e8%bf%b0%e8%bf%99%e9%87%8c%e7%9a%84-gap%e7%a0%94%e7%a9%b6%e7%a9%ba%e7%99%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;比如：“在足式机器人真实场景下，当前Open-vocab检测＋简单动作生成不能有效处理目标暂时丢失和复杂障碍物，导致跟踪／导航失败”。还是要聚焦 “障碍物避障” 或 “三维深度理解”？&lt;/p&gt;
&lt;p&gt;在足式机器人开放世界目标追踪任务中，现有基于纯 2D 视觉目标检测的追踪 - 运动映射方案，因缺乏环境障碍物感知与三维空间理解，且目标暂时丢失后仅采用无环境适配的旋转搜索策略，导致无法应对 “目标被遮挡 / 消失后因路径误判碰撞障碍物” 等真实场景挑战，难以实现稳健的长时追踪与运动控制（具体有没有3D视觉目标检测的论文工作，现在还没有做过调研）&lt;/p&gt;
&lt;h3&gt;三、重要性在哪里？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三重要性在哪里&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e9%87%8d%e8%a6%81%e6%80%a7%e5%9c%a8%e5%93%aa%e9%87%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对学术来说：为什么“足式机器人 + open-vocab目标导航/跟踪”值得研究？是否当前工作少？&lt;/li&gt;
&lt;li&gt;对应用来说：在真实环境（室内／复杂家具／光照变化）中，解决这个问题会带来什么改进？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于我来说，我还只是一个入门新手，打算通过本科毕设的机会，从3D世界理解和具身导航决策这个小角度切入来入门具身领域，所以我也说不清楚学术和应用上的重要性，只求发ccfb以上的paper证明自己&lt;/p&gt;
&lt;h3&gt;四、创新点初步想法？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四创新点初步想法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9b%e5%88%9b%e6%96%b0%e7%82%b9%e5%88%9d%e6%ad%a5%e6%83%b3%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过和导师学长们讨论列出了很多可能的优化方向（环境理解增强、分层决策升级），从这些中最可能做出论文中&lt;strong&gt;可量化贡献&lt;/strong&gt;的一个或两个是什么？
比如：“用 BEV 俯视地图 +轨迹预测 来增强 open-vocab 目标导航”；或者：“在足式机器人上验证视觉+深度融合检测在目标丢失场景下的跟踪稳定性提升”。哪一个更倾向？为什么？&lt;/p&gt;
&lt;p&gt;我不知道量化贡献的指标可以在哪里进一步优化啊，原因也可能在于我读的文献太少了，LOVON在仿真里所使用的指标为衡量 100 次实验中完成任务的平均步数、衡量 100 次实验中成功完成任务的比例两个，而如何去量化现实任务的指标与sim2real的优化，因为文献读的不多所以暂时我还不能回答这个问题&lt;/p&gt;
&lt;h3&gt;五、可量化指标与对比：要发论文，必须有可测量的结果，可以测量哪些指标？例如：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五可量化指标与对比要发论文必须有可测量的结果可以测量哪些指标例如&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94%e5%8f%af%e9%87%8f%e5%8c%96%e6%8c%87%e6%a0%87%e4%b8%8e%e5%af%b9%e6%af%94%e8%a6%81%e5%8f%91%e8%ae%ba%e6%96%87%e5%bf%85%e9%a1%bb%e6%9c%89%e5%8f%af%e6%b5%8b%e9%87%8f%e7%9a%84%e7%bb%93%e6%9e%9c%e5%8f%af%e4%bb%a5%e6%b5%8b%e9%87%8f%e5%93%aa%e4%ba%9b%e6%8c%87%e6%a0%87%e4%be%8b%e5%a6%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;目标被丢失的次数／恢复次数&lt;/li&gt;
&lt;li&gt;障碍物碰撞次数&lt;/li&gt;
&lt;li&gt;成功到达目标的比例&lt;/li&gt;
&lt;li&gt;路径长度／时间／效率&lt;/li&gt;
&lt;li&gt;跟踪保持时间／跟丢时间
− 真机 vs 仿真的差距（sim2real gap）
能够在实机上测这些指标吗？哪些可能无法测？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;六、实验平台／可行性：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六实验平台可行性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e5%ae%9e%e9%aa%8c%e5%b9%b3%e5%8f%b0%e5%8f%af%e8%a1%8c%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;已有的硬件是 Unitree Go2 足式机器人，这很好。你能控制机器人做什么动作（向前、转、停止、避障）？你能获取哪些传感器数据（RGB、深度、IMU、里程计）？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否有仿真实验环境（如 Gym-UnrealCV 场景）可以先做仿真再到实机？仿真与实机之间能记录相同指标吗？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;时间上本科毕设资源有限，预计能做多少场景／多少实验次数？这个对决定指标和可行性很重要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于硬件设备，要关注[官方SDK文档](&lt;a href=&#34;https://support.unitree.com/home/en/developer&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://support.unitree.com/home/en/developer&lt;/a&gt;）：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;一、动作控制能力&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基础运动控制
&lt;ul&gt;
&lt;li&gt;前进 / 后退 / 转向：通过Move(vx, vy, vyaw)函数直接设置线速度（vx/vy）和角速度（vyaw），支持相对于世界坐标系的运动控制。例如，Move(0.5, 0, 0)使机器人以 0.5m/s 速度向前移动。&lt;/li&gt;
&lt;li&gt;停止：调用StopMove()立即终止所有运动，进入静止状态。&lt;/li&gt;
&lt;li&gt;步态切换：通过SwitchGait(int d)选择不同步态（如小跑、踱步），或使用ContinuousGait(bool flag)启用连续步态模式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;高级动作与姿态调整
&lt;ul&gt;
&lt;li&gt;站立 / 坐下：StandUp()和Sit()实现起立和坐下动作，RecoveryStand()用于从侧翻状态恢复。&lt;/li&gt;
&lt;li&gt;身体姿态控制：Euler(roll, pitch, yaw)可调整机身倾角，BodyHeight(float height)动态改变离地高度。&lt;/li&gt;
&lt;li&gt;特技动作：支持FrontFlip()前空翻、FrontJump()跳跃等复杂动作（需硬件支持）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;避障功能
&lt;ul&gt;
&lt;li&gt;自主避障：通过ObstacleAvoidClient类启用避障模块，机器人可实时检测障碍物并调整路径。需调用EnableObstacleAvoidance()激活，并在移动时保持避障服务运行。&lt;/li&gt;
&lt;li&gt;传感器融合：避障依赖激光雷达（PRO/EDU 版）或深度相机（AIR 版）与 IMU 数据融合，实现动态环境下的安全导航。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;二、传感器数据获取&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉传感器
&lt;ul&gt;
&lt;li&gt;RGB 图像：通过 ROS2 话题/camera/image_raw获取 720P/1080P 实时视频流，支持 WebRTC 低延迟传输。&lt;/li&gt;
&lt;li&gt;深度数据：PRO/EDU 版搭载 4D 激光雷达（L1），可输出 360°×90° 点云数据（/go2/camera/depth）；AIR 版通过 Intel RealSense D435i 深度相机提供毫米级深度信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;惯性测量单元（IMU）
&lt;ul&gt;
&lt;li&gt;原始数据：通过 ROS2 话题/imu/data获取加速度（a_x, a_y, a_z）、角速度（ω_x, ω_y, ω_z）和四元数姿态（q_w, q_x, q_y, q_z）。&lt;/li&gt;
&lt;li&gt;坐标系转换：SDK 提供工具函数处理不同框架下的四元数顺序（如 Isaac Gym 与 Isaac Sim 的差异）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;里程计与定位
&lt;ul&gt;
&lt;li&gt;状态估计：通过激光雷达 + IMU 融合（如 LIO-SAM 算法）或腿部运动学模型（关节编码器数据）实现里程计输出。ROS2 话题/odom提供机器人位姿（x, y, θ）和速度信息。&lt;/li&gt;
&lt;li&gt;精度优化：紧耦合 LiDAR-IMU - 腿部里程计系统可在无特征环境下实现亚米级定位精度，在线学习机制适应负载和地形变化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他传感器
&lt;ul&gt;
&lt;li&gt;关节状态：实时获取 12 个关节的角度、角速度和扭矩（/joint_states），支持电机健康监测。&lt;/li&gt;
&lt;li&gt;足端力反馈：PRO/EDU 版配备足端力传感器（F_z），用于复杂地形下的步态调整。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于仿真环境，我有本地的Gym-Unrealcv仿真场景，但是苦恼于LOVON没有开源其仿真代码所以搁置着，不清楚下一步是根据部署代码反推仿真代码还是换一个仿真环境如MatterPort3D
时间本身还是比较充裕的，到开题答辩之前至少有1个月时间&lt;/p&gt;
&lt;h2&gt;研究现状&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究现状&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e7%8e%b0%e7%8a%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;维度&lt;/th&gt;
          &lt;th&gt;内容总结&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;基线模型&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON（LoVi: Open-vocabulary Visual Navigation and Tracking）在仿真中近乎完美（≈100% success rate），但在真实环境严重失效。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心问题&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON 只用 YOLO 的 2D 框坐标来做“视觉 → 动作”映射，没有任何 3D 环境建模或避障机制。目标消失（如进门）时，机器人仍执行“往前走”动作 → 撞门框。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;可用硬件&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Unitree Go2（有RGB、深度、IMU、里程计、足端力传感器）。具备基本避障API、Move(vx,vy,vyaw)控制接口。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;仿真环境&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;有Gym-UnrealCV，但缺少LOVON仿真智能体代码。可能考虑复刻或转向MatterPort3D。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;研究目标雏形&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;希望提升LOVON从2D视觉到更稳健3D环境理解（environment understanding + navigation fusion）的能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;问题定义&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题定义&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e5%ae%9a%e4%b9%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;当前的 open-vocabulary 视觉追踪方法（如 LOVON）在仿真中表现优异，但在真实足式机器人环境中严重退化，其原因在于缺乏对三维环境几何与障碍物的建模能力。
其技术设计恰好规避了仿真环境的局限性，同时最大化了自身优势，具体体现在 3 个 “无冲突”：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “真实场景的 3D 感知需求”，纯 2D 视觉足够&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真环境中，目标的 “2D 图像坐标” 与 “实际空间位置” 完全对齐（如虚拟场景中 xyn=0.5 即代表物理上的正前方，无门框等 3D 遮挡物），无需深度信息即可判断路径是否可行。而 LOVON 的核心是 “2D 视觉 + 运动向量映射”，恰好适配这种需求，无需额外的 3D 深度理解模块。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “不可控干扰”，搜索策略高效&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真中的 “目标丢失” 仅为 “目标移出 90 度扇形视野”（可通过旋转搜索快速重新捕获），无真实场景的 “目标被门框完全遮挡”“机器人被碰撞” 等不可控干扰。LOVON 的旋转搜索策略（vx=0、w_z≠0）在仿真中能高效覆盖视野，而不会像真实场景那样因 “旋转时忽略障碍物” 导致碰撞。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真数据与模型训练 “高度同源”&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真使用的目标类别（背包、椅子、行人）、运动速度（0.3~0.7m/s）、场景光照均与 LOVON 的训练数据集（100 万样本，摘要 1）高度匹配：IOE 对 “椅子”“行人” 的类别映射无误差，L2MM 的运动预测参数（如 β=10）也针对仿真场景校准（摘要 3），避免了真实场景中 “未见过的目标形态”“突发速度变化” 导致的误差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当目标被暂时遮挡（如进入门后）或在复杂结构环境中移动时，机器人仅凭2D像素坐标进行动作决策，无法有效区分“自由空间”与“障碍区域”，导致运动策略失效（如撞门、原地旋转）。
因此，本研究旨在探索一种融合3D环境理解的目标跟踪与导航方法，在保持LOVON开放词汇指令能力的前提下，提高其在真实环境中的鲁棒性与安全性。&lt;/p&gt;
&lt;h2&gt;研究方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;方向&lt;/th&gt;
          &lt;th&gt;名称&lt;/th&gt;
          &lt;th&gt;思路简述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;A. 环境理解增强（BEV / Depth / 3D Occupancy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;给LOVON加“视觉深度感知”，即在YOLO检测的基础上，通过深度图重投影到3D坐标系或BEV平面，建立占用图。再利用该图进行避障或规划。&lt;/td&gt;
          &lt;td&gt;你能做仿真+实机对比，提出一种“轻量级3D-aware追踪方法”。 → 投稿到 &lt;strong&gt;IROS/ICRA workshop 或 CCF-C AI Robotics会议&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;B. 跟踪 + 导航分层融合（Hierarchical Policy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;把“跟踪”和“导航”分成两个层次：高层目标预测、低层路径规划。你可以用简单预测（如卡尔曼滤波预测目标短期轨迹）+ BEV局部避障（A*或DWA）。&lt;/td&gt;
          &lt;td&gt;可以与LOVON对比“复杂场景成功率”→ 写出完整paper。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;选择 A + B 结合的小主题：“基于3D视觉感知与分层导航策略的开放词汇足式机器人目标追踪”(但这个一听就感觉不少人做过类似的课题非常卷)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;含义&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;任务层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Success Rate (SR)&lt;/td&gt;
          &lt;td&gt;机器人在有限步数内到达目标的比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Average Steps (AS)&lt;/td&gt;
          &lt;td&gt;成功任务平均步数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Collision Rate (CR)&lt;/td&gt;
          &lt;td&gt;发生障碍碰撞的任务比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;视觉层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Target Loss Time (TLT)&lt;/td&gt;
          &lt;td&gt;目标丢失后重新识别的平均时间&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Tracking Stability (TS)&lt;/td&gt;
          &lt;td&gt;目标检测框抖动方差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Sim2Real 层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;ΔSR (Sim→Real)&lt;/td&gt;
          &lt;td&gt;仿真与实机成功率差距&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;效率指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;FPS / Latency&lt;/td&gt;
          &lt;td&gt;模型推理帧率与系统延迟&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;安全指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Distance Margin&lt;/td&gt;
          &lt;td&gt;与障碍最近距离的平均值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;在仿真中先实现自动收集 SR、AS、CR。&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;实机可手动统计 SR 和 CR，或用里程计测轨迹。&lt;/p&gt;
&lt;p&gt;可定义 3 个场景（开阔场 / 门框 / 桌椅环境）各跑10次。&lt;/p&gt;
&lt;h2&gt;创新点（暂定）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;创新点暂定&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%88%9b%e6%96%b0%e7%82%b9%e6%9a%82%e5%ae%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们提出一种融合深度视觉感知与分层控制的开放词汇目标追踪框架。
相较于LOVON仅依赖2D目标检测进行运动控制，我们的方法通过深度投影构建局部BEV占用图，并引入预测-驱动的路径规划层，从而显著减少在真实环境中因遮挡或障碍导致的失败&lt;/p&gt;
&lt;h2&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;时间&lt;/th&gt;
          &lt;th&gt;任务&lt;/th&gt;
          &lt;th&gt;目标&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;第1阶段&lt;/td&gt;
          &lt;td&gt;阅读文献：LOVON、LOVi、BEVFusion、LIO-SAM、SceneGPT&lt;/td&gt;
          &lt;td&gt;明确3D环境理解技术路线&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第2阶段&lt;/td&gt;
          &lt;td&gt;在Gym-UnrealCV中复现或简化LOVON策略（YOLO+Motion mapping）&lt;/td&gt;
          &lt;td&gt;建立baseline可控环境&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第3阶段&lt;/td&gt;
          &lt;td&gt;集成深度图或BEV投影模块，实现障碍建模与避障决策&lt;/td&gt;
          &lt;td&gt;形成改进方法&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第4阶段&lt;/td&gt;
          &lt;td&gt;实机测试 + 指标对比 + 论文撰写&lt;/td&gt;
          &lt;td&gt;形成可投稿版本&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;第一阶段的文献调研一方面要包括LOVON引用的和引用LOVON的文献（但是因为VPN节点问题我的Scholar Google给我挂掉了，说我是机器人不让我访问），另一方面是尽可能的调研3D-aware Tracking/Navigation&lt;/p&gt;
&lt;h2&gt;医疗交叉（答辩）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;医疗交叉答辩&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8c%bb%e7%96%97%e4%ba%a4%e5%8f%89%e7%ad%94%e8%be%a9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里值得注意的是，论文里面要写的内容是一个宏大的改进，但是本院答辩时要突出和BME相关、医疗交叉的内容，HexGuide可以作为一个很大的参考&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;《HexGuide: A Hexapod Robot for Autonomous Blind Guidance in Challenging Environments》，一篇期刊论文&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;层级&lt;/th&gt;
          &lt;th&gt;内容&lt;/th&gt;
          &lt;th&gt;对应写作作用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;① 背景&lt;/td&gt;
          &lt;td&gt;世界上有数亿视障人群，对自主出行有刚性需求&lt;/td&gt;
          &lt;td&gt;让读者意识到社会价值和痛点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;② 矛盾&lt;/td&gt;
          &lt;td&gt;现有导盲设备（如导盲犬或轮式机器人）有明显局限，不能稳定地应对复杂地形&lt;/td&gt;
          &lt;td&gt;设置“冲突”——为什么我们必须做新系统&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;③ 概念&lt;/td&gt;
          &lt;td&gt;上交高峰团队设计了一个六足机器人 &lt;strong&gt;HexGuide&lt;/strong&gt;，模仿昆虫式稳定步态，在复杂环境中实现安全引导&lt;/td&gt;
          &lt;td&gt;提出核心创新点和愿景&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;④ 方法&lt;/td&gt;
          &lt;td&gt;通过算法与机械协同，实现&lt;strong&gt;路径规划 + 稳定行走 + 动态避障 + 交通识别 + 人机交互&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;展示技术路线是如何支撑“稳定、安全”这两个关键词的&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;⑤ 验证&lt;/td&gt;
          &lt;td&gt;在机场、十字路口等复杂场景下实测验证，引导成功率高，路径平滑且避障成功&lt;/td&gt;
          &lt;td&gt;用结果“闭环”故事——愿景得以实现&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;论文的立意不是“做一个六足机器人”，而是要证明“六足+智能控制” = 可靠的盲人引导方式。这篇论文的核心任务不是“跟踪一个已知目标”或“视觉跟随”，而是“带领盲人从一个地点到另一个地点”，比如：“from the arrival gate to the baggage claim area in Shanghai Hongqiao Airport.”&lt;/p&gt;
&lt;p&gt;核心流程是盲人用户通过语音指令（如“去出口”）输入目标；在地图上自动规划从当前位置到目标的安全路径；机器人沿着规划路径行走；实时感知环境并修正轨迹。&lt;/p&gt;
&lt;p&gt;目标不是视觉追踪的对象，而是一个空间位置目标，因此这种导航是Goal-based而非Object-based tracking，而且泛化性有限：“The system can autonomously navigate in challenging environments once a map is available.”&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模块&lt;/th&gt;
          &lt;th&gt;故事逻辑&lt;/th&gt;
          &lt;th&gt;手法&lt;/th&gt;
          &lt;th&gt;指标体现&lt;/th&gt;
          &lt;th&gt;补充说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;机械稳定性：六足结构的天然稳态&lt;/td&gt;
          &lt;td&gt;盲人行走必须安全 → 足式比轮式更抗地形 → 六足比四足更稳&lt;/td&gt;
          &lt;td&gt;· “三足支撑步态（Tripod gait）”确保任意时刻三条腿接地&lt;br /&gt;· 单腿轨迹采用三次样条插值，区分支撑相与摆动相以减冲击&lt;br /&gt;· 控制顶点高度以跨越障碍、维持步态连续&lt;/td&gt;
          &lt;td&gt;· 平均支撑腿数 ≥ 3&lt;br /&gt;· 步态周期内质心（CoM）位移波动 &amp;lt; 5 mm&lt;br /&gt;· 10° 坡面及不平地面仍能维持姿态&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;规划稳定性：安全路径生成&lt;/td&gt;
          &lt;td&gt;“安全通行”要求路径不过度摆动、不贴近障碍&lt;/td&gt;
          &lt;td&gt;· 基于 A* 进行全局规划&lt;br /&gt;· 融合人工势场（APF）调整代价，使路径自动远离障碍&lt;br /&gt;· Bézier 曲线平滑路径&lt;br /&gt;· 拐点以贪心方式优化，减少急转角&lt;/td&gt;
          &lt;td&gt;· 路径平滑度提升（转向角波动减少约 40%）&lt;br /&gt;· 路径与障碍最小距离 ≥ 0.3 m&lt;br /&gt;· 平均路径长度仅比最短路径长 ≤ 5%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;运动控制稳定性：MPC 路径跟踪控制&lt;/td&gt;
          &lt;td&gt;六足控制复杂，需让行走对路径偏差“有反馈、能预测”&lt;/td&gt;
          &lt;td&gt;· 使用模型预测控制（MPC）&lt;br /&gt;· 目标函数最小化未来时域的位姿偏差&lt;br /&gt;· 实时约束关节速度与姿态角&lt;br /&gt;· 借助力矩传感器反馈修正步态&lt;/td&gt;
          &lt;td&gt;· 路径跟踪误差 &amp;lt; 3 cm&lt;br /&gt;· 姿态偏角误差 &amp;lt; 2°&lt;br /&gt;· 延迟控制补偿 ≤ 100 ms&lt;/td&gt;
          &lt;td&gt;核心体现“动态预测 + 约束最优控制”，区别于传统 PID&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;环境与交互稳定性：避免危险与错误指令&lt;/td&gt;
          &lt;td&gt;盲人处于动态环境，需识别行人、车辆、信号灯并安全互动&lt;/td&gt;
          &lt;td&gt;· LiDAR + IMU + RGB 摄像头多传感融合&lt;br /&gt;· 基于 LiDAR 点云的区域划分与加权速度修正，实现动态避障&lt;br /&gt;· YOLOv5 交通灯识别结合模板匹配&lt;br /&gt;· 语音识别与反馈交互（“请跟我走”“前方有障碍”）&lt;/td&gt;
          &lt;td&gt;· 动态障碍避让成功率 95%&lt;br /&gt;· 信号灯识别准确率 97.8%&lt;br /&gt;· 平均避障响应时间 &amp;lt; 0.3 s&lt;br /&gt;· 机场/路口场景连续引导成功率 100%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所以这里的Navigation就比较难讲故事了。我们的亮点在于LLM对于自然语言指令能够分解成子任务，但足式机器人比较尴尬的一点是没有手，导致在医疗领域能实现的指令就局限了，比如说有一个RoboNurse-VLA enables the robot to recognize, grasp, and handover surgical instrument.是灵巧手的，但是足式机器人就只是狗了&lt;/p&gt;
&lt;p&gt;我也调研了其他的可能可以相关的领域，秉承**“助残/助盲/助老”**的理念：
第一个是家庭服务，这个还挺好说的，比如越疆 Rover X1/Unitree GO2可在光滑地板、草地、小坡坎等多场景行走，负载能力达日常物品级别，但最大的问题就是没有手，导致比如“帮老人取床头老花镜”“客厅物品递送”这种实现不了 —— 没有机械臂的足式机器人，到底“服务”什么？如果不能取物、开门，它的价值在哪里？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以胡诌做成可语音召唤的移动置物台 ，为上肢失能者提供 室内 5 m 范围内的即时物品可达性 ，用 3D 感知+分层导航解决 家用杂乱环境 下的 安全-连续 难题，从而 以移动代偿 而非 抓取代偿 的方式，提升上肢失能人群的 居家独立指数？
第二个是康复检测，问题是回答不了为什么需要一个狗跟着，而不是穿戴传感器设备/用固定的摄像头进行openpose骨骼分析，就算用了狗也不过是一个移动摄像头，那为什么不让残疾人动或者医生手动挪动摄像头？
第三个就是继续去纯助盲，我想通了，它不是Tracking，而是Object-based Tracking，只是默认命令是跟着person而已，没有说一定要跟在人后面，给他下一个其他目标的指令不就行了？但问题就出在了这里，传统SLAM的方法比结合AI的方法又快又好，你在AI基础上绑一个什么激光雷达/SLAM的话就有点尾大不掉很难绷。&lt;/li&gt;
&lt;li&gt;也有&lt;a href=&#34;https://github.com/Li-Ruiqi777/BlindGuideDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;导盲盲道的参考&lt;/a&gt;，不过是基于A1的本科毕设&lt;/li&gt;
&lt;li&gt;往&lt;a href=&#34;https://zhuanlan.zhihu.com/p/684356655&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RoboGuide这个方向&lt;/a&gt;去做的话也可以，只不过更多是放在VLM而非Tracking/Navigation本身了&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/BestAnHongjun/InternDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InternDog&lt;/a&gt;这篇西工大的工作不知道是怎么做的，看起来很牛，还上了&lt;a href=&#34;https://www.bilibili.com/video/BV1kK421a7sP/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;央视&lt;/a&gt;，据说是我国首个应用在导盲任务/场景下的四足机器人？&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bestxiangest/Intelligent-Guide-Cane&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intelligent-Guide-Cane&lt;/a&gt;或者回归ESP32的导盲？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感觉越调研越有信心了，那就继续往导盲这个领域讲故事应该没有问题！&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>顶会写作指南</title>
      <link>http://localhost:1313/blog/2025/2025-11-20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-20/</guid>
      <description>
        
        
        &lt;h1&gt;顶会写作指南&lt;/h1&gt;&lt;p&gt;今天来研读一下&lt;a href=&#34;https://github.com/hzwer/WritingAIPaper.git&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;如何撰写第一篇AI顶会论文写作指南&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个研究者可能有有趣的发现和实验结果，但不确定如何定义核心主题。大多数已发表论文的主要贡献恰好属于以下三类中的一种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;洞察：你对已经存在的某些事情有了解释。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性能：你可以做得更好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;能力：你可以做以前做不到的事情。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;发现新现象和分享新想法比绩效提升更重要。&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
