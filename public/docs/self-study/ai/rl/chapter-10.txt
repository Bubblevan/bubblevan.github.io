

<NoteBlock title="Algorithm 10.3: Off-policy actor-critic based on importance sampling">

**Initialization**: A given behavior policy $\beta(a|s)$. A target policy $\pi(a|s, \theta_0)$ where $\theta_0$ is the initial parameter. A value function $v(s, w_0)$ where $w_0$ is the initial parameter. $\alpha_w, \alpha_{\theta} > 0$.

**Goal**: Learn an optimal policy to maximize $J(\theta)$.

**At time step in each episode**, do:

1. Generate $a_t$ following $\beta(s_t)$ and then observe $r_{t+1}, s_{t+1}$.
2. **Advantage (TD error)**:
   $$
   \delta_t = r_{t+1} + \gamma v(s_{t+1}, w_t) - v(s_t, w_t)
   $$
3. **Actor (policy update)**: $\theta_{t+1} = \theta_t + \alpha_{\theta} \frac{\pi(a_t|s_t, \theta_t)}{\beta(a_t|s_t)} \delta_t \nabla_{\theta} \ln \pi(a_t|s_t, \theta_t)$
4. **Critic (value update)**: $w_{t+1} = w_t + \alpha_w \frac{\pi(a_t|s_t, \theta_t)}{\beta(a_t|s_t)} \delta_t \nabla_w v(s_t, w_t)$

</NoteBlock>


<NoteBlock title="Algorithm 10.4: Deterministic policy gradient or deterministic actor-critic">

**Initialization**: A given behavior policy $\beta(a|s)$. A deterministic target policy $\mu(s, \theta_0)$ where $\theta_0$ is the initial parameter. A value function $q(s, a, w_0)$ where $w_0$ is the initial parameter. $\alpha_w, \alpha_{\theta} > 0$.

**Goal**: Learn an optimal policy to maximize $J(\theta)$.

**At time step in each episode**, do:

1. Generate $a_t$ following $\beta$ and then observe $r_{t+1}, s_{t+1}$.
2. **TD error**:
   $$
   \delta_t = r_{t+1} + \gamma q(s_{t+1}, \mu(s_{t+1}, \theta_t), w_t) - q(s_t, a_t, w_t)
   $$
3. **Actor (policy update)**: $\theta_{t+1} = \theta_t + \alpha_{\theta}\nabla_{\theta}\mu(s_t, \theta_t)\left(\nabla_a q(s_t, a, w_t)\right)\Big|_{\text{where } a=\mu(s_t)}$
4. **Critic (value update)**:
   $$
   w_{t+1} = w_t + \alpha_w\delta_t\nabla_w q(s_t, a_t, w_t)
   $$

</NoteBlock>


