import CollapsibleBlock from '@site/src/components/CollapsibleBlock';
import { NoteBlock } from '@site/src/components/HighlightBlock';

来入门一下 **VLA** 这篇吹的神乎其神的[《Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware》](https://arxiv.org/pdf/2304.13705)，是在 **2023年** 提出的 **Action Chunking with Transformers**（基于 Transformer 的动作分块算法），所谓 **VLA 领域的鼻祖级动作生成算法**（但是他们自吹自擂为元宇宙）

该 **ACT 算法**是 VLA 领域首个以 **"动作分块（Action Chunking）"** 为核心的 **Transformer 模型**，专门解决机器人从 **"V-L指令"** 到 **"精准动作执行"** 的端到端映射问题 —— 这正是 **VLA 的核心目标**

找这玩意不太容易，Arxiv结果里面还有 **cogACT**、**MM-ACT** 这些，其中 **CogACT** 是将 OpenVLA 的离散动作预测替换为 **DiT**（让我想起cognav了）**MM-ACT** 是通过多模态协同优化 VLA 的感知 - 执行效率

而所谓的 **ALOHA** 就是 **A LOW-COST OPEN-SOURCE HARDWARE SYSTEM FOR BIMANUAL TELEOPERATION**，项目地址在 https://tonyzhaozh.github.io/aloha/

其核心贡献是一个**遥操作（teleoperation）系统**和一个**IL算法**，这样就能不用昂贵的硬件去做到下面的一些事情，比如开瓶盖啥的：

![](/img/vla/act-cando.png)

通过遥操作演示收集数据（**视觉+动作**）之后，就开始用收集的数据训练 **ACT 算法**，从而训练好的模型可以自主完成任务。然而现有**模仿学习算法**在需要**高频控制和闭环反馈**的细粒度任务中表现不佳

<CollapsibleBlock title="什么是PID控制器？">

**PID控制器**（Proportional-Integral-Derivative Controller）是一种经典的**闭环反馈控制算法**，用于让系统输出精确跟踪目标值。

### PID的三个组成部分

1. **P（比例项，Proportional）**
   - 根据当前误差（目标值 - 实际值）产生控制信号
   - 误差越大，控制力度越大
   - 问题：可能产生稳态误差（永远达不到目标）

2. **I（积分项，Integral）**
   - 累积历史误差，消除稳态误差
   - 如果长时间有误差，积分项会逐渐增大控制力度
   - 问题：可能导致系统响应过慢或振荡

3. **D（微分项，Derivative）**
   - 根据误差变化率预测未来趋势
   - 提前"刹车"，防止超调
   - 问题：对噪声敏感

**PID输出公式：**
```
u(t) = Kp × e(t) + Ki × ∫e(t)dt + Kd × de(t)/dt
```
其中：
- `e(t)` = 误差（目标值 - 实际值）
- `Kp, Ki, Kd` = 三个可调参数

### 在ALOHA/ACT中的作用

在 **ALOHA** 系统中，**PID控制器**有两个层面的应用：

#### 1. **底层关节控制（Dynamixel电机内置PID）**

**作用**：让机械臂关节精确到达目标位置

- **输入**：**ACT模型**预测的目标关节位置（如"关节1应该转到45度"）
- **过程**：**PID控制器**实时计算：
  - 读取当前关节实际位置（如"现在在40度"）
  - 计算误差 = 45° - 40° = 5°
  - 根据PID算法计算电机应该施加的力矩
  - 电机转动，关节向目标位置移动
- **输出**：关节实际位置逐渐接近目标位置

**为什么需要PID？**
- **ACT模型**只给出"目标位置"，但实际执行会有误差（摩擦力、负载变化等）
- **PID控制器**通过**高频反馈**（通常几百到几千Hz）实时调整，确保关节精确到达目标

#### 2. **主从遥操作中的力控制**

在数据收集阶段：
- **主臂**：人类操作员手动控制
- **从臂**：跟随主臂动作
- **PID作用**：通过主从臂关节位置的**差异**来间接控制施加的力
  - 如果从臂位置落后主臂，PID会增加力矩让从臂跟上
  - 这样就能"感受"到操作时的力反馈

</CollapsibleBlock>

## A. Action Chunking and Temporal Ensemble

通过 **ALOHA** 采集人类演示数据过程会导致**误差积累**：前序动作的误差会不断叠加，导致机器人状态偏离训练分布。

为在**像素到动作的策略框架**下解决**模仿学习的误差累积问题**，本文选择将一系列动作组合成一个单元进行存储和执行，以降低高频采集的长轨迹任务的**有效时间跨度**。

在实现中，文章将**分块大小固定为 k**：每 k 步，智能体接收一次观测，生成后续 k 步动作并依次执行如下：

![](/img/vla/act-action-chunk.png)

任务的**有效时间跨度被缩短为原来的 1/k**。具体来说，策略建模的是 **πₜ(at:t+k|st)**，而非传统的 **πₜ(at|st)**。**动作分块**还能处理人类演示中的**非马尔可夫行为**：单步策略难以应对时间相关的混杂因素（如演示过程中的停顿）因为此类行为不仅依赖当前状态，还与时间步相关；而当混杂因素处于一个分块内时，动作分块可在不引入历史条件策略因果混淆问题的前提下缓解该问题。

**动作分块的朴素实现存在局限性**：每 k 步才引入新的环境观测，会导致机器人运动卡顿。为提升运动平滑度、避免执行与观测之间的离散切换，我们在每个时间步都查询策略，使得不同动作块相互重叠，单个时间步会存在多个预测动作。所以上面引入了**Temporal Ensemble**，对这些预测进行融合：采用**指数加权方案** **wi = exp(−m×i)**（其中 w₀ 为最早动作的权重）对预测动作进行加权平均。参数 **m** 控制新观测的融合速度，m 越小，融合速度越快。与传统平滑方法（将当前动作与相邻时间步动作聚合，易引入偏差）不同，我们聚合的是针对同一时间步的预测动作，且无需额外训练成本，仅增加推理阶段的计算量。实践证明，**动作分块与时间集成是 ACT 实现精准平滑运动的关键**。

## B. Modeling Human Data

另一项挑战是从**含噪声的人类演示**中学习：面对相同观测，人类可能采用不同轨迹完成任务，且在精度要求较低的区域行为随机性更强。因此，策略需重点关注**高精度需求区域**。

我们通过将**动作分块策略训练为生成模型**来解决该问题，具体采用**条件变分自编码器（CVAE）**以当前观测为条件生成动作序列。**CVAE** 包含编码器和解码器两部分：

- **编码器**：仅用于训练解码器（即策略），测试时被舍弃。具体而言，编码器以当前观测和动作序列为输入，预测**风格变量 z**（服从对角高斯分布）的均值和方差；为加快训练速度，实际训练中仅使用**本体感受观测**和动作序列，不含图像观测。
- **解码器（策略）**：以 z 和当前完整观测（图像 + 关节位置）为输入预测动作序列；测试时，将 z 设为先验分布的均值（即 0），实现**确定性解码**。

模型训练目标是最大化演示动作块的对数似然（即 **minθ −∑st,at:t+k∈D log πₜ(at:t+k|st)**），采用标准 **VAE 损失**（含重建损失和编码器高斯先验正则化损失），并通过超参数 **β** 对正则化损失加权。直观来看，β 越大，z 传递的信息越少。

![](/img/vla/act-architecture.png)

### ACT 架构详解

**核心目的**：ACT 要做的是"看当前画面 + 机器人关节状态，预测未来 k 步动作"。上图展示了完整的"输入→处理→输出"流程。**CVAE** 的作用是处理人类演示的噪声（比如不同人做同一动作的差异），**Transformer** 的作用是处理"多摄像头图像 + 序列动作"的依赖关系（比如先看顶部摄像头再看腕部摄像头，动作要连贯）。

#### 左图：CVAE 编码器（训练时用，测试时丢弃）

**核心功能**：把"人类演示的动作序列 + 机器人关节状态"压缩成一个**风格变量 z**。z 就像一个"动作风格密码"，能捕捉人类操作的随机性（比如有人插电池快、有人慢，但都能成功）。

**具体流程**：

1. **输入**：
   - 人类演示的"动作块"（比如插电池的 k 步连续动作：at~at+k）
   - 机器人的关节观测（不含图像，因为图像信息多、训练慢，且关节状态已能反映核心姿态：ōt）
   - 为了适配 Transformer，会在最前面加一个可学习的 **"[CLS]"** 标记（和 BERT 一样，用来汇总全局信息）
   - 最终形成 **"[CLS] + 关节状态 + k 步动作"** 的序列（长度是 k+2）

2. **处理**：
   - 这个序列输入到"类 BERT 的 Transformer 编码器"后，会被逐层提取特征
   - 重点是把"动作块的规律 + 关节状态的关联"浓缩到 **"[CLS]"** 标记对应的特征向量里
   - 然后用这个特征向量预测一个"对角高斯分布"（就是统计里的"均值 + 方差"）
   - 从这个分布里采样，就得到了**风格变量 z**

3. **为什么需要编码器？**
   - 训练时用来"约束 z 的分布"（让 z 接近标准正态分布），这样 z 能高效编码人类动作的"风格差异"，又不会包含无关噪声
   - 测试时不用它，直接把 z 设为 0（标准正态分布的均值），避免额外计算

#### 右图：CVAE 解码器（核心！测试时实际控制机器人的"策略"）

**核心功能**：把"当前多摄像头图像 + 机器人关节状态 + 风格变量 z"，转换成"未来 k 步的机器人目标关节动作"——这就是机器人实际要执行的指令。

**具体流程**（分 3 步）：

1. **第一步：处理图像（CNN + 位置编码）**
   - 每路图像先过 **ResNet18**（一个图像特征提取网络），把 480×640 的图变成 15×20×512 的特征图（缩小尺寸、提取关键信息，比如夹爪位置、电池轮廓）
   - 然后把特征图展平成 300×512 的序列，再加上**2D 正弦位置编码**
   - 这一步是为了保留图像的空间信息（比如"电池在画面左上角"这个位置关系），不然 Transformer 不知道像素的位置意义
   - 4 路图像都这么处理后，拼接成 **1200×512** 的总图像特征序列

2. **第二步：融合所有信息（Transformer 编码器）**
   - 把"总图像特征序列"和"经线性层投影到 512 维的关节位置 + z"拼接起来，形成 **1202×512** 的"混合特征序列"（1200 来自 4 路图像，2 来自关节 + z）
   - 这个混合序列输入到 Transformer 编码器后，会被充分融合
   - 比如"腕部摄像头看到夹爪快碰到电池（图像特征）+ 关节状态显示夹爪张开（关节特征）+ z 是'轻柔风格'（z 特征）"，这些信息会被整合在一起，形成"当前状态的全局特征"

3. **第三步：生成动作序列（Transformer 解码器）**
   - 解码器的输入是"固定的位置编码"（长度 k，512 维）——相当于告诉模型"要生成 k 步动作，每一步的位置是固定的"
   - 解码器通过**交叉注意力**（把编码器的输出当"键和值"，自己的位置编码当"查询"），一步步生成连贯的动作序列
   - 比如第一步"夹爪对准电池"，第二步"轻微闭合"，…，第 k 步"插入电池"
   - 最后用一个 **MLP**（简单的全连接网络）把生成的 512 维特征，映射成 14 维的关节位置（双臂各 7 个自由度）
   - 最终输出 **k×14** 的张量（未来 k 步的目标关节动作）

**输出**：未来 k 步的机器人目标关节位置——比如"左臂关节 1 角度 30°、关节 2 角度 15°…，右臂关节 1 角度 20°…"，底层 **PID 控制器**会跟踪这些目标位置，让机器人精准执行。

#### 左图和右图的关联：训练时的"闭环"

训练时，左图的编码器和右图的解码器是一起优化的：

1. 编码器生成 z，解码器用 z + 实时观测生成"预测动作块"（ât~at+k）
2. 用"预测动作块"和"人类演示的真实动作块"算**重建损失**（确保预测动作像人类）
3. 同时用编码器预测的"高斯分布"和"标准正态分布"算 **KL 散度**（正则化损失，确保 z 的分布合理）
4. 两个损失加起来（**L = Lreconst + βLreg**），用 **Adam 优化器**更新所有参数
5. 这样解码器（策略）就能学会"看观测、按风格，生成精准动作块"

### 技术细节补充

**观测数据**包含：
- **4 路 480×640 分辨率的 RGB 图像**
- **双机械臂关节位置**（共 7+7=14 个自由度）

**动作空间**为双机械臂的绝对关节位置（14 维向量），因此动作分块策略的输出为 **k×14 维张量**。

**损失函数**：我们采用 **L1 损失**而非常用的 **L2 损失**（MSE），因其能更精准地建模动作序列。实验发现，使用关节位置差值作为动作标签会导致性能下降。训练时使用标准 **VAE 损失**（重建损失 + KL 散度正则化损失）。

### 训练算法

<NoteBlock title="Algorithm 1: ACT Training">

```python
Input: Demo dataset D, chunk size k, weight β
1: Let at, ot represent action and observation at timestep t, 
   ¯ot represent ot without image observations
2: Initialize encoder qφ(z|at:t+k, ¯ot)
3: Initialize decoder πθ(ˆat:t+k|ot, z)
4: for iteration n = 1, 2, ... do
5:   Sample ot, at:t+k from D
6:   Sample z from qφ(z|at:t+k, ¯ot)
7:   Predict ˆat:t+k from πθ(ˆat:t+k|ot, z)
8:   Lreconst = MSE(ˆat:t+k, at:t+k)
9:   Lreg = DKL(qφ(z|at:t+k, ¯ot) ∥ N(0, I))
10:  Update θ, φ with ADAM and L = Lreconst + βLreg
11: end for
```

</NoteBlock>

### 推理算法

<NoteBlock title="Algorithm 2: ACT Inference">

```python
Input: trained πθ, episode length T, weight m
1: Initialize FIFO buffers B[0 : T], where B[t] stores 
   actions predicted for timestep t
2: for timestep t = 1, 2, ... T do
3:   Predict ˆat:t+k with πθ(ˆat:t+k|ot, z) where z = 0
4:   Add ˆat:t+k to buffers B[t : t + k] respectively
5:   Obtain current step actions At = B[t]
6:   Apply at = Σi wiAt[i] / Σi wi, with wi = exp(−m × i)
7: end for
```

</NoteBlock>
