{"/blog/":{"data":{"":"RSS Feed"},"title":"博客"},"/blog/2025/3d-understand/":{"data":{"1实验基础构建-visual-geometry-benchmark#（1）实验基础：构建 Visual Geometry Benchmark":"为避免不同任务评估标准不统一的问题，DA3 首先建立了标准化基准，明确实验的 “输入 - 输出 - 评估” 链路：\n基准覆盖范围： 包含 5 个场景多样化的数据集（共 89+ 场景），涵盖合成数据、真实 LiDAR 数据、低清噪声数据，确保实验泛化性：\n合成数据： HiRoom（29 个室内场景，Blender 渲染，用于验证几何细节） 真实 LiDAR 数据： ETH3D（11 个室内外场景，激光雷达采集，高分辨率）、DTU（22 个物体级场景，控制条件下的真实点云）、ScanNet++（20 个室内场景，iPhone LiDAR + 激光重建深度） 低清噪声数据： 7Scenes（7 个室内场景，低分辨率 + 运动模糊，模拟真实复杂场景） 基准流程： 所有任务遵循 “输入→模型预测→结果对齐→指标计算” 的统一逻辑：\n输入： 随机采样图像（若图像数超 100 张，固定随机种子采样 100 张，保证实验可复现） 预测： DA3 输出深度图、射线图、相机姿态（可选） 对齐： 几何重建时用 RANSAC+evo 工具将预测姿态与真实姿态对齐，确保点云在同一坐标系 评估： 按任务计算对应指标（姿态用 AUC3/AUC30，几何用 F1/Chamfer 距离，渲染用 PSNR/SSIM）","1评估阶段用-真实场景的标注数据-作为-ground-truth#（1）评估阶段：用 \u0026ldquo;真实场景的标注数据\u0026rdquo; 作为 Ground Truth":"实验中模型性能对比的是真实世界的物理测量结果，而非教师模型输出，具体来源：\n姿态真值： ETH3D、DTU 等数据集用 COLMAP（基于特征匹配的 SfM 方法）** 或激光跟踪仪获取相机姿态 几何真值： ETH3D/ScanNet++：用 TSDF 融合 LiDAR 采集的稠密深度图，生成真实场景点云 DTU：在实验室控制条件下，用高精度 3D 扫描仪获取物体的真实点云 渲染真值： NVS 任务（视觉渲染）用 “真实场景的多视图图像” 作为渲染目标真值（如 DL3DV 的测试帧）","2核心任务实验流程以-几何重建-为例#（2）核心任务实验流程（以 \u0026ldquo;几何重建\u0026rdquo; 为例）":"用户关心的 “图片流生成连续点云建模”，正是 DA3 “多视图 / 视频几何重建” 任务的核心，实验流程在论文 7.1 节和 6.1 节明确：\n输入： 支持 “图片流（视频帧）” 或 “多视图图像集”（无数量限制，单帧即单目，多帧即多视图）\n点云生成逻辑：\n模型先为每张图像预测 “深度图 + 射线图”（通过双 DPT 头联合输出，像素级对齐） 用公式 $P = t + D(u,v) \\cdot d$（$t$ 为射线原点，$d$ 为射线方向，$D$ 为深度）计算每个像素的 3D 坐标，生成单帧稀疏点云 对图片流的连续帧，用 TSDF 融合（Truncated Signed Distance Function）将多帧稀疏点云合并为稠密、连续的场景点云（不同数据集的 TSDF 参数不同，如 HiRoom 体素大小 0.007m，ETH3D 为 0.039m） 实验验证： 论文 7.1 节用 ScanNet++、ETH3D 的图片流测试，DA3-Giant 生成的连续点云在 F1 分数上超 VGGT 23.6%，且能保留桌角、墙面边缘等细粒度细节。","2训练阶段用-教师模型伪标签-作为监督非-ground-truth#（2）训练阶段：用 \u0026ldquo;教师模型伪标签\u0026rdquo; 作为监督（非 Ground Truth）":"教师模型的作用是 “为真实噪声数据生成高质量监督信号”，而非作为评估的真值：\n真实数据（如 ARKitScenes、WildRGBD）的深度标注常稀疏 / 噪声大，无法直接用 教师模型（仅用合成数据训练）为这些真实图像生成 “伪深度图”，再通过 RANSAC 对齐真实稀疏深度，得到 “干净的监督信号” 论文 7.2.3 节验证：用教师伪标签训练的 DA3，深度图细节比无教师监督的版本丰富 30%","3d-场景理解工作#3D 场景理解工作":"3D 场景理解工作","depth-anything-3#Depth Anything 3":"Depth Anything 3 (DA3) 是字节跳动提出的 3D 视觉模型，核心目标是从任意数量（单张 / 多张 / 视频流）、已知或未知相机姿态的视觉输入中恢复空间一致的几何结构；其采用单 Transformer 骨干网络（如 vanilla DINOv2 编码器）和深度-射线（depth-ray）表示作为最小预测目标，避免复杂多任务学习，通过师生训练范式（教师模型用合成数据生成高质量伪标签指导学生模型）实现与 DA2 相当的细节度和泛化性；同时构建了涵盖相机姿态估计、任意视图几何重建、视觉渲染的 Visual Geometry Benchmark，在该基准上 DA3 刷新所有任务 SOTA，平均超越此前 SOTA 模型 VGGT 35.7% 的相机姿态精度和 23.6% 的几何精度，且在单目深度估计任务上优于 DA2；此外，DA3 可通过微调扩展至前馈 3D 高斯 splatting（3DGS） 等下游任务，为通用 3D 感知提供基础模型支持。","gpt4scene#GPT4Scene":"3D 视觉-语言任务旨在将 3D 场景理解与自然语言处理相融合。然而，我们希望更进一步：将 3D 内容融入大型语言模型（LLMs），以实现更自然的人机交互。这一方向的研究最初始于 3D 点云大型语言模型（3D Point Cloud LLMs）。此类模型以点云为输入，能够在 3D 场景中实现自然语言生成与交互。\n早期的 3D 大型语言模型主要关注物体级别的几何结构与外观；后续研究扩展至室内场景，开始侧重物体间的空间关系及场景整体特征，通常会利用场景点云，并结合辅助性 2D 多视图图像。为了更精准地捕捉物体间关系，近期的 3D 大型语言模型会先对场景中的物体进行解耦，再将其输入至 LLM。此外，部分方法会更依赖视觉输入来判断场景上下文。\nGPT4Scene 是旨在探索纯视觉输入的 VLM 是否能更有效地处理室内 3D 场景理解任务的框架，仅依赖视频输入（无需 3D 点云），核心通过 3D 重建生成鸟瞰图（BEV） 提供全局场景布局，以及 时空对象标记（STO-markers） 建立 BEV 与视频帧的全局-局部关联，解决 VLMs 缺全局表示、帧与时空上下文错位的问题；同时构建含 165K 文本标注的 ScanAlign 数据集，用于微调开源 VLMs。在零样本设置下，该框架显著提升 GPT-4o、Gemini-1.5-Pro 等大模型性能（如 GPT-4o 在 ScanQA 的 ROUGE 指标从 32.6 提升至 37.7）；微调后 Qwen2-VL-7B 在多任务达 SOTA，如 3D 问答（SQA3D）EM1 指标从 40.7 提升至 60.7（相对 +48%），超过此前 SOTA 模型 Chat-Scene 11.0%，且能让 VLMs 形成内在 3D 理解能力，为 VLMs 扩展 3D 场景理解提供无缝方案。","navigationtracking-适配#Navigation/Tracking 适配":"DA3 适配机器人 SLAM 避障需解决 动态物体过滤、回环检测、语义融合 三大核心问题。这些问题的本质是 “DA3 作为’几何重建模型’的功能边界，与机器人’自主环境交互’的实际需求之间的差距”——DA3 仅负责输出高精度几何信息（深度、点云），但机器人避障需要 “干净的静态环境地图”、“长期建图的一致性”、“可解释的障碍决策依据”，这三个适配问题正是填补这一差距的关键。以下结合你提供的新搜索资源（尤其是动态物体滤除、SLAM 实践相关内容），从 “问题本质、DA3 局限、解决方案、落地细节” 四个维度详细拆解：","partition视频帧采样-生成-带-sto-markers-的采样帧-mathcalv-prime#Partition（视频帧采样）→ 生成 \u0026ldquo;带 STO-markers 的采样帧 $\\mathcal{V}^{* \\prime}$\u0026rdquo;":"核心作用： 给 VLM 提供 “局部物体细节”（如椅子的颜色、杯子的形状），并通过 STO-markers 标记物体 ID，解决 “帧间物体错位” 问题。\n步骤 1：帧采样\n按公式 $s_i = \\lfloor (i-1)\\frac{N}{n} \\rfloor + 1$（$n$ 默认 8 帧）从原始视频中 “均匀采样” $n$ 帧，得到采样帧集合 $V^* = {I_{s_1}, \\ldots, I_{s_n}}$。\n步骤 2：叠加 STO-markers\n这一步需要 “Reconstruct 路径” 的输出（3D 实例掩码）作为前提，并非独立完成：\n从 “Reconstruct 路径” 获取 3D 实例掩码 $M = {M_1, …, M_K}$（每个 $M_k$ 对应 1 个物体的 3D 轮廓） 根据每帧采样帧的 “相机外参”，将 3D 掩码 $M$ 投影到该 2D 帧上，提取每个物体的 “2D 质心坐标” $C_i^{uv}$（第 $i$ 帧中第 $k$ 个物体的标记是 $C_{i,k}^{uv}$） 用函数 $\\mathcal{F}(\\cdot)$ 将这些坐标作为 “标记”（如数字 ID “物体 1\"“物体 2”）叠加到采样帧上，生成带 STO-markers 的采样帧 $\\mathcal{V}^{* \\prime} = {\\mathcal{F}(I_i, C_i^{uv})}$ 关键： 不同采样帧中，同一物体的标记 ID（如 “物体 3”）保持一致，解决 “帧间物体错位”（比如第 1 帧的 “物体 3” 和第 5 帧的 “物体 3” 都是同一个桌子）。","reconstruct--生成-带-sto-markers-的-bev-图像-mathcali_b#Reconstruct → 生成 \u0026ldquo;带 STO-markers 的 BEV 图像 $\\mathcal{I}_b\u0026rsquo;$\u0026rdquo;":"核心作用： 给 VLM 提供 “全局场景布局”（如房间里桌子、椅子的相对位置，墙壁的形状），解决 “缺全局表示” 问题，同时通过 STO-markers 与采样帧关联。\n步骤 1：3D 重建生成点云\n用 “完整原始视频”（而非采样帧）+ 每帧的 “相机外参 $E = {E_1, \\ldots, E_N}$”（描述相机在现实中的位置和姿态），通过 3D 重建技术（如 BundleFusion）生成 3D 点云 $\\mathcal{P}$：\nP=R({(It,Et)})\\mathcal{P} = \\mathcal{R}\\left(\\{(I_t, E_t)\\}\\right)P=R({(It​,Et​)})步骤 2：点云生成 BEV 图像\n设定 “俯视图相机外参 $E_{top}$”（模拟从房间正上方往下拍的相机姿态，$E_{top} \\in SE(3)$），通过渲染函数 $\\mathcal{T}(\\cdot)$ 将 3D 点云投影为 2D 的 BEV 图像 $\\mathcal{I}_b$：\nIb=T(P,Etop)\\mathcal{I}_b = \\mathcal{T}(\\mathcal{P}, E_{top})Ib​=T(P,Etop​)步骤 3：叠加 STO-markers\n与 “路径 1” 共享 3D 实例掩码 $M$，保证标记一致性：\n将 3D 掩码 $M$ 投影到 BEV 的 xy 平面（忽略 z 轴高度，只保留平面位置），提取每个物体投影后的 “边界框中心坐标” $C^{xy} = {C_1^{xy}, \\ldots, C_K^{xy}}$ 用函数 $\\mathcal{F}(\\cdot)$ 将这些坐标作为标记叠加到 BEV 图像上，生成带 STO-markers 的 BEV $\\mathcal{I}_b’ = \\mathcal{F}(\\mathcal{I}_b, C^{xy})$ 关键： BEV 上的标记 ID 与 “路径 1” 采样帧的标记 ID 完全一致（比如 BEV 上的 “物体 3” 和采样帧上的 “物体 3” 是同一个桌子），建立 “全局布局” 与 “局部细节” 的关联。","vlm-输入#VLM 输入":"“The resulting frames and BEV image, enhanced with STO-markers, are inputs for VLM training and inference”—— 即最终输入 VLM 的是两个核心组件：\n$\\mathcal{V}^{ \\prime}$：* 带 STO-markers 的采样帧（提供 “局部物体外观 + 帧间物体一致性”） $\\mathcal{I}_b’$： 带 STO-markers 的 BEV 图像（提供 “全局场景布局 + 物体空间位置”） 这两个组件结合，恰好解决了 VLMs 的 3D 理解缺陷：\n缺全局？ BEV 提供房间整体布局 帧错位？ STO-markers 保证同一物体在 BEV 和所有采样帧中 ID 一致，让 VLM 能关联 “局部看到的物体” 和 “全局中的位置”","一动态物体过滤从-无差别几何重建-到-精准静态环境提取#一、动态物体过滤：从 \u0026ldquo;无差别几何重建\u0026rdquo; 到 \u0026ldquo;精准静态环境提取\u0026rdquo;":"1. 问题本质与影响\n机器人在真实场景（如商场、街道）中，环境充满动态物体（行人、移动车辆、开合的门）。DA3 会将这些动态物体与静态场景（地面、墙壁）一同重建为点云，导致两个严重问题：\n建图污染： 动态物体的点云会被误判为静态障碍（如行人走过的区域，地图会残留 “人形障碍”），后续机器人再次经过时会误触发避障，无法通行 避障误判： 若动态物体突然出现在路径上（如行人横穿），DA3 生成的点云包含该物体，但机器人无法区分 “静态障碍” 和 “动态障碍”，可能出现 “过度避障”（避开缓慢移动的行人导致绕远）或 “避障不及时”（未识别快速移动物体） 2. DA3 当前的核心局限\nDA3 的设计目标是 “恢复空间一致的几何结构”，完全不区分 “静态” 与 “动态”：\n输入图像中的动态物体，会被 DA3 当作 “场景几何的一部分”，输出对应的深度图和射线图，进而生成动态物体的点云 无任何时序动态检测模块（如光流跟踪、多帧运动一致性判断），无法从连续帧中识别 “运动的物体” 3. 解决方案：结合 “几何 + 语义 + 时序” 多维度滤除，适配 DA3 输出\n根据搜索资源（摘要 3：动态物体滤除算法、摘要 6：Dynablox 几何方法），可将 DA3 的输出与以下技术结合，实现动态物体精准滤除：\n技术路径 核心原理（结合 DA3） 优势与优化方向（来自搜索资源） 2D 语义分割反投影 1. DA3 输出 “RGB 图像 + 深度图”；\n2. 用语义分割模型（如 Mask R-CNN、DeepLab，摘要 3）分割 RGB 图像中的动态物体（如 “行人\"“车辆”），得到动态区域掩码；\n3. 将掩码反投影到 DA3 生成的 3D 点云中，滤除掩码对应的点（即动态物体点）。 - 优势： 直接利用 DA3 的 RGB + 深度输出，无需额外传感器；\n- 优化： 结合时序一致性融合（摘要 3），对连续 3-5 帧的分割结果取交集，减少单帧误分割（如将静态停靠的车辆误判为动态）。 光流跟踪 + 运动一致性 1. 对 DA3 处理的连续帧 RGB 图像，用光流模型（如 RAFT、FlowNet，摘要 3）计算像素运动向量；\n2. 筛选运动向量大于阈值的像素（动态物体区域）；\n3. 结合 DA3 的深度图，将动态像素反投影到点云滤除。 - 优势： 无需训练语义模型，适用于 “未知动态物体”（如未见过的包裹）；\n- 优化： 补偿相机运动（Ego-motion）（摘要 3），用 DA3 估计的相机姿态（extrinsics）修正光流，避免将 “相机移动导致的静态物体运动” 误判为动态。 纯几何时序检测（Dynablox） 1. 用 DA3 生成的连续帧点云构建 TSDF 体素地图（摘要 6）；\n2. 跟踪每个体素的 “占用状态时序”（如 “最近 10 帧是否持续被占用”）；\n3. 若体素从 “空闲” 突然变为 “占用”（如行人闯入），标记为动态体素，滤除对应点云。 - 优势： 无依赖外观，适用于黑暗、低纹理场景（DA3 深度图仍有效）；\n- 优化： 引入高置信度自由空间（摘要 6），通过激光射线穿透检测，排除 “静态物体遮挡导致的误判”（如行人挡住墙壁，墙壁仍为静态）。 4. 落地示例\n机器人在商场走廊移动时：\nDA3 实时输出 640×480 分辨率的深度图 + RGB 图像（摘要 1、4 中 Jetson 平台配置） 同步运行 Mask R-CNN 分割 RGB 图像，得到 “行人” 掩码（动态区域） 将掩码与 DA3 的深度图结合，计算动态区域的 3D 坐标范围，从点云中删除这些坐标的点 同时用连续 5 帧的分割结果取交集，避免将 “短暂停留的行人” 误判为静态障碍，最终得到 “仅含墙壁、地面的静态点云”，供避障决策使用","三语义融合从-纯几何点云-到-可解释的避障决策#三、语义融合：从 \u0026ldquo;纯几何点云\u0026rdquo; 到 \u0026ldquo;可解释的避障决策\u0026rdquo;":"1. 问题本质与影响\nDA3 生成的点云仅包含 “3D 坐标 + 深度” 等几何信息，但机器人避障需要语义理解—— 即 “知道障碍物是什么，以及是否需要避开”：\n例如点云中的 “地面点”（可通行）和 “桌子腿点”（需避开），几何上都是 “空间中的点”，但对机器人的意义完全不同 若仅依赖几何信息，机器人可能出现 “荒谬避障”（如避开地面上的阴影点云）或 “漏避障”（如未识别细长的栏杆，几何上点云稀疏易被忽略） 2. DA3 当前的核心局限\nDA3 是 “纯几何模型”，无任何语义输出能力：\n无法给点云添加 “类别标签”（如 “地面\"“行人\"“栏杆”） 无法区分 “可通行区域”（如平坦地面）和 “不可通行障碍”（如台阶、桌椅），仅能通过 “点云是否在路径上” 判断是否避障，精度极低 3. 解决方案：DA3 深度图 + 多模态语义分割，给点云 “贴标签”\n根据搜索资源（摘要 3：RGB-D 语义分割、多模态融合），可利用 DA3 的深度图生成 “RGB-D 数据”，输入语义分割模型，实现 “几何 + 语义” 融合：\n技术路径 核心原理（结合 DA3） 优势与优化方向（来自搜索资源） RGB-D 语义分割 1. DA3 输出 “RGB 图像 + 深度图”，组合为 RGB-D 数据（每个像素含 RGB 颜色 + 深度值）；\n2. 将 RGB-D 数据输入专门的语义分割网络（如 Cylinder3D、RangeNet++，摘要 3）；\n3. 分割网络输出 “语义标签图”（每个像素对应类别，如 “0 = 地面，1 = 行人，2 = 栏杆”）；\n4. 将语义标签反投影到 DA3 生成的 3D 点云中，每个点云添加 “语义属性”。 - 优势： DA3 的深度图能弥补 RGB 图像的遮挡 / 光照问题（如阴影区域，深度图仍能区分地面与障碍）；\n- 优化： 多模态特征融合（摘要 3），将 DA3 的深度特征与 RGB 特征拼接输入分割网络，提升 “细长物体（栏杆）““低纹理物体（白色墙壁）” 的分割精度。 几何辅助语义修正 1. 用 DA3 的深度图计算 “点云曲率”（高曲率区域多为物体边缘，如桌子角）；\n2. 结合语义分割结果，若 “语义标签为地面，但曲率过高”（如地面上的石头），修正标签为 “障碍”；\n3. 若 “语义标签为障碍，但深度值过大”（如远处的树木，超出避障范围），标记为 “无需避障”。 - 优势： 利用 DA3 的几何精度修正语义分割的误判；\n- 优化： 高度阈值约束（摘要 3 提到的 ERASORS 算法），根据机器人高度（如 0.1-1.5 米为障碍范围），过滤掉 “过高（天花板）” 或 “过低（小石子）” 的点，减少无效避障。 4. 落地示例\n服务机器人在办公室避障时：\nDA3 生成 RGB-D 数据（RGB 图像 + 深度图），输入 Cylinder3D 语义分割网络 分割网络输出语义标签图：“地面（绿色）““办公桌（红色）““行人（蓝色）” 点云被标记为三类：绿色点（可通行）、红色点（静态障碍，需绕开）、蓝色点（动态障碍，需实时跟踪） 机器人的避障算法优先避开红色 / 蓝色点，且仅在 “障碍点位于机器人运动范围内（0.1-1.5 米高）” 时触发避障，忽略地面小石子（过低）和天花板管道（过高），避免无效决策","二回环检测从-短期几何一致-到-长期地图全局一致#二、回环检测：从 \u0026ldquo;短期几何一致\u0026rdquo; 到 \u0026ldquo;长期地图全局一致\u0026rdquo;":"1. 问题本质与影响\n机器人在 “长期建图”（如探索整个办公楼）时，会因位姿漂移（相机姿态估计误差累积）导致地图扭曲：\n例如机器人从 “1 楼走廊” 出发，绕一圈回到起点，若没有回环检测，DA3 生成的地图会出现 “起点与终点不重合”（如走廊两端错开 1 米），严重时会导致机器人 “迷路”（不知道自己的真实位置） DA3 仅能保证 “单帧 / 短序列帧” 的几何一致性（如连续 10 帧的点云对齐），但无法判断 “当前场景是否与 10 分钟前访问过的场景相同”—— 这一判断能力正是回环检测的核心 2. DA3 当前的核心局限\nDA3 的定位是 “几何重建模型”，无任何回环检测与位姿修正模块：\n仅能根据输入图像估计 “相对姿态”（相邻帧之间的姿态变化），无法估计 “绝对姿态”（相对于全局地图的位置） 长期运行后，相对姿态误差会累积，导致点云地图出现 “漂移”（如直线走廊建图后变成曲线），进而影响避障精度（如机器人认为前方有障碍，实际是地图漂移导致的虚警） 3. 解决方案：DA3 作为 “几何 backbone”，集成到成熟 SLAM 框架\n根据搜索资源（摘要 5：DA3-Long 提升 SLAM 性能、摘要 3：SLAM 位姿优化），需将 DA3 的输出接入具备回环检测的 SLAM 框架，利用 SLAM 的回环模块修正漂移，具体流程如下：\n选择 SLAM 框架： 优先选择支持 “深度图输入” 的开源框架，如 ORB-SLAM3（适用于单目 / RGB-D）、LDSO（直接法 SLAM，高精度）、VINS-Mono（视觉惯性融合，抗抖动）\nDA3 与 SLAM 的数据交互：\nDA3 将 “深度图（prediction.depth）+ 相机内参（prediction.intrinsics）+ 相对姿态（prediction.extrinsics）” 输出给 SLAM 框架 SLAM 框架将 DA3 的深度图作为 “视觉观测数据”，替代传统的 “单目相机的三角化深度” 或 “LiDAR 点云”，提升姿态估计精度（摘要 5 提到 DA3-Long 替换 VGGT 后，SLAM 漂移显著降低） SLAM 回环检测模块修正漂移：\n当 SLAM 框架检测到 “当前场景与历史场景相似”（如 ORB-SLAM3 通过 ORB 特征匹配判断回环），会触发 “回环优化” 优化过程中，SLAM 会根据 DA3 生成的深度图重新计算 “回环帧” 的姿态，修正之前累积的漂移，并更新全局地图，确保长期建图的一致性 4. 落地示例\n机器人探索办公楼时：\nDA3 在 Jetson AGX Orin 上实时输出深度图（6.35 FPS，摘要 1、4），并将数据传入 ORB-SLAM3 ORB-SLAM3 用 DA3 的深度图优化相邻帧的姿态（减少初始漂移），同时提取图像的 ORB 特征，与历史帧特征库比对 当机器人回到 “1 楼大厅”（之前访问过的场景），ORB-SLAM3 检测到 ORB 特征匹配度超过阈值（回环触发），用 DA3 的深度图验证 “当前深度与历史深度是否一致”，若一致则优化全局位姿，修正之前的漂移 最终生成的地图无明显扭曲，机器人能准确判断自己的位置，避障决策更可靠","实验#实验":"DA3 的实验核心围绕 “统一视觉几何基准” 展开，覆盖 “相机姿态估计、几何重建、视觉渲染” 三大任务，实验流程严格区分 “数据准备、任务执行、指标评估” 三阶段。","射线图ray-map#射线图（Ray Map）":"射线图是 DA3 提出的创新表示方式，用于隐式编码 “相机观测方向与位置”，避免直接预测相机姿态（如旋转矩阵、平移向量）的复杂约束，是实现 “最小建模” 的关键设计。\n定义与数学形式： 射线图用符号 $M \\in \\mathbb{R}^{H \\times W \\times 6}$ 表示，同样与输入图像像素对齐，6 个通道分为两组：\n前 3 通道（$M(:,:,:3)$）：射线原点 $t \\in \\mathbb{R}^3$，代表该像素对应的相机光心在世界坐标系中的位置 后 3 通道（$M(:,:,3:)$）：射线方向 $d \\in \\mathbb{R}^3$，代表该像素从相机光心出发，指向世界空间目标点的单位向量（未归一化，保留投影尺度） 核心作用：\n规避姿态预测约束： 传统方法需直接预测相机外参（旋转矩阵 $R$ 需满足正交性），而射线图通过 “像素级射线” 隐式包含姿态信息，简化建模 反解相机参数： 若需显式获取相机姿态，可从射线图推导 —— 相机中心 $t_c$ 为射线原点的平均值（$t_c = \\frac{1}{H \\times W} \\sum M(h,w,:3)$），通过单应性矩阵 $H=KR$（DLT 算法求解）和 RQ 分解，进一步得到内参 $K$（上三角矩阵）和外参 $R$（正交矩阵） 生成 3D 点云： 与深度图协同，通过 $P = t + D(u, v) \\cdot d$ 直接计算 3D 点，无需额外任务目标（如点云、相机姿态），实现 “最小预测目标” DA3 中的特性： 射线图与深度图通过 “双 DPT 头” 联合预测 —— 共享特征重组模块，独立融合层分别输出两者，确保射线方向与深度的空间一致性，避免多任务目标的纠缠（实验验证 “深度 + 射线” 组合的性能优于 “深度 + 点云 + 相机姿态” 等冗余方案）。\n详细解释：\n第一部分：射线原点（前 3 个通道） 记录 “这条光线的起点”—— 也就是相机光心在 3D 空间中的位置（比如拍桌子时，手机所在的坐标：x=2 米、y=1 米、z=0.5 米）。所有像素的射线原点通常很接近（因为都来自同一台相机），所以射线图前 3 通道的数值差异很小，本质是 “相机位置的像素级体现”。\n第二部分：射线方向（后 3 个通道） 记录 “这条光线的指向”—— 比如 “桌子角像素” 的光线指向是 “向右下方 15°\"，“桌子边缘像素” 的光线指向是 “向左下方 10°\"。用 3 个数值（x/y/z 方向分量）描述这个指向，比如（0.2, -0.3, 0.1）就代表 “在 x 轴正方向、y 轴负方向、z 轴正方向有一定延伸”。\n为什么使用射线图？\n传统 3D 模型需要直接预测 “相机姿态”（比如相机朝哪个方向转、在哪个位置），但姿态计算有严格约束（比如旋转矩阵必须满足 “正交性”，算错一点就会严重偏差）。而射线图绕开了这个复杂问题：\n它不直接算 “相机整体姿态”，而是给每个像素算 “专属光线”—— 这些光线天然包含了姿态信息（比如所有光线的原点平均下来就是相机中心，光线方向的规律就是相机朝向） 后续要显式获取相机姿态时，只需从射线图反推（比如用文档 1-44-46 节的方法：原点求平均得相机中心，方向算单应性矩阵得内参 / 外参），无需在训练时额外优化姿态目标","整体工作流程#整体工作流程":"VLMs 直接处理视频时存在两个问题：\n缺全局场景表示（第一视角视频看不到房间整体布局） 帧与时空上下文错位（不知道不同帧的同一物体是同一个） 因此，GPT4Scene 的框架设计围绕 “补全局”、“建关联” 展开：\n“Reconstruct 路径” 负责 “补全局”（生成 BEV） “Partition 路径” 负责 “提细节”（采样帧） “STO-markers” 负责 “建关联”（让 BEV 和帧的物体对应） 起点： 一段围绕室内场景拍摄的第一视角视频 $V = {I_1, I_2, \\ldots, I_N}$（比如 1000 帧的房间漫游视频）。这段视频会被 “复用” 到两个预处理环节，而非拆分：\n给 “Partition（采样）路径” 用： 取 “部分帧”（采样），目的是减少 VLM 的输入 token 数量（避免 1000 帧计算量过大），同时保留场景局部细节（如物体外观、局部视角） 给 “Reconstruct（重建）路径” 用： 用 “完整帧”，目的是通过连续时序的图像 + 相机外参，还原场景的 3D 结构（点云），进而生成全局布局（BEV） 如上图，给定一段围绕室内场景移动拍摄的视频 $V = {I_1, \\dots, I_N}$，我们首先通过索引近似均匀地采样 $n$ 帧，采样公式为：\nsi=⌊(i−1)Nn⌋+1s_i = \\lfloor (i-1) \\frac{N}{n} \\rfloor + 1si​=⌊(i−1)nN​⌋+1其中 $\\forall i \\in {1, \\dots, n}$，由此形成采样后的视频帧集合 $V^* = {I_{s_1}, \\dots, I_{s_n}}$。该策略在保留场景上下文且无显著信息损失的前提下，减少了视觉语言模型（VLMs）的令牌（token）数量和计算开销。随后，我们利用完整的时间序列进行 3D 场景重建，生成全局鸟瞰图（BEV）。通过后续的 3D 实例分割，可实现物体的精准定位；将该定位结果投影到 BEV 地图和 2D 视频帧上，即可建立时空对象标记（STO-markers）。","架构#架构":"模块 核心设计 作用 单 Transformer 骨干 基于预训练 ViT（如 DINOv2），分 L_s（视图内注意力）和 L_g（交替跨视图 / 视图内）层，L_s:L_g=2:1 继承预训练特征，支持任意视图数（单图自动降级为单目） 相机条件注入 每个视图前缀相机 token：已知姿态→MLP 编码（E_c (f,q,t)）；未知→共享可学习 token 无缝处理有 / 无姿态输入，提供几何上下文 双 DPT 头（Dual-DPT） 共享重组模块（Reassemble），独立融合层 + 输出层，分别预测深度和射线 保证两任务特征交互，避免中间表示冗余，提升预测一致性","深度图depth-map#深度图（Depth Map）":"在 Depth Anything 3（DA3） 的 3D 几何重建框架中，深度图是用于描述 “图像像素与相机之间物理距离” 的稠密矩阵，是构建 3D 结构的核心基础之一。\n定义与数学形式： 深度图用符号 $D \\in \\mathbb{R}^{H \\times W}$ 表示，其中 $H$ 和 $W$ 分别对应输入图像的高度和宽度，每个元素 $D(u,v)$ 代表图像中坐标 $(u,v)$ 的像素到相机光心的真实距离（单位通常为米），且与输入图像严格像素对齐。\n核心作用： 深度图直接提供 “像素位置的远近信息”，是将 2D 图像映射到 3D 空间的关键桥梁 —— 结合相机姿态或射线信息，可通过 $P = t + D(u,v) \\cdot d$（$t$ 为射线原点，$d$ 为射线方向）计算出该像素在世界坐标系中的 3D 坐标 $P$，进而生成完整点云。\nDA3 中的特性： DA3 通过 “师生训练范式” 优化深度图质量 —— 教师模型（仅用合成数据训练）生成高质量伪深度标签，对齐真实场景的稀疏 / 噪声深度，确保深度图的细节完整性与几何一致性，避免近距区域（如物体表面）的距离预测偏差。","点云#点云":"点云生成公式： 3D 点坐标 = 射线原点 + 深度值 × 射线方向\n示例： 比如某像素的射线原点是（2,1,0.5）、方向是（0.2,-0.3,0.1）、深度是 2 米，那么它的 3D 坐标就是（2+2×0.2, 1+2×(-0.3), 0.5+2×0.1）=（2.4, 0.4, 0.7）。所有像素的 3D 坐标拼起来，就是一张完整的点云（比如桌子的点云就是无数个 “桌子表面 3D 点” 组成的）。\n点云评估指标： 把生成的点云和 “真实点云”（比如用 LiDAR 扫描的精准点云）对比：\nChamfer 距离：点云之间的平均距离，越小越准 F1 分数：点云的精度和召回率，越高越完整"},"title":"3D 场景理解工作"},"/blog/2025/3dgswm-manipulation/":{"data":{"":"装模甲样阅读一下文献。 不过说来也巧，如果我选择去港科广读PhD的话通过connection这个组的研究方向就是这个，然后经过他们的几个月的考核，这样就不用被该死的committee折磨了，不过对我来说还是大三的PTSD更impressive一些，所以先冲个Master吧\n黄思源是通讯作者，隶属于香港科技大学（广州）人工智能与数字经济实验室（LAMDA），主要研究方向为通用人工智能、机器人学习与 3D 视觉，但是一作是THU、NTU还有BIGAI，该团队聚焦通过 3D 表征与生成模型解决机器人操作的世界建模问题，即文章中 GWM 的研究方向\nGWM: Towards Scalable Gaussian World Models for Robotic Manipulation这篇文章中了ICCV 2025 该模型通过推断机器人动作作用下高斯基元（Gaussian primitives）的传播过程，实现对未来状态的重建。其核心是一个 latent 扩散 Transformer（Diffusion Transformer, DiT），并结合了 3D 变分自动编码器（3D variational autoencoder），能够借助高斯溅射（Gaussian Splatting）技术完成细粒度的场景级未来状态重建\nGWM 不仅能通过自监督未来预测训练，为模仿学习智能体增强视觉表征能力，还可作为支持模型基强化学习（model-based reinforcement learning, MBRL）的神经模拟器。模拟实验与真实世界实验均表明：GWM 能在不同机器人动作的条件下精准预测未来场景，且进一步用于策略训练时，其性能能以显著优势超越当前SOTA，充分展现了 3D 世界模型在初始数据扩展方面的潜力\n基于视频的生成模型依赖图像输入，且缺乏 3D 几何与空间理解能力，因此易受未见过的视觉变化（如光照、相机姿态、纹理等）影响，尽管 RGB-D（彩色 - 深度）与多视图设置试图弥补这一差距，但在连贯的 3D 空间内隐式对齐图像块特征仍面临挑战\n3DGS将点云等高效 3D 表示与高保真渲染相结合，然而，这些方法主要依赖离线逐场景重建，其计算需求给在机器人操作（尤其是MBRL）中的应用带来了重大挑战","3d-gaussian-vae#3D Gaussian VAE":"由于不同场景与任务中，每个世界状态对应的已学习 3D 高斯核数量差异显著，引入 3D 高斯变分自动编码器（\nEθ,DθE_\\theta, D_\\thetaEθ​,Dθ​），将重建得到的 3D 高斯核 GGG 编码为长度固定为 NNN 的潜在嵌入 x∈RN×Dx \\in \\mathbb{R}^{N \\times D}x∈RN×D，具体步骤如下：\n下采样：采用最远点采样（Farthest Point Sampling, FPS）将重建的 3D 高斯核 GGG 下采样为固定数量 NNN 的高斯核 GNG_NGN​，即：\nGN=FPS(G)G_N = \\text{FPS}(G)GN​=FPS(G)编码：将下采样后的高斯核 GNG_NGN​ 作为查询（query），通过一个 LLL 层基于交叉注意力的编码器 EθE_\\thetaEθ​（参考 [94] 的设计），从所有高斯核 GGG 中聚合信息并生成潜在嵌入 xxx，公式如下：\nX=Eθ(GN,G)=Eθ(L)∘⋯∘Eθ(1)(GN,G),Eθ(l)(Q,G)=LayerNorm(CrossAttn(Q,PosEmbed(G)))(2)\\begin{aligned}\rX \u0026= E_\\theta (G_N, G) = E_\\theta^{(L)} \\circ \\cdots \\circ E_\\theta^{(1)} (G_N, G), \\\\\rE_\\theta^{(l)} (Q, G) \u0026= \\text{LayerNorm}\\left(\\text{CrossAttn}\\left(Q, \\text{PosEmbed}(G)\\right)\\right) \\tag{2}\r\\end{aligned}XEθ(l)​(Q,G)​=Eθ​(GN​,G)=Eθ(L)​∘⋯∘Eθ(1)​(GN​,G),=LayerNorm(CrossAttn(Q,PosEmbed(G)))​(2)解码：利用一个结构对称的基于 Transformer 的解码器 DθD_\\thetaDθ​，在潜在编码集合内传播并聚合信息，最终重建得到高斯核 G^\\hat{G}G^，公式如下：\nG^=Dθ(x)=LayerNorm(SelfAttn(x,x))(3)\\hat{G} = D_\\theta (x) = \\text{LayerNorm}\\left(\\text{SelfAttn}(x, x)\\right) \\tag{3}G^=Dθ​(x)=LayerNorm(SelfAttn(x,x))(3)在 3D 高斯变分自动编码器（\nEθ,DθE_\\theta, D_\\thetaEθ​,Dθ​）的训练过程中，采用两种损失函数进行监督：\n倒角损失（Chamfer Loss）：约束重建高斯核 G^\\hat{G}G^ 与原始高斯核 GGG 的中心对齐 渲染损失（Rendering Loss）：确保重建高斯核 G^\\hat{G}G^ 的渲染效果，为基于图像的策略提供高保真视觉输入 总损失函数公式如下：\nLVAE=Chamfer(G^,G)+∥C(G^)−C(G)∥1(4)\\mathcal{L}_{\\text{VAE}} = \\text{Chamfer}(\\hat{G}, G) + \\left\\| C(\\hat{G}) - C(G) \\right\\|_1 \\tag{4}LVAE​=Chamfer(G^,G)+​C(G^)−C(G)​1​(4)","diffusion-based-dynamics-modeling#Diffusion-based Dynamics Modeling":"已知时刻 ttt 的编码世界状态嵌入 xtx_txt​ 及其未来状态 xt+1x_{t+1}xt+1​，的目标是学习世界动态 p(xt+1∣x≤t,a≤t)p(x_{t+1} | x_{\\leq t}, a_{\\leq t})p(xt+1​∣x≤t​,a≤t​)（其中 x≤tx_{\\leq t}x≤t​、\na≤ta_{\\leq t}a≤t​ 分别表示历史状态与历史动作）。\n具体而言，构建基于扩散的动态模型，将动态学习转化为条件生成问题：以历史状态与动作 yt=(x≤t,a≤t)y_t = (x_{\\leq t}, a_{\\leq t})yt​=(x≤t​,a≤t​) 为条件，从噪声中生成未来状态 xt+1x_{t+1}xt+1​。","feed-forward-3d-gaussian-splatting#Feed-forward 3D Gaussian Splatting":"给定某一世界状态的单视图或双视图图像输入\nI={I}i={1,2}I = \\{I\\}_{i=\\{1,2\\}}I={I}i={1,2}​，的核心目标是先将场景编码为 3D 高斯表示，为后续动态学习与预测提供基础\n3DGS采用多个非结构化 3D 高斯核\nG={xp,σp,Σp,Cp}p∈PG = \\{x_p, \\sigma_p, \\Sigma_p, C_p\\}_{p \\in P}G={xp​,σp​,Σp​,Cp​}p∈P​表示 3D 场景\n每个高斯核是一个小的 3D 椭球，包含位置、大小、颜色等信息\n其中： xpx_pxp​：高斯核中心；（3D 坐标） σp\\sigma_pσp​：高斯核不透明度；（0-1，控制是否可见） Σp\\Sigma_pΣp​：高斯核协方差矩阵；（控制椭球的形状和方向） CpC_pCp​：高斯核球谐系数。（存储颜色信息，支持视角相关颜色）\n为从特定视角获取每个像素的颜色，3DGS 会将 3D 高斯核投影到图像平面，并按以下公式计算像素颜色：\nC(G)=∑p∈Pαp⋅SH(dp;Cp)⋅∏j=1p−1(1−αj)(1)C(G) = \\sum_{p \\in P} \\alpha_p \\cdot \\text{SH}(d_p; C_p) \\cdot \\prod_{j=1}^{p-1} (1 - \\alpha_j) \\tag{1}C(G)=p∈P∑​αp​⋅SH(dp​;Cp​)⋅j=1∏p−1​(1−αj​)(1) 像素颜色 = 所有高斯核的贡献叠加\n其中： αp\\alpha_pαp​：按 z 深度排序的有效不透明度，即由协方差矩阵\nΣp\\Sigma_pΣp​得到的 2D 高斯权重与整体不透明度\nσp\\sigma_pσp​的乘积； dpd_pdp​：从相机到高斯核中心\nxpx_pxp​的视角方向； SH(⋅)\\text{SH}(\\cdot)SH(⋅)：球谐函数（spherical harmonics function）。由于基础版 3D-GS（vanilla 3D-GS）依赖耗时的逐场景离线优化，采用可泛化 3D-GS 学习 “从图像到 3D 高斯” 的前馈映射，以提升效率。\n具体而言，通过 Splatt3R 模型获取 3D 高斯世界状态 GGG，该模型的实现流程为：\n首先利用立体重建模型 Mast3R 从输入图像生成 3D 点图 再通过额外的预测头，基于这些 3D 点图预测每个 3D 高斯核的参数 流程示意：\n输入图像 → Mast3R → 3D 点云\r3D 点云 → 预测头 → 每个高斯核的参数","gwm-for-il#GWM for IL":"在模仿学习（IL）场景中，将 GWM 作为更高效的编码器，为策略学习提供更优特征。\n具体而言，提取扩散过程中 “第一步去噪后的特征向量\"，作为下游策略模型（如行为克隆 Transformer（BC-transformer）、扩散策略（diffusion policy））的输入 —— 第一步去噪后的特征能保留具有代表性的空间信息，可有效应对较高的噪声水平。\n在实现过程中，采用 “按序列块预测动作” 的方式，确保机器人控制的一致性。","gwm-for-policy-learning#GWM for Policy Learning":"","gwm-for-rl#GWM for RL":"从形式化定义来看，马尔可夫决策过程（MDP）由元组 (S,A,p,r,γ,ρ0)(S, A, p, r, \\gamma, \\rho_0)(S,A,p,r,γ,ρ0​) 描述，其中：\nSSS、AAA：分别为状态空间与动作空间 γ\\gammaγ：折扣因子 r(s,a)r(s, a)r(s,a)：奖励函数 ρ0\\rho_0ρ0​：初始状态分布 MBRL 的核心目标是：在通过策略滚动采样（policy roll-outs）构建动态模型 pθ(st+1,rt∣st,at)p_\\theta(s_{t+1}, r_t | s_t, a_t)pθ​(st+1​,rt​∣st​,at​) 的同时，学习使 “期望折扣奖励和” 最大化的策略 π∗\\pi^*π∗，即：\nπ∗=arg⁡max⁡πEπ[∑t=0∞γtrt]\\pi^* = \\arg\\max_\\pi \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]π∗=argπmax​Eπ​[t=0∑∞​γtrt​]Algorithm 1: Monotonic Model-Based Policy Optimization (MBPO) with Gaussian World Model\nfor N epochs do\rInitialize policy π(at|st), Gaussian world model pθ(st+1, rt|st, at), empty replay buffer B;\rCollect data with π in real environment:\rB = B ∪ {(st, at, st+1, rt)}t;\rTrain Gaussian world model pθ on dataset B via maximum likelihood:\rθ ← arg maxθ EB[log pθ(st+1, rt|st, at)];\rOptimize policy under predictive model:\rπ ← arg maxπ Eπ[∑t≥0 γ^t rt];\rend 在该框架下，在 GWM 基础上新增一个奖励预测头，用于参数化动态模型 pθ(st+1,rt∣st,at)p_\\theta(s_{t+1}, r_t | s_t, a_t)pθ​(st+1​,rt​∣st​,at​)。为提升视觉操作任务的性能，参考 [Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. Proceedings of Advances in Neural Information Processing Systems (NeurIPS)] 的设计选择构建基础强化学习策略。","methodology#Methodology":"将真实世界的视觉输入编码为潜在 3DGS 表示，并利用基于扩散的条件生成模型，在给定机器人状态与动作的情况下，学习表示层面的动态特性","world-state-encoding#World State Encoding":"","加噪过程#加噪过程":"对真实未来状态 xt+10=xt+1x_{t+1}^0 = x_{t+1}xt+10​=xt+1​ 添加噪声，通过高斯扰动核得到带噪未来状态样本 xt+1τx_{t+1}^\\tauxt+1τ​，公式如下：\np0→τ(xt+1τ∣xt+10)=N(xt+1τ;xt+10,σ2(τ)⋅I)(5)p^{0 \\to \\tau} (x_{t+1}^\\tau | x_{t+1}^0) = \\mathcal{N}\\left(x_{t+1}^\\tau; x_{t+1}^0, \\sigma^2(\\tau) \\cdot I\\right) \\tag{5}p0→τ(xt+1τ​∣xt+10​)=N(xt+1τ​;xt+10​,σ2(τ)⋅I)(5)其中：\nτ\\tauτ：噪声步数索引 σ(τ)\\sigma(\\tau)σ(τ)：噪声调度（noise schedule），用于控制不同步骤的噪声强度 N(⋅;μ,Σ)\\mathcal{N}(\\cdot; \\mu, \\Sigma)N(⋅;μ,Σ)：正态分布，均值为 μ\\muμ，协方差矩阵为 Σ\\SigmaΣ 上述扩散过程可表示为如下随机微分方程（SDE）的解：\ndx=f(x,τ)dτ+g(τ)dw(6)dx = f(x, \\tau) d\\tau + g(\\tau) dw \\tag{6}dx=f(x,τ)dτ+g(τ)dw(6)其中：\nwww：标准维纳过程（Wiener process） f(x,τ)f(x, \\tau)f(x,τ)：漂移系数（drift coefficient） g(τ)g(\\tau)g(τ)：扩散系数（diffusion coefficient） 在高斯扰动核的设定下，漂移系数 f(x,τ)=0f(x, \\tau) = 0f(x,τ)=0，扩散系数 g(τ)=2⋅σ˙(τ)⋅σ(τ)g(\\tau) = \\sqrt{2 \\cdot \\dot{\\sigma}(\\tau) \\cdot \\sigma(\\tau)}g(τ)=2⋅σ˙(τ)⋅σ(τ)​（\nσ˙(τ)\\dot{\\sigma}(\\tau)σ˙(τ) 为 σ(τ)\\sigma(\\tau)σ(τ) 的导数）。","技术实现#技术实现":"在技术实现上，采用扩散 Transformer（DiT）构建网络 FθF_\\thetaFθ​，具体流程如下：\n带噪潜在生成：给定真实世界状态潜在嵌入序列 {xt0=xt}t=1T\\{x_t^0 = x_t\\}_{t=1}^T{xt0​=xt​}t=1T​，根据公式（5）的高斯扰动生成带噪潜在序列 {xtτ}t=1T\\{x_t^\\tau\\}_{t=1}^T{xtτ​}t=1T​ 输入构建：将带噪潜在嵌入与旋转位置嵌入（RoPE）拼接，作为 DiT 的输入\n条件融入：对于条件 yt=(x≤t0,a≤t,cnoiseτ)y_t = (x_{\\leq t}^0, a_{\\leq t}, c_{\\text{noise}}^\\tau)yt​=(x≤t0​,a≤t​,cnoiseτ​)，通过自适应层归一化（AdaLN）调制时序嵌入，并将当前机器人动作作为 DiT 交叉注意力层的键（key）与值（value），实现条件生成\n训练稳定性优化：为保证所有注意力机制的稳定性与效率，采用带可学习缩放因子的均方根归一化（RMSNorm），在处理空间表示的同时，将时序动作序列作为条件融入训练","条件去噪模型训练#条件去噪模型训练":"通过最小化 “采样未来状态 x^t+10=Dθ(xt+1τ,yt)\\hat{x}_{t+1}^0 = D_\\theta(x_{t+1}^\\tau, y_t)x^t+10​=Dθ​(xt+1τ​,yt​)” 与 “真实未来状态 xt+10x_{t+1}^0xt+10​” 的差异，学习条件去噪模型 DθD_\\thetaDθ​，损失函数如下：\nL(θ)=E[∥Dθ(xt+1τ,ytτ)−xt+10∥22](8)\\mathcal{L}(\\theta) = \\mathbb{E}\\left[ \\left\\| D_\\theta(x_{t+1}^\\tau, y_t^\\tau) - x_{t+1}^0 \\right\\|_2^2 \\right] \\tag{8}L(θ)=E[​Dθ​(xt+1τ​,ytτ​)−xt+10​​22​](8)其中 E[⋅]\\mathbb{E}[\\cdot]E[⋅] 表示期望，\n∥⋅∥22\\|\\cdot\\|_2^2∥⋅∥22​ 表示 L2 平方范数。\n如文献 [[33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565–26577, 2022. 4, A1] 所指出，直接学习去噪器 Dθ(xt+1τ,yt)D_\\theta(x_{t+1}^\\tau, y_t)Dθ​(xt+1τ​,yt​) 易受噪声幅度变化的影响。\n因此，参考 [[1] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari] 的思路，采用 EDM（Elucidating the Design Space of Diffusion-Based Generative Models）[33] 中的预处理策略，转而学习网络 FθF_\\thetaFθ​。\n具体而言，将去噪器 Dθ(xt+1τ,ytτ)D_\\theta(x_{t+1}^\\tau, y_t^\\tau)Dθ​(xt+1τ​,ytτ​) 参数化为：\nDθ(xt+1τ,ytτ)=cskipτ⋅xt+1τ+coutτ⋅Fθ(cinτ⋅xt+1τ,ytτ;cnoiseτ)(9)D_\\theta(x_{t+1}^\\tau, y_t^\\tau) = c_{\\text{skip}}^\\tau \\cdot x_{t+1}^\\tau + c_{\\text{out}}^\\tau \\cdot F_\\theta\\left( c_{\\text{in}}^\\tau \\cdot x_{t+1}^\\tau, y_t^\\tau; c_{\\text{noise}}^\\tau \\right) \\tag{9}Dθ​(xt+1τ​,ytτ​)=cskipτ​⋅xt+1τ​+coutτ​⋅Fθ​(cinτ​⋅xt+1τ​,ytτ​;cnoiseτ​)(9)其中：\ncinτc_{\\text{in}}^\\taucinτ​、coutτc_{\\text{out}}^\\taucoutτ​：输入 / 输出幅度缩放预处理器 cskipτc_{\\text{skip}}^\\taucskipτ​：跳跃连接调节预处理器 cnoiseτc_{\\text{noise}}^\\taucnoiseτ​：噪声水平映射预处理器，将噪声强度作为额外条件输入到 FθF_\\thetaFθ​ 中 预处理器的详细设计见附录 B.1。\n基于上述参数化，可将公式（8）的损失函数重写为：\nL(θ)=E[∥Fθ(cinτ⋅xt+1τ,ytτ)−1coutτ(xt+10−cskipτ⋅xt+1τ)∥22](10)\\mathcal{L}(\\theta) = \\mathbb{E}\\left[ \\left\\| F_\\theta\\left( c_{\\text{in}}^\\tau \\cdot x_{t+1}^\\tau, y_t^\\tau \\right) - \\frac{1}{c_{\\text{out}}^\\tau} \\left( x_{t+1}^0 - c_{\\text{skip}}^\\tau \\cdot x_{t+1}^\\tau \\right) \\right\\|_2^2 \\right] \\tag{10}L(θ)=E[​Fθ​(cinτ​⋅xt+1τ​,ytτ​)−coutτ​1​(xt+10​−cskipτ​⋅xt+1τ​)​22​](10)这一转化的核心价值在于：根据噪声调度 σ(τ)\\sigma(\\tau)σ(τ) 自适应混合信号与噪声，为网络 FθF_\\thetaFθ​ 构建更优的训练目标 —— 在高噪声水平下（\nσ(τ)≫σdata\\sigma(\\tau) \\gg \\sigma_{\\text{data}}σ(τ)≫σdata​），\ncskipτ→0c_{\\text{skip}}^\\tau \\to 0cskipτ​→0，网络主要学习预测干净信号；在低噪声水平下（\nσ(τ)→0\\sigma(\\tau) \\to 0σ(τ)→0），\ncskipτ→1c_{\\text{skip}}^\\tau \\to 1cskipτ​→1，网络目标转为预测噪声分量，避免损失函数趋于 trivial（无学习价值）。","逆时采样#逆时采样":"为从噪声中生成未来状态，通过逆时 SDE 逆转公式（6）的扩散过程，实现采样：\ndx=[f(x,τ)−g(τ)2⋅∇xlog⁡pτ(x)]dτ+g(τ)dwˉ(7)dx = \\left[ f(x, \\tau) - g(\\tau)^2 \\cdot \\nabla_x \\log p^\\tau(x) \\right] d\\tau + g(\\tau) d\\bar{w} \\tag{7}dx=[f(x,τ)−g(τ)2⋅∇x​logpτ(x)]dτ+g(τ)dwˉ(7)其中：\nwˉ\\bar{w}wˉ：逆时维纳过程 ∇xlog⁡pτ(x)\\nabla_x \\log p^\\tau(x)∇x​logpτ(x)：得分函数（score function），即对数边缘概率关于 xxx 的梯度，可通过网络估计 BTW 这里引用的文献是真多，将来对 SDE 这里感兴趣的话看看它的 Reference 吧"},"title":"Gaussian World Model"},"/blog/2025/ai-work/":{"data":{"":"如何评价当前的 AI Agent 落地效果普遍不佳的问题？\n这篇回答给我玩小黄文这一块，**“做AI”**是吧\n不过也让我很好的了解到了从模型生产到部署AI各阶段具体干什么的概念：","1-planning-阶段带来了巨大的耗时#1. Planning 阶段带来了巨大的耗时":"当 tool 变多后，turbo 系列模型的准确率堪忧，因此不得不使用旗舰模型，这让延时进一步增加。\n本质原因：组合优化问题。工具多了以后，搜索空间呈指数级膨胀。弱模型搞不定，强模型 Token 多、推理慢。 解决方案： 分层治理（缩小搜索空间）：意图分类 -\u003e 路由到特定域（Domain） -\u003e 仅暴露少量工具（类似 MCP 协议思路）。 并行化（工程优化）：将串行链改为 DAG（有向无环图），无依赖的任务并行执行（参考 LLMCompiler）。 路由策略（成本优化）：简单任务给小模型（SLM）/硬编码，复杂任务给大模型（参考 RouteLLM）。","1-pre-training-预训练造脑工程#1. Pre-training (预训练)：造脑工程":"这是 AI 的 “基建\"阶段，目标是从海量数据中学习通用知识。","2-planning-的质量不够高#2. Planning 的质量不够高":"原来的 task bot 做任务所使用的 workflow 是人工决定的，现在改成了模型自助决定，从目前的测试来看，由模型构建的复杂工作流的可用率远远不及人类水平。简单工作流使用判别式小模型反而性能更好。\n本质原因：自然语言生成的计划缺乏**“可执行性”和“全局约束”**。模型线性思维（Step A-\u003eB-\u003eC）难以应对复杂多变场景。 解决方案： 解耦规划（HiPlan）：战略（里程碑）与战术（执行细节）分离。 结构化约束（Routine）：强制输出 DSL（领域特定语言） 而非自然语言，由语法保证正确性。 搜索式规划（LATS）：引入 MCTS（蒙特卡洛树搜索），不是赌一把，而是模拟多条路径+打分（Verifier）。 多轮 RL 训练：让模型在多轮交互中\"学会\"长程规划，而不是仅靠 Prompt。","2-post-training-后训练对齐教育工程#2. Post-training (后训练/对齐)：教育工程":"预训练出的模型只是一个**“满腹经纶但满嘴胡话\"的学者**，Post-training 是为了让它变乖、变有用。\nSFT (有监督微调)：喂给模型高质量的 Q\u0026A 对，教会它听从指令。","3-multimodal-多模态五官工程#3. Multimodal (多模态)：五官工程":"让 AI 不仅能看懂文字，还能看图、听声音、甚至看视频。\n模态对齐 (Modality Alignment)：将图像编码器（如 ViT）捕捉到的特征，翻译成大语言模型能听懂的\"语言”。 统一表示 (Unified Tokenization)：尝试把声音、图像、文本都变成同一种数字序列进行处理（如 Chameleon 或 GPT-4o 的思路）。 时序理解：针对视频流，如何让模型理解动作的先后顺序和逻辑。","3-reflection-是一种时间换准确度的策略#3. Reflection 是一种时间换准确度的策略":"然而这个策略非常容易重复进行自我内耗，和死循环。\n本质原因：反馈信号太弱（**“我觉得不对”**太主观），缺乏明确的停机条件，导致错误假设被不断强化。 解决方案： 模型侧：训练模型学会**“诊断错误”并“提出修复方案”**（Failure Makes the Agent Stronger）。 工程侧（兜底）：设置硬性上限（Max rounds）、状态去重（State-hash）、预算控制。","4-inference-optimization-推理优化落地工程#4. Inference Optimization (推理优化)：落地工程":"模型训练好后，如何让它运行得更快、更便宜、更稳。\n量化 (Quantization)：将 16 位浮点数压成 8 位或 4 位，模型体积缩小一倍，速度飞快，但精度损失很小。 算子优化：比如 FlashAttention，通过底层数学技巧极大提升显卡的计算效率。 调度系统： vLLM / TensorRT-LLM：并发处理成千上万个请求，提高吞吐量。 KV Cache 管理：解决模型在生成长文本时内存占用过高的问题。","5-agent-开发#5. Agent 开发":"虽然表面上是鄙视链最底层，很大程度上依赖**“调教 Prompt”**，不过下面这篇回答我觉得说的挺好：\nAI agent到底有多大创新？\n这个知乎提问主要谈到了 AI agent 的缺陷：","alignment-对齐价值观#Alignment (对齐/价值观)":"RLHF (强化学习)：让模型根据人类的打分来优化回答。 DPO (直接偏好优化)：目前最流行的替代 RLHF 的方案，更高效。","反思与强化学习reflection--rl#反思与强化学习（Reflection \u0026amp; RL）":"UFO: Unary Feedback as Observation Failure Makes the Agent Stronger","多轮强化学习multi-turn-rl#多轮强化学习（Multi-turn RL）":"RAGEN (StarPO-S) AgentGym: Evolving Agents via RL MUA-RL","思考#思考":"这么看来，其实 agent 的 Prompt Engineering 已经臭了，应该转向成 Flow Engineering，使用 HiPlan（分层）、DAG（并行）、Router（路由） 等手段。DSL（结构化语言） 依然很重要，即输出 JSON 或特定代码，主要就看你一个 Schema 定义能力本身。然后 MCP 即插即用，Multi-agent System (MAS) 组成一组’专家 Agent’的协作网络。","推理能力强化#推理能力强化":"思维链 (CoT) 激发：通过特定的训练让模型学会**“想好了再说”**。 Reflection (反思)：教会模型在输出前自我检查错误。 合成数据 (Synthetic Data)：当人类数据不够用时，让模型生成高质量数据来训练模型自己。","数据工程-data-curation#数据工程 (Data Curation)":"清洗与去重：处理成百上千 T 的互联网数据，剔除垃圾信息。 数据配比：决定书本、代码、网页、数学题各自占多少比例（这是各家模型的核心秘密）。","架构设计-architecture#架构设计 (Architecture)":"MoE (混合专家模型)：像 DeepSeek 那样，让模型只激活部分参数以节省算力。 长文本窗口：让模型一次能读完一整本书。","相关论文#相关论文":"","算力基础设施-infrastructure#算力基础设施 (Infrastructure)":"分布式训练：如何让几万张显卡同时跑一个模型（3D 并行：数据并行、算力并行、流水线并行）。 算力优化：提高显卡利用率（MFU），防止训练过程中突然崩溃（Checkpoint 恢复）。","规划优化planning-optimization#规划优化（Planning Optimization）":"HiPlan: Hierarchical Planning for Complex Tasks Routine: Structured Instruction for Agents LATS: Language Agent Tree Search","路由routing#路由（Routing）":"RouteLLM: Learning to Route LLMs with Preference Data MoMA: Multimodal LLM Adapter for Mobile Agents"},"title":"AI 上下游工作概念"},"/blog/2025/commit-message/":{"data":{"":"COMMIT MESSAGE我完全把blog当成杂碎知识点的记录了……不过COMMIT MESSAGE还是很有必要规范一下的，规范的COMMIT MESSAGE是团队协作的“沟通密码”，能让代码变更清晰可追溯，那些只写一行空消息或“改东西”“修复bug”的做法，简直是给未来埋坑。\n规范的COMMIT MESSAGE需遵循「类型(可选范围)：简洁描述」格式，搭配正文和脚注，常用类型包括feat、fix、docs等，目的是让任何人快速看懂变更目的和影响。","1-feat新增功能新特性新功能新接口#1. feat：新增功能（新特性、新功能、新接口）":"feat(登录模块)：新增手机验证码登录功能 feat(订单页)：添加物流轨迹实时查询接口 feat(用户中心)：支持第三方账号（微信/QQ）绑定 feat(支付模块)：新增支付宝分期支付选项","2-fix修复bug线上测试环境问题逻辑错误#2. fix：修复bug（线上/测试环境问题、逻辑错误）":"fix(首页)：修复轮播图在iOS15下自动播放失效的问题 fix(购物车)：解决商品数量为0时仍可结算的bug fix(搜索框)：修复输入特殊字符导致接口报错的问题 fix(个人资料)：更正手机号格式校验逻辑（支持境外号码）","3-docs文档变更readme注释接口文档#3. docs：文档变更（README、注释、接口文档）":"docs(API文档)：补充用户登录接口的错误码说明 docs(README)：更新项目启动步骤（新增依赖安装命令） docs(注释)：为工具类函数添加参数说明和使用示例 docs(部署文档)：修正生产环境nginx配置示例","4-style代码格式调整不影响逻辑仅格式排版#4. style：代码格式调整（不影响逻辑，仅格式/排版）":"style(utils.js)：统一代码缩进为2个空格 style(login.vue)：删除多余空行和未使用的注释 style(api.js)：调整函数参数顺序，优化代码可读性 style(全局)：统一变量命名规范（下划线转小驼峰）","5-refactor重构代码不新增功能不修复bug优化结构#5. refactor：重构代码（不新增功能、不修复bug，优化结构）":"refactor(支付逻辑)：拆分过大的pay()函数，提取工具方法 refactor(列表组件)：用TS重构JS代码，添加类型定义 refactor(请求封装)：优化axios拦截器结构，简化错误处理 refactor(数据处理)：替换for循环为数组方法，提升代码简洁度","6-test测试相关新增修改测试用例测试代码#6. test：测试相关（新增/修改测试用例、测试代码）":"test(登录模块)：新增验证码登录的单元测试用例 test(订单接口)：补充异常场景（参数为空、权限不足）的测试 test(工具函数)：修复测试用例中过期的断言逻辑 test(全局)：新增E2E测试，覆盖核心业务流程","7-chore构建依赖工具相关不影响业务代码#7. chore：构建/依赖/工具相关（不影响业务代码）":"chore(package.json)：更新axios依赖版本至1.6.0 chore(webpack)：新增打包分析插件，优化构建体积 chore(CI)：配置GitHub Actions自动部署测试环境 chore(脚本)：新增数据库备份脚本（daily-backup.sh）","8-perf性能优化提升代码运行速度减少资源占用#8. perf：性能优化（提升代码运行速度、减少资源占用）":"perf(列表渲染)：使用虚拟滚动优化长列表加载性能 perf(图片加载)：新增图片懒加载，减少首屏加载时间 perf(接口请求)：添加请求缓存，减少重复接口调用 perf(本地存储)：优化localStorage读写逻辑，提升响应速度","9-revert回滚代码撤销之前的提交#9. revert：回滚代码（撤销之前的提交）":"revert: feat(支付模块)：回滚“新增支付宝分期”功能（因兼容性问题） revert: fix(首页轮播)：撤销#123提交的修复（导致其他功能异常）","commit-message#COMMIT MESSAGE":"","一为什么不能只写一行模糊消息#一、为什么不能只写一行模糊消息？":"后续排查问题时，翻log像“开盲盒”，找不到对应变更的上下文。 团队协作时，同事不知道你改了啥，需要反复沟通确认。 自动化工具（如生成CHANGELOG）无法识别，失去版本迭代的清晰记录。 隔几个月自己回头看，完全忘了当初为啥改这段代码。","三常用类型#三、常用类型":"","二核心规范结构通用angular规范最常用#二、核心规范结构（通用Angular规范，最常用）":"完整格式分3部分，一行消息只占“简洁描述”，核心是前半段的「类型」：\n类型(可选范围)：简洁描述（不超过50字）\r可选正文：详细说明变更原因、实现方式、影响范围（换行写，每行不超过72字）\r可选脚注：关联Issue、PR或Breaking Change（如：Fixes #123 / BREAKING CHANGE: 接口参数变更）","四反面例子#四、反面例子":"错误1：git commit -m \"\"（空消息，完全不知道改了啥） 错误2：git commit -m \"改东西\"（模糊到极致，毫无意义） 错误3：git commit -m \"修复bug\"（没说清哪个bug、哪个模块） 错误4：git commit -m \"今天的修改\"（时间维度没用，无法追溯） 错误5：git commit -m \"新增功能+修复bug+改文档\"（一个提交干多件事，应该拆分） 这就把"},"title":"Commit Message"},"/blog/2025/deepresearch-workflow/":{"data":{"a-langgraph#A. LangGraph":"智能体框架大致分为两类：一类追求配置驱动的简单性（如 CrewAI），另一类提供图驱动的编排控制能力（如 LangGraph）。CrewAI 在角色分工明确的简单任务中表现良好，但面对复杂的条件执行或流程分支时缺乏灵活性。AutoGen 擅长对话式协作，却难以满足科研任务所需的高确定性。\nLangGraph 基于 LangChain 原语构建运行时，其核心即状态机。对于复杂且长周期的 DRW，图结构是刚性需求，因为它能够：\n确保确定性工作流：明确定义节点（规划、执行、合成）与状态转换。 实现条件执行：强制执行 if/then/else 逻辑，例如“RAG 失败则重试，成功则验证”。 支持持久状态：原生提供状态管理与检查点，对耗时数小时甚至数天的流程至关重要。","a-mcp-在学术研究中的作用#A. MCP 在学术研究中的作用":"深度研究需要可靠、标准化的数据接口。MCP（Model Context Protocol）通过开放协议，定义了应用如何向 LLM 提供工具与上下文，确保数据摄取的一致性与可验证性。\nMCP 服务器可同时暴露：\n工具（Tools）：执行特定任务，如网络搜索、文件解析。 提示（Prompts）：针对任务的提示模板，例如“系统综述大纲提示”。 资源（Resources）：提供学术论文 PDF 或文本片段作为上下文。 借助 MCP，DRW 能以统一方式连接 GitHub、Slack、Google Drive 等资源。若未来接入授权学术数据库，只需替换 MCP 服务器实现，高层编排无需变动。","a-委托循环与条件执行#A. 委托循环与条件执行":"TTA 持续监控共享状态，依次选择 paper_list 中的待处理论文，更新 current_paper_id 并路由给 STA。凭借 LangGraph 的条件路由能力，可构建稳健的错误处理：\n首次失败：立即重试 MCP 调用。 二次失败：记录错误并返回 TTA 重新评估，例如改用网络摘要工具。 最终失败：将论文标记为 Failed，继续下一项，避免流程阻断。","a-数据摄取与准备#A. 数据摄取与准备":"学术论文结构复杂，RAG 的数据处理质量直接决定结果准确性：\n复杂文档处理：必须使用能识别章节、段落、图表的文本分割器，保障语义连贯。 向量存储与嵌入模型：可采用 Pinecone 或本地 Chroma，嵌入模型由本地 Ollama 提供，构建混合式 RAG，结合生成与检索优势，降低幻觉率。","a-深度研究工作流drw范式的界定#A. 深度研究工作流（DRW）范式的界定":"传统的单体式大型语言模型（LLM）在执行多步骤、高复杂度且需长期记忆的学术研究任务时存在明显局限。单个模型同时承担规划、执行、检索和综合，往往缺乏维持状态、处理复杂决策以及精确纠错的机制。\n完整的文献综述任务包含“识别论文—逐篇阅读—交叉引用—合成结论”等数十个顺序与条件步骤。要让 LLM 从文本生成器转型为可执行复杂任务的自主系统，必须赋予其智能体（Agentic）能力，使其能够适应输入、调用外部工具并自主执行预设或自适应的工作流。\n在深度研究场景中，LLM 需要具备实时信息检索、任务进度管理以及失败后的自我恢复能力。因此，分层多智能体架构是 DRW 的必要条件，它模拟组织化的管理体系，将复杂任务拆解并交给专业化的子智能体，从而保证系统的专业性与鲁棒性。\n分层架构的关键优势包括：\n专业化与解耦：避免单一智能体承担全部工作导致的效率低下与脆弱性，通过专职智能体（如 RAG 执行者）实现功能隔离。 控制流与维护性：提供清晰的委托规则、退避重试（backoff retries）与故障转移逻辑。 当外部工具调用失败时，总任务智能体可以决定重试、更换工具或标记论文失败后继续下一项，从而保障系统稳定与可维护性。","b-主管子团队分层结构#B. 主管—子团队分层结构":"总任务智能体（Total Task Agent, TTA）/ 主管：负责高层规划、任务拆解、paper_list 进度跟踪、状态条件路由以及最终合成。 委托机制：根据用户查询与子智能体描述决定任务路由，必要时将论文检索任务委托给执行智能体。 研究执行智能体（Research Executor Agent, STA）：与外部环境（Model Context Protocol, MCP 服务器）交互，负责文档检索、RAG 调用与结构化摘要生成。","b-数据质量事实基础与引文校验#B. 数据质量：事实基础与引文校验":"为最大程度降低幻觉风险，需要验证智能体对 STA 生成的摘要进行原文核查。关键指标如下：\n指标 定义 对 DRW 的重要性 正确性（Correctness） 事实点可在引用文档中核实的比例 学术诚信的底线 完整性（Completeness） 查询或文档关键要点的覆盖程度 确保分析全面 关联性（Relevancy） 引用资源与生成内容的相关度 验证任务匹配性","b-智能体式-rag-与迭代摘要#B. 智能体式 RAG 与迭代摘要":"STA 在 LangGraph 的指导下进行智能决策，仅在需要外部上下文时调用 RAG。它通过多轮定向查询（如“提取架构细节”“总结消融实验”）收集事实，再综合输出高质量摘要。","b-本地-llm隐私控制与成本效益#B. 本地 LLM：隐私、控制与成本效益":"“本地搭建”是架构设计的关键约束。采用 Ollama 等本地 LLM 服务平台能够增强数据隐私，减少对第三方云服务的依赖，并完全掌控模型版本与参数，从而满足成本与安全性的双重需求。但是这里我们暂且按下不表，选择调用 LLM API。","b-构建论文检索用-mcp-服务器#B. 构建论文检索用 MCP 服务器":"STA 必须完成“通过 MCP 联网读取指定论文并总结”的职责，因此需要搭建封装 RAG 前置流程的 MCP 服务器，核心能力包括：\nfetch_and_prepare_resource(url)：下载 PDF 并转换成标准资源对象。 perform_rag_query(resource_id, question)：对摄取后的论文执行检索增强生成。 传输方案：本地可采用 stdio；若需并行或远程访问，可切换到支持流式的 HTTP 传输。","c-共享状态架构#C. 共享状态架构":"共享状态对象是整个工作流的“单一事实来源”，用于记录流程状态、中间结果与进度。所有智能体必须通过标准化接口读写该状态，以实现模块化解耦：\nTTA 无需了解 STA 执行 RAG 的细节，只需读取写回的结果。 即便替换 RAG 流程或底层工具链，也不会影响 TTA 的高层逻辑，实现可扩展的软件架构原则。 示例字段如下：\n字段 类型 说明 使用方 query_topic 字符串 初始研究查询 TTA（规划） paper_list 列表（字典） 论文主列表：URL、标题、状态 TTA（分配、追踪） current_paper_id 字符串 当前 STA 处理的文档 ID TTA / STA next_task_route 字符串 下一节点条件字段 TTA（路由） task_output 字符串 / 字典 STA 产出的摘要或错误信息 STA / TTA verified_summaries 字典 经核查的摘要索引存储 TTA / 合成智能体 error_log 字符串列表 失败调用与幻觉警告 TTA（回退） synthesis_draft 字符串 文献综述草稿 TTA / 合成智能体","c-合成智能体整合最终输出#C. 合成智能体：整合最终输出":"当所有论文分析完成或达到失败阈值，TTA 将流程路由至合成阶段。合成智能体通过 VCM 检索已验证摘要，按照标准综述结构（如非 SLAM 3D 方法分类、传感器融合对比、挑战识别）生成报告。LangGraph 的流式输出能力保证用户实时查看长文档生成过程。","c-在-sta-中集成-mcp#C. 在 STA 中集成 MCP":"STA 作为 MCP 客户端，可使用 MultiServerMCPClient 安全调用服务器工具。论文分析通常需要顺序推理（先“方法”，再“实验”，最后“总结”），因此 STA 需要通过 ClientSession 维持跨调用状态，实现类似研究员的深度迭代分析。","c-高级上下文工程与长期记忆vcm#C. 高级上下文工程与长期记忆（VCM）":"LLM 的上下文窗口限制是深度研究的主要挑战。可引入受 MemGPT 启发的虚拟上下文管理（Virtual Context Management, VCM）：\n核心记忆（Core Memory）：相当于 RAM，存储当前指令与摘要。 归档上下文（Archival Context）：相当于磁盘，存储所有已验证摘要（如向量数据库）。 VCM 在固定窗口内模拟“无限上下文”，支持跨论文事实汇总。合成智能体具备自定向检索能力，可调用工具按主题调取历史摘要，例如“检索所有提及‘非视觉里程计’的摘要”。","d-人在回路hilt检查点#D. 人在回路（HILT）检查点":"在高度自主系统中引入 HILT 至关重要。LangGraph 支持在工作流中暂停、等待用户输入并从相同状态继续。建议的人工介入点包括：\n大纲审查：TTA 制定初始论文列表与研究计划后，人工确认与调整。 合成审查：最终报告提交前，由人工审阅草稿，确保学术质量与方向。 结合评估指标与可观察性工具（如 LangSmith），系统不仅能执行任务，还能自我改进。error_log 与验证得分帮助持续优化提示工程与 RAG 管道，将智能体系统视为可度量、迭代的软件产品。","i-自主研究智能体的基础架构#I. 自主研究智能体的基础架构":"","ii-核心编排框架的选择与实施#II. 核心编排框架的选择与实施":"","iii-通过模型上下文协议mcp实现工具访问#III. 通过模型上下文协议（MCP）实现工具访问":"","iv-子任务智能体论文-rag-管道#IV. 子任务智能体：论文 RAG 管道":"","v-工作流执行质量保障与稳健合成#V. 工作流执行、质量保障与稳健合成":"","深度研究工作流drw构建#深度研究工作流（DRW）构建":"深度研究工作流（DRW）构建"},"title":"深度研究工作流（DRW）构建"},"/blog/2025/docusaurus-content-workflow/":{"data":{"":"作为一套以文档驱动为核心的前端框架，Docusaurus 3 天然把「内容生产」拆成两个入口：docs 和 blog。最近整理站点时，我也顺手理了一遍这套流程，写下来算是备忘。\n最初我完全没有博客的需求，因此从 docs 入手，把课程笔记、强化学习章节放进 docs/self-study/ai 一类的目录。\n每个 Markdown 或 MDX 文件都可以用 front matter 决定 id、标题和侧边栏标签，然后通过 sidebars.ts 统一组织结构。\n只要在某个类别下列出文档路径，访问 /docs/... 时就能看到层级清晰的目录树。\n但是文档的内容太多太长了。如果想让一篇文档拆成多个小章节，除了常规的多文件方案，这里另一个常规解决方案是吧父级文件改成 .mdx，像 React 组件一样 import 子文档，再内联渲染，你就能在同一个页面里拼装章节，同时保留侧边栏导航。\nimport Chapter1 from './rl/chapter-1.md';\rimport Chapter2 from './rl/chapter-2.md';\rimport Chapter3 from './rl/chapter-3.md';\r\u003cChapter1 /\u003e\r\u003cChapter2 /\u003e\r\u003cChapter3 /\u003e 写到这里就不可避免要碰 docusaurus.config.ts。classic 预设里 docs 与 blog 的配置都在这里：routeBasePath 决定入口路径，editUrl 可以连到 GitHub。\n想让数学公式在博客和文档里都生效，就把 remark-math、rehype-katex 同时塞进两个插件配置。\n导航栏是在 themeConfig.navbar.items 里布置的，像 type: 'docSidebar' 这样的条目，可以把整个文档侧边栏挂到顶栏里；而 to: '/blog' 则直接跳转到博客首页。\n说到博客，Docusaurus 会自动扫描 ./blog 目录，按照日期或 slug 生成路由。文章同样支持 Markdown、MDX，还有 authors.yml、tags.yml 管理作者与标签。比如这篇文章就使用 front matter 里的 tags: [docusaurus, docs, blog]，最终会渲染成标签页链接。\n写作体验跟文档差不多，但因为博客默认暴露 RSS、Atom 订阅，还能在配置文件里打开阅读时长统计，所以我常把学习心得、踩坑记录放在这里。\nMDX 则是打通 React 组件和 Markdown 的桥梁：你可以引入现成的组件，也可以在 src/components 写一个自定义卡片，然后在任意文档、博客中 \u003cHighlight\u003e 一下。如果要引用别的 Markdown 片段，直接 import Section from '../foo/bar.md'，接着 \u003cSection /\u003e，Docusaurus 在构建时会内联处理。这在编写重复的导言、FAQ 时很好用，既不会复制粘贴，也不怕链接失效。\n管理多章节文档时，我越来越喜欢把大纲拆成子目录，再在 _category_.json 里加入 generated-index。这样一来，访问目录本身就能看到自动生成的索引页，还能写简介。必要时也能通过 link 配置生成手写的 md 页面。随着文档越来越多，还可以在类别层级上应用 collapsed: false 让关键模块默认展开，读者体验会好很多。\n最后补一笔 React 组件的故事。Docusaurus 的主题本质是 React 应用，所以自带的布局、DocItem 等部件都能 override。只要在 src/theme 下按需复制对应组件，就能自定义渲染逻辑，例如替换文档页眉、给博客加上分享按钮。我自己常把一些重复的提示条、总结卡片抽成小组件，既能保持风格统一，又方便在 MDX 中复用。"},"title":"在 Docusaurus 3 里搭建内容生产流水线"},"/blog/2025/embodied-jobs/":{"data":{"":"灵感主要来源于这篇26二硕具身秋招小结的帖子，因为以前我只知道互联网开发岗的实习和工作，对其他领域实在未知且畏惧。这篇帖子最好的就在于博主是半路出身，和我基本一模一样。将来我无论是去港科广还是港中文，提前规划自己的实习或科研都是必要的。将来的工作，如果具身找不到，也可以往多模态转，像是LLM应用开发之类的岗位。\n还有一些更多的内容，基本上就是按照具身、秋招、实习之类的关键词在社交媒体上检索，下面分别是：","a-具身算法岗-the-dream-job#A. 具身算法岗 (The Dream Job)":"JD高频词： Imitation Learning (模仿学习), PPO/RL, VLA, Diffusion Policy, Sim2Real.\n现状判定： 也许懂深度学习基础，但可能没跑过Diffusion Policy。\n补课重点： 去看 ACT (Action Chunking with Transformers) 和 Diffusion Policy 的开源代码。这是目前最火的两个模仿学习基线。","b-机器人软件岗-the-engineer-job#B. 机器人软件岗 (The Engineer Job)":"JD高频词： ROS 2, Linux, C++, Docker, Python, SLAM, Planning.\n现状判定： 本科做过Demo，但代码规范吗？会用CMake吗？懂进程间通信吗？\n补课重点： ROS 2 Humble。不要再学ROS 1了。学会如何在一个Docker容器里配置好所有环境，这是多人协作的基础。","c-仿真与合成数据岗-the-hidden-gem#C. 仿真与合成数据岗 (The Hidden Gem)":"JD高频词： Isaac Gym, Isaac Sim, Isaac Lab, MuJoCo, Python.\n现状判定： 这是一个非常适合BME转行的切入点。\n补课重点： NVIDIA Isaac Lab (Isaac Gym的继任者)。现在的具身智能全是靠仿真数据堆出来的。如果能熟练搭建一个\"机器狗在火星表面行走\"的仿真环境，就是抢手货。","上海人工智能实验室招聘#上海人工智能实验室招聘":"机构介绍： 上海人工智能实验室具身智能中心，面向国家具身智能领域的重大需求，以构建\"一体、可泛化的具身人工智能系统\"为目标。科研方向涵盖具身生成与数字化、具身智能大模型、人形机器人与运动智能、世界模型、具身多模态感知、物理仿真等。\nBase： 上海-徐汇滨江。校招、实习、社招都招。\nGitHub： Intern Robotics，建议准备简历的同时先关注相关仓库，欢迎 star 和提出宝贵意见。","下一步#下一步":"目前处于\"信息过载但行动迷茫\"的状态。\n现在的核心任务不是海投，而是\"备弹\"。手中的Unitree机器狗就是核武器。大多数CS专业的学生只能在仿真里跑代码，而有真机，这意味着Sim2Real经验将是真实的，这是最大的竞争力。要做的就是毕设所言的面向JD的Unitree机器狗全栈项目。","两条发展路线#两条发展路线":"两周后的面试将决定未来两年的主线任务。无论结果如何，利用大四做项目是公约数，但在侧重点上需要完全不同的打法。","中层小脑层---运控rl#中层（小脑层 - 运控/RL）":"代表机构： 宇树（Unitree）、小鹏（Robot Center）、以及大多数人形机器人初创。\n特征： 核心是 Sim2Real（仿真到真机）。PPO算法、Isaac Gym/Lab仿真平台是标配。\nBME的策略： 这是主战场。有工程背景，理解物理世界（力、摩擦、惯性），这比纯CS背景的人更有优势。","主要岗位类型#主要岗位类型":"校招/社招/实习岗位包括：\n具身智能-AIGC青年研究员\n研究方向：三维重建与生成、几何处理、三维物体/人体生成、运动捕捉 要求：硕士+，有三维视觉经验，顶级会议论文优先 具身智能-仿真平台青年研究员\n研究方向：具身控制、大模型驱动的智能体、Sim-Real transfer 要求：硕士+，熟悉IsaacGym/Sim、Gibson、Habitat、Mujoco等仿真平台 具身智能-人体运动策略青年研究员\n研究方向：数字人/人型机器人/灵巧手运动控制 要求：博士，熟悉生成模型、动捕动画、物理仿真、大模型 具身智能-具身智能大模型青年研究员\n研究方向：VLA模型框架设计、模型优化、数据生成、持续学习 要求：硕士+，1年以上经验，熟悉Omniverse等仿真平台 具身智能-足式机器人青年研究员\n研究方向：双足/四足机器人运控算法 要求：硕士+，有足式机器人项目经验，熟悉运动学动力学 具身智能-强化学习青年研究员\n研究方向：机器人运控与操作、现实世界强化学习 要求：硕士+，精通PPO、SAC、DDPG等算法 实习岗位主要类型：\nAIGC算法实习生：三维重建/生成方向，要求博士/硕士，6个月以上 机械臂操作算法实习生：VLA模型、Sim-to-Real，本硕博均可 具身智能大模型算法实习生：大模型框架设计、模型优化，本科+ 人体运动策略算法实习生：物理仿真、动作生成，博士/硕士，需发表论文 仿真平台算法实习生：本科+，有数据可视化经验优先 足式机器人算法实习生：双足/四足运控算法，本硕博均可 强化学习算法实习生：RL前沿技术研究，本硕博均可 仿真-渲染研发实习生：3D重建大模型、物理引擎优化，需并行计算经验 仿真平台系统研发实习生：系统架构设计、分布式计算，需开源项目经验 社招岗位主要类型：\n机器人操作工程师：仿真引擎平台搭建，硕士+，有工作经验 开放平台产品经理：产品设计、需求挖掘，有技术类产品经验优先 强化学习青年研究员：RL前沿研究，硕士+，有论文发表 强化学习算法工程师：大规模并行仿真环境构建，硕士+，有项目经验 机械臂操作青年研究员：VLA模型、端到端操作，博士，1年以上研究经历 注： 所有岗位均支持校招、实习、社招。具体岗位详情建议关注Intern Robotics GitHub","互联网大厂#互联网大厂":"日期 公司 职位方向 类型 2025.11.28 京东物流 具身智能 寒假实习生 2025.11.24 阿里巴巴达摩院 具身智能招聘(杭州/北京) 全职/实习 2025.11.24 高德视觉技术中心 世界模型 全职（学历不限） 2025.9.29 TikTok 多模态基础模型\u0026多模态推荐大模型/MLLM合成数据研究型/大模型后训练RL 社招/校招/实习生 2025.10.13 蚂蚁天玑实验室 机器人算法专家/具身智能遥操作系统开发工程师 实习 2025.8.28 蚂蚁集团 语言模型/大模型系统/具身智能/多模态/大模型基础设施/AI安全攻防 2026届校招 2025.8.20 阿里达摩院具身智能大模型团队 大模型的具身智能大脑/世界模型与VLA方向 实习 2025.9.3 阿里达摩院 具身智能招聘大模型、多模态方向 实习 2025.8.19 蚂蚁集团 具身智能方向财务专家 全职 2025.8.14 腾讯RoboticsX实验室 具身智能方向 实习生 2025.7.21 微软 图像/视频/3d生成or pretraining/posttraining相关 实习 2025.7.14 阿里星顶尖人才计划 基础模型/Al Infra/大模型应用/产业AI/计算架构等方向 2025/2026届本硕博毕业生 2025.7.10 腾讯 技术研究-机器学习/机器人/多模态方向 实习 2025.7.2 字节跳动 机器人研究员/机器人运动控制算法工程师/机器人多模态大模型算法工程师/NLP算法工程师-应用算法 - 2025.7.2 Momenta(北京/上海/苏州) 端到端大模型算法工程师 面向26届博士 2025.6.24 蚂蚁集团(上海) 整机硬件产品专家 具身智能方向 全职 2025.6.6 京东探索研究院(北京亦庄) 具身智能VLA/自动驾驶VLA 社招|校招 2025.6.6 三星电子(北京) VLA/RL/点云处理 社招/校招 2025.6.6 微软亚洲研究院机器学习组工业创新中心 具身智能 实习 2025.6.6 蚂蚁集团(上海) 具身感知 实习 2025.6.6 京东TGT顶尖青年技术天才计划 包含具身岗位 校招/实习 2025.6.5 Figure AI Training Infra/Large Scale Training/Large Scale Model Evals/Reinforcement Learning Full Time 2025.5.29 英伟达(北京) 大规模数据集/仿真benchmark/算法部署 实习 2025.5.29 百度(北京/上海/深圳) 多模态算法/自动驾驶感知算法/深度学习决策规划算法 实习 2025.4.25 蚂蚁集团PlanA人才专项 - - 2025.4.12 三星中国研究院 Robot Leading Scientist - 2025.4.8 字节跳动实习生招聘——筋斗云人才计划 - - 2025.4.8 小米集团2026届转正实习招聘 - - 2025.4.7 京东探索研究院具身智能团队 VLA算法研发 实习 2025.4.3 Intel(北京) Robotic System Research Intern 2025.4.2 美团大模型北斗实习计划2025 - 截止日期7月31日 2025.3.29 商汤2025春季校招 - - 2025.3.28 蚂蚁集团 具身算法/运动控制 算法工程师 2025.3.28 字节跳动 SLAM/运动控制/移动规划 算法工程师 2025.3.28 Vivo 机器人首席科学家 - 2025.3.23 Waymo Deep Learning \u0026 Modeling Research Summer Intern 2025.3.23 Waymo World Modeling Research Scientist 2025.3.23 NVIDIA Foundation Model Training Infrastructure Senior Research Engineer 2025.3.23 Google DeepMind Robotics Research Scientist 2025.3.23 华为中央研究院类脑计算团队(北京/上海/杭州/南京/合肥/深圳) 具身智能 算法实习生 2025.5.15 Vivo Lab(上海) 技术规划工程师 实习 2025.3.23 NVIDIA Generalist Embodied Agent Research Research Scientist (New College Grad 2025) 2025.3.23 京东探索研究院(北京亦庄京东总部) 具身智能/人形机器人 实习生 2025.3.15 Amazon Robotics (Germany) Motion Planning \u0026 Control Applied Scientist 2025.3.15 Huawei (Munich, Germany) Robot Learning Industrial PhD（中国学生可能因签证问题被拒） 2025.3.12 Qualcomm Netherlands Reinforcement learning/MPC/differentiable world models Intern 2025.3.12 Microsoft UK (Cambridge) Robotics Intern 2025.3.8 Microsoft (Redmond, Washington, US) Spatial AI Research Intern 2025.3.6 三星电子中国研究院(北京) 机器人/具身智能算法 实习 2025.2.23 微软亚洲研究院 具身智能算法实习生 实习 2025.2.19 蚂蚁集团 人型机器人工程师 (上海浦东/上海黄埔/杭州西湖) 请在猎聘搜索 2025.2.19 Amazon Robotics Germany - PostDoc Scientist 2025.2.19 Qualcomm Automotive Engineering Internship - 2025.2.16 Toyata US Multimodal Learning Intern 2025.2.16 Waymo US Planning Selection PhD Intern 2025.2.16 Samsung US Embodied Intelligence Intern - 2025.2.15 NVIDIA PhD Research Intern - Robotics and/or Autonomous Vehicles Remote! 2025.2.15 NVIDIA Principal Autonomous Vehicles Engineer - Mapping and Localization - 2025.2.13 Amazon Robotics - Intern/FullTime 2025.2.12 字节跳动机器人 - 实习/全职 2025.2.11 阿里巴巴达摩院视觉技术实验室 VLA方向 研究实习生","信息来源#信息来源":"知乎：\n【具身智能】招聘帖：校招、实习、社招 小红书：\n26二硕具身秋招小结 具身秋招小结 其中知乎和小红书的垃圾信息最多，容易陷入其中无意义的刷，这里就暂时贴这些\nGithub：\n具身智能招贤榜 CC98：\n【求职广场】offer帮选 具身or互联网 【求职广场】机械-\u003e具身智能入坑指南 (附公司红黑榜) 【求职广场】offer犹豫，二选一，具身人形 【实习兼职】具身智能实习生（模仿学习） 【求职广场】具身菜博offer预选（拼尽全力版） 【求职广场】（10.20更新）具身智能方向还能投什么？一图速览 【求职广场】具身老博offer二选一（大结局） 【求职广场】本科毕业可以做具身智能吗","值得投递梯队榜#值得投递梯队榜":"基于提供的列表，结合地理位置（深圳/香港），整理了一份\"梯队榜\"：\n筛选标准： Base大湾区优先 + 有AI/具身基因 + 薪资尚可 + 对转行友好。\n梯队 公司/机构 关键词与评价 行动建议 T0 (灯塔级) 腾讯 Robotics X 极难，技术最强，学术圈认可度高。科研路线必投。如果没有顶会，大概率简历挂，但值得一试。 科研路线必投 T0 (灯塔级) IDEA 研究院（粤港澳大湾区数字经济研究院） 沈向洋带队，VLA很强。极佳的跳板。这里有很多港科/港中文的学生实习，学术资源好。 极佳的跳板 T0 (灯塔级) 华为 (2012/制造部) 薪资天花板，但分工极细。慎重。制造部可能偏传统，天才少年计划难度太高。可关注其具身智能应用组。 可关注其具身智能应用组 T1 (独角兽) 小鹏 (Robot Center) 深圳具身龙头，做人形，薪资55w+ (红榜)。重点目标。他们非常缺懂\"运控+AI\"的人。BME背景在这里不减分。 重点目标 T1 (独角兽) 大疆 (DJI) 难度极大，但技术栈极佳。可以投，但大疆更偏向极致的工程控制，对纯End-to-End AI持保留态度。 可以投，但需注意匹配度 T1 (独角兽) Unitree (宇树) 使用的设备厂商，虽然总部在杭州，但行业地位高。利用优势。如果能用他们的狗做出比官方Demo还牛的功能，直接发邮件给CTO。 利用优势 T2 (潜力股) 众擎机器人 人形，初创，势头猛。刷实习首选。初创公司流程快，能接触核心代码。 刷实习首选 T2 (潜力股) 帕西尼 感知触觉传感器+灵巧手。刷实习首选。BME背景懂传感器（Sensor），这里可能是降维打击区。 BME背景的降维打击区 T2 (潜力股) 越疆 / 普渡 机械臂 / 移动底盘。保底选择。商业化成熟，适合去学习规范的工程开发流程。 保底选择","其他教育投资内容医疗等#其他（教育/投资/内容/医疗等）":"日期 机构 职位方向 类型 2025.11.24 元枢智汇(上海) AI数据开源社区运营 全职 2025.11.24 上海人工智能实验室具身智能中心 具身开源社区运营 - 2025.11.24 秦皇岛中秦智能装备有限公司(秦皇岛) 机械研发/机器人 全职 2025.11.24 长智具身智能(海南三亚) 销售代表/解决方案工程师/研发工程师/数采员 全职 2025.11.24 新生纪智能科技有限公司(德国 可远程) FAE工程师 全职 2025.11.24 西湖机器人(杭州) 强化学习/具身智能算法/虚幻引擎/VR/SLAM/机械结构/市场 全职 2025.11.24 南洋理工大学LinsLab(新加坡) 灵巧操作/VLA/世界模型 26Fall PhD 2025.10.19 深蓝学院 机器人算法实习生 实习 2025.10.14 深蓝学院 人工智能教育产品经理/人工智能教研 实习 2025.10.18 Motphys 初级引擎开发工程师/具身智能场景美术/具身智能技术应用工程师 全职/实习 2025.10.10 Motphys(武汉) 仿真解决方案/工具链/视觉感知/仿真资产制作/产品经理/仿真训练/物理引擎开发 全职 2025.9.30 NVIDIA（北京） 三维重建/世界模型/VLM/VLA Solution Architect全职 2025.9.26 中金公司 AI\u0026机器人产业研究 实习 2025.9.17 辉羲智能 AI编译器工程师/专家、NPU算子开发工程师/专家 全职 2025.9.17 Leading Future 多模态大模型科学家 全职 2025.9.11 凤麟核集团 具身智能机器人项目经理/AI医疗产品经理/AI医学影像软件工程师 全职 2025.9.10 香港大学MMLab 具身智能内容运营(深圳) 实习/全职 2025.9.10 香港大学MMLab 机器人数据工程师(深圳) 实习/全职 2025.9.10 香港大学MMLab 深度学习平台工程师(深圳) 实习/全职 2025.9.10 香港大学MMLab 腿足/人形全身控制算法专家(深圳) 全职 2025.9.10 香港大学MMLab 科研采购工程师(深圳) 全职 2025.9.10 香港大学MMLab 深度学习推理工程师(深圳) 全职/实习 2025.8.19 自动驾驶之心 内容运营(自驾/大模型/具身相关研究方向) 实习 2025.8.16 Leading Future 多模态大模型科学家（Embodied AI / Robotics Foundation Model） 全职 2025.8.15 深蓝学院 机器人算法实习生/机械臂研发实习生 实习 2025.8.9 智子力控(宁波) 机器人电气与控制工程师(ROS和柔顺控制) 全职 2025.8.7 稳正资产 智能硬件投资总监/具身智能投资总监 社招 2025.8.1 天奇股份 机器人视觉工程师/机器人工程师/机器人销售经理/机器人数据采集工程师 社招 2025.7.28 亮源新创(北京/上海/深圳) 产品经理 全职 2025.7.26 中能坤域科技控股(浙江)有限公司 具身智能/产品研发/产品运营等相关岗位 社招 2025.7.16 小米(北京) 机器人多模态大模型研究专家 急招全职 2025.7.7 稳正资产 智能硬件投资总监/具身智能投资总监 社招 2025.7.2 机器人算法实习生(具身智能方向) - 实习 2025.6.24 小米(北京) 具身智能/世界模型 暑期实习 2025.6.11 具身智能之心 课程讲师/硬件开发者 - 2025.6.6 具身研习社 具身智能机器人深度内容作者 - 2025.5.21 中金研究院(北京) 具身智能产业研究 实习 2025.4.20 北京中关村学院 具身智能方向 研究员/工程师/实习生 2025.4.10 北京中关村学院 全球招募副院长/助理院长 - 2025.4.10 中关村人工智能研究院 2025年超能实习生 - 2025.4.6 音波迭代Embodied Pioneering 一级市场具身智能方向 投资实习生 2025.3.28 小米 传统运动控制/强化学习运动控制/SLAM 算法工程师 2025.3.28 海尔 SLAM/运动控制/抓取/视觉 算法工程师/算法总监 2025.3.29 汇川技术 传统运动控制/仿真/电机控制 算法工程师 2025.3.23 国金证券具身智能组 人形机器人板块 实习生(可远程可留用) 17家国企/央企/研究院岗位包括：\n上海电气中央研究院 - 具身智能机器人算法/增材仿真/模拟计算/优化算法 - 校招 中国电信人工智能研究院(TeleAI) - 具身智能大模型/具身仿真/具身硬件/机器人操作控制/具身感知与规划/灵巧操作/嵌入式开发 - 校招 中兵智能创新研究院 - 机器人算法研究员 中兴通讯 - 强化学习/视觉大模型/人形机器人机电 - 校招 中国移动具身智能产业创新中心 西安航天自动化 - 机器人开发工程师 - 校招 国核电站运行服务技术有限公司 - 机器人电气工程师 中科航天人才服务有限公司 - 人形机器人研发总监 南方海洋科学与工程广东省实验室 - 水下机器人运动控制 新兴际华北京智研院 - 智能机器人专业总工/总师","具身智能岗位三层架构#具身智能岗位三层架构":"总结上述的内容，可以总结目前的具身智能岗位不再是浑浑噩噩的一团，而是清晰地分为了三层：","内容精选#内容精选":"","基座层工程层---部署基础软件#基座层（工程层 - 部署/基础软件）":"代表机构： 各大机器人公司的\"工程落地组”、自动驾驶公司的\"工具链组\"。\n特征： ROS2、C++、Docker、通信协议、传感器驱动。\nBME的策略： 这是保底牌和敲门砖。很多算法岗其实都需要强工程能力来落地。","学术研究机构#学术/研究机构":"日期 机构 职位方向 类型 2025.11.28 UC San Diego Biwei Huang组 因果驱动的世界模型 PostDoc 2025.11.28 浙江大学机器人与物理智能实验室 机器人控制 RA 2025.11.28 中科院自动化所水下机器人团队 水下机器人 实习 2025.11.28 普林斯顿PRISM实验室 Robot Foundation Models/Interaction and Autonomous Improvement/Open-World Systems 全奖PostDoc/PhD 2025.11.26 灵心巧手(北京大钟寺) 学术合作 实习生 2025.11.24 上海人工智能实验室 具身手术机器人 实习 2025.11.24 清华深研院江勇课题组 研究型实习生 实习 2025.11.24 上交ScaleLab 具身仿真方向 实习 2025.11.24 港中文李钟毓组 人形/灵巧手/多智能体 PhD/PostDoc 2025.11.24 北京大学先进制造与机器人学院庞智博组 - PostDoc 2025.11.24 浙江大学高飞组 - 科研实习 2025.11.24 华东师范大学计算机科学与技术学院 具身智能方向学者 - 2025.9.29 上海交通大学自动化与感知学院 机器人研究和应用方向 科研助理 2025.9.25 北京人形机器人创新中心 具身仿真链路 实习 2025.9.24 MBZUAI 机器人系RCL实验室 机器人/混合现实/人工智能 博士/硕士/访问学生/实习生 2025.9.22 长三角国创中心智慧农业机器人研究所 具身智能机器人系统工程师/数据工程师 社招 2025.9.9 MBZUAI 机器人系RCL实验室 机器人/混合现实/人工智能 博士/硕士/访问学生 2025.9.5 南京大学机器人与自动化学院空中机器人课题组 空中机器人设计与自主导航 实习/全职 2025.9.2 上海交通大学ScaleLab - 科研实习生 2025.9.2 中科院自动化所多模态人工智能系统全国重点实验室 具身设计/感知智能系统/灵巧操作/多模态大模型 实习 2025.9.1 朗毅机器人 SLAM算法/AI视觉算法/结构设计 全职/实习/2026校招 2025.8.29 香港大学MMLab 人形机器人全身控制 RA 2025.8.28 西湖大学机器智能实验室（MiLAB） 机器人具身大模型/深度强化学习（3-5人） 科研助理 2025.8.27 灵巧智能 灵巧操作基础模型 研究实习 2025.8.24 北京理工大学（珠海）跨域智能无人团队 AI机器人与具身智能 国家高层次人才/准聘教授/博士后 2025.8.18 中豫具身智能实验室 科研岗位 博士 2025.8.17 北航国新院杨顺昆老师 软件工程/具身智能方向 博士/硕士 2025.8.17 香港科技大学（广州）钟秉灼老师 具身智能安全/信息物理系统控制和形式化方法 博士后/全奖博士生/研究助理 2025.8.20 北大信研院应急管理具身智能联合实验室 具身智能算法 实习生 2025.7.27 香港大学 具身智能方向 博士生 2025.7.25 NUS CLeAR Lab 机器人操作方向 全职/兼职 2025.7.18 NUS CLeAR 实验室 机器人操作 全职/兼职/实习 2025.7.17 浙江大学 FAST LAB 实验室 「机器人打羽毛球」和「水上漂机器人」项目招聘科研助理 实习 2025.7.16 荷兰特文特大学手术机器人实验室 手术机器人 博士研究生 2025.7.16 ELIXIR Lab 软体机器人的具身协作操控研究 博士后 2025.7.11 香港科技大学郭嵩教授 大模型/多模态等方向 博士/RA/博后 2025.7.9 北京大学计算机学院张史梁老师课题组 具身智能/多模态大模型轻量化/AIGC 实习生 2025.7.8 香港理工大学校长青年学者吴郁杰老师 脑启发AI/脑认知科学 2-5名全奖博士生 2025.7.8 香港科技大学高揚教授 空间机器人中的人工智能/用于空间可持续性的机器人系统/主动太空碎片清除技术（ADR） 3名博士后 2025.7.2 同济大学设计创意学院CDI数字创新中心 机器人终端用户编程/大模型驱动/具身机器人交互/Unity开发 实习生 2025.7.2 清华大学设计未来课题组 具身智能机器人开发与设计 实习生 2025.6.30 加州大学洛杉矶分校Bolei Zhou老师 计算机视觉/机器自主系统 博士/visiting students 2025.6.25 香港中文大学李钟毓组 VLA/Humanoid/Control Theory/Design PhD/Postdoc/Intern 2025.6.25 TU Eindhoven (Netherlands) Dynamic Manipulation in Semi-Structured Industrial Settings PhD 2025.6.25 KTH (Sweden) Industrial automation and intelligent robotics Assistant Professor 2025.6.24 北京大学董豪组/UC Berkeley 计算机视觉、机器人和具身智能 RA 2025.6.24 University of Luxembourg SLAM \u0026 Situational Awareness for Robotics PostDoc 2025.6.24 NTU Medical Robotics RA 2025.6.24 Harvard University (US) Postdoctoral Fellow in Robotics PostDoc 2025.6.24 Lule? University of Technology(Sweden) Field Robotics with a focus on Extreme Environments PhD 2025.6.24 Lule? University of Technology(Sweden) Data-driven and learning based control PhD 2025.6.24 King’s College of London robotics and neuro-technology Research associate 2025.6.24 NTU Acoustic Soft Robotics Research Fellow 2025.6.24 NTU Fluidic Robotics Research Fellow 2025.6.24 北大-灵初联合实验室 具身智能算法 实习 2025.6.24 南洋理工PineLab王子为组 具身基础模型学习/具身策略迁移部署/具身模型高效推理 PostDoc/PhD/RA 2025.6.24 清华深圳研究院王智组 空间场景数据标注与构建/VLA空间推理部署与验证 实习 2025.6.24 上海交大黄涛组 跨本体VLA/VLA推理加速/具身世界模型/仿真平台和数据生成 实习 2025.6.24 港科大郑展鹏组 多旋翼无人机的开发和应用/基于水下机器人的水下目标识别和三维重建 PostDoc/PhD/RA 2025.6.24 南洋理工大学Chuanxia Zheng组 三维重建与数字孪生 PostDoc/PhD/Intern/RA 2025.6.24 南洋理工大学王林组 多模态人工智能/基于生物激发的多模态传感融/面向操作与感知的机器人学习 PhD/PostDoc/访问博士 2025.6.11 意大利技术研究院IIT Arash Ajoudani组 VLM+机器人 全奖PhD 2025.6.11 香港大学Zhongliang Jiang组 Robotic Learning/Autonomous Surgical Robotics 博士/研究助理/访问学者 2025.6.11 上海交通大学汶川组 机器人控制策略模型/硬件设计/机器人系统 PhD 2025.6.11 香港科技大学人工智能机器人与空间可持续性研究中心 AI for Space Robotics/Robotic Systems for Space Sustainability/Active Debris Removal PostDoc/研究助理 2025.6.6 同济大学齐鹏组 具身智能血管介入手术机器人/多模态医学影像人工智能 PostDoc 2025.6.6 上海交通大学SCALE LAB 人形机器人/双臂协作VLA/RoboTwin仿真开发 实习 2025.6.6 University of Southampton Multimodal Large Language Model in Human-Robot Interaction Funded PhD 2025.6.6 University of Oxford Multi-collaborative scouting and mapping with a team of highly mobile robots Funded PhD 2025.6.6 University of Southampton Soft Robots with Integrated Sensing, On-Demand Therapy and AI-assisted Control Funded PhD 2025.6.6 University of Bristol Dexterous 3D-printed Robot Hands Funded PhD 2025.6.6 University of Glasgow AI-Powered Resilient Teleoperation for Autonomous Robotics Funded PhD 2025.6.6 University of Manchester/University of Glasgow/University of Oxford Robotics and AI for Net Zero Funded PhD 2025.6.6 University of Bath Modelling and analytical framework for developing dexterous soft robotic manipulators Funded PhD 2025.6.6 MBZUAI(UAE) Robotics and AI Funded PhD 2025.6.6 The University of Manchester AI Driven Robotics for Intelligent Construction PhD 2025.6.6 The University of Manchester Cloud robotics/Networked Systems Funded PhD 2025.6.5 Heriot Watt University 华威大学 (UK) Chair in Artificial Intelligence - 2025.5.30 香港科技大学徐英豪组 具身智能/3D重建与生成/世界模型 PhD/PostDoc/RA 2025.5.15 卡迪夫大学玛丽居里学者项目(英国) SLAM/强化学习/可微分物理 博后 2025.5.15 港科广褚晓文李昊昂组 具身智能 25Fall or 26 Fall PhD/RA 2025.5.15 复旦大学智能人机交互实验室 可穿戴AGI/开源具身智能 2026级硕士博士 2025.5.15 上交李永露卢策吾组RHOS 具身智能/人机协同学习 博士/硕士/实习 2025.5.15 北京大学智能学院钟亦武组 多模态推理/具身智能 博士 2025.4.28 南科大机械系STAR课题组 空中机器人/具身智能 PhD 2025.4.27 哈工大深圳研究院杨硕组 具身智能/多模态模型 PhD 2025.4.23 KTH robotics with specialization in visual domain adaptation PostDoc 2025.4.11 香港岭南大学陈曦组 软体机器人 PhD 2025.4.10 KTH Robotics and geometric machine learning PhD 2025.4.10 香港科技大学（广州）李昊昂组 具身智能操纵 PhD 2025.3.29 北京大学Hao Tang组 Embodied AI/AIGC PostDoc/PhD/Master/Intern 2025.3.28 KTH Machine Learning and Robotics Postdocs 2025.3.28 BMW Group (Munich) Neuromorphic Multimodal Perception and Learning Intern 2025.3.28 Lule? University of Technology (Sweden) Robotics and Artificial Intelligence WASP PhD 2025.3.28 Toyota US Robotics \u0026 Machine Learning for Human-Robot Interaction and Intelligent Vehicles Research Engineer 2025.3.26 University of Nottingham Soft Robotics and Wearables PhD (UK students only) 2025.3.26 University of Surrey Neuro-muscular state estimation and control for physical human-robot interaction PhD 2025.3.26 North Dakota State University Precision Agriculture \u0026 Robotics PhD 2025.3.26 Institut de Robòtica i Informàtica Industrial (Spain) Learning robot behaviors in collaborative manufacturing of large components PhD 2025.3.25 KTH Robotics with spatial understanding and Modelling PostDoc 2025.4.6 Imdea Materials (Spain) Collaborative robots and laboratory automation in materials discovery Intern 2025.3.23 中南大学 医疗多模态大模型和具身智能 2名博士补录 2025.3.23 西交利物浦大学/中科院沈阳自动化所 具身智能 联培PhD 2025.3.23 University of Surrey Evaluating the next generation of warehouse robotics using generative world models PhD 2025.3.23 Horizon Europe MSCA-DN Project AIGreenBots robotics and AI for agricultural robotics 11 PhD Positions 2025.3.23 哈工大深圳 微型机器人 博士(3月31日前急招) 2025.3.23 NTU Multi-Sensor Fusion in Mobile Manipulation RA 2025.3.23 Singapre Institute of Technology Robotics \u0026 Automation Professor 2025.3.22 MBZUAI (Abu Dhabi, United Arab Emirates) Robotics Assistant, Associate and Full Professor 2025.3.22 南京大学智能科学与技术学院 具身智能 副教授/助理教授 2025.3.21 KTH Royal Institute of Technology Research engineers within Robotics - 2025.3.17 清华大学深研院王智组 三维视觉与具身智能 实习生 2025.3.17 清华大学脑与智能实验室 机器人硬件/VLA训练和部署/人机交互与心智理论 实习 2025.3.17 University of Sheffield Deep Reinforcement Learning with Interactive Human Feedback PhD 2025.3.17 西交利物浦大学(苏州) 多模态脑机接口大模型 博士 2025.3.17 南京大学单彩峰司晨阳组/南洋理工(NTU)刘子纬组 具身视觉理解 联培博士生/硕士生/实习生 2025.3.16 南京大学龙霄潇组 3D生成/自动驾驶世界模型/3D视觉 Master/PhD/Visiting students 2025.3.16 OpenAI (US) Mechanical Architect/Research Engineer/Systems Integration Electrical Engineer, Robotics FullTime 2025.3.16 中国人民大学信息学院AIM3实验室 具身智能/多模态 博士/硕士/访问学生 2025.3.16 Georgia Tech Lunar Lab Robot Learning PostDoc/PhD/Master/Undergraduate 2025.3.16 University of Nottingham Soft Robotics and Wearables PhD (UK students only) 2025.3.16 New York University Abu Dhabi (NYUAD) Automation, Robotics, and AI Research Instrumentation Scientist 2025.3.16 西交利物浦大学(苏州) 机器人学院 R1研究员 2025.3.16 Cranfield University (UK) Robust Motion Planning for Hopping Robots PhD (self-funded, 3-year) 2025.3.16 Princeton University Construction Robot PostDoc 2025.3.15 Cambridge University (Rika Antonova team) robot learning / robot hardware design / reinforcement learning Fully-funded PhD 2025.3.15 Cranfield University (UK) Risk aware planning for multi agent systems PhD (self-funded, 3-year) 2025.3.15 University of Liverpool Adaptive Robotic Chemists for Resilient Pharmaceuticals PhD 2025.3.15 ETH Artificial Visual Intelligence group (AVI) Computer Vision for Embodied AI PhD（对中国学生有限制） 2025.3.15 Chalmers University of Technology (Sweden) Dynamics and Control of Legged Robots PhD 2025.3.15 Technical University of Denmark (DTU) Active Perception and End-to-End AI-driven Intuitive Inspection for Autonomous Aerial Robots PhD 2025.3.12 克莱姆森大学(美国)Luyang Zhao组 机器人学 PhD 2025.3.11 ?rebro University (Sweden) Embodied Learning PhD 2025.3.11 香港科技大学(广州)许人镜组 强化学习/仿生机器人运动控制/具身智能芯片 PhD 2025.3.10 ETH Stretchable Optical Skin Postdoc 2025.3.10 Singapore Institute of Technology Assistant Professor / Associate Professor in Robotics - 2025.3.9 NTU Energy Research Institute (南洋理工能源研究所) Computer Science/Robotics/Automation Research Engineer 2025.3.9 Loughborough University (UK) Agri-Robotics Research Associate 2025.3.7 清华大学协同交互智能研究中心周伯文组 人机/多智能体协同 PhD / Visiting Student 2025.3.7 Arizona State University - Bo Liu’s Group - PostDoc 2025.3.6 Tampere University (Finland) Robotics and AI Postdoctoral Research Fellow 2025.3.6 香港科技大学(广州)李昊昂组 具身智能(人形机器人/机械臂) PhD 2025.3.6 香港大学刘希慧组 - PhD/RA 2025.3.5 MIT Soft and Micro Robotics PhD 2025.3.2 CMU Robotics Institute - Biorobotics Lab - Research Associate 2025.3.2 Lule? University of Technology (Sweden) Robotics and AI Senior Lecturer 2025.3.2 NTU南洋理工 insect-machine hybrid robot Research Assistant 2025.3.2 NTU南洋理工 - Robotics Research Associate 2025.2.28 香港中文大学(深圳) 医疗具身智能 PhD/PostDoc 2025.2.27 Eindhoven University of Technology Mechanical Contact Information Processing of Soft and Large-area Robot Skin PhD 2025.2.26 宁波东方理工大学 机器人与控制 博士后 2025.2.22 The University of Western Australia Lecturer in Automation and Robotics - 2025.2.22 University of Luxembourg Research Associate in Space Robotics Manipulation - 2025.2.19 香港大学MMLab罗平组 \u0026 深圳星际光年 - 实习生/联培PhD 2025.2.19 Washington State University Robotics Planning PhD/Master 2025.2.19 港科广 三维空间感知/运动估计/导航 PhD 2025.2.19 University of Lincoln Robotic Fleet Coordination PostDoc / Research Associate 2025.2.18 Wageningen University (Netherlands) Robotics Sensor Fusion and Robotics Transferable skills PhD 2025.2.18 Technical University of Munich Space Robotics or GNC PhD/PostDoc（不推荐中国学生） 2025.2.17 University of Nottingham Robotics Manipulation Learning PhD 2025.2.16 KTH Research Engineer in Robotics, Perception and Learning - 2025.2.16 KTH - PhD Social Robotics 2025.2.15 ETH Zurich \u0026 Tethys Robotics Robotics Hardware Engineer - 2025.2.14 University of Surrey Robotics Lecturer -","实习机会#实习机会":"三一集团耘创新实验室（杭州）- 具身智能算法实习生\n研究方向： 模仿学习、VLA 团队背景： 顶级会议论文30+篇（NeurIPS、ICML、ICLR、CVPR等） 资源： 充足尖端显卡 工作内容： 智能体策略模型研发、算法研究、系统验证 要求： 研究生在读，精通Python/C++，熟悉Pytorch/TensorFlow，有模仿学习基础 联系方式： yunze.pan@irootech.com","当前困境与策略#当前困境与策略":"现在的一个问题是，本科大四空有时间但是无法投递实习——这里选择的领域虽然新兴，但是要求也高，很多都是得在读研究生期间才行。因此最初拟定的策略是大四就忙三件事：\n毕设（论文） 申请季（找硕士读） 知识准备（面向JD/PhD） 但是未来其实也有很多路可以走，这里也可以考虑论文之类的，但是对口实习至少得1份，最好有2份。论文这一块至少现在的不确定性比较大，将来如果进入港科广获得MPhil的机会那就可以往这方面努力，只刷1份实习；如果面试失败的话那就老老实实港中文港深通勤深圳刷2份实习。关键的问题是第一份实习要怎么才能拿到手，这里得面向JD努力，或者找一个初创类似性质的地方去，尽可能不要太底层Robotics（但是基础知识又不能不掌握，主要是单纯Robotics的不太好迁移到LLM/VLM之类的工作上去）","技术方向建议#技术方向建议":"ZJU机械出身学长转具身的推荐路线： 以VLA算法为主，主打综合全栈能力，通过错位竞争避免与纯算法同学硬拼。\n四个主要技术方向：\nVLA/WA（决策层）\n目前主流demo是VLA，框架分为分层式和一段式 本质是VLM的posttrain，大规模pretrain学术界资源有限 掌握diffusion用于生成轨迹 **WA（World Action）**算力需求更高，需生成符合物理规则的动作并对齐视频-action gap，目前较遥远 RL/IL（运控层）\nPPO仍是主流，RL发展相对缓慢 全身运控效果出色，固定任务上成功率有优势（RL100） 重点掌握：PPO、SAC、TD3 VLA+RL是长远趋势，工厂场景需要few-shot和zero-shot 纯RL运控竞争激烈，薪资不如以前高 嵌入式\n掌握模型轻量化和部署流程（ROS 2节点封装） 确保算法在边缘设备（如Jetson）实时运行 掌握CAN和485通信，能写PID和MPC 结构设计\n使用SolidWorks，手搓滚珠丝杠、连杆、线驱系统 液压已过时，现基本为电机驱动 重视演示能力（“No BB, show me your demo”） 面试重点： 主要考察前两个方向（VLA和RL/IL）。","技能黑洞与补课重点#技能黑洞与补课重点":"","投递公司速览表#投递公司速览表":"根据ZJU学长的投递经验整理（59投，3offer，2入池）：\n智能驾驶 制造/硬件 互联网大厂 具身智能（中厂） 研究院 具身智能（初创） 理想 宇量晟（入池）? 字节 三一云 上海AI Lab（转正实习 oc/一面）? 亦方创新（意向）? 地平线 中兴蓝剑（入池）? 腾讯 旷世 鹏城（博后）? 极佳? MMT 美的 阿里 同花顺 北京通用人工智能 它石智航（意向）? 卓驭 联想 快手 宇树 北京人形 普渡 小鹏 OPPO 网易 深信服 招商局 RoboSense 元戎 vivo 蚂蚁 优必选（笔试）? 智源（一面挂）? 傅里叶 文远知行 大疆 京东 讯飞飞星（二面）? 众擎 小米 算能 滴滴 安克（?） 自变量 百度（?） TP 米哈游 银河通用（笔试）? 蔚来（?） Shopee（?） 智元 千里（?） 美团（?） 帕西尼 哈啰 越疆 SharpA（?） 标注说明： ?一面挂、?简历挂、?还有机会","机器人具身智能公司#机器人/具身智能公司":"日期 公司 职位方向 类型 2025.11.24 小鹏机器人中心(北上广深北美) 多模态数据/灵巧操作 全职 2025.11.24 极佳视界(北上广深杭州武汉) 具身智能校园招聘 全职 2025.11.24 优必选(深圳) 感知算法/系统开发 全职 2025.11.24 晨昏线(深圳) 视觉感知/嵌入式/VLA/SLAM/强化学习/机械结构 全职 2025.11.24 深圳市大寰机器人 - 校招 2025.11.24 越疆机器人(深圳) - 校招 2025.11.24 普渡机器人(深圳/成都) - 校招 2025.11.24 北京术锐机器人 - 校招 2025.11.24 埃夫特机器人(吉林) - 校招 2025.11.11 星际光年(深圳) 灵巧操作算法 全职 2025.10.27 仁新机器人 机器人 AI算法工程师(5名) 社招 2025.10.25 松灵机器人 具身智能算法量化工程师 实习/社招 2025.10.23 松灵机器人 高级机械结构工程师/导航算法工程师/高级硬件工程师 社招 2025.10.21 聆动通用 机器人运动控制/机器人视觉感知/机械臂运动规划 2026届校招 2025.10.18 松灵机器人 强化学习算法/量化工程师 实习/社招 2025.10.17 人形机器人(上海)有限公司 算法工程师/软件工程师 2026届校招 2025.10.15 星动纪元 具身大模型算法工程师 2026届校招 2025.9.30 首形科技 机械工程师/嵌入式软件工程师/算法工程师（大模型多模态交互方向） 2026届招聘 2025.9.29 上海市人形机器人创新孵化器 具身智能算法工程师/机器人装配工程师/机械结构设计等 全职/实习 2025.9.26 穹彻智能(上海) 视觉/三维重建/位姿估计/模仿学习/视频理解 实习生 2025.9.26 Infermove 规划算法负责人/规划算法工程师/资深全栈开发工程师 全职 2025.9.19 智平方 算法/工程/产品/硬件/设计/生产岗 2026校招 2025.9.17 星动纪元 多模态强化学习算法工程师/具身智能大模型算法工程师/多模态数据算法工程师 社招/实习 2025.9.16 星动纪元 具身智能大模型算法工程师 全职 2025.9.16 星动纪元 多模态强化学习算法工程师 全职 2025.9.16 星动纪元 多模态数据算法工程师 全职 2025.9.12 智拓科技 具身智能工程师 全职/实习生 2025.9.7 聆动通用机器人 研究算法类/研发类 2026届校招 2025.9.6 云鲸智能具身智能团队 具身智能 实习生 2025.9.4 零次方机器人 机械工程师/硬件工程师/仿真工程师/具身数据工程师/深度强化学习算法工程师等 2026校招 2025.9.3 智身科技 解决方案工程师 全职 2025.8.30 诺亦腾机器人Noitom Robotics 算法研究员/具身智能系统应用开发工程师/结构工程师 实习/全职 2025.8.30 微分智飞 强化学习算法工程师/机器人大模型算法工程师/机器人系统工程师 2026校招/全职/实习 2025.8.29 非夕科技 机器人应用 实习生 2025.8.27 光轮智能 机器人仿真框架用户测试 实习 2025.8.23 千寻智能 Spirit Al 具身智能算法工程师/机器学习系统工程师/机器学习平台后端工程师 2026应届校招/实习 2025.8.22 小鹏机器人 Research Scientist/Research Intern 实习+校/社招 2025.8.21 上海人工智能实验室 算法、研发、产品、运营、解决方案、职能/支持 2026届校招 2025.8.20 忆生科技 三维重建与生成算法工程师/多模态算法工程师/机器人遥操作系统开发工程师 2026届校招/社招 2025.8.20 智元机器人 大模型类/算法类/软件系统类/测试类/其它 2026届校招 2025.8.20 银河通用机器人 具身多模态大模型/具身智能操作算法/人形强化学习控制/机器人规划与控制 2026届校招 2025.8.16 原力灵机 具身智能大模型算法研究员/具身智能强化学习算法研究员/机器人系统算法工程师 2026校招 2025.8.13 Dyna Robotics 机械工程师/高级机器人工程师/机器学习工程师 全职 2025.8.13 宇树机器人 操作员 - 2025.8.12 PaXini 具身智能算法/机器人智能系统/运动控制算法 社招 2025.8.12 河南具身智能产业发展有限公司 招聘10人 国企 2025.8.11 舞肌科技 具身大模型强化学习算法工程师/大模型部署工程师 校招 2025.8.7 自变量机器人 多模态生成算法工程师/3D生成算法工程师/语音算法工程师/运动控制算法工程师 2026校招 2025.7.31 小鹏机器人中心多模态智能部 具身多模态大模型/世界模型方向 社招/校招/实习 2025.7.30 原力灵机 具身智能大模型算法研究员/机器人系统算法工程师/具身智能传感器工程师 2026校招 2025.7.29 Sharpa 机器人机械工程师/机器人电子工程师/机器人系统工程师/触觉应用算法工程师等 2026校招 2025.7.29 智元机器人\"优才计划\" 真机强化学习(具身操作)/运动控制算法/感知/规控/力控算法/多模态大模型(VLA/VLM) 校招 2025.7.28 硅基方舟(杭州) 运动控制算法工程师 全职 2025.7.28 跨维智能(深圳) 数据生成研发工程师 全职 2025.7.27 汉宇晨星 机器人研发专家 全职 2025.7.27 帕西尼感知科技（天津）有限公司 数据采集员/机器人助理工程师/视觉算法工程师 社招 2025.7.26 厨芯科技 具身智能算法工程师（数据方向） 校招 2025.7.23 智元机器人 仿真开发工程师/硬件工程师/数据分析与挖掘工程师 校招 2025.7.22 珞石机器人 软件工程师/机械工程师/测试工程师/控制工程师 2026届校招 2025.7.22 灵心巧手/赛那德招聘 海外销售经理/具身智能大客户经理/机器人KA销售 全职 2025.7.18 深圳星际光年 灵巧手嵌入式软件工程师/灵巧手设计工程师(机械方向)/灵巧操作学习算法工程师 全职 2025.7.18 RoboSense 2026\"天才罗伯特\"人才计划 硬件/算法/产品方向 校招 2025.7.17 苏州一星机器人 具身智能算法岗/软件开发岗/硬件开放岗/GPU计算集群岗/产品岗 实习/全职 2025.7.17 深庭纪 机器学习工程师/机器人运动控制工程师/高性能计算工程师/强化学习工程师 全职 2025.7.10 智元机器人 机器人解决方案实习生/具身感知算法工程师/大语言模型算法实习生 全职/实习 2025.7.9 一星机器人(苏州) 硬件开发/集群运维工程师 全职 2025.7.8 智元机器人 大模型类/算法类/软件系统类/测试类/其它 2026届校招 2025.7.8 Sharpa灵巧手(上海长宁) 机器人软件主架构师/强化学习 全职 2025.7.8 真友科技(武汉) 人形强化学习运动控制 全职 2025.7.8 加速进化(北京) 强化学习运动控制算法 全职 2025.7.8 自变量(深圳优先) slam/导航/点云/传感器标定/VLN 全职 2025.7.7 RoboSense速腾聚创 研发技术类/产品管理类/制造技术类 2026届校招 2025.7.7 知象光电 具身智能算法方向/产品研发方向/市场营销方向 2026届校招 2025.7.7 深圳特修斯机器人有限公司新余研究所 机械/结构设计工程师、工业设计工程师 全职 2025.7.7 源络科技 多模态算法工程师/机器人学习工程师/感知算法工程师 - 2025.7.7 众擎机器人科技 机器人本体研发/具身智能/运动控制/生产制造 - 2025.7.6 星猿哲 视觉算法/运动算法/机器人仿真/机械设计研发工程师 2026届校招 2025.7.4 眸深智能 机器人工程师/机器人算法 全职/实习生 2025.7.3 它石智航 机器人算法/机器人多模态感知/VLA/机器人SLAM等算法工程师 校招/社招 2025.7.3 妙动科技 硬件/机械/算法 全职/实习 2025.7.2 银河通用机器人 具身大模型/灵巧手/机器人全身控制/足式强化学习/感知/仿真等算法工程师 社招/2026届实习生/暑期实习 2025.7.2 干寻智能 机器人高级机械工程师 - 2025.7.2 干寻智能 机器人资深硬件工程师 - 2025.6.30 一星机器人 具身智能算法 苏州园区 2025.6.30 一星机器人 硬件开放工程师 苏州园区 2025.6.28 卓驭(大疆车载, 深圳) SLAM/决策规划/大模型 全职/实习 2025.6.27 银河通用 灵巧手操作/机器人全身规划/机器人规划/强化学习控制算法工程师 全职/实习 2025.6.27 宇树科技 机器人运动控制/深度强化学习/激光SLAM/AI算法工程师 - 2025.6.26 跨维（深圳）智能数字科技有限公司 机器人算法工程师 - 2025.6.26 穹彻智能(上海) 机械臂运动规划与控制 实习 2025.6.25 普渡机器人/零次方(深圳) 强化学习/运动控制算法/具身通用操作 实习/社招/校招 2025.6.25 唯实具身智能(北京) 步态算法工程师 社招/校招 2025.6.25 莱福(北京) 机器人运动控制 实习/社招/校招 2025.6.25 逐际动力(北京/深圳) 具身大模型/RLHF/数据/后端/AI软件/Python后端/前端/人形全身控制/产品经理 全职/实习 2025.6.24 同方鼎欣科技股份有限公司 具身机器人系统工程师 北京海淀区-全职 2025.6.24 自变量机器人(深圳/北京) 强化学习算法工程师/多模态理解算法工程师 全职 2025.6.24 逐际动力 RLHF强化学习/多模态具身大模型/具身仿真Benchmark 实习/全职 2025.6.24 傅里叶智能 视觉感知工程师 全职 2025.8.6 无界智慧 机器人操作工程师/医疗Agent开发工程师/机器人导航工程师/机器人硬件运动控制工程师 全职/实习 2025.8.5 深圳无界智慧 机器人导航/医疗Agent开发工程师/具身操作算法 全职/实习 2025.8.4 无界智慧 操作算法/导航算法/运动控制 社招/实习 2025.8.4 它石智航 机器人算法工程师-强化学习/机器人SLAM算法工程师/机器人算法工程师-决策规划 校招 2025.8.2 银河通用 具身智能课程研究员 全职/实习 2025.6.22 安克创新(深圳/北京/上海/杭州) 具身智能系列岗位 全职 2025.6.11 PNDbotics(北京) 人形机器人与生成式AI 全职/实习 2025.6.11 留形科技(深圳/香港) 三维重建/SLAM/VLA/定位/结构 全职 2025.6.11 它石智航(上海) 机器人运动控制/计算机视觉/感知算法/端到端/VLA算法/SLAM算法/机器人软件开发/底软开发/自动驾驶开发 全职 2025.6.6 智元机器人(北京/上海) 人形模仿学习/强化学习 全职 2025.6.6 小米机器人(北京亦庄) 双足人形 实习 2025.6.6 小雨机器人(北京) RL/运控/VSLAM/软件开发 实习 2025.6.6 RoboScience(北京) 具身智能算法 - 2025.6.6 Light Robotics(上海/新加坡) 算法/软件/硬件/产品 实习/全职 2025.5.28 地瓜机器人(北京) VLA/多模态infra/机器人应用开发 全职 2025.5.26 无限工坊(上海) 具身智能算法研发 实习/全职 2025.5.21 千觉机器人(上海) 触觉传感器/电机/算法/结构/材料/机械/强化学习/仿真/嵌入式软件工程师 全职/实习 2025.5.15 Industrial Next(苏州) 工业机器人多模态感知与自主决策算法/强化学习/模仿学习/VLA/仿真训练/实际落地验证 实习 2025.5.15 阿加犀智能科技(成都) 具身智能 暑期实习 2025.5.5 浙江人形机器人中心 VLA,灵巧手，操作，仿真急招 校招/社招/实习皆可 2025.4.19 银河通用 三维重建 全职 2025.4.10 星海图 VLA/算法/Infra/三维重建/机械 全职 2025.4.6 DJI大疆 图像算法 实习生 2025.4.6 DJI大疆 数据闭环工程师 全职 2025.4.3 银河通用(北京) 足式控制 全职急招 2025.4.3 傅利叶智能(上海) 具身智能VLA多模态大模型 实习生 2025.4.3 灵初智能 2025春季招聘 - 2025.4.3 逐际动力(北京/深圳) 具身算法/强化学习工程师 全职 2025.3.29 众擎机器人 运动控制 算法工程师 2025.3.29 星动纪元 大模型算法/电机控制 校招/社招 2025.3.29 石头科技 运动控制/导航/视觉 算法工程师 2025.3.29 云鲸智能 运筹优化/机械臂操作/3D感知 算法工程师 2025.3.29 海康机器人 运动控制/计算机视觉 算法工程师 2025.3.28 它石智航 运动控制/SLAM/感知 算法工程师 2025.3.26 松灵机器人 2025年春季社招 - 2025.4.6 荣耀(北京) 机器人软件系统开发工程师 全职 2025.3.23 Neura Robotics (Germany) Embodied AI Intern/Expert 2025.3.17 小鹏机器人 后训练/RL/reasoning/agentic llm/world model/humanoid vla/机器人仿真 全职/实习 2025.3.17 自变量机器人(深圳/北京) 多模态大模型/数据筛选算法/运动控制 全职 2025.3.11 松应科技(上海/深圳/北京) 3D仿真引擎开发/强化学习/数据系统/系统测试和解决方案/产品经理 全职 2025.3.11 智元机器人 多模态大模型/空间智能算法/具身算法/强化学习算法 研究员/工程师/实习生 2025.3.10 新松机器人(沈阳) 视觉算法工程师/具身智能专家/控制算法工程师 社招 2025.3.10 国家地方共建人形机器人创新中心/人形机器人(上海)有限公司 - 2025校招/社招集中招聘 2025.3.10 星尘智能 - 2025校招/社招/实习集中招聘 2025.3.4 星尘智能 机器人交互研究实习生 实习 2025.3.2 光轮智能 模仿学习/强化学习算法实习生 实习 2025.2.19 梅卡曼德机器人 VLA全职工程师/实习 - 2025.2.18 VLAI未来动力 强化学习全职工程师/实习/远程 - 2025.2.17 NOETIX Robotics 算法实习生 - 2025.2.13 千寻智能 具身业务算法工程师 - 2025.2.13 艾欧智能 具身智能实习生 - 2025.2.12 北京启物科技 机器人算法实习/校招-操作/导航/仿真 - 2025.2.12 银河通用人型机器人 强化学习or运动规划实习生 - 2025.2.12 星尘智能 具身智能算法实习生 - 2025.2.11 极佳科技 具身智能机器人算法实习生 - 2025.2.11 速腾聚创 多模态大模型算法工程师 - 2025.2.11 中科慧灵 VLA方向 社招/校招 2025.2.11 比亚迪 人型机器人算法工程师 - 2025.2.11 国家地方共建人形机器人创新中心/人形机器人(上海)有限公司 具身大模型实习生 - 2025.2.11 清华大学\u0026地瓜机器人 具身智能算法实习生 - 2025.2.11 逐际动力 具身大模型算法+物理仿真+视频生成+世界模型+运动控制实习生 - 2025.2.11 星海图许华哲组 - 实习/全职","案例一转专业二硕求职经验#案例一：转专业二硕求职经验":"背景： 9本美硕，本科和一硕均与具身智能无关，24年九月同时修读第二个硕士专业Robotics，一年紧急完成一段对口实习，一篇A会（后面求职发现不太对口），预计12月提前毕业。\n投递策略：\n早投递： 因为转专业的弱势背景，选择了在8月中就开始投递，后面证明这其实不一定是一个好的策略 只投递初创，不投递大厂： 基于对大厂具身部门\"规模小，门槛高\"的判断。大厂具身部门offer总体上确实呈现一种赢者通吃的状态，冲大厂可能需要非常过硬的Pub/实习 除头部初创外只投递上海的公司： 个人原因，对象在上海工作 投递结果：\n简历挂： 智元/银河 一面挂： Sharpa/光轮/忆生科技 二面挂： 极佳/维他动力 Offer： 穹彻智能/源络科技/小雨智造 Offer后结束推进： 矩阵超智/留形科技/帕西尼 最终选择： 穹彻智能具身算法工程师\n选择考量：\n穹彻： 之前实习过，公司氛围、工作强度、title都很不错，工作内容也很喜欢 源络： 虽然强度偏大，但给的多，成长快并且leader都比较坦诚开放 小雨： 整体公司非常严谨，从上到下对于公司要干嘛、优先级的划分非常清楚，细分赛道有一套完整的落地预期，非常看好他们未来的商业模式，只可惜因为在北京不得不拒绝 个人收获和想法：\n对口实习就是第二学历： 对于不追求顶级大包（硕士80+）的同学来说，对口实习非常非常重要，通常比论文在找工作的时候更好用 开奖时机： 有时候开奖晚，找得晚一些不是坏事。因为找得比较早所以没有和大部队一起开奖，当时开出来的已经觉得非常高，尤其是源络，但这几天其他初创陆续开奖抢人才发现还是小巫见大巫了 人才红利期： 具身总体处于人才缺口相当大的红利期，今年开始学具身的人在明显变多，但这批人可能还需要一段时间才会进入就业池，过几年人才饱和之后红利期可能不复存在。在红利期结束之前要尽快积累完整项目经验 预研vs落地： 预研和落地是两条完全不同的路线，目前技术未收敛，企业更愿意为科研支付溢价。技术收敛之后可能会迎来具身初创和大厂的超级混战，落地经验就会更值钱","案例二非科班零论文转行经验#案例二：非科班零论文转行经验":"背景： 9本德硕非科班，0论文，1个相关实习，2个本科做的水实习，代码能力一般。\n投递情况：\n投递了大概200-300个公司 初创公司和机器人公司基本都没怎么投递，因为觉得不太稳定 主要投递的还是制造业 关键经验：\n对口实习的重要性： 秋招期间对口的实习还是非常重要的，在询问的时候基本都只问实习的内容，做的项目也不会问。算法方向还是2段实习最为稳妥，不然背景不够只能海投 转行时机： 今年3月份的时候有幸找到了一个具身的实习，在面试之前没有任何具身方向的认知，随便问了几个问题就招进去了 学习路径： 从0开始入门，完整跑了一遍从运动控制、代码采集系统、数据处理、大模型微调、模型部署的整个pipeline。做完实习之后毕设就开始准备做具身方向，找了个博士生带着做，在秋招期间就读了很多这方面的论文，复现了一下模型，正好就补全了模型框架理论薄弱这方面的问题 时间线： 从8月底开始投，11月底才开始收到比较好的offer 总结： 非常幸运，正好赶在具身这个方向热门之前就入局了。秋招是一个长线的过程，过往的所有经历都会成为你的一部分，不要泄气。","汽车自动驾驶#汽车/自动驾驶":"日期 公司 职位方向 类型 2025.11.24 东风汽车研发总院(武汉) 模型训练/运控/机械设计 实习 2025.8.6 苏州博世XC事业部 自动驾驶高精地图算法工程师(两年以上智驾行业经验) 社招 2025.7.25 小鹏汽车 多模态理解/多模态生成/三维视觉/自动驾驶/大模型/CV/Audio/NLP 社招(急)/校招/实习 2025.7.14 华为车BU天才少年 自动驾驶世界模型/VLA/强化学习/并行训练/仿真/数据/多模态理解生成 - 2025.7.11 小鹏汽车 人形机器人运动控制/VLA/VLN/灵巧手/多模态/大模型 2026届校招 2025.7.2 小米 自动驾驶与具身智能算法研究员 (VLA/具身方向) 社招/校招 2025.7.1 华为 研发算法岗 应届生/实习生 2025.6.27 橙子运力 规划算法工程师 正式/实习 2025.6.27 理想汽车『自动驾驶』 大模型/端到端/强化学习算法工程师 - 2025.6.27 小米 机器人操作抓取/足式机器人强化学习/大模型强化学习/多模态/机器学习/感知/机器人算法工程师 实习 2025.4.23 蔚来 春季招聘 实习/全职 2025.3.29 科大讯飞 自动驾驶工程师 - 2025.3.29 文远知行 春招 - 2025.3.29 酷睿程(地平线大众合资) 生成式算法/规控/控制/SLAM 算法工程师 2025.3.20 小马智行 - 2025校招 2025.3.20 地平线(北京/上海/南京) - 2026春季实习生招聘 2025.3.15 Hyundai (US) Autonomous Driving Intern 2025.3.11 IAV GmbH (Germany) Autonomous Driving: Path Planning in Unstructured Environments Internship","滚动招聘信息按领域分类#滚动招聘信息（按领域分类）":"","研究院实验室#研究院/实验室":"日期 机构 职位方向 类型 2025.11.24 光明实验室(深圳) 具身智能机器人研究人员/工程师 全职 2025.9.28 江淮前沿技术协同创新中心 博士后一具身智能方向/智能机器人算法专家/多模态大模型研发工程师 社招 2025.9.8 中国空间技术研究院CAST+AI·R人工智能专班 大模型算法研发岗/大模型数据治理岗/大模型应用系统开发岗/具身智能算法研发岗 - 2025.8.18 华为制造部 视觉算法开发与高级应用工程师/具身智能机器人应用高级工程师 2026届博士 2025.8.5 智源研究院 机器人系统/数据/多模态/具身智能等方向 校招 2025.8.1 智源 具身大模型研究员 社招/校招/实习 2025.7.31 上海人工智能实验室具身智能中心 具身智能-AIGC青年研究员/仿真平台青年研究员/人体运动策略青年研究员/具身智能大模型青年研究员/足式机器人青年研究员/强化学习青年研究员/AIGC算法实习生 校招/实习/社招 2025.7.27 中国科学院空间应用工程与技术中心 空间实验技术研究室 空间具身智能系统/空间灵巧机构研发岗/传感器与感知算法开发 2025年招聘 2025.7.27 嘉陵江实验室 大模型工程师(含LLM 与时空模型)/嵌入式与实时系统工程师 社招 2025.7.25 深圳人工智能与机器人研究院 大模型算法研究员/工程师、具身智能研究员/工程师、机器人导航算法研究员/工程师 2026届招聘 2025.7.24 中豫具身智能实验室 机械工程/自动化/机器人学等专业相关科研岗 博士研究生 2025.7.18 清华+智谱 AI基础模型研究/AI机器人技术研究/记忆机理与实时学习算法研究 博士后 2025.7.8 北京智源人工智能研究院 机器人系统/多模态/数据/信息检索与知识计算/具身智能 2026届校招 2025.7.8 上海算法创新研究院/上海交大人工智能学院具身智能团队 机器人学习研究工程师 全职 2025.7.4 北大银河通用 具身智能科研 实习生 2025.7.3 IDEA机器人感知/VLA 研发招聘 - 实习生 2025.6.30 中国信通院人工智能研究所 具身智能研究岗/具身智能实验室运营岗 全职/实习生 2025.6.24 中国信息通信研究院 具身智能研究员 北京海淀区 2025.8.4 杭州湾具身智能创新中心 机器人数采实习生/机器人数据审核实习生 实习 2025.6.6 上海人工智能实验室 三维重建 全职/实习 2025.6.6 上海人工智能实验室夏季招聘 包含具身岗位 领军科学家/青年科学家/PostDoc/工程师 2025.6.6 北京大学信息技术高等研究院MAII Lab 机器学习/具身智能 PostDoc 2025.5.26 北京具身智能机器人创新中心 具身智能数据实习生 实习 2025.5.15 上海算法创新研究院\u0026上海交大人工智能学院 空间智能/具身智能 实习生 2025.4.25 招商局先进院(深圳) 具身数据/多模态/动作捕捉/VSLAM/人体姿态估计/大模型训练/大模型数据工程/机械臂运动控制 全职/实习（可给香港身份） 2025.4.17 OpenDriveLab RAP/PostDoc/机器人硬件工程师 - 2025.4.10 上海人工智能实验室 2026联培博士招生简章 - 2025.4.8 深圳河套学院 24/25/26级各方向PhD 4月10日截止 2025.3.29 深圳科创学院具身智能团队 机器人系统工程师/具身智能算法工程师 全职 2025.3.29 上海期智研究院 全职研究员 - 2025.3.29 智源研究院 端到端VLA 实习 2025.3.17 北京通用人工智能研究院(BIGAI)机器人团队 跨本体操作策略/异构多机任务规划 实习 2025.3.10 西安交大/优艾智合具身智能机器人联合研究院 海外优青/青年拔尖人才/博士后 - 2025.3.6 复旦大学可信具身智能研究院 海外优青 博士后/助理教授/副教授/教授 2025.3.6 17家国企/央企/研究院 具身智能招聘岗位汇总 具身智能之心(微信: AIDriver002) 2025.2.17 CVTE中央研究院机器人创新部 机器人具身智能算法实习生/全职 -","薪资行情与offer选择案例#薪资行情与offer选择案例":"2024年薪资行情参考（非浙地区）：\n硕士： 有项目或优质实习经历，没顶会的情况下，普遍 60-70w，最高可达 85w 博士： 有机会接近 100w 案例：6个offer选择困境\n讯飞飞星 - 40-80w（不确定性高），995，二线城市，半国企，VLA算法，浮动大 普通初创 - 70×14，965，一线城市，全栈具身研究员，被裁风险++ 明星初创 - 预计可谈到85w左右，9105，一线城市，VLA算法，资金充足 央企军工 - 50w+，965，二线城市，技术管理，AI控制算法，后续难转具身 香港普通初创 - 90w+（港币），base深圳，HK工签（满7年永居），VLA算法，965+，资金有限 等待中 - AILab、TeleAI、IDEA，进度缓慢 选择考虑： 对具身短期看空、长期看好，预计3-5年后冷静期，考虑转LLM或进入研究院过渡。\n案例：互联网vs机器人方向选择\n互联网方向： 字节多模态算法（搜推组），未开奖 机器人方向： 小鹏（已开，总包约55w） Sharpa 中兴（可能给蓝剑，但技术栈可能较落后） 考虑因素： 机器人发展路径不明朗，可能泡沫破裂；互联网存量竞争；薪资差异不大。","路线-a港科广-hkust-gz--mphil--走学术科研流#路线 A：港科广 (HKUST-GZ) / MPhil / 走学术科研流":"假设顺利拿到了MPhil Offer，目标是：两年后进大厂研究院或读博。\n核心逻辑： Novelty (创新性) \u003e Engineering (工程量)。简历上必须要有Paper。\n大四空窗期策略（Unitree项目）：\n不要满足于\"跑通\"，要追求\"算法改进\"。 课题方向建议： 针对Unitree机器狗在极端非结构化环境（如松软沙地、楼梯废墟）下的RL适应性研究。或者，研究如何用更少的算力实现VLA模型在狗身上的边缘侧推理。 目标产出： 一篇由自己一作（或共一）的Workshop论文或会议投稿，哪怕没中，Draft本身也是申请RA实习的硬通货。 实习投递方向：\n首选： 腾讯Robotics X（深圳）、IDEA研究院（深圳）、鹏城实验室。 JD匹配： 关注JD里写着\"探索…前沿算法\"、“发表过ICRA/IROS者优先\"的岗位。","路线-b港中文-cuhk--msc--走业界就业流#路线 B：港中文 (CUHK) / MSc / 走业界就业流":"假设去了港中文或港科广的MSc，目标是：两年后秋招拿高薪Offer（华为/小鹏/初创独角兽）。\n核心逻辑： Complexity (系统复杂度) \u003e Novelty。面试官不关心算法多新，只关心Demo多稳、技术栈多全。\n大四空窗期策略（Unitree项目）：\n必须做一个\"全栈闭环”。 课题方向建议： 给Unitree Go2上跑通 “语音指令 -\u003e VLM解析 -\u003e 导航规划 -\u003e RL运动控制” 的全流程。 核心技能点： 必须熟练掌握 ROS 2 (Humble)，这是工业界通用的语言。代码要写得漂亮（C++为主，Python为辅），要会用Docker打包环境。 实习投递方向：\n策略： 大四下/研一入学前，先去一家中小型初创（如帕西尼、众擎、或者深圳无数的机器人Startup）刷简历。 理由： 大厂（如华为、大疆）很难进，但初创公司急缺能干活的人。有了第一份实习，研二再冲大厂。","顶层大脑层---vla多模态#顶层（大脑层 - VLA/多模态）":"代表机构： 上海AI Lab、腾讯Robotics X、华为天才少年、智元/银河通用等明星初创的核心组。\n特征： 极其看重**Paper（CVPR/ICRA/CoRL）**或顶尖竞赛。这是\"神仙打架“的领域。\nBME的策略： 不要硬碰硬。除非在读研期间发了顶会，否则很难直接作为第一份实习切入。"},"title":"具身职业规划"},"/blog/2025/embodied-navigation/":{"data":{"":"首先，我们的Baseline —— LOVON (Legged Open-Vocabulary Object Navigator, 2025) 是一个在 Gym-Unreal（即 Gym-UnrealCV 风格的仿真 benchmark）上做了大规模仿真实验来验证其开阔词表目标搜索与导航能力；文中强调用虚幻环境来做长航时、动态目标搜索的系统验证（包括视觉抖动、目标短暂消失等问题）并在仿真里验证 Laplacian Variance Filtering、语言→运动模型等模块。也进行了真实腿式机器人（Unitree 系列）上的跨域验证以检验 sim→real。","一现状定位用-lovon-的方法在真机上效果很差最关键的失败点是什么机器狗撞门框那是什么原因#一、现状定位：用 LOVON 的方法在真机上效果很差──最关键的失败点是什么？“机器狗撞门框”？那是什么原因？":"是因为纯2D检测＋动作映射，缺乏深度／3D理解？ 是因为没有障碍物避障规划？ 是因为导航规划缺失，仅“向目标走”而不考虑路径？ 还是别的问题（如机器狗控制延迟、检测误差大、目标消失后无追踪策略）？ LOVON的原理，也就是视觉追踪的原理在于：\n目标提取与筛选（_yolo_image_post_process 方法）\n先通过 object_extractor 从任务指令（默认任务是 run to the person at speed of 0.36 m/s，提取目标为 “person”）中提取目标类别。YOLO 模型输出所有检测框后，只保留类别与提取目标一致的框，过滤无关目标。\n滑动窗口历史管理\n初始化 5 个历史缓存列表，分别存储目标类别、置信度、归一化坐标（xyn）、归一化宽高（whn）、像素坐标（xyxy）。每帧仅保留置信度最高的检测框，加入缓存列表；当列表长度超过 lengthen_filter 时，删除最早的帧，维持窗口大小。\n追踪结果计算\n对缓存列表中的数据取平均值，得到平滑后的置信度、坐标和宽高。\nobject_xyn[0] 是目标中心的水平归一化坐标（0~1，0 为左边界、0.5 为图像中心、1 为右边界）。 若目标在图像中心（xyn[0] ≈ 0.5）：机器狗沿前后方向运动（v_x 按任务指令速度，如 0.36m/s，v_y = 0，w_z = 0），即 “往前走”。 若目标偏左（xyn[0] \u003c 0.5）：w_z 为正（顺时针旋转），同时 v_x 降低，直到目标回到中心；偏右则相反。 任务指令中的 “speed” 仅限制 v_x 的最大值，而非强制固定 v_x。 统计列表中出现次数最多的目标类别，作为当前追踪目标（避免单帧误检影响）。若目标类别为 “NULL”（无有效检测），则重置追踪结果为默认值。\n跟丢的判定标准\n单帧无检测：YOLO 未检测到与 extracted_object 匹配的框 → 往历史缓存中添加 “NULL” 和 0 置信度。 连续跟丢：当历史缓存（长度由 lengthen_filter 控制）中 “NULL” 出现次数最多 → most_common_object 变为 “NULL”，avg_confidence 设为 0 → 判定为 “跟丢”。 motion_predictor 接收 “跟丢状态” 后，生成搜索型 motion_vector：\n通常是「旋转搜索」：v_x = 0（不前后动）、v_y = 0（不左右动）、w_z ≠ 0（缓慢旋转，扫描周围环境）。\n机器狗撞门框的原因在于，这里现实环境的部署代码通过 YOLO 只识别到了目标但是没有理解环境与障碍物，而当人消失在门后时，最后一帧这个目标是在画面中心的，因此机器狗会往前走直到撞到门框，又或者笨笨的在门框那个位置旋转搜索。因为没有开源其仿真智能体的代码所以不知道模拟环境是怎么规避这个问题的","三重要性在哪里#三、重要性在哪里？":"对学术来说：为什么“足式机器人 + open-vocab目标导航/跟踪”值得研究？是否当前工作少？ 对应用来说：在真实环境（室内／复杂家具／光照变化）中，解决这个问题会带来什么改进？ 对于我来说，我还只是一个入门新手，打算通过本科毕设的机会，从3D世界理解和具身导航决策这个小角度切入来入门具身领域，所以我也说不清楚学术和应用上的重要性，只求发ccfb以上的paper证明自己","二gap-与定位基于你上面的回答问题在哪儿用一句话描述这里的-gap研究空白#二、Gap 与定位：基于你上面的回答，问题在哪儿？用一句话描述这里的 gap（研究空白）":"比如：“在足式机器人真实场景下，当前Open-vocab检测＋简单动作生成不能有效处理目标暂时丢失和复杂障碍物，导致跟踪／导航失败”。还是要聚焦 “障碍物避障” 或 “三维深度理解”？\n在足式机器人开放世界目标追踪任务中，现有基于纯 2D 视觉目标检测的追踪 - 运动映射方案，因缺乏环境障碍物感知与三维空间理解，且目标暂时丢失后仅采用无环境适配的旋转搜索策略，导致无法应对 “目标被遮挡 / 消失后因路径误判碰撞障碍物” 等真实场景挑战，难以实现稳健的长时追踪与运动控制（具体有没有3D视觉目标检测的论文工作，现在还没有做过调研）","五可量化指标与对比要发论文必须有可测量的结果可以测量哪些指标例如#五、可量化指标与对比：要发论文，必须有可测量的结果，可以测量哪些指标？例如：":"目标被丢失的次数／恢复次数 障碍物碰撞次数 成功到达目标的比例 路径长度／时间／效率 跟踪保持时间／跟丢时间 − 真机 vs 仿真的差距（sim2real gap） 能够在实机上测这些指标吗？哪些可能无法测？","六实验平台可行性#六、实验平台／可行性：":"已有的硬件是 Unitree Go2 足式机器人，这很好。你能控制机器人做什么动作（向前、转、停止、避障）？你能获取哪些传感器数据（RGB、深度、IMU、里程计）？\n是否有仿真实验环境（如 Gym-UnrealCV 场景）可以先做仿真再到实机？仿真与实机之间能记录相同指标吗？\n时间上本科毕设资源有限，预计能做多少场景／多少实验次数？这个对决定指标和可行性很重要。\n对于硬件设备，要关注[官方SDK文档](https://support.unitree.com/home/en/developer）：\n一、动作控制能力\n基础运动控制 前进 / 后退 / 转向：通过Move(vx, vy, vyaw)函数直接设置线速度（vx/vy）和角速度（vyaw），支持相对于世界坐标系的运动控制。例如，Move(0.5, 0, 0)使机器人以 0.5m/s 速度向前移动。 停止：调用StopMove()立即终止所有运动，进入静止状态。 步态切换：通过SwitchGait(int d)选择不同步态（如小跑、踱步），或使用ContinuousGait(bool flag)启用连续步态模式。 高级动作与姿态调整 站立 / 坐下：StandUp()和Sit()实现起立和坐下动作，RecoveryStand()用于从侧翻状态恢复。 身体姿态控制：Euler(roll, pitch, yaw)可调整机身倾角，BodyHeight(float height)动态改变离地高度。 特技动作：支持FrontFlip()前空翻、FrontJump()跳跃等复杂动作（需硬件支持）。 避障功能 自主避障：通过ObstacleAvoidClient类启用避障模块，机器人可实时检测障碍物并调整路径。需调用EnableObstacleAvoidance()激活，并在移动时保持避障服务运行。 传感器融合：避障依赖激光雷达（PRO/EDU 版）或深度相机（AIR 版）与 IMU 数据融合，实现动态环境下的安全导航。 二、传感器数据获取\n视觉传感器 RGB 图像：通过 ROS2 话题/camera/image_raw获取 720P/1080P 实时视频流，支持 WebRTC 低延迟传输。 深度数据：PRO/EDU 版搭载 4D 激光雷达（L1），可输出 360°×90° 点云数据（/go2/camera/depth）；AIR 版通过 Intel RealSense D435i 深度相机提供毫米级深度信息。 惯性测量单元（IMU） 原始数据：通过 ROS2 话题/imu/data获取加速度（a_x, a_y, a_z）、角速度（ω_x, ω_y, ω_z）和四元数姿态（q_w, q_x, q_y, q_z）。 坐标系转换：SDK 提供工具函数处理不同框架下的四元数顺序（如 Isaac Gym 与 Isaac Sim 的差异）。 里程计与定位 状态估计：通过激光雷达 + IMU 融合（如 LIO-SAM 算法）或腿部运动学模型（关节编码器数据）实现里程计输出。ROS2 话题/odom提供机器人位姿（x, y, θ）和速度信息。 精度优化：紧耦合 LiDAR-IMU - 腿部里程计系统可在无特征环境下实现亚米级定位精度，在线学习机制适应负载和地形变化。 其他传感器 关节状态：实时获取 12 个关节的角度、角速度和扭矩（/joint_states），支持电机健康监测。 足端力反馈：PRO/EDU 版配备足端力传感器（F_z），用于复杂地形下的步态调整。 对于仿真环境，我有本地的Gym-Unrealcv仿真场景，但是苦恼于LOVON没有开源其仿真代码所以搁置着，不清楚下一步是根据部署代码反推仿真代码还是换一个仿真环境如MatterPort3D 时间本身还是比较充裕的，到开题答辩之前至少有1个月时间","创新点暂定#创新点（暂定）":"我们提出一种融合深度视觉感知与分层控制的开放词汇目标追踪框架。 相较于LOVON仅依赖2D目标检测进行运动控制，我们的方法通过深度投影构建局部BEV占用图，并引入预测-驱动的路径规划层，从而显著减少在真实环境中因遮挡或障碍导致的失败","医疗交叉答辩#医疗交叉（答辩）":"这里值得注意的是，论文里面要写的内容是一个宏大的改进，但是本院答辩时要突出和BME相关、医疗交叉的内容，HexGuide可以作为一个很大的参考\n《HexGuide: A Hexapod Robot for Autonomous Blind Guidance in Challenging Environments》，一篇期刊论文\n层级 内容 对应写作作用 ① 背景 世界上有数亿视障人群，对自主出行有刚性需求 让读者意识到社会价值和痛点 ② 矛盾 现有导盲设备（如导盲犬或轮式机器人）有明显局限，不能稳定地应对复杂地形 设置“冲突”——为什么我们必须做新系统 ③ 概念 上交高峰团队设计了一个六足机器人 HexGuide，模仿昆虫式稳定步态，在复杂环境中实现安全引导 提出核心创新点和愿景 ④ 方法 通过算法与机械协同，实现路径规划 + 稳定行走 + 动态避障 + 交通识别 + 人机交互 展示技术路线是如何支撑“稳定、安全”这两个关键词的 ⑤ 验证 在机场、十字路口等复杂场景下实测验证，引导成功率高，路径平滑且避障成功 用结果“闭环”故事——愿景得以实现 论文的立意不是“做一个六足机器人”，而是要证明“六足+智能控制” = 可靠的盲人引导方式。这篇论文的核心任务不是“跟踪一个已知目标”或“视觉跟随”，而是“带领盲人从一个地点到另一个地点”，比如：“from the arrival gate to the baggage claim area in Shanghai Hongqiao Airport.”\n核心流程是盲人用户通过语音指令（如“去出口”）输入目标；在地图上自动规划从当前位置到目标的安全路径；机器人沿着规划路径行走；实时感知环境并修正轨迹。\n目标不是视觉追踪的对象，而是一个空间位置目标，因此这种导航是Goal-based而非Object-based tracking，而且泛化性有限：“The system can autonomously navigate in challenging environments once a map is available.”\n模块 故事逻辑 手法 指标体现 补充说明 机械稳定性：六足结构的天然稳态 盲人行走必须安全 → 足式比轮式更抗地形 → 六足比四足更稳 · “三足支撑步态（Tripod gait）”确保任意时刻三条腿接地\n· 单腿轨迹采用三次样条插值，区分支撑相与摆动相以减冲击\n· 控制顶点高度以跨越障碍、维持步态连续 · 平均支撑腿数 ≥ 3\n· 步态周期内质心（CoM）位移波动 \u003c 5 mm\n· 10° 坡面及不平地面仍能维持姿态 —— 规划稳定性：安全路径生成 “安全通行”要求路径不过度摆动、不贴近障碍 · 基于 A* 进行全局规划\n· 融合人工势场（APF）调整代价，使路径自动远离障碍\n· Bézier 曲线平滑路径\n· 拐点以贪心方式优化，减少急转角 · 路径平滑度提升（转向角波动减少约 40%）\n· 路径与障碍最小距离 ≥ 0.3 m\n· 平均路径长度仅比最短路径长 ≤ 5% —— 运动控制稳定性：MPC 路径跟踪控制 六足控制复杂，需让行走对路径偏差“有反馈、能预测” · 使用模型预测控制（MPC）\n· 目标函数最小化未来时域的位姿偏差\n· 实时约束关节速度与姿态角\n· 借助力矩传感器反馈修正步态 · 路径跟踪误差 \u003c 3 cm\n· 姿态偏角误差 \u003c 2°\n· 延迟控制补偿 ≤ 100 ms 核心体现“动态预测 + 约束最优控制”，区别于传统 PID 环境与交互稳定性：避免危险与错误指令 盲人处于动态环境，需识别行人、车辆、信号灯并安全互动 · LiDAR + IMU + RGB 摄像头多传感融合\n· 基于 LiDAR 点云的区域划分与加权速度修正，实现动态避障\n· YOLOv5 交通灯识别结合模板匹配\n· 语音识别与反馈交互（“请跟我走”“前方有障碍”） · 动态障碍避让成功率 95%\n· 信号灯识别准确率 97.8%\n· 平均避障响应时间 \u003c 0.3 s\n· 机场/路口场景连续引导成功率 100% —— 所以这里的Navigation就比较难讲故事了。我们的亮点在于LLM对于自然语言指令能够分解成子任务，但足式机器人比较尴尬的一点是没有手，导致在医疗领域能实现的指令就局限了，比如说有一个RoboNurse-VLA enables the robot to recognize, grasp, and handover surgical instrument.是灵巧手的，但是足式机器人就只是狗了\n我也调研了其他的可能可以相关的领域，秉承**“助残/助盲/助老”**的理念： 第一个是家庭服务，这个还挺好说的，比如越疆 Rover X1/Unitree GO2可在光滑地板、草地、小坡坎等多场景行走，负载能力达日常物品级别，但最大的问题就是没有手，导致比如“帮老人取床头老花镜”“客厅物品递送”这种实现不了 —— 没有机械臂的足式机器人，到底“服务”什么？如果不能取物、开门，它的价值在哪里？\n可以胡诌做成可语音召唤的移动置物台 ，为上肢失能者提供 室内 5 m 范围内的即时物品可达性 ，用 3D 感知+分层导航解决 家用杂乱环境 下的 安全-连续 难题，从而 以移动代偿 而非 抓取代偿 的方式，提升上肢失能人群的 居家独立指数？ 第二个是康复检测，问题是回答不了为什么需要一个狗跟着，而不是穿戴传感器设备/用固定的摄像头进行openpose骨骼分析，就算用了狗也不过是一个移动摄像头，那为什么不让残疾人动或者医生手动挪动摄像头？ 第三个就是继续去纯助盲，我想通了，它不是Tracking，而是Object-based Tracking，只是默认命令是跟着person而已，没有说一定要跟在人后面，给他下一个其他目标的指令不就行了？但问题就出在了这里，传统SLAM的方法比结合AI的方法又快又好，你在AI基础上绑一个什么激光雷达/SLAM的话就有点尾大不掉很难绷。 也有导盲盲道的参考，不过是基于A1的本科毕设 往RoboGuide这个方向去做的话也可以，只不过更多是放在VLM而非Tracking/Navigation本身了 InternDog这篇西工大的工作不知道是怎么做的，看起来很牛，还上了央视，据说是我国首个应用在导盲任务/场景下的四足机器人？ Intelligent-Guide-Cane或者回归ESP32的导盲？ 感觉越调研越有信心了，那就继续往导盲这个领域讲故事应该没有问题！","四创新点初步想法#四、创新点初步想法？":"通过和导师学长们讨论列出了很多可能的优化方向（环境理解增强、分层决策升级），从这些中最可能做出论文中可量化贡献的一个或两个是什么？ 比如：“用 BEV 俯视地图 +轨迹预测 来增强 open-vocab 目标导航”；或者：“在足式机器人上验证视觉+深度融合检测在目标丢失场景下的跟踪稳定性提升”。哪一个更倾向？为什么？\n我不知道量化贡献的指标可以在哪里进一步优化啊，原因也可能在于我读的文献太少了，LOVON在仿真里所使用的指标为衡量 100 次实验中完成任务的平均步数、衡量 100 次实验中成功完成任务的比例两个，而如何去量化现实任务的指标与sim2real的优化，因为文献读的不多所以暂时我还不能回答这个问题","思考#思考":"我们第一步要做的就是 Define Problem。若能有清晰的问题定位 + 合理指标 +实证结果 +对比分析，就有很大机会产出成果。一个很好的方法就是自问自答：","研究方向#研究方向":"方向 名称 思路简述 A. 环境理解增强（BEV / Depth / 3D Occupancy） 给LOVON加“视觉深度感知”，即在YOLO检测的基础上，通过深度图重投影到3D坐标系或BEV平面，建立占用图。再利用该图进行避障或规划。 你能做仿真+实机对比，提出一种“轻量级3D-aware追踪方法”。 → 投稿到 IROS/ICRA workshop 或 CCF-C AI Robotics会议。 B. 跟踪 + 导航分层融合（Hierarchical Policy） 把“跟踪”和“导航”分成两个层次：高层目标预测、低层路径规划。你可以用简单预测（如卡尔曼滤波预测目标短期轨迹）+ BEV局部避障（A*或DWA）。 可以与LOVON对比“复杂场景成功率”→ 写出完整paper。 选择 A + B 结合的小主题：“基于3D视觉感知与分层导航策略的开放词汇足式机器人目标追踪”(但这个一听就感觉不少人做过类似的课题非常卷)\n类别 指标 含义 任务层面 Success Rate (SR) 机器人在有限步数内到达目标的比例 Average Steps (AS) 成功任务平均步数 Collision Rate (CR) 发生障碍碰撞的任务比例 视觉层面 Target Loss Time (TLT) 目标丢失后重新识别的平均时间 Tracking Stability (TS) 目标检测框抖动方差 Sim2Real 层面 ΔSR (Sim→Real) 仿真与实机成功率差距 效率指标 FPS / Latency 模型推理帧率与系统延迟 安全指标 Distance Margin 与障碍最近距离的平均值 在仿真中先实现自动收集 SR、AS、CR。 实机可手动统计 SR 和 CR，或用里程计测轨迹。\n可定义 3 个场景（开阔场 / 门框 / 桌椅环境）各跑10次。","研究现状#研究现状":"维度 内容总结 基线模型 LOVON（LoVi: Open-vocabulary Visual Navigation and Tracking）在仿真中近乎完美（≈100% success rate），但在真实环境严重失效。 核心问题 LOVON 只用 YOLO 的 2D 框坐标来做“视觉 → 动作”映射，没有任何 3D 环境建模或避障机制。目标消失（如进门）时，机器人仍执行“往前走”动作 → 撞门框。 可用硬件 Unitree Go2（有RGB、深度、IMU、里程计、足端力传感器）。具备基本避障API、Move(vx,vy,vyaw)控制接口。 仿真环境 有Gym-UnrealCV，但缺少LOVON仿真智能体代码。可能考虑复刻或转向MatterPort3D。 研究目标雏形 希望提升LOVON从2D视觉到更稳健3D环境理解（environment understanding + navigation fusion）的能力。","规划#规划":"时间 任务 目标 第1阶段 阅读文献：LOVON、LOVi、BEVFusion、LIO-SAM、SceneGPT 明确3D环境理解技术路线 第2阶段 在Gym-UnrealCV中复现或简化LOVON策略（YOLO+Motion mapping） 建立baseline可控环境 第3阶段 集成深度图或BEV投影模块，实现障碍建模与避障决策 形成改进方法 第4阶段 实机测试 + 指标对比 + 论文撰写 形成可投稿版本 第一阶段的文献调研一方面要包括LOVON引用的和引用LOVON的文献（但是因为VPN节点问题我的Scholar Google给我挂掉了，说我是机器人不让我访问），另一方面是尽可能的调研3D-aware Tracking/Navigation","问题定义#问题定义":"当前的 open-vocabulary 视觉追踪方法（如 LOVON）在仿真中表现优异，但在真实足式机器人环境中严重退化，其原因在于缺乏对三维环境几何与障碍物的建模能力。 其技术设计恰好规避了仿真环境的局限性，同时最大化了自身优势，具体体现在 3 个 “无冲突”：\n仿真无 “真实场景的 3D 感知需求”，纯 2D 视觉足够 仿真环境中，目标的 “2D 图像坐标” 与 “实际空间位置” 完全对齐（如虚拟场景中 xyn=0.5 即代表物理上的正前方，无门框等 3D 遮挡物），无需深度信息即可判断路径是否可行。而 LOVON 的核心是 “2D 视觉 + 运动向量映射”，恰好适配这种需求，无需额外的 3D 深度理解模块。 仿真无 “不可控干扰”，搜索策略高效 仿真中的 “目标丢失” 仅为 “目标移出 90 度扇形视野”（可通过旋转搜索快速重新捕获），无真实场景的 “目标被门框完全遮挡”“机器人被碰撞” 等不可控干扰。LOVON 的旋转搜索策略（vx=0、w_z≠0）在仿真中能高效覆盖视野，而不会像真实场景那样因 “旋转时忽略障碍物” 导致碰撞。 仿真数据与模型训练 “高度同源” 仿真使用的目标类别（背包、椅子、行人）、运动速度（0.3~0.7m/s）、场景光照均与 LOVON 的训练数据集（100 万样本，摘要 1）高度匹配：IOE 对 “椅子”“行人” 的类别映射无误差，L2MM 的运动预测参数（如 β=10）也针对仿真场景校准（摘要 3），避免了真实场景中 “未见过的目标形态”“突发速度变化” 导致的误差。 当目标被暂时遮挡（如进入门后）或在复杂结构环境中移动时，机器人仅凭2D像素坐标进行动作决策，无法有效区分“自由空间”与“障碍区域”，导致运动策略失效（如撞门、原地旋转）。 因此，本研究旨在探索一种融合3D环境理解的目标跟踪与导航方法，在保持LOVON开放词汇指令能力的前提下，提高其在真实环境中的鲁棒性与安全性。"},"title":"重新思考三维空间感知与具身导航决策在毕设中的研究点"},"/blog/2025/embodied-survey/":{"data":{"1-few-shot-imitation-learning#1. Few-shot Imitation Learning":"该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。\n传统方法：Behavior Cloning、DAgger\n当前主流方法：ACT、Diffusion Policy\n这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。","1-graphics#1. Graphics":"图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。\n**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等 **Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。 **系统性学习图形学推荐课程：**Games 101, 103","1-grasping#1. Grasping":"抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。\n常见任务有（难度依次递增）：\nSingle object grasping（单物体抓取）：抓取一个物体，物体通常放在桌子上。 Clutter scene grasping（堆叠场景抓取）：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。 Functional grasping（带语义抓取）：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。 常用机械手末端有（难度依次递增）：\nSuction cup（吸盘）：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。 Parallel gripper（平行夹抓）：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real） Multi-fingered hand（多指手），又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。 常见的做法：\nOpen-loop methods（开环执行）：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。 Closed-loop methods（闭环执行）：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。","1-simulation-environments#1. Simulation Environments":"","1-什么是具身智能#1、 什么是具身智能":"具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。\n典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。\n核心特征：\n拥有多模态感知能力（视觉、触觉、语音等） 能够执行动作并影响环境 学习可以通过与环境交互而不仅仅是被动监督完成","2-hardware#2. Hardware":"硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。\nTele-operation（遥操作）\n**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。 **主从臂系统：**如 Gello，实现高精度的力控遥操作。 **可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。 Sensors（传感器）\n**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。 **Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。 **Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。 **Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定","2-manipulation#2. Manipulation":"操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。\n**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control） **Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。 **Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。 **Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。 **Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。 **Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同","2-robot-foundation-model#2. Robot Foundation Model":"该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。 目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：\nOpenVLA：第一个开源且易于follow的VLA。 Pi0 / Pi0.5：目前公认最work的VLA，10K+ hours teleop data训练的。 GraspVLA：基于纯仿真数据的抓取任务的VLA。 还有少量工作没有借助VLM，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)","2-robot-platform#2. Robot Platform":"","2-具身智能与其他ai的区别#2. 具身智能与其他AI的区别":"具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，\n如何scale up机器人数据？ 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置） 在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？ 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）","3-daily-arxiv#3. Daily ArXiv":"原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了： 具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：\nhttps://github.com/jiangranlv/robotics_arXiv_daily","3-mainstream-models#3. Mainstream Models":"Transformer Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。","3-navigationnow#3. Navigation(NOW)":"Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。\n常见任务包括：\nPoint Goal Navigation (PointNav)：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。 Object Goal Navigation (ObjectNav)：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。 Vision-Language Navigation (VLN)：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。 Embodied Question Answering (EQA)：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。 Tracking：机器人持续感知并跟随动态目标（如人或移动物体）。 常见做法：\nMap-based Navigation：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration Prompting-Large-Model Navigation：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav Video-based VLM Navigation：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid Unified Embodied Navigation：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。","3-sim-to-real-reinforcement-learning-distillation#3. Sim-to-Real Reinforcement Learning (Distillation)":"从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。\n目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。\n核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。","3-研究具身智能的核心原则-core-principles#3. 研究具身智能的核心原则 (Core Principles)":"首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；\n但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）\n必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？","4-foundation-models#4. Foundation Models":"LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。\nVision Encoder\nDINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。 CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。 VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。","4-locomotion#4. Locomotion":"Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。\n技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为\nLearning from manually designed reward (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】 Learning from human data (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】 如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -\u003e whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了 下一个阶段会涉及到一点除了control之外的东西，就是\n引入【视觉】实现户外自主化（perceptive locomotion）；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】 引入【物体】实现loco-manipulation；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】 对上述两种task的组合 强调【语义的泛化性】，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】 强调一些特殊的capability（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）","4-real-world-reinforcement-learning#4. Real-World Reinforcement Learning":"Real-world RL 指直接在现实环境中进行探索式学习。\n这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。\n**从零开始的真实世界强化学习：**Hil-Serl **基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。","5-3d-vision#5. 3D Vision":"详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。\n三维生成与重建\n**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。 **单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。 **单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。 **位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。 三维表示\n**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。 **点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。 **Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。 三维理解\n包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。","5-world-models#5. World Models":"World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。\n代表性工作包括 Dreamer 系列（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。 在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。","一基础概念-basic-concepts#一、基础概念 (Basic Concepts)":"","三研究平台与工具#三、研究平台与工具":"","二ai-and-robotics-basis#二、AI and Robotics Basis":"以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。\nIntro-to-Embodied-AI 王鹤老师《具身智能导论》，找找类似课程替代\nIntro-to-CV Stanford CS231N\nDeep Reinforcement Learning (CS285) Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替","五learning-based-research-field#五、Learning based Research Field":"","六相关领域#六、相关领域":"","具身调研#具身调研":"具身调研对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。\n本调研主要基于**PKU EPIC Lab、Lumina具身智能社区**","四research-field-on-robots#四、Research Field on Robots":""},"title":"具身调研"},"/blog/2025/falcon/":{"data":{"":"来拜读梁俊卫老师和龚泽颖学长的工作，没准之后红鸟面试还能用上。","1-人类计数估计human-count-estimation数清楚周围有几个人#1. 人类计数估计（Human Count Estimation）——\u0026ldquo;数清楚周围有几个人\u0026rdquo;":"任务目标： 估计场景中人类的总数\n输入： 主网络的潜在变量 $\\delta_R$（包含前几步的环境记忆，如\"前 3 步看到 2 个人类轮廓\"）\n网络结构：\nLSTM 编码器： 将 $\\delta_R$ 编码为时序特征 $\\Phi_R$ 自注意力层 [54]： 以 $Q=K=V=\\Phi_R$ 处理特征，目的是\"聚焦与人类数量相关的关键记忆\"（如\"忽略墙的特征，重点关注人类轮廓的变化\"），输出注意力特征 $A_t$ 分类器 $\\phi_{count}$： 基于 $A_t$ 预测\"场景中人类数量为 $k$“的概率 $\\hat{n}_k$： n^k=ϕcount(At),k∈{0,1,…,M},M=6 \\hat{n}_k = \\phi_{count}(A_t), \\quad k \\in \\{0, 1, \\ldots, M\\}, M=6 n^k​=ϕcount​(At​),k∈{0,1,…,M},M=6损失函数： 采用交叉熵损失（衡量\"预测概率\"与\"真实数量\"的差异）：\nLcount=−∑k=0Mnklog⁡(n^k) L_{count} = -\\sum_{k=0}^{M} n_k \\log(\\hat{n}_k) Lcount​=−k=0∑M​nk​log(n^k​)其中 $n_k$ 是\"真实人类数量为 $k$“的指示变量（如真实有 2 人，则 $n_2=1$，其余 $n_k=0$）。","1-核心定位#1. 核心定位":"它是智能体的\"行动决策中心\"—— 既要保证导航效率（尽快到达目标），又要遵守社交规则（不碰撞、不挡路、保持安全距离），是直接指导智能体移动的核心模块。","1-算法短视性#1. 算法短视性":"传统 RL 方法仅依赖当前环境信息，易出现**“短视避障”*问题；规则类算法（A/ORCA）或依赖全局地图，或无法动态适应人类运动。下面以 Proximity-Aware 为例详细说明传统方法的局限性。\n传统 RL 方法的基本流程：\n状态表示： 通常使用当前时刻的传感器数据（如深度图像、激光雷达）和人类当前位置作为状态输入 奖励设计： 设计基于当前距离的奖励函数，例如： 当机器人与人类距离过近时给予惩罚 当机器人成功到达目标时给予奖励 考虑当前时刻的碰撞风险 策略学习： 通过强化学习（如 PPO、A3C）训练策略网络，学习在当前状态下选择最优动作 动作执行： 策略网络直接输出下一步动作（如前进、转向、停止） 传统方法的局限性：\n仅考虑当前状态： 只基于当前时刻的人类位置和距离进行决策，无法预测人类未来的移动轨迹 反应式避障： 当人类突然改变方向时，机器人只能被动反应，容易出现\"短视避障\"（即只关注眼前障碍，导致后续路径不佳） 缺乏前瞻性： 无法提前规划路径以避免与人类未来轨迹发生冲突，导致效率低下或碰撞风险增加 Proximity-Aware 是一个典型的基于当前距离的 RL 方法：\n状态空间： $s_t = [d_t, \\theta_t, g_t]$，其中 $d_t$ 是当前时刻机器人与人类的距离，$\\theta_t$ 是相对角度，$g_t$ 是目标方向 奖励函数： $r_t = r_{goal} + \\alpha \\cdot r_{proximity}$，其中： $r_{goal}$ 是到达目标的奖励 $r_{proximity} = -1/d_t$ 是基于当前距离的惩罚（距离越近惩罚越大） 问题： 这个奖励函数只考虑当前时刻的距离 $d_t$，无法考虑人类未来可能移动到的位置 假设机器人在走廊中需要绕过前方正在移动的人类。Proximity-Aware可能： 看到人类在左侧，选择向右避让 但人类可能正在向左移动，导致机器人向右避让后反而与人类未来位置冲突 需要多次调整，效率低下","1-障碍碰撞惩罚obstacle-collision-penalty#1. 障碍碰撞惩罚（Obstacle Collision Penalty）":"该惩罚针对机器人与静态障碍或人类发生碰撞的行为，计算公式如下：\nrcoll=βs⋅Is_coll+βh⋅Ih_coll r_{coll} = \\beta_s \\cdot I_{s\\_coll} + \\beta_h \\cdot I_{h\\_coll} rcoll​=βs​⋅Is_coll​+βh​⋅Ih_coll​其中：\n$I_{s_coll}$ 和 $I_{h_coll}$ 分别为表示\"与静态障碍碰撞\"和\"与人类碰撞\"的指示变量（发生碰撞时为 1，否则为 0） $\\beta_s$ 和 $\\beta_h$ 为对应的惩罚权重","1输入智能体看到和知道的信息#（1）输入：智能体\u0026quot;看到\u0026quot;和\u0026quot;知道\u0026quot;的信息":"核心输入 1：深度图像（RGBD 中的 Depth 部分）\n相当于智能体的\"眼睛\"，能看到周围的墙壁、障碍物、人类的轮廓和距离（比如\"前方 3 米有个人，左侧 2 米有个柜子\"）。\n核心输入 2：相对目标坐标\n相当于智能体的\"导航目的地\"，比如\"目标在我前方 5 米、偏右 10 度的位置\"（不需要全局地图，只需要自己和目标的相对位置）。","2-人类距离惩罚human-proximity-penalty#2. 人类距离惩罚（Human Proximity Penalty）":"该惩罚确保机器人与人类保持安全距离，计算公式如下：\nrprox=∑i=1N{βprox⋅exp⁡(−dit)若 dit\u003c2.0 m0若 dit≥2.0 m r_{prox} = \\sum_{i=1}^{N} \\begin{cases} \\beta_{prox} \\cdot \\exp(-d_i^t) \u0026 \\text{若 } d_i^t \u003c 2.0 \\text{ m} \\\\ 0 \u0026 \\text{若 } d_i^t \\geq 2.0 \\text{ m} \\end{cases} rprox​=i=1∑N​{βprox​⋅exp(−dit​)0​若 dit​\u003c2.0 m若 dit​≥2.0 m​式中，$d_i^t = |\\tau_a(t) - \\tau_i(t)|$ 表示时间步 $t$ 时机器人与第 $i$ 个人类的欧氏距离。当 $d_i^t$ 减小时，惩罚呈指数增长，以促使机器人主动避开人类；当机器人距离目标不足 2.0 米时，该惩罚自动取消。","2-内部结构与工作流程输入处理输出#2. 内部结构与工作流程（输入→处理→输出）":"可以把它想象成\"一个带’翻译官’‘记忆大师’和’决策+评估团队’的指挥系统\"，步骤如下：","2-基准不真实性#2. 基准不真实性":"现有数据集（如 iGibson、Habicrowd）场景类型单一、人类行为简化（随机行走 / 无动画）、人类密度失衡，且常假设机器人可获取全局信息，与真实场景脱节。","2-当前位置跟踪current-position-tracking知道每个人在哪#2. 当前位置跟踪（Current Position Tracking）——\u0026ldquo;知道每个人在哪\u0026rdquo;":"任务目标： 跟踪人类相对于机器人的二维位置\n输入： 主网络的 $\\delta_R$ + 场景中人类的真实数量 $N$（Oracle 信息，仅用于训练，推理时无需）\n网络结构：\nLSTM 编码器： 将 $\\delta_R$ 和 $N$ 融合编码为特征 $\\Phi_{R;N}$ 自注意力层： 以 $Q=K=V=\\Phi_{R;N}$ 处理，聚焦\"人类位置相关的特征”（如\"分辨’人类 A 在左前方’和’人类 B 在右后方’\"），输出 $A_t$ 回归器 $\\phi_{pos}$： 基于 $A_t$ 预测第 $i$ 个人类的相对 2D 位置 $\\hat{P}_i^t$： P^it=ϕpos(At) \\hat{P}_i^t = \\phi_{pos}(A_t) P^it​=ϕpos​(At​)损失函数： 采用均方误差（MSE，衡量\"预测位置\"与\"真实位置\"的距离）：\nLpos=1∣M∣∑i∈M∥P^it−Pit∥2 L_{pos} = \\frac{1}{|M|} \\sum_{i \\in M} \\|\\hat{P}_i^t - P_i^t\\|^2 Lpos​=∣M∣1​i∈M∑​∥P^it​−Pit​∥2其中：\n$P_i^t$ 是人类 $i$ 的真实位置 $M$ 是\"真实存在的人类\"的掩码（如场景中只有 2 人，就只计算这 2 人的位置损失）","2处理把原始信息变成决策依据#（2）处理：把\u0026quot;原始信息\u0026quot;变成\u0026quot;决策依据\u0026quot;":"这个过程分 3 步，对应模块里的关键组件：\nResNet-50 编码器：视觉翻译官\n深度图像是\"像素组成的图片\"，机器看不懂，ResNet-50 的作用就是把图片\"翻译\"成机器能理解的\"数字特征向量\"（比如用一串数字代表\"前方 3 米有人类\"“左侧是静态柜子”）。\n简单说：它负责提取环境的\"关键视觉信息\"，过滤无用细节（比如墙壁的纹理、人类的衣服颜色），只保留和导航相关的核心特征（距离、障碍物类型、人类位置）。\n2 层 LSTM：时间记忆大师\n导航是\"连续的过程\"，不是单张图片能决定的（比如\"前一秒那个人在走，这一秒停了，下一秒可能继续走\"）。LSTM 的作用是\"记住时间序列的变化\"，处理\"时序依赖\"。\n比如：它会整合\"过去 5 个时间步的视觉特征\"，判断人类的移动趋势（“这个人一直在朝我这边走，速度大概 0.5 米/秒”），而不是只看\"当下这一帧\"的静态位置 —— 这能避免智能体\"短视\"，比如不会因为当下距离够远就忽视正在靠近的人类。\nActor-Critic（演员-评论家）头：决策+评估团队\n经过 ResNet-50 和 LSTM 处理后，得到了\"当前环境特征 + 历史变化趋势\"，接下来由这个\"团队\"输出最终决策：\n演员头（Actor Head）：\"决策者\"—— 输出具体的导航动作，比如\"向前移动 0.3 米\"“左转 15 度\"“保持静止”（动作是连续的，不是固定的几个选项） 评论家头（Critic Head）：”评估师\"—— 不直接做决策，而是评估\"演员头做出的这个动作好不好\"，输出一个\"价值分数\"（比如\"这个动作能让你更快到目标，且不会撞人，得分 8 分\"“这个动作会挡路，得分 2 分”）","3-未来轨迹预测future-trajectory-forecasting预判人类未来走哪#3. 未来轨迹预测（Future Trajectory Forecasting）——\u0026ldquo;预判人类未来走哪\u0026rdquo;":"任务目标： 预测人类未来多个时间步的轨迹\n注： 这是 SPM 中最重要的任务（实验证明其对性能提升最大），因\"预测未来轨迹\"需要处理更复杂的时序关系，所以用 **BiLSTM（双向 LSTM）**替代普通 LSTM。\n输入： 主网络的 $\\delta_R$ + 人类真实数量 $N$ + 当前人类位置 $P_i^t$\n网络结构：\nBiLSTM 编码器： 双向处理 $\\delta_R$、$N$、$P_i^t$ 的融合信息，输出特征 $\\Phi_{R;N;P}$（双向 LSTM 能同时利用\"过去记忆\"和\"未来趋势”，更适合长时序预测） 自注意力层： 以 $Q=K=V=\\Phi_{R;N;P}$ 处理，聚焦\"人类运动趋势相关的特征\"（如\"人类 A 前 2 步朝电梯走，预判他会继续向电梯移动\"），输出 $A_t$ 回归器 $\\phi_{traj}$： 基于 $A_t$ 预测未来 $H$ 步的人类轨迹 $\\hat{P}_i^{t+1:t+H}$： P^it+1:t+H=ϕtraj(At) \\hat{P}_i^{t+1:t+H} = \\phi_{traj}(A_t) P^it+1:t+H​=ϕtraj​(At​)损失函数： 采用 MSE（衡量\"预测轨迹\"与\"真实轨迹\"的差异）：\nLtraj=1∣M∣∑i∈M∥P^it+1:t+H−Pit+1:t+H∥2 L_{traj} = \\frac{1}{|M|} \\sum_{i \\in M} \\|\\hat{P}_i^{t+1:t+H} - P_i^{t+1:t+H}\\|^2 Ltraj​=∣M∣1​i∈M∑​∥P^it+1:t+H​−Pit+1:t+H​∥2其中 $P_i^{t+1:t+H}$ 为第 $i$ 个人类未来 $H$ 步的真实轨迹。","3-训练逻辑怎么让大脑学会好策略#3. 训练逻辑：怎么让\u0026quot;大脑\u0026quot;学会\u0026quot;好策略\u0026quot;？":"主策略网络是通过\"奖励机制“学习的 —— 就像训练宠物：做得好就给奖励，做得差就给惩罚，慢慢形成条件反射。核心是之前提到的 RtsocialnavR_t^{socialnav}Rtsocialnav​ 奖励函数，简化理解就是：\n加分项：\n靠近目标（−βdΔd-\\beta_d \\Delta_d−βd​Δd​，距离目标越近，加分越多） 成功到达目标（βsucc⋅Isucc\\beta_{succ} \\cdot I_{succ}βsucc​⋅Isucc​，直接加大额奖励） 扣分项：\n无意义动作（−rslack-r_{slack}−rslack​，比如原地打转） 碰撞（−rcoll-r_{coll}−rcoll​，撞墙或撞人扣大分） 离人太近（−rprox-r_{prox}−rprox​，距离小于 2 米扣分，越近扣越多） 挡人类轨迹（−rtraj-r_{traj}−rtraj​，挡住别人要走的路扣分） 通过不断迭代训练（7500 万步），主策略网络会逐渐学会”平衡效率和合规\"—— 既不会为了快而撞人，也不会为了合规而绕远路。","3-轨迹阻碍惩罚trajectory-obstruction-penalty#3. 轨迹阻碍惩罚（Trajectory Obstruction Penalty）":"该惩罚用于阻止机器人阻碍人类未来 $H$ 步的轨迹。它通过同时考虑\"当前与未来位置\"预判潜在阻碍，且对\"早期轨迹重叠\"的惩罚更重，计算公式如下：\nrtraj=∑k=t+1t+H∑i=1N{βtraj⋅1k−t+1若 dtraj−ik\u003c0.05 m0若 dtraj−ik≥0.05 m r_{traj} = \\sum_{k=t+1}^{t+H} \\sum_{i=1}^{N} \\begin{cases} \\beta_{traj} \\cdot \\frac{1}{k-t+1} \u0026 \\text{若 } d_{traj-i}^k \u003c 0.05 \\text{ m} \\\\ 0 \u0026 \\text{若 } d_{traj-i}^k \\geq 0.05 \\text{ m} \\end{cases} rtraj​=k=t+1∑t+H​i=1∑N​{βtraj​⋅k−t+11​0​若 dtraj−ik​\u003c0.05 m若 dtraj−ik​≥0.05 m​其中：\n$d_{traj-i}^k = |\\tau_a(k) - \\tilde{\\tau}_i(k)|$ 表示时间步 $k$ 时机器人与第 $i$ 个人类\"未来轨迹\"的距离 $\\frac{1}{k-t+1}$ 为时间衰减因子，确保距离当前时间越近的轨迹重叠，惩罚权重越大 当机器人距离目标不足 2.0 米时，该惩罚同样取消","3输出具体的导航动作#（3）输出：具体的导航动作":"最终由演员头输出\"连续的控制指令\"（比如速度、转向角度），直接驱动智能体移动。","4-与时空预认知模块的协作关系#4. 与时空预认知模块的协作关系":"主策略网络： 负责\"当下该做什么动作\"（比如现在走还是转）\n时空预认知模块： 负责\"未来会发生什么\"（比如人类会走到哪）\n协作逻辑： 主策略网络根据\"当下状态 + 未来预测\"做决策 —— 比如当下离人 3 米（安全），但预测 1 秒后会到 1.5 米（危险），主策略网络就会提前调整轨迹，而不是等进入危险区再反应。\n主策略网络包含两个核心组件：\n状态编码器（State Encoders）：从观测中提取视觉与时序特征 社交认知惩罚（SCP）：促进社交合规性的惩罚机制","a-算法#A* 算法":"A 算法*是一个经典的静态路径规划算法，广泛应用于机器人导航和游戏AI中。\n核心思想：\n使用启发式搜索在静态地图上找到从起点到终点的最优路径 综合考虑已走路径成本（$g(n)$）和预估剩余成本（$h(n)$） 评估函数：$f(n) = g(n) + h(n)$，其中 $h(n)$ 通常是欧氏距离或曼哈顿距离 工作原理：\n初始化： 将起点加入开放列表（open list） 迭代搜索： 从开放列表中选择 $f(n)$ 值最小的节点 将该节点移入关闭列表（closed list） 检查该节点的所有邻居节点 对于每个邻居节点，计算新的 $g$ 值，如果更优则更新 终止条件： 当目标节点被加入关闭列表时，回溯路径 在社交导航中的应用：\n预先计算一条从起点到终点的固定路径 假设环境是静态的，不考虑动态人类的存在 当遇到人类时，需要重新规划路径 局限性：\n无法适应动态环境：预先确定的路径无法应对人类移动 易导致碰撞：当人类移动到规划路径上时，机器人可能直接碰撞 需要全局地图：算法需要完整的静态地图信息","orca-算法#ORCA 算法":"ORCA（Optimal Reciprocal Collision Avoidance）算法是一个基于速度障碍的多智能体避障算法，可获取智能体的位置和速度\"先知信息\"（oracle access），以动态调整规划路径。\n核心思想：\n基于**速度障碍（Velocity Obstacle）**概念 每个智能体选择一个新的速度，使得在假设其他智能体也选择最优速度的情况下，避免碰撞 使用线性规划在速度空间中寻找可行速度 工作原理：\n速度障碍计算：\n对于每个其他智能体，计算一个速度障碍区域 该区域包含所有会导致碰撞的速度 速度障碍是一个圆锥形区域 ORCA 半平面：\n将速度障碍转换为ORCA 半平面 每个半平面定义了一个速度约束：$v \\cdot n \\geq u \\cdot n$ 其中 $n$ 是半平面的法向量，$u$ 是参考速度 线性规划求解：\n在满足所有 ORCA 半平面约束的条件下，选择最接近期望速度的速度 优化目标：$\\min |v - v_{pref}|$，其中 $v_{pref}$ 是期望速度 在社交导航中的应用：\n可以获取人类的位置和速度信息（oracle access） 实时计算速度障碍，动态调整机器人速度 假设所有智能体都遵循 ORCA 规则，实现相互避让 局限性：\n假设运动不受限制：ORCA 假设智能体可以在任意方向移动，但实际机器人可能有运动学约束（如非完整约束） 可能导致与静态障碍物碰撞：算法主要关注动态避障，可能忽略静态障碍 需要精确的位置和速度信息：在实际应用中，这些信息可能难以准确获取 计算复杂度：当环境中智能体数量较多时，线性规划的计算成本较高","pointnav-奖励函数#PointNav 奖励函数":"训练过程中，策略的更新由一个鼓励\"达成目标\"行为的奖励函数引导。每个时间步 $t$ 采用经典的 PointNav 奖励函数：\nRtpointnav=−βdΔd−rslack+βsucc⋅Isucc R_t^{pointnav} = -\\beta_d \\Delta_d - r_{slack} + \\beta_{succ} \\cdot I_{succ} Rtpointnav​=−βd​Δd​−rslack​+βsucc​⋅Isucc​其中：\n$\\Delta_d$ 为机器人到目标的测地线距离变化量 $r_{slack}$ 为防止不必要动作的步长惩罚项 $I_{succ}$ 为导航成功的指示变量（成功时为 1，否则为 0） $\\beta_d$ 与 $\\beta_{succ}$ 为权重系数","proximity-aware#Proximity-Aware":"Proximity-Aware 是一个基于强化学习的社交导航方法，该方法通过两个辅助任务建模人类与机器人的距离和方向，能有效捕捉当前时刻的人机距离关系。\n核心思想：\n使用当前时刻的人机距离和相对方向作为状态输入 通过两个辅助任务学习人机距离关系： 距离预测任务：预测机器人与人类的距离 方向预测任务：预测人类相对于机器人的方向 奖励函数基于当前距离：$r_{proximity} = -1/d_t$，其中 $d_t$ 是当前时刻的距离 工作原理：\n状态表示： $s_t = [d_t, \\theta_t, g_t]$，其中 $d_t$ 是当前距离，$\\theta_t$ 是相对角度，$g_t$ 是目标方向 辅助任务： 在训练过程中同时学习预测距离和方向，增强对当前人机关系的理解 策略学习： 使用 PPO 算法训练策略网络，学习在当前状态下选择最优动作 局限性：\n只考虑当前时刻的距离和方向，无法预测人类未来的移动轨迹 当人类突然改变方向时，只能被动反应，容易出现\"短视避障\" 无法提前规划路径以避免与人类未来位置发生冲突","一核心输入主策略网络的状态编码-delta_r#一、核心输入：主策略网络的状态编码 $\\delta_R$":"来源： 主策略网络中\"ResNet-50 编码器 + 2 层 LSTM“的输出结果。\n先由 ResNet-50 把\"深度图像\"翻译成\"视觉特征向量”（比如\"前方 3 米有人类、左侧 2 米是柜子\"） 再经 2 层 LSTM 处理\"时序依赖\"（比如\"这个人前 3 步朝我走，速度 0.5 米/秒\"） 最终输出的 $\\delta_R$ 是\"浓缩了当前环境 + 历史变化\"的高维数字特征，相当于主策略网络整理好的\"环境情报汇总\" 作用： 时空预认知模块直接用这个\"情报汇总\"做预测，不用再重复处理原始深度图像，既高效又能保证\"预判和决策基于同一套环境理解\"（比如主策略网络认为\"那是个人\"，预认知模块不会误判为\"障碍物\"）。","三auxiliary-information-的具体来源#三、Auxiliary Information 的具体来源":"这三个信息不是\"凭空产生\"，而是主策略网络处理原始数据后，从\"视觉特征 + 时序特征\"中解析出来的，流程如下：\n原始数据（深度图像 + 相对目标坐标）→ ResNet-50 → 提取\"静态视觉特征\"（包含障碍物、人类的轮廓、距离信息） 静态视觉特征 → 2 层 LSTM → 融合\"时序特征\"（人类移动趋势、环境变化）→ 生成 $\\delta_R$ 从 $\\delta_R$ 中进一步解析： 通过\"空间关系提取器\"得到 $S_R$（相对角度、距离） 通过\"初步人数检测\"得到 $N$（比如先判断\"有 2 个人类\"，后续由 classifier 优化精度） 通过\"初步位置回归\"得到 $P$（比如先粗略预测\"人类在前方 2-3 米处\"，后续由 regressor 优化坐标精度） 关键结论： Auxiliary Information 是从主策略网络的状态编码 $\\delta_R$ 中解析出的结构化信息，不是额外传感器数据，也不是预定义的固定值——会随着 $\\delta_R$ 的更新（每个时间步都更新）而动态变化，比如人类移动后，$S_R$ 和 $P$ 会实时更新。","主策略网络main-policy-network#主策略网络（Main Policy Network）":"Main Policy Network（主策略网络）—— 导航的\"核心决策大脑\"\n其实这个模块是整个系统的\"指挥官\"，核心作用是：接收传感器数据，分析当前环境和自身状态，最终输出具体的导航动作（比如向前走、左转、右转、减速），同时通过奖励/惩罚机制不断学习\"更优的导航策略\"。","二辅助输入auxiliary-informations_r-n-p#二、辅助输入：Auxiliary Information（$S_R, N, P$）":"这部分是从 $\\delta_R$ 中解析出的\"具体结构化信息\"，不是额外输入，而是对 $\\delta_R$ 的\"拆解和明确\"，方便模块针对性预测：\n$S_R$（Spatial Relationship）：人类与智能体的\"相对空间关系\"，比如\"人类在智能体前方 30 度、距离 2.5 米\"，从 $\\delta_R$ 中的视觉特征和目标坐标推导而来 $N$（Number of Humans）：当前环境中检测到的人类数量（初始值来自主策略网络的初步特征解析，后续会被辅助任务的 classifier 优化） $P$（Human Positions）：人类相对于智能体的\"当前位置坐标\"（比如\"人类 A 在 $(x=2.3, y=1.5)$ 米处\"），同样从 $\\delta_R$ 中提取的视觉特征回归得到 简单说： 核心输入是\"浓缩情报\" $\\delta_R$，辅助输入是\"拆解后的具体情报\" $S_R, N, P$，两者都来自主策略网络，没有额外传感器开销。","四classifier-和两个-regressor-的具体对应#四、Classifier 和两个 Regressor 的具体对应":"这三个组件是时空预认知模块的\"核心工作单元\"，分别对应 Falcon 框架的三个辅助任务，本质是\"通过多任务学习，让模块同时掌握’数人数、定位置、猜轨迹’三种能力\"，最终提升时空理解能力。\n组件类型 对应辅助任务 核心作用 输入（均为 $\\delta_R$） 输出结果 损失函数 Classifier（分类器） 人数估计（Population Estimation） 预测当前环境中\"真实人类数量\"（0-M） 状态编码 $\\delta_R$ 每个可能人数的概率（比如\"0 人：5%、1 人：30%、2 人：65%\"） 交叉熵损失 $\\mathcal{L}_{count}$ Regressor 1（回归器） 位置估计（Position Estimation） 精准回归\"人类当前相对智能体的坐标\" 状态编码 $\\delta_R$ 每个人类的 $(x, y)$ 坐标（比如\"人类 A：$(2.3, 1.5)$ 米\"） MSE 损失 $\\mathcal{L}_{pos}$ Regressor 2（回归器） 轨迹预测（Trajectory Forecasting） 预测\"人类未来 $H$ 个时间步的坐标\" 状态编码 $\\delta_R$ 每个人类的未来位置序列（比如\"人类 A：$t+1$ 步 $(2.1,1.6)$、$t+2$ 步 $(1.9,1.7)$\"） MSE 损失 $\\mathcal{L}_{traj}$ 逐个拆解（通俗理解）：\nClassifier（分类器）= “人数计数器”\n任务本质： 分类问题（比如\"当前人数是 0、1、2、…、$M$\"，$M$ 是最大预测人数，比如 6 人） 工作逻辑： 输入 $\\delta_R$（包含当前环境的视觉 + 时序特征），输出\"每个人数选项的概率\"，最终选择概率最高的作为预测人数 举个例子： $\\delta_R$ 中包含\"两个移动的人形轮廓特征\"，分类器会输出\"2 人：90% 概率\"，通过交叉熵损失（惩罚预测概率与真实人数的差异）不断优化，让\"计数\"更准确 第一个 Regressor（位置估计回归器）= “实时定位仪”\n任务本质： 回归问题（预测连续的坐标值，不是离散类别） 工作逻辑： 输入 $\\delta_R$，针对每个检测到的人类，输出精准的 $(x, y)$ 相对坐标 核心价值： 解决\"视觉特征只能判断’有人类’，但不知道’具体在哪’“的问题——比如主策略网络知道\"前方有人”，这个回归器能精准告诉它\"在前方 2.3 米、偏右 10 度的位置\"，为避障提供精确依据 第二个 Regressor（轨迹预测回归器）= “未来预言家”\n任务本质： 序列回归问题（预测未来多个时间步的连续坐标） 工作逻辑： 输入 $\\delta_R$（包含人类的历史移动趋势，比如\"前 3 步朝智能体移动，速度 0.5 米/秒\"），输出未来 $H$ 个时间步（比如 $H=5$）的人类位置序列 核心价值： 解决\"只能看当下，看不到未来\"的短视问题——比如现在人类在 3 米外，但回归器预测\"1 秒后会到 1.8 米处\"，主策略网络就能提前调整轨迹，而不是等靠近了才反应 关键补充：三个组件的协同关系\n共享输入： 都用主策略网络的 $\\delta_R$，确保\"基于同一套环境理解\"做预测 结果互补： 人数估计（classifier）确定\"有多少人要预测\"，位置估计（regressor1）确定\"现在在哪\"，轨迹预测（regressor2）确定\"未来会到哪\" 损失合并： 三个组件的损失（$\\mathcal{L}{count} + \\mathcal{L}{pos} + \\mathcal{L}{traj}$）乘以权重 $\\beta{aux}$ 后，加入总损失函数（公式 12），一起优化——比如人数预测不准、位置回归偏差大，都会让总损失上升，倒逼模型同时提升三个任务的精度 总结：核心逻辑链\n主策略网络生成\"环境情报\" $\\delta_R$ → 拆解出辅助信息 $S_R, N, P$ → 时空预认知模块用 $\\delta_R$ 驱动三个组件（classifier + 两个 regressor）→ 分别输出\"人数、当前位置、未来轨迹\" → 三个组件的损失共同优化模型 → 提升智能体的时空理解能力，最终帮助主策略网络做出更精准的导航决策。\n根据论文第三章 3.3 节，SPM 包含 3 个辅助任务，每个任务的网络结构都是**“LSTM/BiLSTM + 自注意力（Self-Attention） + 分类器/回归器”**，且输入均依赖主网络的潜在变量 $\\delta_R$。","在-autodl-上复现-falcon#在 Autodl 上复现 Falcon":"conda create -n falcon python=3.9 cmake=3.14.0 # 在base环境安装mamba（仅需一次） conda install mamba -n base -c conda-forge -y # 用mamba给falcon环境安装habitat-sim（核心：-n falcon 指定目标环境） mamba install habitat-sim=0.3.1 withbullet headless -n falcon -c conda-forge -c aihabitat -y conda activate falcon 接下来使用Robosense比赛所提供的代码而不是原生。原版 SocialNav 侧重于让人形机器人（Humanoid）在环境中跟随或者寻找人，它的 Reward（奖励函数）和 Episode（任务集）定义是通用的；RoboSense 比赛版 SocialNav 目标是从 A 点走到 B 点，同时避开动态的人，它使用的是 Social-HM3D 这个特定的数据集，里面的 NPC（路人）的轨迹是经过特殊生成的（基于 ORCA 算法），且有人口密度分级。 但是说白了就是 Robosense 是对其的发展，直接战未来：\ngit clone --recurse-submodules https://github.com/robosense2025/track2.git mv track2 Falcon cd Falcon/Falcon # 安装Python依赖 pip install -e habitat-lab pip install -e habitat-baselines pip install -r requirements.txt 下面是数据集部分：\ncd /root/Falcon/Falcon mkdir -p data mkdir -p /root/autodl-fs/habitat_data/scene_datasets # 创建软链接 cd /root/Falcon/Falcon/data ln -s /root/autodl-fs/habitat_data/scene_datasets ./scene_datasets 接下来是场景数据集： 点击这个链接登录，但是我一直卡在最终的这个地方： 在页面上找到 “API Tokens” 部分。 点击 “Generate” (生成)。 Token ID 就是你的 –username。 Token Secret 就是你的 –password。 中途面临一个ImportError: libEGL.so.1的问题，直接apt-get update \u0026\u0026 apt-get install -y libegl1-mesa libgl1-mesa-glx libgl1-mesa-dev掉\n实际上这里不应该装mesa，详见SocialNav-map的复现 首先下载HM3D：\npython -m habitat_sim.utils.datasets_download --username xxx --password xxx --uids hm3d_minival 接下来下载任务数据 (Episode Datasets)，这是告诉机器人“从哪里走到哪里”的指令包。给的是 ™ HuggingFace 的链接\n#!/usr/bin/env python3 \"\"\" 使用 Python huggingface_hub 库下载数据，支持镜像站和SSL配置 \"\"\" import os import ssl from pathlib import Path # 配置镜像站 os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\" # 如果遇到SSL证书问题，可以临时禁用验证（不推荐生产环境） # ssl._create_default_https_context = ssl._create_unverified_context try: from huggingface_hub import snapshot_download # 设置下载目录 local_dir = Path(__file__).parent / \"track2-social-navigation\" local_dir.mkdir(parents=True, exist_ok=True) # 下载指定路径的文件 # 注意：allow_patterns 和 ignore_patterns 同时使用时，需要正确配置 snapshot_download( repo_id=\"robosense/datasets\", repo_type=\"dataset\", local_dir=str(local_dir), allow_patterns=[\"track2-social-navigation/**\"], resume_download=True, token=\"token!!!\", ) print(f\"下载完成，文件保存在: {local_dir}\") except ImportError: print(\"请先安装: pip install huggingface_hub\") except Exception as e: print(f\"下载失败: {e}\") print(\"\\n如果遇到SSL证书错误，可以取消注释脚本中的SSL禁用行（第15行）\") 然后是下载机器人动画 (Leg Animation)，即 Spot 机器人的走路动作文件\n# 回到项目根目录 cd /root/Falcon/Falcon # 创建目录并下载 mkdir -p data/robots/spot_data wget https://github.com/facebookresearch/habitat-lab/files/12502177/spot_walking_trajectory.csv -O data/robots/spot_data/spot_walking_trajectory.csv 再之后是下载多智能体资产 (Multi-agent Assets)，这里包含人类模型（Humanoids）和一些仿真资产，而被 Git LFS 折磨惨了，选择不用python -m habitat_sim.utils.datasets_download --uids habitat_humanoids hab3_bench_assets hab_spot_arm而是额外写一个Python下：\n#!/usr/bin/env python3 \"\"\" 使用 huggingface_hub 直接下载数据集，避免 Git LFS 问题 实现与 habitat_sim.utils.datasets_download 相同的功能 \"\"\" import os import sys from pathlib import Path # 配置镜像站（可选） os.environ[\"HF_ENDPOINT\"] = os.environ.get(\"HF_ENDPOINT\", \"https://hf-mirror.com\") try: from huggingface_hub import snapshot_download except ImportError: print(\"错误：请先安装 huggingface_hub\") print(\"运行: pip install huggingface_hub\") sys.exit(1) # 数据集配置（从 habitat_sim 的数据源配置中提取） DATASETS = { \"habitat_humanoids\": { \"repo_id\": \"ai-habitat/habitat_humanoids\", \"version\": \"main\", \"link_path\": \"humanoids/humanoid_data\", \"version_dir\": \"habitat_humanoids\", }, \"hab3_bench_assets\": { \"repo_id\": \"ai-habitat/hab3_bench_assets\", \"version\": \"main\", \"link_path\": \"hab3_bench_assets\", \"version_dir\": \"hab3_bench_assets\", }, \"hab_spot_arm\": { \"repo_id\": \"ai-habitat/hab_spot_arm\", \"version\": \"v2.0\", \"link_path\": \"robots/hab_spot_arm\", \"version_dir\": \"hab_spot_arm\", }, } # 默认数据路径（相对于执行脚本时的当前工作目录） # 与 habitat_sim.utils.datasets_download 的行为一致 DEFAULT_DATA_PATH = Path(\"./data\").resolve() def download_dataset(uid: str, data_path: Path = None, token: str = None): \"\"\" 下载指定的数据集 Args: uid: 数据集唯一标识符 data_path: 数据存储根目录（默认为 ./data） token: HuggingFace token（如果需要认证） \"\"\" if uid not in DATASETS: print(f\"错误：未知的数据集 ID: {uid}\") print(f\"支持的数据集: {', '.join(DATASETS.keys())}\") return False if data_path is None: data_path = DEFAULT_DATA_PATH data_path = Path(data_path).resolve() config = DATASETS[uid] version_dir = data_path / \"versioned_data\" / config[\"version_dir\"] link_path = data_path / config[\"link_path\"] print(f\"\\n{'='*60}\") print(f\"下载数据集: {uid}\") print(f\"仓库: {config['repo_id']}\") print(f\"版本目录: {version_dir}\") print(f\"符号链接: {link_path}\") print(f\"{'='*60}\\n\") # 检查是否已存在 if version_dir.exists(): print(f\"警告：版本目录已存在: {version_dir}\") response = input(\"是否删除并重新下载？(y/n): \").strip().lower() if response == 'y': import shutil shutil.rmtree(version_dir) if link_path.exists() or link_path.is_symlink(): if link_path.is_symlink(): link_path.unlink() else: shutil.rmtree(link_path) else: print(\"跳过下载\") return True # 创建版本目录 version_dir.parent.mkdir(parents=True, exist_ok=True) try: # 使用 huggingface_hub 下载 print(f\"开始下载 {config['repo_id']}...\") snapshot_download( repo_id=config[\"repo_id\"], repo_type=\"dataset\", local_dir=str(version_dir), local_dir_use_symlinks=False, revision=config[\"version\"], token=token, resume_download=True, ) print(f\"下载完成: {version_dir}\") # 创建符号链接 link_path.parent.mkdir(parents=True, exist_ok=True) if link_path.exists() or link_path.is_symlink(): if link_path.is_symlink(): link_path.unlink() else: import shutil shutil.rmtree(link_path) link_path.symlink_to(version_dir, target_is_directory=True) print(f\"符号链接已创建: {link_path} -\u003e {version_dir}\") print(f\"\\n{'='*60}\") print(f\"数据集 {uid} 下载成功！\") print(f\"源目录: {version_dir}\") print(f\"符号链接: {link_path}\") print(f\"{'='*60}\\n\") return True except Exception as e: print(f\"下载失败: {e}\") import traceback traceback.print_exc() return False def main(): \"\"\"主函数\"\"\" import argparse parser = argparse.ArgumentParser( description=\"使用 huggingface_hub 下载 Habitat 数据集（无需 Git LFS）\" ) parser.add_argument( \"--uids\", nargs=\"+\", default=[\"habitat_humanoids\", \"hab3_bench_assets\", \"hab_spot_arm\"], help=\"要下载的数据集 ID 列表（默认: habitat_humanoids hab3_bench_assets hab_spot_arm）\", ) parser.add_argument( \"--data-path\", type=str, default=None, help=f'数据存储根目录（默认: {DEFAULT_DATA_PATH}）', ) parser.add_argument( \"--token\", type=str, default=None, help=\"HuggingFace token（如果需要认证）\", ) parser.add_argument( \"--list\", action=\"store_true\", help=\"列出所有支持的数据集\", ) args = parser.parse_args() if args.list: print(\"支持的数据集:\") for uid, config in DATASETS.items(): print(f\" {uid}: {config['repo_id']} (版本: {config['version']})\") return data_path = Path(args.data_path) if args.data_path else DEFAULT_DATA_PATH print(f\"数据路径: {data_path}\") print(f\"要下载的数据集: {', '.join(args.uids)}\\n\") success_count = 0 for uid in args.uids: if download_dataset(uid, data_path, args.token): success_count += 1 print(f\"\\n总计: {success_count}/{len(args.uids)} 个数据集下载成功\") if success_count \u003c len(args.uids): sys.exit(1) if __name__ == \"__main__\": main() 对了还不能忘记 pretrained_models，但是它很烦是Google云盘里的，好在只有243MB，可以下下来之后再传到autodl上去\n万事俱备，开始运行评测脚本：\npython -u -m habitat_baselines.eval --config-name=social_nav_v2/falcon_hm3d.yaml 运行后立即遇到了 OpenGL 上下文创建失败的问题：GL::Context: cannot retrieve OpenGL version: GL::Renderer::Error::InvalidValue\n问题分析：habitat-sim 试图在 AutoDL 容器里创建一个图形渲染上下文，但是失败了。随后的 BrokenPipeError 和 core dumped 只是因为主进程试图跟一个已经崩溃的渲染进程通信导致的后果。这个问题通常是 EGL（无头渲染接口）配置不当引起的。在 AutoDL 这种无显示器的服务器上，必须强制指定渲染模式为无头模式。\n虽然系统确实找到了 NVIDIA 显卡（found 3 EGL devices, choosing EGL device 0），但是在创建 OpenGL 上下文时失败了。\n初步尝试：先尝试设置简单的环境变量：\nexport EGL_PLATFORM=surfaceless export MAGNUM_LOG=verbose export HABITAT_SIM_LOG=verbose 但问题依然存在，出现了内存错误：malloc_consolidate(): unaligned fastbin chunk detected，随后进程崩溃。\n简单的环境变量设置不够，需要更完整的 NVIDIA EGL 配置。编写了一个脚本来强制使用 NVIDIA 的 EGL 库：\n为什么需要强制使用 NVIDIA 库？Mesa vs NVIDIA 冲突详解：\nMesa 是什么：\nMesa 是开源的 OpenGL/EGL/Vulkan 实现 提供软件渲染（纯 CPU）或通过 DRI 使用集成显卡（Intel/AMD） 是 Linux 发行版的默认图形库 NVIDIA 驱动：\nNVIDIA 提供专有的硬件加速 OpenGL/EGL 实现 直接访问 NVIDIA GPU，性能远高于 Mesa 软件渲染 库文件：libEGL_nvidia.so.0、libGLX_nvidia.so.0 冲突原因：\nLinux 动态链接器默认按库搜索路径顺序加载库 系统通常先找到 Mesa 的 libEGL.so.1（在 /usr/lib/x86_64-linux-gnu/） Mesa 无法访问 NVIDIA GPU，只能使用 CPU 或集成显卡 导致 habitat-sim 无法创建硬件加速的 OpenGL 上下文 为什么先安装 Mesa？：\n解决 ImportError: libEGL.so.1 缺失问题（提供符号链接） 但 Mesa 只是\"占位符\"，实际渲染需要 NVIDIA 库 解决方案：\n使用 LD_PRELOAD 强制优先加载 NVIDIA 库 设置 __EGL_VENDOR_LIBRARY_FILENAMES 指向 NVIDIA 配置 确保子进程也继承这些设置（多进程环境） #!/bin/bash # 强制使用 NVIDIA EGL 运行 Habitat-Sim echo \"=== 设置 NVIDIA EGL 环境 ===\" # 1. 强制指定 NVIDIA 的 EGL 配置 export __EGL_VENDOR_LIBRARY_FILENAMES=/usr/share/glvnd/egl_vendor.d/10_nvidia.json # 2. 设置无头模式 export EGL_PLATFORM=surfaceless # 3. 确保使用 NVIDIA 的 GL 库（GLX用于OpenGL） export __GLX_VENDOR_LIBRARY_NAME=nvidia # 4. 设置额外的 EGL 相关环境变量，确保子进程也能正确初始化 export EGL_DEVICE_ID=0 export __GL_SYNC_TO_VBLANK=0 # 5. 确保 NVIDIA 驱动相关的环境变量被设置 export __NVIDIA_BUG_REPORT=0 # 6. 使用 LD_PRELOAD 强制加载 NVIDIA 的 EGL 库，确保子进程也能正确初始化 # 查找 NVIDIA 库的实际位置 NVIDIA_EGL_LIB=\"\" NVIDIA_GLX_LIB=\"\" # 尝试多个可能的库位置 for egl_lib in /lib/x86_64-linux-gnu/libEGL_nvidia.so.0 /usr/lib/x86_64-linux-gnu/libEGL_nvidia.so.0; do if [ -f \"$egl_lib\" ]; then NVIDIA_EGL_LIB=\"$egl_lib\" break fi done for glx_lib in /usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0 /usr/lib/libGLX_nvidia.so.0; do if [ -f \"$glx_lib\" ]; then NVIDIA_GLX_LIB=\"$glx_lib\" break fi done # 如果找到了库，设置 LD_PRELOAD if [ -n \"$NVIDIA_EGL_LIB\" ] \u0026\u0026 [ -n \"$NVIDIA_GLX_LIB\" ]; then export LD_PRELOAD=\"${NVIDIA_EGL_LIB}:${NVIDIA_GLX_LIB}\" echo \" 设置 LD_PRELOAD 强制使用 NVIDIA 库\" elif [ -n \"$NVIDIA_EGL_LIB\" ]; then export LD_PRELOAD=\"$NVIDIA_EGL_LIB\" echo \" 设置 LD_PRELOAD 强制使用 NVIDIA EGL 库\" fi echo \"环境变量设置：\" echo \" __EGL_VENDOR_LIBRARY_FILENAMES=$__EGL_VENDOR_LIBRARY_FILENAMES\" echo \" EGL_PLATFORM=$EGL_PLATFORM\" echo \" EGL_DEVICE_ID=$EGL_DEVICE_ID\" echo \" LD_PRELOAD=$LD_PRELOAD\" echo \" __GLX_VENDOR_LIBRARY_NAME=$__GLX_VENDOR_LIBRARY_NAME\" echo \"\" # 检查 NVIDIA 库是否存在 if [ -z \"$NVIDIA_EGL_LIB\" ]; then echo \"警告：找不到 NVIDIA EGL 库，可能无法正常工作\" echo \" 请确保已安装 NVIDIA 驱动和相应的库文件\" fi echo \"开始运行命令...\" echo \"\" # 获取 falcon conda 环境的 Python 路径 FALCON_PYTHON=\"/root/miniconda3/envs/falcon/bin/python\" # 如果第一个参数是 python，替换为 falcon 环境的 python # 这样可以确保使用正确的 Python 环境和所有环境变量都被继承 if [ \"$1\" = \"python\" ] || [ \"$1\" = \"python3\" ]; then # 替换 python 为 falcon 环境的 python，保持其他参数 shift exec \"$FALCON_PYTHON\" \"$@\" else # 如果不是 python 命令，尝试使用 conda run（环境变量可能需要额外处理） # 这里我们假设用户知道自己在做什么 exec \"$@\" fi 使用方法：将脚本保存为 run_with_nvidia_egl.sh 并赋予执行权限，然后通过脚本运行评测命令：\n./run_with_nvidia_egl.sh python -u -m habitat_baselines.eval \\ --config-name=social_nav_v2/falcon_hm3d.yaml \\ habitat_baselines.num_environments=1 这个脚本其实是最终版本了，在这之前还有下列一系列问题的，第一个问题：\nModuleNotFoundError: No module named 'habitat_baselines' 检查发现 run_with_nvidia_egl.sh 脚本使用了系统的 Python，而 habitat_baselines 模块安装在 falcon conda 环境中。\n修复方案：修改 run_with_nvidia_egl.sh，自动使用 falcon conda 环境的 Python：\nFALCON_PYTHON=\"/root/miniconda3/envs/falcon/bin/python\" if [ \"$1\" = \"python\" ] || [ \"$1\" = \"python3\" ]; then shift exec \"$FALCON_PYTHON\" \"$@\" fi 修复 conda 环境后，程序可以导入模块，但在初始化多进程环境时崩溃：\nmalloc_consolidate(): unaligned fastbin chunk detected ConnectionResetError: [Errno 104] Connection reset by peer Aborted (core dumped) 问题在于多进程环境下，子进程无法正确继承 EGL 相关环境变量。虽然主进程设置了 EGL_PLATFORM=surfaceless 等环境变量，但子进程在初始化 EGL/OpenGL 时仍然失败。\n修复方案：在 run_with_nvidia_egl.sh 中添加 LD_PRELOAD 强制加载 NVIDIA 库，并添加额外的 EGL 环境变量：\n# 查找并设置 NVIDIA 库的 LD_PRELOAD NVIDIA_EGL_LIB=\"/lib/x86_64-linux-gnu/libEGL_nvidia.so.0\" NVIDIA_GLX_LIB=\"/usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0\" export LD_PRELOAD=\"${NVIDIA_EGL_LIB}:${NVIDIA_GLX_LIB}\" # 额外的 EGL 环境变量确保子进程正确初始化 export EGL_DEVICE_ID=0 export __GL_SYNC_TO_VBLANK=0 export __NVIDIA_BUG_REPORT=0 修复 EGL 问题后，程序可以初始化 Simulator，但遇到数据集路径错误：\nFileNotFoundError: Could not find dataset file `data/datasets/pointnav/social-hm3d/val` 坑爹的是，这个地方的报错和上面的一模一样，然后日志没有专门写出导一个文件中，轻易就超出了终端的最大长度，导致排查了好久，这里进行了最小元测试：\ntest_egl_step_by_step.py: 测试 EGL/OpenGL 初始化，验证环境变量、habitat_sim 导入、Simulator 创建、多进程环境 #!/usr/bin/env python3 # -*- coding: utf-8 -*- \"\"\" 最小元测试：逐步验证每个配置环节 \"\"\" import os import sys import json import gzip def test_step(step_num, step_name): \"\"\"打印测试步骤标题\"\"\" print(\"\\n\" + \"=\" * 60) print(f\"步骤 {step_num}: {step_name}\") print(\"=\" * 60) def main(): print(\"=\" * 60) print(\"最小元测试：逐步验证配置\") print(\"=\" * 60) # 步骤1: 检查数据集文件是否存在 test_step(1, \"检查数据集文件\") dataset_file = \"data/datasets/pointnav/hm3d/val/val.json.gz\" if not os.path.exists(dataset_file): print(f\" [FAIL] 数据集文件不存在: {dataset_file}\") return False print(f\" [OK] 数据集文件存在: {dataset_file}\") # 步骤2: 检查数据集文件内容 test_step(2, \"检查数据集文件内容\") try: with gzip.open(dataset_file, 'rt') as f: data = json.load(f) print(f\" [OK] 数据集文件可以读取\") print(f\" Episodes 数量: {len(data.get('episodes', []))}\") if len(data.get('episodes', [])) == 0: print(f\" [WARN] 数据集文件为空（没有episodes）\") return False # 获取第一个episode的场景路径 first_episode = data['episodes'][0] scene_id = first_episode.get('scene_id', '') print(f\" 第一个episode的场景ID: {scene_id}\") except Exception as e: print(f\" [FAIL] 读取数据集文件失败: {e}\") import traceback traceback.print_exc() return False # 步骤3: 检查场景文件是否存在 test_step(3, \"检查场景文件路径\") if not scene_id: print(f\" [FAIL] 场景ID为空\") return False # 场景ID可能是相对路径或绝对路径 print(f\" 数据集中的场景路径: {scene_id}\") # 尝试多个可能的路径 possible_paths = [ scene_id, # 原始路径 os.path.join(os.getcwd(), scene_id), # 相对于当前目录 scene_id.replace('data/', ''), # 去掉data前缀 ] scene_found = False for path in possible_paths: if os.path.exists(path): print(f\" [OK] 场景文件存在: {path}\") print(f\" 文件大小: {os.path.getsize(path) / (1024*1024):.2f} MB\") scene_found = True break if not scene_found: print(f\" [FAIL] 场景文件不存在\") print(f\" 尝试过的路径:\") for path in possible_paths: print(f\" - {path}\") # 检查场景目录结构 print(f\"\\n 检查场景目录结构...\") scene_dir = os.path.dirname(scene_id) if os.path.exists(scene_dir): print(f\" [OK] 场景目录存在: {scene_dir}\") files = os.listdir(scene_dir) print(f\" 目录中的文件: {files[:10]}\") else: print(f\" [FAIL] 场景目录不存在: {scene_dir}\") # 检查hm3d目录结构 hm3d_base = \"data/scene_datasets/hm3d\" if os.path.exists(hm3d_base): print(f\"\\n 检查 {hm3d_base} 目录结构...\") subdirs = os.listdir(hm3d_base) print(f\" 子目录: {subdirs}\") # 检查val目录 val_dir = os.path.join(hm3d_base, \"val\") if os.path.exists(val_dir): print(f\" [OK] val目录存在\") val_subdirs = os.listdir(val_dir)[:5] print(f\" val目录中的前5个条目: {val_subdirs}\") else: print(f\" [FAIL] val目录不存在\") return False # 步骤4: 检查配置文件 test_step(4, \"检查配置文件\") # 检查数据集配置 dataset_config = \"habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml\" if os.path.exists(dataset_config): print(f\" [OK] 数据集配置文件存在\") with open(dataset_config, 'r') as f: content = f.read() if 'hm3d' in content: print(f\" [OK] 配置使用hm3d路径\") else: print(f\" [FAIL] 数据集配置文件不存在\") return False # 检查任务配置 task_config = \"habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml\" if os.path.exists(task_config): print(f\" [OK] 任务配置文件存在\") with open(task_config, 'r') as f: content = f.read() if 'social_nav_v2: hm3d' in content or 'social_nav_v2:hm3d' in content.replace(' ', ''): print(f\" [OK] 任务配置引用hm3d数据集\") else: print(f\" [FAIL] 任务配置文件不存在\") return False # 步骤5: 测试实际加载数据集 test_step(5, \"测试加载数据集（使用habitat API）\") try: import habitat from omegaconf import DictConfig # 创建配置 dataset_cfg = { 'type': 'PointNav-v1', 'split': 'val', 'data_path': 'data/datasets/pointnav/hm3d/{split}/{split}.json.gz' } print(f\" 尝试加载数据集...\") # 这里只是测试配置，不实际加载（因为可能需要更多依赖） print(f\" [OK] 数据集配置格式正确\") except Exception as e: print(f\" [WARN] 无法测试habitat API: {e}\") print(\"\\n\" + \"=\" * 60) print(\"测试完成\") print(\"=\" * 60) return True if __name__ == '__main__': success = main() sys.exit(0 if success else 1) test_minimal_step_by_step.py: 最小元测试，验证数据集文件、场景文件路径、配置文件 test_scene_paths.py: 专门诊断场景路径问题 test_complete_flow.py: 完整流程测试 #!/usr/bin/env python3 # -*- coding: utf-8 -*- \"\"\" 完整流程测试：模拟实际运行环境 \"\"\" import os import sys def main(): print(\"=\" * 60) print(\"完整流程测试\") print(\"=\" * 60) # 测试1: 场景文件 print(\"\\n1. 测试场景文件路径...\") scene_path = \"data/scene_datasets/hm3d/val/00808-y9hTuugGdiq/y9hTuugGdiq.basis.glb\" if os.path.exists(scene_path): print(f\" [OK] 场景文件存在: {scene_path}\") print(f\" 文件大小: {os.path.getsize(scene_path) / (1024*1024):.2f} MB\") else: print(f\" [FAIL] 场景文件不存在: {scene_path}\") return False # 测试2: 数据集文件 print(\"\\n2. 测试数据集文件...\") dataset_path = \"data/datasets/pointnav/hm3d/val/val.json.gz\" if os.path.exists(dataset_path): print(f\" [OK] 数据集文件存在: {dataset_path}\") # 数据集文件可能为空，但这不是致命错误 print(f\" [WARN] 数据集文件可能为空，但程序可能从其他地方获取场景信息\") else: print(f\" [FAIL] 数据集文件不存在: {dataset_path}\") return False # 测试3: 配置文件 print(\"\\n3. 测试配置文件...\") configs = [ \"habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml\", \"habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml\", ] for config in configs: if os.path.exists(config): print(f\" [OK] {config}\") else: print(f\" [FAIL] {config}\") return False print(\"\\n\" + \"=\" * 60) print(\"所有基本检查通过！\") print(\"=\" * 60) print(\"\\n现在可以尝试运行评估命令:\") print(\" ./run_with_nvidia_egl.sh python -u -m habitat_baselines.eval \\\\\") print(\" --config-name=social_nav_v2/falcon_hm3d.yaml \\\\\") print(\" habitat_baselines.num_environments=1\") return True if __name__ == '__main__': success = main() sys.exit(0 if success else 1) 中间的两步一方面是最小元测试的过程中通过排除法检查发现配置文件引用了 social-hm3d，但实际数据集目录是 hm3d。\n修复方案：\n创建新的数据集配置文件 habitat-lab/habitat/config/habitat/dataset/social_nav_v2/hm3d.yaml，设置 data_path: data/datasets/pointnav/hm3d/{split}/{split}.json.gz 修改 habitat-lab/habitat/config/benchmark/nav/socialnav_v2/falcon_hm3d_task.yaml，将数据集引用从 social-hm3d 改为 hm3d 修复数据集配置后，程序可以加载数据集，但场景文件路径错误： AssertionError: ESP_CHECK failed: No Stage Attributes exists for requested scene 'data/scene_datasets/hm3d/val/00808-y9hTuugGdiq/y9hTuugGdiq.basis.glb' 检查发现：\ndata/scene_datasets/hm3d 是指向 /root/autodl-fs/habitat_data/versioned_data/hm3d-0.2/hm3d 的符号链接 实际目录中只有 minival 子目录，没有 val 子目录 场景文件实际存储在 minival 目录下 修复方案：在场景数据实际目录中创建符号链接： cd /root/autodl-fs/habitat_data/versioned_data/hm3d-0.2/hm3d ln -sf minival val 这样 data/scene_datasets/hm3d/val/ 就可以正确访问到场景文件了。\n最后程序总算成功启动：\nEGL/OpenGL 初始化成功（检测到 NVIDIA RTX 4090，OpenGL 3.0.0） 场景文件加载成功 多进程环境正常工作 评估进度正常（2/1087 episodes） 注意：运行过程中会出现大量 Debug 日志 Can't project end-point to navmesh，这是正常的调试信息，表示某些 episode 的坐标无效，程序会自动跳过，不影响评估结果。","基线方法#基线方法":"","实验与结果#实验与结果":"","实验结果与分析#实验结果与分析":"","实验设置#实验设置":"训练算法： 强化学习智能体采用 DD-PPO 算法 [53] 训练，且所有模型使用相同超参数 随机性控制： 每种算法均采用 3 个不同随机种子独立运行 3 次，最终结果取各指标的均值与标准差 模型初始化： 模型初始化权重来自预训练的 PointNav 模型 [57]，并在社交导航任务上进行 1000 万步微调 训练硬件： 训练过程使用 4 块 Nvidia RTX 3090 显卡，同时运行 8 个并行环境 数据集划分： 模型在 Social-HM3D 训练集上训练，在 Social-HM3D 测试集和 Social-MP3D 测试集上测试 泛化评估： Social-MP3D 的测试结果用于评估 Falcon 的零样本泛化能力","局限性#局限性":"Falcon 虽能实现较高的成功率，但 Proximity-Aware（成功率约 20%）在避障方面表现更优。这一现象暴露了现有评估指标的局限性：\n在拥挤环境中，指标可能过度优先考虑**“社交舒适度”，而忽视“任务完成度”** 本基准目前未涵盖**“礼让”**等更高阶的人类社交行为","总奖励函数#总奖励函数":"社交导航的总奖励函数为\"目标导向奖励\"与\"社交认知惩罚\"的差值：\nRtsocialnav=Rtpointnav−Rtscp R_t^{socialnav} = R_t^{pointnav} - R_t^{scp} Rtsocialnav​=Rtpointnav​−Rtscp​其中，$R_t^{scp}$ 为社交认知惩罚的总和：\nRtscp=rcoll+rprox+rtraj R_t^{scp} = r_{coll} + r_{prox} + r_{traj} Rtscp​=rcoll​+rprox​+rtraj​主策略网络采用 DD-PPO 算法进行训练，优化目标为 PPO 损失 $L_{main}$。","方法论#方法论":"考虑这样一个社交导航任务：机器人 $a$ 在存在 $N$ 个动态人类（记为 $i \\in {1, \\ldots, N}$）的环境中导航。机器人从初始位形 $q_a \\in Q$ 出发，需持续选择动作以生成一条通往目标位形 $g_a \\in Q$ 的路径 $\\tau_a$，同时避免与静态障碍及动态人类发生碰撞。\n其总体目标可建模如下：\nτa=arg⁡min⁡τ∈T(ca(τ)+λacas(τ,τ1:N))s.t.Aa(τa)∉Cobs,Aa(τa)∩Ai(τi)=∅,τa(0)=qa,τa(T)=ga \\begin{aligned} \\tau_a \u0026= \\arg \\min_{\\tau \\in T} \\left( c_a(\\tau) + \\lambda_a c_a^s(\\tau, \\tau_{1:N}) \\right) \\\\ \\text{s.t.} \\quad \u0026A_a(\\tau_a) \\notin C_{obs}, \\quad A_a(\\tau_a) \\cap A_i(\\tau_i) = \\emptyset, \\\\ \u0026\\tau_a(0) = q_a, \\quad \\tau_a(T) = g_a \\end{aligned} τa​s.t.​=argτ∈Tmin​(ca​(τ)+λa​cas​(τ,τ1:N​))Aa​(τa​)∈/Cobs​,Aa​(τa​)∩Ai​(τi​)=∅,τa​(0)=qa​,τa​(T)=ga​​其中：\n$c_a$ 为引导机器人前往目标的路径成本 $c_a^s$ 为考虑社会规范的成本项 $A(\\tau)$ 表示轨迹 $\\tau$ 所占据的体积 $C_{obs}$ 表示静态障碍 $T$ 为任务回合结束时间 $\\lambda_a$ 为权重因子 约束条件确保机器人在到达目标前，不会与静态障碍或人类发生碰撞。\n如上图，主策略网络（Main Policy Network）在每个时间步以深度图像（depth image）和点目标（point goal）为输入，直接输出机器人下一步的动作。该网络的训练结合了点目标导航（PointNav）的奖励与本文提出的社交认知惩罚（Social Cognition Penalty, SCP）。此外，主策略网络还搭配了时空预知模块（Spatial-Temporal Precognition Module），该模块在训练过程中支持多个辅助任务的执行。","时空预知模块spatial-temporal-precognition-module#时空预知模块（Spatial-Temporal Precognition Module）":"SPM 的核心作用： “利用主网络的记忆（$\\delta_R$），学习与人类相关的’经验’，辅助主网络更聪明地决策”。","核心创新#核心创新":"因此 Falcon 框架的核心贡献在于是首个融合显式人类轨迹预测的未来感知 RL 架构，通过 SCP 和 SPM 实现主动避障与社交合规。\n一方面，该框架引入**“社交认知惩罚”**（含轨迹阻碍惩罚），鼓励智能体主动规避潜在碰撞并遵守社交礼仪 另一方面，框架搭载**“时空预知模块”**，该模块融入包含轨迹预测在内的社交感知辅助任务，以增强智能体在训练过程中对未来动态的理解 注： 这里提到社交导航最初是在 iGibson 社交导航挑战赛提出，是 PointNav基础上增加了移动人类这一元素，那这个领域真的非常新颖了，还是这篇文章首创的Habitat替代这一静态人形模型。","模块架构与输入设计#模块架构与输入设计":"**时空预认知模块（Spatial-Temporal Precognition Module）**的核心输入完全来自主策略网络的\"中间处理结果\"，没有额外新增传感器数据。核心目的是\"复用特征、节省计算\"，同时让\"预判能力\"和\"决策能力\"基于同一套环境理解，避免信息脱节。","相关工作#相关工作":"根据 Related Works，往期人类轨迹预测方法可分为三类：\n基于物理学的方法： 此类方法从牛顿运动定律中推导显式动力学模型，用于轨迹预测 基于学习的方法： 聚焦于从观测到的历史轨迹中学习运动模式 基于规划的方法： 其核心目标是推理理性智能体的运动意图，通过理解智能体的目标及其决策过程来预测轨迹 Falcon借鉴了这些研究思路，提出的方法不仅能预测人类轨迹，还能将社交感知信息融入智能体的导航策略，从而确保智能体在动态场景中实现安全、高效的导航。","社交认知惩罚scp#社交认知惩罚（SCP）":"然而，PointNav 奖励函数未考虑动态环境与社交交互，无法满足社交导航（SocialNav）的需求。为此，本文引入社交认知惩罚（SCP）——一套用于促进机器人遵守社会规范的惩罚机制，具体包含以下三类惩罚：","第一步输入编码linear-encoder--visual-encoder#第一步：输入编码（Linear Encoder + Visual Encoder）":"点目标 + GPS+Compass → 线性编码器（Linear Encoder）：\n点目标的坐标（如 $g_a \\in Q$）和 GPS 定位的机器人当前位置（$q_a \\in Q$） 先计算\"目标相对距离与方向\" 通过线性变换转化为低维特征向量 $f_{goal}$ 深度图像 → 视觉编码器（Visual Encoder：ResNet-50）：\n通过预训练的 ResNet-50 模型 [51] 提取视觉特征 $f_{depth}$","第三步输出决策actor-head--value-head#第三步：输出决策（Actor Head + Value Head）":"时序特征 $h_t$ 会输入到两个\"头网络\"：\nActor Head（动作头）：\n基于 $h_t$ 输出机器人的下一步动作，决定了机器人的实际导航路径 $\\tau_a$ Value Head（价值头）：\n基于 $h_t$ 预测当前动作的\"预期奖励\" $V(h_t)$，辅助强化学习的训练（PPO 算法需要通过\"预测奖励\"与\"实际奖励\"的差异更新策略）","第二步时序建模recurrent-state-encoder2-层-lstm#第二步：时序建模（Recurrent State Encoder：2 层 LSTM）":"输入： 将线性编码器输出的 $f_{goal}$ 和 ResNet-50 输出的 $f_{depth}$ 拼接，得到融合特征： ffusion=[fgoal,fdepth] f_{fusion} = [f_{goal}, f_{depth}] ffusion​=[fgoal​,fdepth​]输出： LSTM 会输出两个关键结果：\n时序特征 $h_t$：捕捉\"前 t 步环境变化\"的动态信息 潜在变量 $\\delta_R$：LSTM 的隐藏状态，其核心作用是\"将主网络的时序记忆传递给 SPM\"，让 SPM 能基于主网络的\"观察记忆\"学习辅助任务","第四步主损失-l_main-的来源#第四步：主损失 $L_{main}$ 的来源":"主策略网络的训练目标是\"让机器人既快又安全地抵达目标\"，其损失 $L_{main}$ 来自 DD-PPO 算法 [53] 的 PPO 损失。\nPPO 损失的核心逻辑： “约束策略更新的幅度，避免更新过快导致不稳定”\n其简化公式为：\nLPPO=Eπ^[min⁡(πθ(a∣s)πθold(a∣s)Aπθold(s,a),clip(πθ(a∣s)πθold(a∣s),1−ϵ,1+ϵ)Aπθold(s,a))] L_{PPO} = \\mathbb{E}_{\\hat{\\pi}} \\left[ \\min\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)} A^{\\pi_{\\theta_{old}}}(s,a), \\text{clip}\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}, 1-\\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_{old}}}(s,a) \\right) \\right] LPPO​=Eπ^​[min(πθold​​(a∣s)πθ​(a∣s)​Aπθold​​(s,a),clip(πθold​​(a∣s)πθ​(a∣s)​,1−ϵ,1+ϵ)Aπθold​​(s,a))]其中：\n$\\pi_{\\theta}(a|s)$ 是当前策略（主网络）在状态 $s$ 下选择动作 $a$ 的概率 $\\pi_{\\theta_{old}}(a|s)$ 是上一轮策略的概率（用于约束更新幅度） $A^{\\pi_{\\theta_{old}}}(s,a)$ 是优势函数（衡量\"当前动作比平均动作好多少\"，与奖励函数 $R_t^{socialnav}$ 直接相关，而 $R_t^{socialnav}$ 包含了 SCP 惩罚，因此 SCP 会间接影响 $L_{main}$） 简言之， $L_{main}$ 是\"主策略网络决策质量的量化指标\"：动作越接近\"抵达目标 + 遵守社交规则\"，$L_{main}$ 越小。","结论-1具备未来感知能力的方法比静态方法和情境感知方法更高效更安全#结论 1：具备未来感知能力的方法比静态方法和情境感知方法更高效、更安全":"A* 等静态路径规划算法会预先确定一条固定路径，无法适应动态环境，易导致人机碰撞（见图 4 (a)） ORCA、Proximity-Aware 等情境感知避障方法虽能通过调整路径避开当前人类与障碍物，但存在局限性： 路径重规划需耗时，延迟响应会增加碰撞风险 ORCA 因假设运动不受限制，导致与静态障碍物碰撞（见图 4 (b)） Proximity-Aware 因无法预判人类运动，短期调整失效，最终发生碰撞（见图 4 (c)） Falcon 能主动适应人类动态运动，高效抵达目标位置","结论-2辅助任务可提升模型性能其中轨迹预测任务作用最显著#结论 2：辅助任务可提升模型性能，其中轨迹预测任务作用最显著":"表 3 展示了不同辅助任务组合的实验结果。与 PointNav 基线模型 [57] 相比，单个辅助任务即可提升导航性能；其中，轨迹预测任务（SPM.Traj）效果最突出——将成功率从 40.94% 显著提升至 54.00%。这一结果证明，在社交导航中引入显式轨迹预测具有重要价值。\n表 3：Falcon 消融实验结果\n表格说明： 仅使用 PointNav 算法 [57] 训练的模型作为基线；SPM.Count、SPM.Pos、SPM.Traj 分别指\"人类计数估计\"“当前位置跟踪\"“未来轨迹预测\"三个辅助任务；数据以百分比表示，\"↑” 表示指标值越高越好，\"↓” 表示指标值越低越好\nSPM. 计数（Count） SPM. 位置（Pos） SPM. 轨迹（Traj） SCP 成功率（Suc.）↑ 路径长度加权成功率（SPL）↑ 时间长度加权成功率（STL）↑ 个人空间合规率（PSC）↑ 人机碰撞率（H-Coll）↓ （无辅助任务） （无辅助任务） （无辅助任务） 无 40.94 34.14 11.50 90.82 53.54 ✓ 无 无 无 51.43 51.42 51.16 90.53 46.46 无 ✓ 无 无 53.17 53.17 52.95 90.06 44.07 无 无 ✓ 无 54.00 53.99 53.92 89.46 43.88 无 无 无 ✓ 51.24 51.24 51.08 90.41 48.11 ✓ ✓ ✓ 无 53.63 53.63 53.40 89.33 44.89 ✓ ✓ ✓ ✓ 55.15 55.15 54.94 89.56 42.96","结论-3scp-与-spm-协同互补显著提升模型性能并加快训练收敛#结论 3：SCP 与 SPM 协同互补，显著提升模型性能并加快训练收敛":"表 3 显示，SCP 对模型性能提升至关重要，尤其在与 SPM 结合时效果更明显：\n仅使用 SPM 的三个辅助任务（计数、位置、轨迹）时，模型性能较单个辅助任务无显著提升（SPM 组合的成功率为 53.63%，而 SPM.Pos 为 53.17%、SPM.Traj 为 54.00%） 加入 SCP 后，完整模型的成功率提升至 55.15%，显著优于单独使用 SPM 的情况 图 5 显示，同时包含 SPM 和 SCP 的模型在训练过程中收敛速度更快（1400K 步前即可体现） 这些结果表明，若缺乏 SCP 的引导，SPM 的辅助任务无法有效整合——SCP 能帮助模型平衡各项任务，更充分地利用现有信息。","网络架构技术细节#网络架构（技术细节）":"主策略网络的处理流程分为四个步骤，具体技术实现如下：","论文研读#论文研读":"为解决社交导航（SocialNav）中机器人“短视避障”和现有评估基准不真实的问题，论文提出Falcon 框架（一种基于强化学习的未来感知社交导航架构），通过社交认知惩罚（SCP）（含障碍碰撞、人类距离、轨迹阻碍三类惩罚）和时空预知模块（SPM）（含人类计数估计、当前位置跟踪、未来轨迹预测三个辅助任务）实现主动避障与社交合规；同时构建SocialNav 基准，包含Social-HM3D（844 个场景）和Social-MP3D（72 个场景）两个高真实感室内数据集，平衡人类密度与自然运动模式。\n实验结果： Falcon 在该基准上实现55% 的任务成功率，同时保持约90% 的个人空间合规率，显著优于 A*、ORCA 等规则算法及 Proximity-Aware 等 RL 方法，且具备零样本泛化能力。","评估指标#评估指标":"任务完成度指标：\n成功率（Success Rate, Suc.）：机器人成功到达目标的比例 路径长度加权成功率（Success weighted by Path Length, SPL）：考虑路径效率的成功率 时间长度加权成功率（Success weighted by Time Length, STL）：考虑时间效率的成功率 社交合规性指标：\n人机碰撞率（Human-Robot Collision Rate, H-Coll）：机器人与人类发生碰撞的比例 个人空间合规率（Personal Space Compliance, PSC）：机器人保持适当社交距离的比例 考虑到人类碰撞半径为 0.3 米、机器人碰撞半径为 0.25 米，本实验将个人空间合规（PSC）的距离阈值设定为 1.0 米。","辅助损失与总损失#辅助损失与总损失":"3 个任务的损失相加，得到 SPM 的辅助损失：\nLaux=Lcount+Lpos+Ltraj L_{aux} = L_{count} + L_{pos} + L_{traj} Laux​=Lcount​+Lpos​+Ltraj​训练过程中，主策略网络与辅助任务的优化同步进行，模型的总损失为\"主策略损失\"与\"辅助损失\"的加权和：\nLtotal=βmainLmain+βauxLaux L_{total} = \\beta_{main} L_{main} + \\beta_{aux} L_{aux} Ltotal​=βmain​Lmain​+βaux​Laux​其中，$\\beta_{main}$ 和 $\\beta_{aux}$ 分别为主策略损失与辅助损失的权重系数。","问题背景#问题背景":"**社交导航（SocialNav）**要求机器人在人类共享环境中遵守社交规范并安全导航，但现有方案存在两大关键问题："},"title":"Falcon"},"/blog/2025/habitat-common-sense/":{"data":{"":"今天做的另一件事情是去autodl里复现falcon，具体可见blog\\2025-11-29-falcon.md下半部分，图形学驱动这些真的很难绷的住。再就是今天我疑似罹患流感，晚上昏昏沉沉的，效率不高，RT-1没有看。","eglembedded-system-graphics-library#EGL（Embedded-System Graphics Library）":"定义：OpenGL 和底层显示系统之间的接口层 作用：创建 OpenGL 上下文（context），连接 OpenGL 和 GPU 驱动 为什么重要：在服务器上（没有显示器），EGL 是创建 OpenGL 上下文的唯一方式 无头模式：EGL_PLATFORM=surfaceless 告诉 EGL “我不需要显示窗口，只需要在内存中渲染”","eglopengl-配置核心难点#EGL/OpenGL 配置（核心难点）":"问题诊断流程：\n检查错误信息：GL::Context: cannot retrieve OpenGL version → EGL 初始化失败 检查 GPU：nvidia-smi 确认 GPU 可用 检查库文件：确认 NVIDIA EGL 库存在 设置环境变量：配置 EGL 平台和库路径 多进程问题：使用 LD_PRELOAD 确保子进程继承设置 调试技巧：\n设置 MAGNUM_LOG=verbose 和 HABITAT_SIM_LOG=verbose 查看详细日志 使用最小测试脚本逐步验证每个环节 检查日志中的 found X EGL devices 确认 GPU 被检测到","glxopengl-extension-to-the-x-window-system#GLX（OpenGL Extension to the X Window System）":"定义：Linux 上连接 OpenGL 和 X11 窗口系统的接口 作用：在有显示器的 Linux 系统上创建 OpenGL 上下文 服务器场景：在无头服务器上通常不需要，但 NVIDIA 驱动可能需要它来初始化","openglopen-graphics-library#OpenGL（Open Graphics Library）":"定义：跨平台的图形渲染 API，用于在 GPU 上绘制 3D 图形 作用：告诉 GPU 如何渲染三角形、纹理、光照等 类比：就像告诉画家\"用红色画笔在画布上画一个圆\"","x11x-window-system是什么#X11（X Window System）是什么？":"定义：Linux/Unix 系统上的图形窗口系统，用于管理窗口、鼠标、键盘等 作用： 提供图形界面基础：窗口管理、事件处理、输入输出 客户端-服务器架构：X Server（显示服务器）和 X Client（应用程序）分离 支持远程显示：可以通过网络在远程机器上显示图形界面 为什么服务器上没有 X11？： 服务器通常没有显示器，不需要窗口系统 X11 服务器需要显示器硬件支持 无头服务器不需要图形界面，节省资源 与 OpenGL 的关系： GLX 是 X11 的扩展，用于在 X11 窗口系统中创建 OpenGL 上下文 有显示器时：应用程序 → GLX → X11 → 显示器 无头服务器：无法使用 GLX，必须使用 EGL 的无头模式","一habitat-sim-是什么#一、Habitat-Sim 是什么？":"Habitat-Sim 是 Meta（Facebook）开发的 3D 场景仿真器，主要用于训练和评估机器人导航算法。它的核心作用是：\n3D 场景渲染：加载真实的 3D 场景（如 HM3D 数据集中的室内场景），生成机器人\"看到\"的图像 物理仿真：模拟机器人的移动、碰撞检测等物理行为 传感器仿真：模拟 RGB 相机、深度相机、激光雷达等传感器数据 多智能体仿真：可以同时模拟多个机器人或 NPC（如行人）的行为 为什么需要图形渲染？\n机器人需要通过\"视觉\"感知环境，这需要从 3D 场景生成 2D 图像 即使不显示图像，也需要在 GPU 上渲染以获取传感器数据（RGB、深度图等） 这就是为什么即使是无头（headless）模式，也需要 GPU 和图形驱动","三为什么服务器上需要特殊配置#三、为什么服务器上需要特殊配置？":"","二opengleglglx-是什么#二、OpenGL、EGL、GLX 是什么？":"","五复现其他-3d-视觉工作的通用检查清单#五、复现其他 3D 视觉工作的通用检查清单":"","六常见错误与解决方案速查#六、常见错误与解决方案速查":"错误信息 原因 解决方案 GL::Context: cannot retrieve OpenGL version EGL 初始化失败 设置 EGL_PLATFORM=surfaceless 和 LD_PRELOAD malloc_consolidate(): unaligned fastbin chunk detected 多进程环境下库加载冲突 使用 LD_PRELOAD 强制加载 NVIDIA 库 ImportError: libEGL.so.1 EGL 库未安装 apt-get install libegl1-mesa FileNotFoundError: Could not find dataset file 数据集路径不匹配 检查配置文件中的路径，创建符号链接 No Stage Attributes exists for requested scene 场景文件路径错误 检查场景数据集目录结构，创建缺失的符号链接","关键环境变量解释#关键环境变量解释":"# 1. 指定 EGL 平台为无头模式（不需要窗口） export EGL_PLATFORM=surfaceless # 2. 强制使用 NVIDIA 的 EGL 实现（而不是 Mesa 软件渲染） export __EGL_VENDOR_LIBRARY_FILENAMES=/usr/share/glvnd/egl_vendor.d/10_nvidia.json # 3. 指定使用哪个 GPU（多 GPU 系统） export EGL_DEVICE_ID=0 # 4. 强制加载 NVIDIA 的图形库（确保子进程也使用正确的库） export LD_PRELOAD=\"/lib/x86_64-linux-gnu/libEGL_nvidia.so.0:/usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0\" 为什么需要 LD_PRELOAD？\nLinux 的动态链接器默认会按顺序查找库文件 系统可能有多个 EGL 实现（NVIDIA、Mesa 等） LD_PRELOAD 强制优先加载指定的库，确保使用 NVIDIA 的硬件加速版本 在多进程环境下，子进程也需要继承这个设置 关于 Mesa vs NVIDIA 冲突的说明：\nMesa 是开源的 OpenGL/EGL 实现，提供软件渲染（CPU）或通过 DRI（Direct Rendering Infrastructure）使用集成显卡 NVIDIA 驱动 提供专有的硬件加速 OpenGL/EGL 实现，直接使用 NVIDIA GPU 冲突原因：Linux 系统默认会优先加载 Mesa 库（libEGL.so.1），但 Mesa 无法访问 NVIDIA GPU，导致无法创建硬件加速的 OpenGL 上下文 解决方案：虽然安装了 Mesa 库解决了 libEGL.so.1 缺失的问题，但后续需要通过 LD_PRELOAD 和环境变量强制使用 NVIDIA 的 EGL 库（libEGL_nvidia.so.0），这样才能利用 GPU 硬件加速","图形渲染配置#图形渲染配置":"设置无头模式：export EGL_PLATFORM=surfaceless 指定 GPU 设备：export EGL_DEVICE_ID=0（多 GPU 时） 强制使用硬件加速：设置 LD_PRELOAD 指向 NVIDIA 库 多进程环境：确保子进程继承环境变量","复盘3d-视觉仿真与图形渲染基础科普#复盘：3D 视觉仿真与图形渲染基础科普":"","数据集路径#数据集路径":"场景数据集路径正确：检查 scene_datasets/ 目录 Episode 数据集路径正确：检查 datasets/ 目录 配置文件中的路径与实际目录匹配 符号链接正确：使用 ls -la 检查符号链接","环境检查#环境检查":"GPU 驱动已安装：nvidia-smi 能正常显示 CUDA 版本匹配：检查项目要求的 CUDA 版本 EGL 库已安装：apt-get install libegl1-mesa（或使用 NVIDIA 版本） Python 环境正确：conda/venv 环境已激活","解决方案无头渲染headless-rendering#解决方案：无头渲染（Headless Rendering）":"使用 EGL 的 surfaceless 模式：\n不创建可见窗口 直接在 GPU 内存中渲染 渲染结果可以保存为图像或用于计算","调试技巧#调试技巧":"启用详细日志：设置 *_LOG=verbose 环境变量 最小测试：创建简单的测试脚本验证每个组件 逐步排查：先测试 EGL 初始化，再测试场景加载，最后测试完整流程 日志重定向：\u003e log.txt 2\u003e\u00261 保存日志避免终端溢出","配置文件路径问题#配置文件路径问题":"常见问题：\n配置文件引用的数据集名称与实际目录不匹配 场景数据集的实际目录结构与配置期望不一致 排查方法：\n检查配置文件中的 data_path 和 scene_id 路径 使用符号链接统一路径结构 创建最小测试脚本验证路径是否正确","问题本质#问题本质":"服务器（如 AutoDL）通常：\n没有显示器：无法创建传统的窗口来显示图形 没有 X11 服务器：无法使用 GLX 创建 OpenGL 上下文（GLX 依赖 X11） 需要 GPU 加速：仍然需要 GPU 来加速渲染计算 X11 vs EGL 的区别：\nX11 + GLX：需要显示器，创建可见窗口，适合桌面环境 流程：应用程序 → GLX → X11 Server → 显示器 示例：在本地 Linux 桌面运行图形程序 EGL（无头模式）：不需要显示器，直接在 GPU 内存渲染，适合服务器 流程：应用程序 → EGL → GPU（无窗口） 示例：在服务器上运行 3D 仿真，渲染结果保存为图像或用于计算"},"title":"Habitat 相关项目常识"},"/blog/2025/habitat-env/":{"data":{"":"","autodl--habitat--vnc--vglrun#AutoDL + Habitat + VNC + VGLRun":"真的没招了…下面的错误层出不穷：\nGL::Context: cannot retrieve OpenGL version: GL::Renderer::Error::InvalidValue 因此咨询学长，考虑根据AUTODL官方远程GUI文档设置VNC。\n# 安装基本的依赖包\rapt update \u0026\u0026 apt install -y libglu1-mesa-dev mesa-utils xterm xauth x11-xkb-utils xfonts-base xkb-data libxtst6 libxv1\r# 安装libjpeg-turbo和turbovnc\rexport TURBOVNC_VERSION=2.2.5\rexport LIBJPEG_VERSION=2.0.90\rwget https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/vnc/libjpeg-turbo-official_${LIBJPEG_VERSION}_amd64.deb\rwget https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/vnc/turbovnc_${TURBOVNC_VERSION}_amd64.deb\rdpkg -i libjpeg-turbo-official_${LIBJPEG_VERSION}_amd64.deb\rdpkg -i turbovnc_${TURBOVNC_VERSION}_amd64.deb\rrm -rf *.deb\r# 启动VNC服务端，这一步可能涉及vnc密码配置（注意不是实例的账户密码）。另外如果出现报错xauth未找到，那么使用apt install xauth再安装一次 在启动前，需要卸载无头版本：\npip uninstall habitat-sim -y conda remove habitat-sim-mutex headless -y # 安装非无头版本（带渲染支持） conda install habitat-sim withbullet -c conda-forge -c aihabitat -y # mamba! /root/miniconda3/bin/mamba install! # conda config --set ssl_verify false 启动！\nrm -rf /tmp/.X1* # 如果再次启动，删除上一次的临时文件，否则无法正常启动\rUSER=root /opt/TurboVNC/bin/vncserver :1 -desktop X -auth /root/.Xauthority -geometry 1920x1080 -depth 24 -rfbwait 120000 -rfbauth /root/.vnc/passwd -fp /usr/share/fonts/X11/misc/,/usr/share/fonts -rfbport 6006\r# 检查是否启动，如果有vncserver的进程，证明已经启动\rps -ef | grep vnc 接下来cmd里配置SSH转发：\nssh -CNg -L 6006:127.0.0.1:6006 root@connect.nmb2.seetacloud.com -p 24413 然后设置export DISPLAY=:1，随便打开一个VNC Client “连接地址” 栏输入：127.0.0.1:6006（本地被 SSH 隧道代理的端口）； 点击连接，前面你设置的 VNC 密码（不是实例 root 密码）\n在这之后我们需要VirtualGL (vglrun)，其拦截了 Habitat 的 OpenGL 渲染请求，重定向到物理 GPU（NVIDIA）上运行，最后再把画面传回 VNC 的虚拟显示器。\n# 下载并安装 wget https://github.com/VirtualGL/virtualgl/releases/download/3.1/virtualgl_3.1_amd64.deb dpkg -i virtualgl_3.1_amd64.deb # 安装依赖 apt install -y libegl1-mesa libgl1-mesa-glx libglu1-mesa 这之后就可以用它来运行了：\nvglrun -d :1 python test.py 这里会遇到一个非常麻烦的问题：\nPlatform::WindowlessEglApplication::tryCreateContext(): unable to find CUDA device 0 among 4 EGL devices in total 参考知乎经验，实际上这个问题在Habitat-lab/TROUBLESHOOTING.md 中提到了。\n# 设置 EGL vendor library（解决 EGL 设备匹配问题） export __EGL_VENDOR_LIBRARY_FILENAMES=/usr/share/glvnd/egl_vendor.d/10_nvidia.json 接下来因为我是换了一个Autodl实例从头开始跑examples的，这里我们顺便来学习一下Habitat本身：","autodl-tmphabitat-labexamplesshortest_path_follower_examplepy#autodl-tmp/habitat-lab/examples/shortest_path_follower_example.py":"这个里面没有cv2.imshow()这种弹窗指令，所以逻辑是“静默运行 -\u003e 保存 MP4”。\n可以在autodl-tmp/habitat-lab/examples/images/shortest_path_example看到生成的mp4\n注意其数据：\n# 下载测试场景（存放在 habitat-lab/data 下）\rpython -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path data/\r# 下载导航任务数据\rpython -m habitat_sim.utils.datasets_download --uids habitat_test_pointnav_dataset --data-path data/ 这个在4090那个容器里也跑起来了。中间报了一个小错误：\nTypeError: write() got an unexpected keyword argument 'fps' imageio 的 tifffile 插件不支持 fps 参数，问题是 ffmpeg 未安装，导致 imageio 无法使用 ffmpeg 插件写入 MP4，回退到其他插件（如 tifffile）不支持 fps 参数，如果是这样那就很简单了：\n/root/miniconda3/bin/mamba install -n falcon ffmpeg -c conda-forge -y\rconda run -n falcon pip install imageio-ffmpeg 解决"},"title":"从头开始配置 Habitat Autodl 环境"},"/blog/2025/hk-master/":{"data":{"cuhk-mscai#CUHK MscAI":"CUHK无需笔面，直接根据申请材料下offer。这里聊一聊一般网上不会详细说明的拿到conditional offer之后的流程：\n这里分为两步，首先是交留位费：\n这样一来这个offer就会为你保留，你不去的话就会亏掉这一笔钱。留位费理论上应该算是学费的一部分（将在第一学期部分学生费用中抵消），而我感觉CUHK AI是比较新也比较贵的那一批。据称其深圳校区的留位费可以退还90%，不过我在写这篇博客的同时还没有给我下面试通知所以无从验证。\n上图所描述的可选支付方式（选一个即可）：\n信用卡支付（VISA / 万事达 / 银联）\n勾选对应的选项，点 “Confirm” 按钮，会跳转到在线信用卡支付页面完成付款。 电子钱包类（微信支付 / 支付宝 / AlipayHK/BoC Pay / 银联 App）\n勾选后点 “Confirm”，会跳转到第三方支付平台 SwiftPass 完成付款。 快速支付系统（FPS）\n勾选后点 “Confirm”，跳转到 CUHK 的 FPS 支付系统操作。 本地支付方式（仅香港地区可用：PPS/ATM/ 网上银行 / 现金 / 支票）\n勾选后，先下载 “Payment Advice”，按照页面里 “See more” 的指引操作。 电汇（仅非本地支付）\n勾选后，点 “See more” 查看 CUHK 的银行账户详情，直接向该账户电汇（注意电汇金额是 HK$126,916）。 注意事项：\n付款时要填对参考号：88103073875 非本地支付（如电汇）可能需要 7 个工作日到账，建议提前操作避免逾期 付款后确认可能需要 1-3 个工作日（本地）或 7 个工作日（非本地） 小红书上据称是微信支付的手续费最少，不过既然都读港校了也不差这点钱。","hkust-gz-mphil-rb#HKUST-GZ MPhil RB":"面邀到线下面试之间有2周时间左右，足够你去准备英文PPT与5min讲稿、看面经、准备机票与衣物等工作，别忘了带身份证等。\n通勤方面，直接选择萧山机场→白云机场，从ZJU到前者和从HKUST-GZ到后者的时间差不多都是1h左右，飞机飞2h多一点，差不多当天直接注册入住，第二天开面。\n上午群面，下午个面。这里的信息来源来自港科广官方微信公众号以及小红书经验贴，当然你也可以在面邀邮件里找到对应的流程：","pi调研#PI调研":"以免个人面试的时候被问到，所以这里得先准备一下。\n陈昶昊\n陈昶昊博士（Dr. Changhao Chen）是香港科技大学（广州）系统枢纽智能交通学域、信息枢纽人工智能学域联聘助理教授，副研究员，博士生导师。获英国牛津大学计算机科学博士学位，并在英国工程和自然科学研究委员会（EPSRC）资助下从事博士后研究。入选全球前2%顶尖科学家榜单（2024）、**中国科协青年人才托举工程（2022）**和国际机器人科学与系统大会（Robotics: Science and Systems）先锋者（2020）。\n组建港科大（广州）PEAK-Lab (Perception, Embodiment, Autonomy and Kinematics)课题组，现有博士生及科研助理10人。课题组聚焦具身智能和无人系统研究，致力于构建在动态开放环境交互的具身智能体，服务低空经济、智能交通和智慧城市等应用领域。主持国家自然科学基金等纵向项目5项，先后三次获/提名权威学术会议优秀论文奖。在人工智能、机器人和智能交通领域已发表高水平论文50多篇，包括TITS、TNNLS、TMC、TIP、TIV等领域权威期刊以及CVPR、ICCV、AAAI、ECCV、ICRA、IROS、WWW、MobiCom、MobiSys、SenSys等国际顶级会议，谷歌学术引用超过2900次。长期担任NeurIPS、ICML、ICLR、CVPR、TRO、IJRR、IJCV、TAC等30多个国际会议、期刊的程序委员会委员/元审稿人/审稿人，中国科协会刊《科技导报》首届青年编委，受邀担任国际机器人与自动化会议（ICRA-2024）以及中俄\"导航与运动控制\"青年学者论坛的分会场主席。已授权国家发明专利、国际PCT专利、美国和欧洲专利共14项，包含1项在英国成功成果转化。\n主页：https://changhao-chen.github.io/\n研究方向：\n人工智能在计算机视觉和自然语言处理领域取得显著进展，为**通用人工智能（AGI）**奠定基础。尽管大语言模型在虚拟环境中展现巨大潜力，将人工智能融入现实世界仍面临诸多挑战。未来十年，具身智能将推动下一次技术革命。实现机器智能从虚拟、受限环境延伸到物理、开放世界，需要深入理解三维场景、本体运动、具身智能以及高效计算。\n三维空间感知\n三维空间感知将多视图几何与深度神经网络结合，提供从低层级特征感知、中间层级几何表达到高层级语义理解。课题组已取得三方面突破：构建自监督学习的空间感知框架，实现二维到三维数据的直接特征提取匹配、基于特征元学习的视觉定位与建图、自监督位姿与深度估计等；构建城市级别的视觉定位与三维重建框架，包括视觉定位Transformer大模型以及多神经渲染网络的合并渲染、匹配与定位；针对烟、雾等视觉受限环境，实现红外相机和毫米波雷达的稀疏数据信号处理，构建超越人类视界的感知系统。未来将进一步面向动态变化的实际场景，开展新场景自适应的空间感知研究，实现感知系统的长效终身学习以及学习模型的可信度分析。 运动状态估计\n三维空间运动的智能体需进行速度、姿态、位置等系统状态的估计，以支撑规划和决策。课题组开展以本体感知为核心的运动估计研究，在世界范围内首次提出基于深度学习的惯性导航算法并公开相关数据集，得到全球40多个国家的研究人员关注和使用，被后续研究者拓展为数十种算法模型，用于解决四足机器人、无人机、水下机器人、车辆、智能穿戴设备等载体的本体运动状态估计难题。未来将进一步研究结合物理模型和机器学习的动态建模，通过可学习的状态空间模型和神经常微分方程等方法，实现理解物理世界的运动估计、预测以及状态分析。 具身导航决策\n实现三维空间导航与交互是通往通用智能的重要目标。课题组面向高效可靠自主导航的需求，提出基于特征选择的多传感器融合方法，从数据中学习到传感器间的融合策略，实现强鲁棒多源导航。课题组构建融合深度神经网络的卡尔曼滤波模型，实现具备李雅普诺夫稳定的导航动力学模型自动构建。课题组探索了融合传统控制策略与强化学习的决策控制模型，实现导航策略更高效学习。未来研究将借鉴人类大脑与小脑的功能分工，提出层次化的具身导航与操纵决策框架，通过大语言模型理解语言指令用于全局规划与决策，通过具身基础模型实现局部运动规划和控制，最终实现智能体自主导航、探索、操纵与协作。 高效神经网络计算\n未来社会将依赖数十亿台具身智能系统，传统通用计算架构难以满足低成本、低能耗和低延迟的需求，需探索面向具身智能的专用计算框架。课题组已突破轻量化神经网络知识蒸馏和量化、视觉感知FPGA硬件加速单元、硬件加速的视觉定位与建图系统等关键技术。未来将进一步研究通用计算与神经网络计算融合的新型智能计算框架，同时考虑计算特性和约束条件进行软硬件协同设计，优化系统并行计算需求，加速矩阵乘法等热点函数，高效实现系统整体迭代优化。 汪军\n3DGS World Model\n马骏\nLOVON (Legged Open-Vocabulary Object Navigator)：足式机器人、开放世界导航、大语言模型任务规划、机器人学习\n分层任务规划：利用大语言模型（LLMs）将复杂的自然语言指令（如\"先跑向椅子，再靠近行人”）拆解为一系列可执行的子任务。 开放词汇视觉检测：让机器人能够识别超出预定义类别的各种物体，大大增强了在陌生环境中的适应能力。 语言-运动模型：将文字指令和视觉信息直接转化为控制机器人运动的向量，实现精准的导航控制。 抗干扰与鲁棒性：项目专门设计了应对视觉画面抖动、目标临时丢失等现实问题的策略，提升了系统在真实场景下的可靠性。 梁俊卫\n三维场景理解、具身智能；为零样本3D视觉定位方法SeeGround的博士生导师\nSeeGround方法，创新性地利用2D视觉语言模型来完成零样本的3D物体定位，无需昂贵的3D标注数据，在复杂场景中表现出色。这项技术对于机器人在未知环境中进行自主操作至关重要。","个人面试#个人面试":"总时长：15分钟\n1. 口头演讲\n重要提示：申请者的口头演讲使用中文，问答环节必将使用英文；如果口头演讲使用英文，问答环节将可使用中文或英文。\n因此，这部分必须使用英文。\n5分钟内阐述你攻读该领域硕士学位的原因，从以下五个主题中选择一个：\n未来健康保健技术 可持续生活 智能工业化 低空经济 海洋科技与经济 评分指标（演讲与主题契合类）：\n指标 描述 分数（0-2） 主题相关性 演讲是否聚焦于申请人选择的五个主题之一？ 0-2 视角与愿景 申请人是否展示了多角度的思考，并解决了现实问题或未来挑战？ 0-2 组织与结构 演讲是否结构合理，有明确的开头、主体和结论？ 0-2 内容熟悉度 申请人是否展示了对内容的深入理解，并简化了复杂的主题？ 0-2 幻灯片设计 幻灯片是否无误，与内容一致，并有效支持演讲？ 0-2 语言表达 申请人表达是否流畅清晰，避免了多余叹词和模糊不清的表达？ 0-2 与评委的互动 演讲过程中，申请人是否有效地与评委进行眼神接触并互动？ 0-2 创造性思维 申请人在演讲中是否提供了创造性或独特的观点？ 0-2 动机与契合度 申请人是否清晰阐述了自己为何适合该项目，将个人目标与项目目标对齐？ 0-2 2. 问答环节\n评分指标（问答与综合能力类）：\n指标 描述 分数（0-2） 理解能力 申请人是否理解并准确回应了评委的问题？ 0-2 回答深度 回答是否经过深思熟虑，并显示了对主题的深入理解？ 0-2 情绪控制 面对压力时，申请人是否保持冷静镇定，沉着应对难题？ 0-2 危机处理能力 申请人是否能够很好地应对挑战性或意外问题，并提供相关的解决方案或观点？ 0-2 批判性思维 申请人是否展示了逻辑推理能力，分析问题并提供有见地的回答？ 0-2 团队合作的关注 申请人是否提供了团队合作的具体例子，并表达了对协作的强烈兴趣？ 0-2 说服力 申请人的观点是否具有说服力，并能够有效支持其立场？ 0-2 自我意识与成长 申请人是否展示了自我意识，讨论了自身的优势、劣势和需要改进的地方？ 0-2 共情与参与 申请人是否根据评委的反馈调整了自己的回答，并与他们进行有意义的互动？ 0-2 详细的面试流程可以参考面邀邮件里的文件：","推免失败#推免失败":"本科大三时经历了一段痛苦的PTSD，导致本科规划出现了巨大偏差。理论上应该是以二作CCF文章加上刚刚过推免线的绩点保研，但后来拼尽全力把3年GPA稳到了88分，也算刚刚好拿到推免资格（22/25）。由于对直接读博产生了巨大阴影，且不太喜欢生仪研究院的4个方向，因而选择无论如何都要读一个CS/AI相关的硕士。\n然而硕士申请需要面临committee的拷打，因此推免浙计失败。面试时被问到的5个问题：\n排序算法 联邦学习 数组排序 职业规划 还有一个忘记了（记忆自动删除了） 选择浙计确实有点冲动，主要是有老师愿意给我他们组里专硕的名额，但是老师又不能在委员会里直接捞人（这和博士不一样，但出于上述的原因，我又不愿直博）。在9月22日左右结束之后，就立刻开始捡起托福复习英语了。","推荐信与申请进度#推荐信与申请进度":"推荐信这一块还是拜本科期间干了点事，拿到了那位计院老师以及本院书记的两封强推（硕士居然要推荐信），所以基本上刚申请就交了。\n11月3日，CUHK IE发邮件过来催补材料；11月5日回复我说我的申请会被更深入处理。结果最先发offer的居然是CUHK的AI，在11月20日前。\n因为申请比较晚，港科广红鸟MPhil属于第二批，在11月26日才发面邀，时间为线下的12月10-11日。","方向确定与选校#方向确定与选校":"大三暑假通过对计院/信电老师（比如计院院长那个组，真是年少轻狂）的套磁，排除了AI4Sci等选项后，确定了可能会感兴趣的方向——空间智能这一块，目前国内也比较火的所谓具身智能。但是机器人这一块我确实不太喜欢，本科的时候一直在避免接触Robotics这个概念，当然也和认识一个比较讨厌的控制学院的同学有关系。不过既然决定了方向，下一步就是选校了。\nAI、CS领域，想要获得长足的发展，在我看来只有两个国家可以选择：中与美。美国在我大三的时候特朗普刚开始第二任任期，在签证与大学经费（后果是削减招生名额）以及工作机会（美国本地企业实习）都出现了重大利空，因此不做考虑，当然现在可能又回暖了。但是托福是路径依赖的语言，可以和雅思互相替代。因此选择中国这一块。而在本院预推免结束之后，其他（不被视为下保的）学校的也都差不多来不及报名了，唯一一个中科院大学还留有一点点，思来想去还是放掉选择港硕。\n工科信息这一块，对口实习甚至比论文一作还重要，而新加坡限制中国学生数量并且控制工作签证对外国人的发放，因此不考虑。选择离中国大陆更近的香港。至少因为我还有一个88的GPA，看不太上港五那两所，只选港三申请：","最终结果#最终结果":"","最终选择与规划#最终选择与规划":"比较感兴趣的CUHKSZ居然还不发笔面通知，这样就只能选择CUHK本部了。交了这个留位费之后，剩下像占尽地理劣势的HKUST其实都可以不用考虑了，甚至CUHKSZ都可以不用考虑，专心等港科广的面试结果。如果成功了，每月发的10000RMB凑起来去掉学费完全能把这11万留位费赚回来。\n最坏的情况是港科广面试不过，那样就需要好好进行后面的规划：\n留服认证问题：不同于那两所内地校区的港校，CUHK本部算留学要进行留服认证，但是留学生进央国企的话，很多国央企都要求本硕一致。 学制与实习规划：CUHK的CS是一年制，AI是1.5年制。两者都可以延期毕业从而多次参与秋招，从而争取攒出两段3个月以上的实习出来进入这个还未收敛的具身行业。 信息获取渠道：像是毕业去向、具体的规划等还是得上cc98问问校友学长之类的，小红书上面中介之类的杂鱼信息太多了。","未来城市小组项目活动#未来城市小组项目活动":"1. 头脑风暴阶段\n4-6名申请人，抽取一个**A+B（AI随机形容词生成）**的主题：\n未来健康保健技术 可持续生活 智能工业化 低空经济 海洋科技与经济 示例：寒冷的 + 智能工业化\n时间：30+10分钟\n评分指标（创意与沟通类）：\n指标 描述 分数（0-2） 创意贡献 申请人在头脑风暴中是否提出了至少一个具体的想法？ 0-2 积极倾听与反馈 申请人是否提出了至少一个澄清性问题或重述了队友的想法？ 0-2 创造力与独创性 申请人是否提出了至少一个在方法或内容上不同于其他人的想法？ 0-2 接受反馈的态度 申请人是否口头承认了反馈，或根据反馈提出了调整建议？ 0-2 2. 制作阶段\n使用基础材料（积木、纸、绳子、竹木、乐高等），在一个长、宽、高分别为 75.5cm、51.5cm、\u003e43cm 的立体空间内创作一个主题关键词的未来城市模型，包含不少于 5 个模块（模块功能或内容由小组自行定义）。\n示例：具备高度智能工业化生产能力，且能够在严寒环境中提供可持续的生活条件和高效管理的未来城市，包含了智能工业中心、严寒能源供应与管理系统、智能交通网络、常温智能居住区、抗寒垂直农业与生态等模块。\n2名评委全过程观察、记录、评分。\n时间：110分钟\n评分指标（执行与协作类）：\n指标 描述 分数（0-2） 制作贡献 申请人是否在项目的某个方面进行了实际操作（例如，放置材料、调整设计等）？ 0-2 任务责任感 申请人是否负责完成了至少一项分配的任务，并且无需提醒？ 0-2 协作能力 申请人是否与另一名队员在至少一项具体任务上合作（如规划或构建）？ 0-2 主动性与解决问题的能力 申请人是否在构建阶段遇到问题时提出了至少一个解决方案？ 0-2 领导能力 申请人是否至少一次分配或委派任务给队友？ 0-2 3. 交易与适应阶段\n随机抽取另一小组作为观察对象进行观察，并结合自己小组的作品情况，有针对性地为被观察的小组提出一个挑战任务（例如：地震、海啸、核污染等人类社会可能面临的重大灾难或者挑战，具体内容由各小组自行定义），同时要求自己小组的作品必须能够满足自己所提给对方的挑战任务要求。此期间允许各小组之间使用金币进行模块的自由交易，但最终需标注出所交换的模块。\n时间：10分钟\n每个小组必须在20分钟内完成挑战任务的应对方案，并在白板上写好小组作品介绍，要求包含应对方案，形式不限。此期间，允许各小组调整各自的作品以及进行组间的模块自由交易。建议各小组妥善使用金币，争取使投资产生最大价值。\n时间：20分钟\n评分指标（交易与适应类）：\n指标 描述 分数（0-2） 战略思维 申请人是否在交易阶段提出了至少一次交易或模块交换的建议？ 0-2 谈判技巧 申请人是否至少与其他团队进行了一次谈判，无论是提供还是接受条件？ 0-2 设计调整的灵活性 申请人是否根据挑战任务提出了至少一个设计调整建议或进行调整？ 0-2 压力下的适应性 申请人在挑战任务引入时是否保持参与而没有退缩或失去参与感？ 0-2 4. 挑战与最终展示\n每个小组面向各自评委进行团队陈述，结束后按照现场指引完成组内互评。\n时间：30分钟\n评分指标（展示与团队互动类）：\n指标 描述 分数（0-2） 演讲清晰度 申请人是否清晰地展示了自己在项目中的角色，无需队友提示？ 0-2 挑战任务中的领导力 申请人是否提出了至少一项团队应对挑战任务的行动建议？ 0-2 回应挑战的创造力 申请人是否提出了一个直接应对挑战的解决方案（例如，功能改动，增加新特性等）？ 0-2 团队动态 申请人是否鼓励了至少一位队友参与或征求了他们的意见？ 0-2 与评委的互动 申请人在展示过程中是否至少回答了评委的一个问题？ 0-2 活动地点：香港科技大学（广州）校园的 Highbay，全部制作环节必须在指定区域内完成；各小组在活动结束后须快速整理物资，清理现场后，方可离场。","注册流程#注册流程":"下一步是完成注册。在接受录取通知书并缴纳留位费后，要按照以下步骤完成注册，以便在开学前完成所有手续。\n重要提示：\n只有在完成注册程序后，才能领取CU Link（学生证）。 所有CUHK学生都需要在校园入口出示CU Link Card。有关迎新活动的详情，请访问 Orientation Website 2025。 自2023年8月1日起，CUHK Mobile Pass App 可供新生使用，可生成二维码用于校园通行和学习空间访问。详情请访问 ITSC网站。 注册步骤：\n步骤 1：提交在线注册\n你必须在指定的付款截止日期前缴纳留位费。 注册链接将在付款完成后生效：本地学生为第3个工作日，非本地学生为第14个工作日。 完成在线注册后，你可以在此处查看注册状态。 步骤 2：提交学生签证申请（仅限非本地学生）\n建议学生通过快递将原始签证申请表及所有所需支持文件寄送至CUHK，时间安排如下： 研究型课程学生：4月中旬前 授课型课程学生：5月中旬前（有预科课程的学生应在预科开始日期前至少8周提交签证申请） 步骤 3：提交研究生宿舍申请（如适用）\n修读全日制研究型课程或全日制UGC资助授课型课程的学生可以申请宿舍。请从研究生宿舍网站查看申请截止日期，并相应地向研究生宿舍办公室提交申请。 步骤 4：提交文件以完成录取条件（如有）\n在条件完成截止日期前，通过邮寄或快递将录取通知中指定的所需文件寄送： 授课型课程学生：寄送至课程办公室 研究型课程学生（MPhil \u0026 PhD）：寄送至研究生院 如果你未能在规定截止日期前完成录取条件，你的录取通知书将失效。 步骤 5：执行签证文件提交第一步（仅限非本地学生）\n步骤 6：激活 MyCUHK 账户\n完成上述步骤后： 本地学生：将从7月中旬开始通过ITSC的电子邮件和/或短信（仅限香港手机）收到v-code 非本地学生：将从7月中旬开始通过ITSC的电子邮件收到v-code 使用v-code从学生计算账户收集系统检索OnePass密码，以便通过MyCUHK进行选课和其他服务。 步骤 7：执行签证文件提交第二步（仅限非本地学生）\n步骤 8：领取 CU Link（学生证）\n本地学生：可在录取日期或之后领取CU Link。请联系CU Link Card Centre查看CU Link是否已准备好并确认领取安排。 非本地学生：在完成上述步骤7后7-10个工作日，请联系CU Link Card Centre查看CU Link是否已准备好并确认领取安排。 部分课程会为学生统一领取CU Link。如有疑问，请联系课程办公室。 步骤 9：更新 MyCUHK 信息（仅限非本地学生）\n在MyCUHK更新邮寄地址和联系方式。 步骤 10：提交研究生奖学金支付指示表\n填写并返回\"研究生奖学金支付指示表\"至财务处会计运营与系统组（邵逸夫楼1楼）。 步骤 11：选课\n选课将在8月初进行。请根据你课程的学习计划和研究生院的建议选课。详情请参考研究生院网站（学生 –\u003e 选课和加退选 –\u003e 选择相应的学年）。 注意事项：\n如果未完成在线注册，将无法进行选课和领取CU Link。 请不要通过电子邮件将完成录取条件的证明文件发送至任何研究生院邮箱账户，这可能会延迟处理提交。 到时候看看要不要申请宿舍","注意事项#注意事项":"证件准备：请准备申请者有效身份证件原件（身份证、护照或其他有效证件）入校及签到。 住宿安排： 所有申请者在报到当晚将安排住宿（8人公寓，135元/晚，自费） 如果申请在面试后延长一晚住宿（续住房型：3人公寓，214元/晚，自费），学校将为你安排三人间住宿 如因住房紧缺，将由校方另作安排 未申请续住的申请者应在面试当天19:00之前离开学校 重要提示：因此通勤非常紧凑，当天去注册，第二天晚上就要连夜回来，需要注意买票这一块。\n纪律要求：迟到、缺席或不遵守规定，甚至对面试现场造成破坏或不良影响者，将被直接取消面试资格。 录音录像：主办方将根据实际工作需求，在活动现场安排相关录音、录像或拍摄等。 差旅费用：申请者参加线下面试的差旅自理，校方将不提供订票服务。","港硕申请回顾#港硕申请回顾":"港硕申请回顾","第一批申请9月30日#第一批申请（9月30日）":"港科广 红鸟 港中深 MAIR 港中深 CS（怕不稳补申）","第二批申请10月25日之后考出托福成绩后补申#第二批申请（10月25日之后，考出托福成绩后补申）":"CUHK AI CUHK CS CUHK IE HKUST CS HKUST IE 没有申请HKU的原因是，HKU CS属于超级大班，感觉含金量基本上靠的是【香港大学】的综排牌子；第二个原因就是这是唯一一个需要笔面的，虽然笔试难度没有咱ZJU的期末考试难度高，但就很恶心。与之形成鲜明对比的就是CUHK，没有笔面，直接花600RMB去抽奖。 BTW，这里面申请页面的前端UI做的，我没有想到会这么拉，看下来港科广做的是最好看的，港中深其次但是它的密码不支持特殊字符（@#￥这些），关键是还不会专门提醒报错是这个原因，我试了好几次换了几个邮箱都输密码错误，最后才被这个问题的原因猜出来气消了。港中港科两所港校本部的申请界面丑的颇有上世纪网站的美感，值得吐槽的是HKUST的Country那一栏不像其他的学校那样是China(Mainland)，它是The Mainland of China，首字母是T还是M来着，导致第一眼根本看不到，只找到一个Chili\n申请建议不要请中介，如果你是想要找港新的选校的话，欧美啥的另说，但是这几个地区靠ZJU的同学完全可以自己做到。中介的最大作用就是你去加他跟他聊的第一次，它会给你一些关于流程上的信息，比如我就找了一个中介来帮我介绍港科广的申请流程、面试之类的常识信息，后面直接忽略掉他的消息就行。","面经分享#面经分享":"因为红鸟的特殊机制，面试的时候尽量伪装自己，或者说释放天性，选一个相对冷门的Hub尽情阐述，毕竟进去之后有半年才正式选导。\n小红书链接汇总： 1. 港科广第二批面经（推研版）\n前一天：\n下午到达科广签到，五点老师带队参观校园 面试当天：\n离开宿舍时要收房卡退房 8:30开始签到 团队面试：\n题目：未来健康保健技术 + 宁静的 个人整体感受：team2队员都很友好，过程愉快。评委老师似乎在最后的陈述部分才动笔记录（似乎在打分？所以这个part要重视）。最好开始前上好厕所，带杯水进来场地，全程站着特别累，12:30才结束。 个人面试：\n中午要抓紧吃饭，时间很紧 13:00签到，14:00开始，我是最后一个等到17:00面完 问题：\n项目经历中的合作与困难 你未来想研究这个方向（老龄化）要达到什么目的 研究方法？ 假设要研究这个项目，你想要什么队友（两个老师分别问了这个问题，答了两次） 如果你来到这个学校，我要你换个研究方向你愿意吗？你要换什么方向，为什么 什么情况下你会换研究方向？（逐渐抽象） 你对你现在的方向感兴趣，还是你想换的另一个方向？ 假如以后没人生小孩，全是老人，没钱发养老金，你是决策者你要怎么办 PS：我和评委老师专业不对口，几乎没有专业知识的提问，反而future plan引起了他们的兴趣所以一直在抽象地拷打（maybe我是最后一个，老师们都开始放松了随便乱问）。\n2. 港科广第二批MPhil面经\n上午团队面试：\n围绕未来健康技术、宁静的两个关键词进行团队合作搭积木，目标是未来城市的设计 我负责的是老本行，搭建的能源电力部分 感觉这个团队合作需要多张嘴多动手，自圆其说即可，队友都特别给力，也非常nice 对于我这个i人其实面完精疲力竭，幸亏抽到了比较e的队友，被带动着也就慢慢热络了 下午个人面试（10分钟的提问）：\n解释一下CLLLC，通俗一点给我们讲一下 你为什么不保研 你还申请了哪些学校（或单位） 通俗解释一下High power与high power density是什么，怎么实现，作用是什么 你的变换器怎么实现的low ripple低纹波 你的参数是在仿真里枚举就行了吗，设计逻辑是什么 你的意向导师是谁 （英文提问）你觉得你的变换器还有什么地方可以改进，这个你能靠自己完成吗还是需要teamwork 注意：存在英文提问但是可以中文回答，老师们都很好，果然是一所氛围特别好的学校，两天体验下来非常非常好。\n3. 港科广红鸟MPhil线下面试复盘\n申请时间线：\n1.29春节准备材料 → 1.31完成投递 → 3.31收到面邀 → 4.17面试 → 4.25收到通知 线下面试复盘：\n小组群面项目为搭建未来城市，随机抽取的关键词为奇思妙想的 + 可持续生活。\n我主要关注的点为两个：\n1. 快速满足客观要求\n我把规则分为了客观要求和主观要求，我的第一任务是快速满足最低的客观要求。搭建规则只对模块数量和尺寸做了要求，尺寸中相对最难满足的是最低高度。问题就转化为了如何在材料有限的情况下，用最简单的方法达到指定高度。\n最直接的是用手边的置物箱，但空间太小施展不开手脚。经过测量，椅子的长和宽刚好满足所给的尺寸要求。我的策略是将椅子直接放在桌子上作为城市的地表部分，椅子下面则作为地下部分。这样在满足要求的前提下，空间也更宽敞，搭建的时候也可以施展手脚。在快速把客观要求全部满足的前提后，就可以放开手脚对各模块进行自由发挥了。\n2. 主观要求的思考\n我的策略只有三个：做局部区域最优解，命运共同体，以人为本\n第一，面试整体的玩家数量是固定的，所以在资源和时间有限的情况下，我认为关键不是闭门造车，尽善尽美追求极致，而是只需要在某些方面或某个模块做得比其他组好就可以，做到局部最优解。 第二，小的来说，每个人所在的小组是一个独立的城市，小组之间是竞争关系；但从大局来看，所有的组又都是一个命运共同体，如果把城市与城市连接起来，所有的城市会构成一个更大的城市，连接起来后形成的效益将远远大于自身建设的效益。这一点最难的地方是在于如何把所有的组连通在一起，但最好玩的点也在于当把其中一部分组连接在一起之后，其余没有连接的组反而会失去越来越多的东西，从而迫使其加入连接。其实当把这场面试的设计看作如今世界局势的缩影时，也会很有趣。攒局搭建这个连接平台的团队当然会失去一部分精力和时间来做客户的开拓和运营，在自己的城市搭建上会吃亏，但这是长期可以一本万利的事情，所以值得去做。另外，在连接其他组的过程中，也可以提前了解各个组的进度和搭建模块特色，可以有效的为第一点中的局部最优解策略提供信息。 第三，城市是由人构成的。城市的设计是以用户为中心来展开的，所以搭建策略从一开始就从人的衣食住行四方面来进行规划的。 4. 港科广第三批个人面面经（已推研）\n下午个人面试要点：\n首先，能脱稿一定要脱稿脱稿！！！ 我就抱有侥幸心理没脱稿，老师上来第一个问题就是：你手里拿着的是小抄吗？（真的很压力）还好本人机智，笑着解释了一下。最后答辩完评委老师还特地叮嘱\"下次就不要带着稿子了\"（万幸是过了，当时面完差点以为自己凉了）。而且听说隔壁组一个没脱稿的uu被狠狠压力了。\n不同老师问的问题风格有很大差异，有些老师可能会问很专业的问题，有些老师的问题就比较general，遇到什么样的老师是不可控的，所以我们能做的就是准备好每个可能的问题。PPT上所有涉及到自己以前的论文、项目一定要再看看弄清楚每个细节，老师提的问题不管多不可控都是基于PPT的。我也遇到比较厉害的uu，有意在pre的时候挖一些坑，刚好评委老师就问到了。\n下午面试包含一定压力面的成分，所以面对评委老师的刨根究底一定要思路清晰，保持镇定。\n我被提问的问题：\nQ：为什么转专业？ A：因为巴拉巴拉巴拉巴拉（提前准备了顺利完成） Q：你的研究计划好像港科广没有这方面的老师啊？ A：详细阐述研究计划，有哪个老师是做这方面的…巴拉巴拉… Q：研究计划里某某某是什么意思呀？ A：巴拉巴拉…解释一下 Q：联系意向导师了吗？ A：还没有（确实是本人拖延症又犯了） Q：你手里拿着的是小抄吗？ A：是我的讲稿，我想把所有细节都注意到，所以带了讲稿 Q：你能具体讲讲xxx项目的结果吗？PPT上图是什么意思？ A：巴拉巴拉巴拉巴拉（意料之中准备了） 总之，以不变应万变 5. 港科广红鸟线下面试-面经分享最后一批随缘\n报道前一天：晚八点落地白云机场，坐地铁两个小时到学校附近，找了家宾馆住下开始做PPT，当时PPT完成度只有大概一半（换模版+稿子没写）。\n我PPT内容比较多，精简了好几次之后还是有大概20页。其实也不是东西有多少，主要是之前东西都放到一页，导致图片和文字都很小很密，几次模拟面试反馈都不是很好（加上模版不太学术有点幼稚）。我索性直接重写了。\n第二天：报道之后五点下楼有老师领着逛学校，走了一下第二天面试地点路线。晚上吃饭之后就回宿舍做PPT写讲稿，整个Q\u0026A和群面环节几乎没准备（之前忙毕设，就大致扫了几眼面经）。一直做到凌晨四点，上床五点睡到七点半。\n上午群面：关键词是未来保健技术和sophisticated，我们组氛围还ok，但是大家思路其实比较乱。搭积木还是有点费手的，而且我们的模块很容易倒塌，因为这个原因差一点没搭建完。老师提问的时候一定要听清，当时一个问题：每个人为了应对别的组提出的挑战都做了什么。我看当时没人说话就想打破冷场但是急匆匆的没有回答到点上，感觉是个失分项。群面在这里不详细讲，可以看其他帖子，推荐看我们同组队员的帖子\"港科广红鸟硕士面经(被恨版)“里的描述。\n下午个面：我是最后一个，前边的人反应被拷打了或者提问时间没用完。我问答没有准备特别多，就准备了一些例如为什么选择项目这种比较general的问题，最后都没用到……一直在熟悉稿子，从一开始800词删到400词还是背不下来，索性带平板进去念了，大概念两句背一句看一眼老师。\n主题我选择的是低空经济，项目都是实体机器人和上面的算法，三个小项目一个大项目。中间穿插一些学工和比赛之类的经历。时间分配大概是个人介绍和动机一分钟，项目和经历三分钟，最后原因总结致谢一分钟。没有讲技术细节，图片视频比较多但是每页一两张。\nQ\u0026A：我的感觉是要多说，另外，最好表现的自信一点，本人平时比较e，面试的时候也是语速很快，思路也比较清晰。一开始几个老师是想压力的，但是每个问题问一个方面，我一般会多说好几个点综合性的讲，都超出了他们的预期，后面整个氛围都非常好了。面试结束的时候我说期望再见到你们，几个老师都笑了，有一个老师还让我去官网看一下他的实验室和研究方向（暗示我？）最后关门出去的时候听见几个老师笑着说：年轻人还是热情啊。感觉整体上是认可的。\n6. 港科广RBM第一批面经\n申请时间线：\n9.1提交网申 9.13通知补充材料 10.14面邀 10.23线下面试 11.4推研 上午小组活动：\n感觉总体很玄学，注意要全程保持参与，然后在交易阶段做出贡献 下午个人面：\n感觉也很玄学，准备了很多专业和研究项目的问题，完全没问到，都是很general的问题。 RP的应用场景，为什么工业4.0一直没有得到全面的推广 是否准备创业 如果让你选择，你是会深入学习本领域的知识，还是会去学习其他领域的知识 本科竞赛，自己做了什么 本科的TA工作经历 RP能给计划的研究领域（智能工业化）带来什么提升 RP的具体创新点 感觉面试老师应该都不是CS/AI/CE相关专业的，没问到专业问题。\n7.港科广红鸟26fall正式批面经 📌tl 10.22 报道、刷身份证、签名、付住宿费，有车接送到宿舍楼下，找到宿舍收拾东西/认识新舍友；会领到一张纸，上面有地图以及第二天面试时间 10.22 17:00，步行、宿舍楼下集合逛校园，可以看到第二天上午群面的场地 10.22 ～18:00之后，自行吃饭（可以体验一下学校食堂），回宿舍休息，想晚起的话记得准备明天的早饭 10.23 上午，先退房，然后把行李箱搬到面试的地方，拿身份证签到，现场抽签分组，抽主题，开始面试，有电子钟，注意把握时间 10.24 下午，拿身份证签到，现场抽签面试教室，拷贝ppt，到等候室等待面试，面试完可以直接走人\n📌面经： 1⃣上午小组面： 我们当天抽到的是“可持续生活”+“ 独自漫游的”，算是关联性比较直接的两个主题，当然这会导致大家能想到的东西差不多。总体的流程就是构思讨论、搭乐高、看其他小组的模型并提出挑战、交易以解决自己收到的挑战、向面试老师汇报。 建议：\n务必仔细看官网的评分表 可以自己找AI模拟一下提示词，想想怎么布局、切合题目 搭积木以及城市规划不是本质的考核内容，基本切题即可，搭的结构也不需要特别复杂，能说得通就行，抽象表述也ok 但是，老师也有可能在大家都介绍完之后询问城市的功能区和内容，正常回答即可，没考虑到就及时承认 感觉自己群面表现一般，在无领导小组中，不是发言很多的或者经常帮助组员的角色（没遇到机会，玄学），但是在交易环节比较活跃，老师应该也关注到了。所以不必太焦虑和纠结，正常表现就可以 2⃣下午个面： 我选的是英文汇报+中文回答。 PPT的思路是讲故事，把自己的经历融入到故事里面去：为什么未来想做这个主题的工作，我有什么专业基础（这里可以展现标化的绩点、获奖、论文）来支撑未来的研究，然后也可以讲讲科研探索小故事，比如做了第一个项目之后思考到了什么问题，从而想做主题方向的研究，为此又做了哪些努力（论文、项目、实践），最后再往你的主题和构想上面靠一靠，讲讲为什么选择红鸟。 提问环节应该有压力面，正常回答即可。可能会问你的项目需要哪些学域的老师指导、需要哪些专业的同学参与、不同专业的合作者分别负责项目的哪些部分、有无创业经历、（有经历）为什么没有创业成果等等。 建议：\n准备充分，汇报只有五分钟，内容多的话语速要快，但是亲测现场语速更快（甚至会漏了一些点忘记讲），自己准备大概5.30，现场只有4分钟 一些很难的问题一般都是压力面，所以如实回答，稳住心态，保持微笑 ：） 8.红鸟计划第一批推研！ 10.1面邀-10.24面试-10.31下推 有始有终的十月！\n群面：智能工业化+Aurora 一开始担心和四个工科生一组会不会一句话说不出来？但很快联想到极光粒子能收集和有色金属后完美的发挥了商科生+e人的诡辩能力，个人觉得群面真的超超常发挥！\n个面： ①陈述：极限准备了一周，但我真的觉得我把个人陈述环节做的非常非常好（需要参考可以私信），紧扣“可持续生活”主题的同时，用了个word play搭了两条很巧妙的主线和暗线联系自身经历贯彻全部，三个导师真的一直在点头！（所以哪怕科研和项目背景较弱也不要怕！！商科生有自己的扬长避短的打法）\n②盘问：详细说了我CV里的一个项目经历和创业经历，前期自认为得心应手，直到有个老师突然灵魂发问，质疑我能否达到红鸟的学术要求？于是乱了阵脚…后面一顿胡说，时间到了之后，这个老师又给了我个追问，潦草收场。（好在结果满意） 总结： 庆幸自己作为极少数商科生脱颖而出，群面五进一（如果组员刷到这里，认出了我，我想说我也真心的感谢你们没有push我，和你们度过了非常快乐两小时）个面也是水来土掩。\n9.拿下港科广红鸟面试，只因我做对了这几件事 主播是25届mphil的最后一批面试，凭借一些面试的小技巧成功通过！ 红鸟的面试分为团面与个面，这篇就先讲讲团面的部分。\n团面主要考察的是团队协作能力与创新思维，内容是用积木搭建未来城市。 主办方会在未来健康保健技术、可持续生活、智能工业化等题材中抽取一个范围，并用AI随机生成一个关键词。 我记得当时我们抽中的是海洋科技与经济，生成的关键词是空灵的。\n在团面环节，我认为有一些会对红鸟面试很加分的点： 第一，做你自己，不必强行角色扮演。 在无领导小组讨论过程中，你只要完成好自己负责的板块，在小组讨论中提出建设性意见，负责好自己里面的任务。 我认为团面的本质是看你是否具有与人合作的潜力，能否利用好团队优势进行分工，不必为了展现自己的领导能力而“过度表演”。\n第二，善用身边工具，带好便签纸。 在团面中，面试官并不纠结你搭建的是否能实现，重视你对想法的表达。这时候运用好便签纸就很重要，可以通过便签纸记下自己搭建部件想要达成的功能是什么，在便签纸中记下一条从What到How的路径，帮助自己与团队更好地记忆与表达设计理念。\n第三，天马行空不一定是贬义词 在积木环节，你可以提一些富有想像力的点，不必太拘泥于技术能否实现，只要能解释通，就可能成为设计的闪光点。 当时我们小组有位同学提出设计“泡泡”作为交通运输工具，灵感源于小时候在公园玩的浮力球。我们并没有详细阐述这个泡泡是怎么驱动，但提出了这一个新奇想法，显得我们有独特的创造力。\n第四，模块化思考是设计的重要能力 模块化建设城市我是很好的思路，我们小组把城市分为了多个不同功能模块，通过模块组合的方式进行城市规划。在团面开始前与小组进行头脑风暴，提出这个想法，可以在早期向面试官展现你们的建设思维，为后续的高效率建设提供方向。\n第五，言简意赅总结自己的贡献 在个人陈述部分，可以把表达重点放在这几个问题，尽可能展现自己的价值： 在头脑风暴环节中，你在团队中扮演的角色是什么？ 你在头脑风暴中提出了什么建设性意见？ 在制作过程，你和谁进行了合作？合作过程中如何分工的？你们的合作达到了什么效果？ 在交易环节，你如何跟其他小组谈判？谈判发挥了什么作用？"},"title":"港硕申请回顾"},"/blog/2025/hkustgz-preparation/":{"data":{"1-低空经济-low-altitude-economy#1. 低空经济 (Low-altitude Economy)":"","1-低空经济-low-altitude-economy-1#1. 低空经济 (Low-altitude Economy)":"M1 核心产业：eVTOL垂直起降枢纽（像蜂巢一样的停机坪） M2 能源：分布式换电站（无人机飞累了直接换电池，不用充） M3 交通：3D空域航道（用绳子拉出空中轨道，分层飞行） M4 生活：空投接收阳台（每家每户窗外有个伸缩平台接外卖） M5 大脑：空域流量监管塔（雷达+视觉识别，防止撞机）","1-负面困难类形容词如寒冷的焦虑的破碎的危险的#1. 负面/困难类形容词（如：寒冷的、焦虑的、破碎的、危险的）":"策略：加\"防御\"和\"冗余\"\n寒冷的 → 给M2能源加\"供热管道\"，给M4住宅加\"保温层\"，M1工厂变成\"全封闭式\" 焦虑的 → 给M4生活区加\"心理疗愈花园\"，M5大脑加强\"隐私加密算法\"，M3交通强调\"绝对安全防撞\" 危险的 → 给整个城市加\"防护罩\"，M5大脑变成\"灾难预警中心\"","2-未来健康-future-health#2. 未来健康 (Future Health)":"","2-未来健康-future-health-1#2. 未来健康 (Future Health)":"M1 核心产业：个性化基因治疗中心 或 脑机接口康复仓 M2 能源：生物质能发电厂（利用医疗废弃物发电） M3 交通：急救绿色通道（地下真空管道，胶囊列车运送器官/病人） M4 生活：全适老化社区（防摔倒地板+情绪监测墙壁） M5 大脑：全民健康云平台（实时处理所有市民的心跳血压数据）","2-正面抽象类形容词如快乐的甚至漫游的无形的#2. 正面/抽象类形容词（如：快乐的、甚至、漫游的、无形的）":"策略：加\"体验\"和\"连接\"\n快乐的 → M1产业里增加\"多巴胺制造\"，M4生活区增加\"游戏化设施\" 独自漫游的 → M3交通变成\"单人飞行器\"，M4住宅变成\"移动房车\" 无形的 → 强调M5大脑（看不见的网），实体建筑都做成透明的或者地下的","3-可持续生活-sustainable-living#3. 可持续生活 (Sustainable Living)":"","3-可持续生活-sustainable-living-1#3. 可持续生活 (Sustainable Living)":"M1 核心产业：垂直农业塔（每一层都种菜，像DNA螺旋结构） M2 能源：光伏玻璃幕墙 + 雨水收集净化罐 M3 交通：共享单车/步行绿道网络（禁止机动车） M4 生活：模块化可降解住宅（房子是积木拼的，不想住了拆了换地方） M5 大脑：碳足迹追踪中心（计算每个人排了多少碳，以此发货币）","4-智能工业化-smart-industry#4. 智能工业化 (Smart Industry)":"","4-智能工业化-smart-industry-1#4. 智能工业化 (Smart Industry)":"M1 核心产业：黑灯工厂（无人工厂，机械臂自动作业） M2 能源：微型核聚变反应堆（工业耗电大，需要强能源） M3 交通：AGV自动物流小车轨道（地面全是二维码，小车跑来跑去） M4 生活：职住一体胶囊公寓（工人住在工厂楼上，下楼上班） M5 大脑：工业数字孪生中心（你的测控强项：预测性维护，机器坏之前先报警）","5-海洋科技-ocean#5. 海洋科技 (Ocean)":"","5-海洋科技-ocean-1#5. 海洋科技 (Ocean)":"M1 核心产业：深海矿产采集站 或 海底数据中心 M2 能源：潮汐能/波浪能发电机（利用海浪晃动发电） M3 交通：潜水艇驳接港口 M4 生活：海上漂浮居住岛（像荷叶一样漂在水面） M5 大脑：水下声呐监测网（监控鱼群和海啸）","ai辅助诊断#AI辅助诊断":"人话：“AI看片”。医生看CT要10分钟，AI看只要1秒，还能发现肉眼看不到的小结节。","evtol-电动垂直起降飞行器#eVTOL (电动垂直起降飞行器)":"人话：“空中的士\"或\"放大版的大疆无人机（能坐人）\"。不需要跑道，电动的，声音小，适合在城市楼顶起降。","slide-1-2-title#Slide 1-2: Title":"Good afternoon, professors. I am Bao Bowen from Zhejiang University. Today, I present my proposal for the Redbird program\nI major in Biomedical Engineering with a GPA of 3.97. During my undergraduate studies, I didn’t limit myself to coursework. I actively participated in interdisciplinary research and innovation competitions, winning the University Scholarship. This rigorous engineering training provided me with a solid foundation in Artificial Intelligence, preparing me for complex system design.\nSo, why Smart Industrialization? Please look at this diagram from the World Economic Forum’s latest report. It shows a ‘Lighthouse Factory’ achieving a 67% increase in productivity using AMR. This is impressive, but… it is mostly limited to isolated, caged zones. To me, Smart Industrialization means moving from Rigid Automation to Embodied Collaboration. It is not just about robots working faster; it is about robots working safely alongside humans in unstructured environments. This is where Embodied AI creates real value\nTo achieve this, I first built my foundation in data processing. In my research on Wavelet Convolutions, I optimized time-series analysis for medical data. While this was for healthcare, the core capability is universal: I learned to design lightweight, efficient algorithms that extract precise features from noisy data. This intuition is critical for any real-time robotic system\nIn the Innovation Competition, where we won the National Bronze Award, we identified a real pain point: the labor-intensive digitization of archaeological reports. I led the team to build a product that fused YOLO detection with layout analysis to automate this process. This experience taught me how to transform technical solutions into viable products, which aligns perfectly with Redbird’s maker spirit\nMoving forward, I have identified two key research directions for my Master’s. First is End-to-End Control, similar to XPeng’s recent work on VLA models, which cuts out the middle language translation step, enabling direct mapping from visual signals to action commands. Second is World Models—giving robots an internal simulator to predict future consequences before acting.\nCurrently, I am using my Bachelor Thesis as an entry point into this field. I chose Social Navigation because it allows me to focus on Reinforcement Learning logic without getting bogged down by complex kinematics yet. I am optimizing the Falcon baseline by adding a Risk Preperception Module. The goal is to train an agent that doesn’t just reach the goal, but knows how to be ‘polite’ and aware of environmental obstacles\nBeyond simulation, I am also diving into hardware deployment. I am porting the LOVON algorithm to Unitree Go2. It’s a learning process. For example, currently, the robot lacks occlusion awareness—if a person hides behind a wall, the dog might crash into the wall trying to track them. Debugging these real-world failures is exactly where I am gaining my Sim-to-Real experience.\nLooking at my roadmap for Redbird: Short-term: I will finish my thesis and continue solving those hardware navigation issues.\nMid-term: At HKUST(GZ), I plan to dive deeper into my research interest, exploring how to integrate better environmental representations.\nLong-term: I hope to leverage the GBA scenarios to realize “Smart Industrialization, Become a leading EAI Researcher\nFinally, why Redbird? It has provided access to over 90 enterprises and 10 more labs in the GBA. I specifiy 3 things here:\nThe Interdisciplinary Synergy that allows me to collaborate across hubs;\nThe opportunities to leverage GBA Supply Chain \u0026 Industry Giants;\nAnd the university’s proven track record of incubation,\nThat’s my presentation. Thank you","个面#个面":"0、 很多学科都跟AI相关，有必要吗？像是你BME本身不太需要？\n1、 拷打BME为什么做这么多CS相关\n2、 必须放弃推免才能申请吗\n3、 如果你是项目制，你打算怎样的项目和怎样的队友，你的缺陷是什么\n总的来说，面试总共在5个房间进行，不同房间的老师提问风格不一样，我所在的421B就属于非常友善的那一组，只有最左边那个老师（可能是化工相关专业）提的问题不算general。\n同学A：\n上了红鸟之后如何提升自己 排名够吗？为什么不去保研 有没有offer（Mphil新南威尔士） 化工吸收（专业问题） 对机器学习代码有多少了解 同学B：\n郑大为什么不接北理推免offer？ （回答不接之后问）你是不是想创业？为什么想创业？ 质疑项目经历：工程化的东西，没有很难 有没有什么小组合作项目经历 同学B就是421A的老师问的，而同学A和我一样是421B，可以看出421B确实相当温柔了，下面还有其他同学被压力的吐槽： 不拉我通过的话也就算了吧。毕竟这样复盘下来，失分点主要在这个群面上，因为他们自己的筛选机制错过了我这样优秀的学生（迫真），那是他们招生组的损失，更何况他们上一学年貌似也出现了一些状况，广州南沙也非常郊区看起来不方便找实习。而且第二批面试分为3天，我隶属的第2天就有66名学生参与，整个第二批可想而知池子有多大。","个面-1#个面":"","交易适应阶段#交易/适应阶段":"在交易阶段提出了至少一次交易或模块交换的建议 至少与其他团队进行了一次谈判，无论是提供还是接受条件 根据挑战任务提出了至少一个设计调整建议或进行调整 在挑战任务引入时保持参与而没有退缩或失去参与感","人机协作机器人-cobots#人机协作机器人 (Cobots)":"人话：“不伤人的机械臂”。以前机械臂要把人围起来怕打死人，现在的Cobot碰到人会自动停，可以跟人肩并肩干活。","低空旅游#低空旅游":"人话：坐直升机/eVTOL看风景。","共享经济#共享经济":"人话：“只用不买”。共享单车、共享充电宝、共享雨伞。","农业植保无人机#农业植保无人机":"人话：“农田洒水机”。大疆在这个领域很强，用无人机喷农药、播种，效率是人的几十倍。","制作城市阶段#制作城市阶段":"尺寸要求：长、宽、高分别为 75.5cm、51.5cm、\u003e43cm 在项目的某个方面进行了实际操作（例如，放置材料、调整设计等） 负责完成了至少一项分配的任务，并且无需提醒 与另一名队员在至少一项具体任务上合作（如规划或构建） 在构建阶段遇到问题时提出了至少一个解决方案 至少一次分配或委派任务给队友体现领导能力","可穿戴监测#可穿戴监测":"人话：Apple Watch的医疗级进阶版。贴在皮肤上的柔性电路，能测汗液成分、血糖等。","可降解材料#可降解材料":"人话：“吃土塑料”。袋子扔土里，几个月就被细菌吃没了，变回泥土。","垂直农业#垂直农业":"人话：“摩天大楼种菜”。在市中心的写字楼里，一层层架子上用水培技术种菜，省地、省水、不需要运输。","城市基础五大系统#城市基础五大系统":"城市类型分类：根据产业定位，城市可分为农业城（第一产业）、工业城（第二产业）、旅游城（第三产业）等。不同城市类型会突出不同的系统模块。例如：寒冷的智能工业化城市 = 具备高度智能工业化生产能力，且能够在严寒环境中提供可持续的生活条件和高效管理的未来城市，包含智能工业中心、严寒能源供应与管理系统、智能交通网络、常温智能居住区、抗寒垂直农业与生态等模块。\n模块序号 模块名称 功能定义 ZJU仪器切入点 Module 1 核心产业区 (The Core) 这是题眼。根据抽到的5大主题变身（如工厂、医院、机场）。 部署自动化流水线 / 手术机器人 / 植保无人机。 Module 2 能源动力区 (Energy Hub) 给城市供电。风/光/核/地热。 能源管理系统(EMS)：部署传感器监控能耗，进行PID调节。 Module 3 立体交通网 (Transport) 人流物流。地面/地下/空中。 轨迹规划算法：无人驾驶调度，防碰撞系统。 Module 4 生活与生态区 (Living \u0026 Eco) 给人住的 + 处理垃圾/水的。 非侵入式健康监测：智能家居里藏着传感器，养老不用穿戴设备。 Module 5 中央指挥大脑 (The Brain) 你的主场。控制中心/数据中心。 数字孪生/IoT平台：所有传感器数据汇聚于此，大屏幕实时报警。","基因编辑-crispr#基因编辑 (CRISPR)":"人话：“上帝的手术刀”。像编辑Word文档一样修改DNA，把致病的片段剪掉，换上好的。","头脑风暴阶段#头脑风暴阶段":"提出了至少一个具体的想法 提出了至少一个澄清性问题或重述了队友的想法 提出了至少一个在方法或内容上不同于其他人的想法 口头承认了队友反馈，或根据反馈提出了调整建议","工业物联网-iiot#工业物联网 (IIoT)":"人话：“机器说话”。螺丝刀、机械臂、传送带都连网，互相发数据，协同工作。","提问#提问":"","提问环节评分标准#提问环节评分标准":"理解并准确回应了评委的问题 回答经过深思熟虑，并显示了对主题的深入理解 面对压力时保持冷静镇定，沉着应对难题 很好地应对挑战性或意外问题，并提供相关的解决方案或观点 展示逻辑推理能力，分析问题并提供有见地的回答 提供团队合作的具体例子，并表达对协作的强烈兴趣 观点具有说服力，并能够有效支持其立场 展示自我意识，讨论了自身的优势、劣势和需要改进的地方 根据评委的反馈调整自己的回答，并与他们进行有意义的互动","数字孪生-digital-twin#数字孪生 (Digital Twin)":"人话：“虚拟克隆体”。工厂里有一台机器，电脑里有一个一模一样的3D模型。机器动，模型也动；模型预测机器明天会坏，机器明天真就坏了。","无人机物流#无人机物流":"人话：“美团/顺丰空投”。外卖不走马路，走窗户。关键技术是航路规划（不撞楼）和末端投放（不砸人）。","最终展示阶段#最终展示阶段":"清晰地演讲自己在项目中的角色，无需队友提示 提出了至少一项团队应对挑战任务的行动建议体现领导力 提出了一个直接应对挑战的解决方案（例如，功能改动，增加新特性等） 鼓励了至少一位队友参与或征求了他们的意见 在展示过程中至少回答了评委的一个问题","柔性制造#柔性制造":"人话：“个性化定制流水线”。以前一条线只能产黑色T型车，现在这条线上一秒产红色SUV，下一秒产蓝色跑车。","模版#模版":"“各位评委老师好，我是包博文。在这个项目中，我主要担任的是技术落地与系统整合（Technical Integrator）的角色。” “在头脑风暴阶段，面对【题目形容词 + 主题】这个命题，大家一开始主要关注在概念发散上。 我的贡献是引入了系统工程/测控专业的视角，提出了【你的核心Idea】的概念。 具体来说，我建议不仅要关注建筑外观，更要通过【具体技术手段，如传感器/IoT/闭环控制】来解决【题目形容词】带来的挑战。 这个建议确立了我们后续**‘技术驱动’**的搭建基调。”\n(示例：面对‘脆弱的海洋’，我建议不仅要建岛屿，更要引入**‘分布式传感器网络’，通过实时监测环境数据来预警灾害，从而化解‘脆弱’这一难题。) “在制作阶段，我主要负责【你的模块】的搭建，期间我与负责【队友模块】的【队友名】进行了深度合作**。 分工上，他负责【外观/结构/上层建筑】，而我负责【底层逻辑/能源/传感器/内部连接】。 合作中，为了解决【具体困难，如高度/稳定性】的问题，我利用【物理原理/材料特性】辅助他进行了加固。 最终效果是，我们的模型不仅满足了【硬性指标】，更实现了【功能上的互联互通】。” (示例：我与负责主体结构的B同学合作。他负责搭建高塔，我负责底部的海上风力发电底座。为了解决高度不够的问题，我利用风机叶片的长度优势，帮助团队轻松突破了43cm的限制，同时为他的建筑提供了稳定的能源概念支撑。) “在交易环节，面对【挑战名称】的突发状况，我主动与【对手组名】进行了谈判。 我的策略是**‘价值交换’，我用我们多余的【本组资源】换取了他们的【急需模块】。 这一谈判的关键作用在于，它让我们在不推翻原有设计的前提下，以最低成本引入了【外部技术】，成功解决了【挑战难题】，体现了红鸟倡导的跨学科融合精神**。” (示例：面对**‘噪音污染’挑战，我用我们的深海鱼油换取了隔壁组的‘多孔吸音材料’。这让我们引入了物理降噪层**，配合我的主动降噪算法，成功化解了生态危机。) “总的来说，我认为我们组不仅建成了一个模型，更构建了一个有生命力的系统。当然，这个系统的美学设计离不开【队友名】的贡献，下面请他来补充。”","水循环系统#水循环系统":"人话：“中水回用”。洗澡水过滤后用来冲厕所，雨水收集起来浇花。","海上漂浮城市#海上漂浮城市":"人话：像巨型航母或积木一样的城市，浮在水面上，应对海平面上升。\n注意：我不知道可不可以带平板，可以带平板的话能不能问AI，根据面经下午的个面有人带平板过的（当稿子）","海上风电#海上风电":"人话：把大风车插在海里。海风比陆地风大且稳。","海底数据中心#海底数据中心":"人话：“把服务器扔海里”。微软干过。因为海底冷，省了空调电费；而且海底没氧气，机器不容易氧化。","海水淡化#海水淡化":"人话：把咸水变纯净水。中东国家常用。","深海采矿#深海采矿":"人话：去海底捡\"土豆”（多金属结核），里面全是稀有金属。","演讲评分标准#演讲评分标准":"演讲要聚焦于申请人选择的五个主题之一——智能工业化 要展示多角度的思考，并解决了现实问题或未来挑战 演讲应该结构合理，有明确的开头、主体和结论 展示对内容的深入理解，并简化复杂的主题 幻灯片无误，与内容一致，并有效支持演讲 表达要流畅清晰，避免多余叹词和模糊不清的表达 演讲过程中，申请人要有效地与评委进行眼神接触并互动 在演讲中要提供创造性或独特的观点 要清晰阐述自己为何适合RB项目，将个人目标与项目目标对齐","用形容词进行针对性调整#用\u0026quot;形容词\u0026quot;进行针对性调整":"","示例问题#示例问题":"","稿子#稿子":"","第一类研究计划与学术愿景-research-proposal--vision#第一类：研究计划与学术愿景 (Research Proposal \u0026amp; Vision)":"核心逻辑： 考察你的 RP 是否经过深思熟虑，是否有现实意义，以及你的思维深度（Critical Thinking）。\n宏观视角类：\n你未来想研究这个方向要达到什么目的？ RP 的应用场景是什么？ 为什么工业 4.0 一直没有得到全面的推广？（考察对行业痛点的理解） RP 能给计划的研究领域（智能工业化）带来什么提升？ RP 的具体创新点在哪里？ 抽象思维/假设类（考察应变）：\n假如以后没人生小孩，全是老人，没钱发养老金，你是决策者你要怎么办？ 研究方法是什么？","第一维度个人背景与动机-the-pivot--motivation#第一维度：个人背景与动机 (The \u0026ldquo;Pivot\u0026rdquo; \u0026amp; Motivation)":"核心考察： 你为什么\"弃医从工\"？你的逻辑能否自洽？\n[必问] Why did you switch from Biomedical Engineering to Robotics? Isn’t it a waste of your 4 years in BME? (你为什么转行？不觉得浪费吗？) [进阶] Why didn’t you choose Medical Robotics (e.g., Da Vinci)? That seems like a more natural fit for your background. (为什么不选医疗机器人？那不是更顺理成章吗？) Why HKUST(GZ) Redbird? Why not a traditional CS Master’s at ZJU or HKUST Clear Water Bay? (为什么是红鸟，不是浙大或清水湾的传统 CS？) You have a high GPA (3.97). Why apply for an MPhil instead of a PhD directly? (绩点这么高，为什么不直接申博？) What is the biggest “gap” you feel you have compared to a pure CS student, and how do you plan to fill it? (你觉得自己比纯 CS 学生缺什么？怎么补？) How did your “Internet+” competition experience influence your decision to join Redbird? (那次\"互联网+“比赛怎么影响了你来红鸟的决定？) If I force you to change your topic from “Smart Industrialization” to “Sustainable Living”, what would you research? (参考面经：如果我强迫你换个主题，比如可持续生活，你会做什么？)","第三类个人动机与红鸟契合度-motivation--fit#第三类：个人动机与红鸟契合度 (Motivation \u0026amp; Fit)":"核心逻辑： 考察你\"为什么来这里\"以及\"能不能适应红鸟的模式（Project-based Learning）\"。\n转专业/择校类：\n[高频] 为什么转专业？（BME -\u003e Robotics） 为什么不保研（Baoyan）？ 你还申请了哪些学校？ 红鸟模式类：\n[高频] 假设要研究这个项目，你想要什么样的队友？（考察对跨学科的理解） 是否准备创业？（考察 Maker 精神） 如果你来到这个学校，我要你换个研究方向你愿意吗？为什么？ 你对现在的方向感兴趣，还是你想换的另一个方向？ 如果让你选择，你会深入学习本领域知识，还是去学习其他领域知识？（Deep vs. Broad）","第三维度本科毕设深挖-social-navigation--thesis#第三维度：本科毕设深挖 (Social Navigation \u0026amp; Thesis)":"核心考察： 技术细节，验证你是否亲自做了项目\n[必问] Explain “Prioritized Experience Replay (PER)” in one sentence to a non-expert. (用一句话给外行解释 PER。) How exactly do you define “Risk” in your Risk Awareness Module? Is it a hard threshold or a gradient? (风险怎么定义的？是硬阈值还是梯度？) Why did you choose Falcon as your baseline? Are there newer SOTA methods in 2024? (为什么选 Falcon 做基线？没有更新的方法了吗？) You mentioned the “Brain-Cerebellum” architecture. How do you handle the latency difference between the LLM (Brain) and the Motor Control (Cerebellum)? (大脑和小脑的频率不同，延迟怎么解决？) In your simulation (Habitat), how did you model human behavior? Are they static or dynamic? (仿真里的人是静止的还是动态的？用的什么模型？) What is the “Action Space” of your robot in the RL training? Continuous or Discrete? (动作空间是连续的还是离散的？) How do you balance the trade-off between “Safety” (Risk Module) and “Efficiency” (getting to the goal fast)? (安全和效率冲突了怎么办？)","第二类技术细节与硬实力-technical-deep-dive#第二类：技术细节与硬实力 (Technical Deep Dive)":"核心逻辑： 如果碰巧遇到懂行的老师，或者老师对你 PPT 里的某张图感兴趣，会抓住细节\"拷打”，确认项目是你自己做的。\n原理阐释类：\n通俗解释一下 CLLLC（或你的核心算法，如 PER/LOVON） High power 与 high power density 的区别，怎么实现？ 你的变换器怎么实现的 low ripple（低纹波）？ PPT 上这张图是什么意思？能具体讲讲结果吗？ 工程逻辑类：\n你的参数是在仿真里枚举就行了吗？设计逻辑是什么？ 你觉得你的项目还有什么地方可以改进？能靠自己完成还是需要团队？","第二维度ppt-核心叙事-smart-industrialization#第二维度：PPT 核心叙事 (Smart Industrialization)":"核心考察： 你对你选的主题（Topic）是否有真正的洞察？\nWhy do you think “Smart Industrialization” is the best entry point for Embodied AI, rather than Home Service? (为什么觉得工业是具身智能最好的切入点，而不是家庭服务？) Factories are structured. Why do we need “Social Navigation” in a factory? Workers wear uniforms and follow lanes. (工厂是很结构化的，工人穿制服走通道，真的需要\"社交导航\"这么复杂的东西吗？) You mentioned the “Lighthouse Factory” productivity boost. Do you think robots will replace human workers? Is that ethical? (机器人替代工人，这符合伦理吗？) What is the biggest bottleneck for “Industry 4.0” deployment right now? (工业 4.0 现在最大的落地瓶颈是什么？不要只说技术，也要谈谈成本/信任。) Your solution seems expensive (Unitree Go2 + LiDAR + Chips). How can small factories afford this? (你的方案成本太高，小工厂怎么用？)","第五类压力测试与行政问题-pressure--admin#第五类：压力测试与行政问题 (Pressure \u0026amp; Admin)":"核心逻辑： 考察抗压能力，或者单纯是老师没话找话/核实信息。\n压力面：\n你的研究计划好像港科广没有这方面的老师啊？ 你手里拿着的是小抄吗？ （英文提问）这个问题你能靠自己完成吗？ 行政类：\n联系意向导师了吗？意向导师是谁？","第五维度过往项目-archaeology--signal#第五维度：过往项目 (Archaeology \u0026amp; Signal)":"核心考察： 技术栈的广度与迁移能力\nIn your Archaeology project, how did you align the visual data (YOLO) with the text data (LLM)? (考古项目里，视觉和文本怎么对齐的？) Why use Wavelet Convolutions instead of Transformers for signal processing? (信号处理为什么用小波不用 Transformer？) You mentioned “deploying efficient models”. Have you ever quantized a model? (你说部署高效模型，你做过量化吗？) Did you publish any papers from these projects? If not, why? (这些项目发论文了吗？没发是为什么？)","第六维度红鸟契合度与抽象压力面-the-redbird-style#第六维度：红鸟契合度与抽象压力面 (The \u0026ldquo;Redbird Style\u0026rdquo;)":"核心考察： 软实力、团队合作、以及应对\"抽象\"问题的能力\n[面经原题] If you want to build this project at Redbird, what kind of teammates do you need? (你想找什么样的队友？) [面经原题] If we have no professors specializing in “Social Navigation” here, what will you do? (如果这里没有做社交导航的老师，你怎么办？) Do you plan to start a startup? If yes, what is your product? If no, why did you mention the “Internet+” award? (你想创业吗？产品是什么？不想的话为什么提创赛？) [抽象题] Explain “Embodied AI” to an elderly person. (给老奶奶解释什么是具身智能。) [抽象题] If robots take over all logistics, what is the value of humans in the loop? (如果机器人接管了物流，人的价值在哪？) What is your “Plan B” if your Sim-to-Real transfer fails completely in the first year? (如果第一年 Sim-to-Real 彻底失败，你的 B 计划是什么？) How do you handle conflict in a team? Give an example from your “Internet+” competition. (举个你在创赛中处理团队冲突的例子。) [压力面] Your research plan sounds very ambitious for a 2-year MPhil. How can you finish all this (World Model + VLA + Hardware)? (两年读完这些是不是太贪心了？怎么可能做完？) What is the most creative thing you have ever done? (你做过最创造性的事是什么？) Which “Hub” (学域) do you think you belong to? (你觉得自己属于哪个域？) Describe a time you failed and what you learned. (描述一次失败经历。) [最后] In one sentence, why should we choose you over a student with a pure Robotics background? (用一句话概括，为什么选你而不选纯机器人背景的学生？)","第四类项目经历与软实力-experience--soft-skills#第四类：项目经历与软实力 (Experience \u0026amp; Soft Skills)":"核心逻辑： 考察你的过往经历是否真实，以及你在团队中的角色。\n项目经历中的合作与困难是什么？ 本科竞赛，你自己做了什么？ 本科的 TA（助教）工作经历？","第四维度硬件部署与-sim-to-real-the-crash--reality#第四维度：硬件部署与 Sim-to-Real (The \u0026ldquo;Crash\u0026rdquo; \u0026amp; Reality)":"核心考察： 动手能力，以及面对失败的态度\n[必问] You said the robot crashes into walls due to occlusion. Why didn’t you use a map (SLAM)? (你提到撞墙，为什么不用 SLAM 建图？) What sensors are on the Unitree Go2? Which one is the most critical for your algorithm? (Go2 上有哪些传感器？哪个对你最重要？) Have you encountered any “Reality Gap” issues other than occlusion? (除了遮挡，Sim-to-Real 还有什么坑？比如摩擦力？光照？) LOVON is an open-source project. What specific modifications did YOU make to it? (LOVON 是开源的，你具体改了哪里？) If the robot fails in a real factory, how do you ensure it doesn’t hurt anyone? (如果真在工厂里失控了，怎么保证不伤人？Fail-safe 机制是什么？)","红鸟面试准备#红鸟面试准备":"红鸟面试准备上面那则hk-master的文档有点太满了，所以放在这里。","群面#群面":"群面很糟糕，因为我是第一个发言的，住宅区不太好体现quintessencial 低空经济，只能更多地去说区位优势，并且没有cue队友（得分点的teamwork我觉得废了）。老师提问两个问题，第二个老师的问题是我们的作品是如何体现精髓典范这个关键词的（在我们已经在第一个问题回答了的前提下），通过这个可以判断前期他们根本没有关注（或者说他们自己也不理解这个关键词），自然也没法关注到我确实做了一些teamwork，而且我发言没有框架，串联头脑风暴和交易适应的也不好。\n我还是挺委屈的，交易前的头脑风暴阶段我搭了三个冗余模块（工作量相对单个积木那种来说非常大，模块大小超过掌心），其中一个被两名队友拿去当救生舱用成为他们的模块功劳了（我不好意思指出），一个被别组的舍友入室抢劫般以一个金条的价格交易出去了，一个被另一个队友交易出去了，没有办法成为我的工作成果（演示时没法拿起来给老师们看），而多出去Social拿到交易模块的人就可以举起来给老师看看其他组所做的。\n这有点让我回忆起ITP面试时候的场景了，貌似大差不差，最后倒在了二面校友面。","群面-1#群面":"在流程方面，上午10:00开始：头脑风暴，A+B，B是五大主题，A由AI随机生成一个形容词。","脑机接口-bci#脑机接口 (BCI)":"人话：“意念控制”。在大脑植入芯片或戴个头盔，捕获脑电波，让瘫痪的人能控制鼠标或机械臂（像马斯克的Neuralink）。","评分标准#评分标准":"说明：这一块有点冷冰冰（形式主义）了，有点像卡戴珊那种莫名其妙的东西必须得做。","远程手术#远程手术":"人话：“5G隔空开刀”。医生在北京操作机械臂，给新疆的病人做手术，延迟极低。","针对五大主题的应用#针对五大主题的应用":"","零碳建筑#零碳建筑":"人话：“自给自足的房子”。房子本身发电（光伏）= 房子消耗的电。不给地球排二氧化碳。","面经#面经":"12/12 Update","面经总结#面经总结":""},"title":"港科广红鸟面试准备"},"/blog/2025/how-to-write-paper/":{"data":{"顶会写作指南#顶会写作指南":"顶会写作指南今天来研读一下如何撰写第一篇AI顶会论文写作指南\n一个研究者可能有有趣的发现和实验结果，但不确定如何定义核心主题。大多数已发表论文的主要贡献恰好属于以下三类中的一种：\n洞察：你对已经存在的某些事情有了解释。\n性能：你可以做得更好。\n能力：你可以做以前做不到的事情。\n发现新现象和分享新想法比绩效提升更重要。"},"title":"顶会写作指南"},"/blog/2025/lovon-baseline/":{"data":{"ad-vat-an-asymmetric-dueling-mechanism-for-learning-and-understanding-visual-active-tracking#AD-VAT+: An asymmetric dueling mechanism for learning and understanding visual active tracking":"","ad-vat-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning#AD-VAT: End-to-end active object tracking and its real-world deployment via reinforcement learning":"","dimp-learning-discriminative-model-prediction-for-tracking#DIMP: Learning discriminative model prediction for tracking":"","evt-empowering-embodied-visual-tracking-with-visual-foundation-models-and-offline-rl#EVT: Empowering embodied visual tracking with visual foundation models and offline RL":"","lovon-相关-baseline-调研#LOVON 相关 Baseline 调研":"LOVON 相关 Baseline 调研本篇内容主要集中在针对 LOVON 论文中所对比的 paper 工作，他的仿真指标，一方面顺着前人的工作一路做下来思路比较直接也比较连贯，另一方面我还是觉得gym-unrealcv这个模拟仿真的引擎相对MatterPort3d还是小众了一点，也没有现成的博客文章去汇总有哪些工作用到了这个。","rspt-reconstruct-surroundings-and-predict-trajectory-for-generalizable-active-object-tracking#RSPT: reconstruct surroundings and predict trajectory for generalizable active object tracking":"","sarl-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning#SARL: End-to-end active object tracking and its real-world deployment via reinforcement learning":"","trakvla-embodied-visual-tracking-in-the-wild#TrakVLA: Embodied visual tracking in the wild":"PKU在2025年5月的工作，VLA对训练算力和时间的要求堪称恐怖，所以这里单纯参考一下思想\nEmbodied visual tracking enables an agent to follow a specific target in dynamic environments using only egocentric vision.\nThis task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics, 也就是遮挡和高动态性。\n研究方向 现有方法特点 局限性 具身视觉跟踪 分模型基（IBVS）、RL 基（AD-VAT、EVT [6]）、IL 基（Uni-NaVid） 误差累积；Uni-NaVid 依赖离散动作空间 具身导航 聚焦静态室内环境（如视觉 - 语言导航） 忽略真实世界动态性 VLA 模型 用于操纵/导航，基于预训练 VLM 扩展动作生成 推理效率低，仅在低动态环境验证 而现有的相关工作都是将Recognition和Trajectory Planning给decouple出来的，而these methods are limited to category-level tracking in relatively open areas, 看来大家都意识到了这个问题，而作者指出这是因为上面decoupling的两个模块会产生error accumulation——识别错误可能导致规划失效，反之亦然。\n因此要用unified framework统合起来，共用token encoding（deconding的时候再分为两个头，一个language modeling head解码识别任务的文本响应，一个anchor-based diffusion head生成航点轨迹应用于规划任务）","ts-towards-distraction-robust-active-visual-tracking#TS: Towards distraction-robust active visual tracking":""},"title":"LOVON 相关 Baseline 调研"},"/blog/2025/manim/":{"data":{"manim入门#manim入门":"manim入门Manim (Mathematical Animation Engine) 是由 3Blue1Brown (Grant Sanderson) 开发的 Python 库，用于制作高质量的数学动画。\n用它来复现和理解论文中的 Methodology 和数学推导是一个非常棒的想法，不仅能吃透公式背后的逻辑，还能产出极具展示价值的可视化素材。","环境配置#环境配置":"conda install -c conda-forge ffmpeg LaTeX环境MikTex我先前已经安好了，这里不再继续 然后\npip install manim 接下来安装Manim Sideview插件，这样一来就能在IDE的侧边栏预览效果了\n实际体验下来，gemini生成的并不优秀，不能用其作为理解数学公式的方法（也可能是我的提示词工程做的不好），可视化overview、pipeline倒是可以。\n具体我不打算深入语法这些，作为工具，仅把其中文文档放在这里。"},"title":"Manim 入门与数学动画制作"},"/blog/2025/paper-agent/":{"data":{"":"科研paper读着实在是不得劲。独立研究者有着自己的一套taste，正如杨振宁先生所说，这种taste是可以通过大量的喂养培养出来的——但是这有一点太厚积薄发了，看着zotero里面贫瘠的二十多篇论文，我打算做一个全文献检索的工具。\n首先想到的当然是ArXiv提供的官方库，但是养taste的话，顶会中稿+开源的才有用，普通的preprint属于垃圾食品，然而arXiv API 本身并没有一个专门的 strict 字段来筛选\"是否被会议录用\"，完全依赖于作者手动更新 metadata 中的 comment。","arxiv作为内容仓库#ArXiv（作为\u0026quot;内容仓库\u0026quot;）":"作用： 负责下载 优势： 一旦知道了 arXiv ID，用 Python 的 arxiv 库下载 PDF 或源码是最稳定、最合规的 解决痛点： Semantic Scholar 的 API 有时会限流或不直接提供 PDF 文件流","dblp作为权威判官#DBLP（作为\u0026quot;权威判官\u0026quot;）":"作用： 只负责提供**“真·录用名单”** 优势： DBLP 是计算机领域的**“户籍科”，它的数据是人工校对的**，只有真正被 CVPR 录用的论文才会出现在 conf/cvpr/2025 列表里 解决痛点： 解决了 arXiv 上作者**“自吹自擂\"或\"撒谎”**的问题（比如有的作者被拒稿了也敢写 Accepted）","semantic-scholar作为连接器#Semantic Scholar（作为\u0026quot;连接器\u0026quot;）":"作用： 负责把 DBLP 的会议论文标题，映射到 arXiv ID 优势： DBLP 的会议条目和 arXiv 条目通常是独立的（两条记录）。Semantic Scholar 构建了巨大的图谱，它能识别出\"这篇 CVPR 论文其实就是那篇 arXiv 预印本\"，并提供 externalIds 字段 解决痛点： 解决了 DBLP 不直接提供 PDF 下载链接，以及 DBLP 和 arXiv 割裂的问题","各顶会的处理方法#各顶会的处理方法":"这里还是继续完善离线方法，前面CVF的方法只适用于3个：\nECCV： 虽然不属于 CVF，但 ECVA 的网站结构和 CVF 惊人的相似 ICLR 和 CoRL： 最优雅的方法不是爬网页，而是使用 OpenReview 的官方 Python 库 ICRA, IROS（机器人双雄）： 是最麻烦的两个。因为它们版权属于 IEEE；IEEE Xplore 有很强的反爬，且下载 PDF 需要学校 IP 验证 NeurIPS \u0026 RSS： 非常良心，历史归档都在静态网页上，HTML 结构十年不变 所以对于双雄，只能用DBLP的方法，然后用Arxiv库交叉验证。\n根据经验，机器人领域（Robotics）的 ArXiv 覆盖率大约在 60% - 80%。剩下 20% 的文章，作者可能根本没传 ArXiv，或者传了但是标题改得面目全非（比如从 “A Fast Method…” 改成了 “FastMethod:…\"）。\n计算机视觉顶会有严格的**“奇偶年份”**规律？！这里修正了我的爬取ICCV和ECCV的方法","存在的问题#存在的问题":"DBLP 的标题有时候会把句号放在最后，或者包含特殊字符。Semantic Scholar 的搜索能力很强，通常能模糊匹配，但偶尔也会因为特殊符号对不上。代码里做一点简单的 strip() 清洗很有必要，但这不是最大的问题。\nSemantic Scholar 和 DBLP 都有频率限制。如果名单有几千篇，一定要加 time.sleep(1) 或者使用 API Key（Semantic Scholar 申请 Key 后并发度更高），但是这导致了一个非常大的问题：调用Semantic Scholar太久了","方案一dblp--semantic-scholar--arxiv#方案一：DBLP + Semantic Scholar + ArXiv":"所以我想到了下面这样的产品流：","方案二直接爬取-cvf#方案二：直接爬取 CVF":"所以接下来又想了第二条路：直接爬取 CVF (Computer Vision Foundation) 的官方 Open Access 仓库。\nCVPR、ICCV、WACV 的所有论文（包括 Supplemental Material）都全量、免费托管在这里，它就是简单的静态 HTML 页面，没有复杂的反爬机制，没有 API 限流，直接 requests 请求一次就能拿到几千篇论文的列表，通过暴力爬虫直接访问 CVPR 2022 的\"所有论文\"页面，瞬间解析出所有论文的标题和 PDF 下载链接，并保存为 CSV 文件。\n然而没有投过paper的本科生被这个数量吓晕了：\nCVPR 2020: ~1,467 篇 CVPR 2021: ~1,660 篇 CVPR 2022: 2,074 篇 CVPR 2023: ~2,359 篇 CVPR 2024: ~2,719 篇 实在是太疯狂了。如果你一天读 5 篇，读完这一年的会，明年的会都开完了。"},"title":"全文献检索工具方案设计"},"/blog/2025/social-nav/":{"data":{"":"起因是在小红书上刷到了这一篇2025年11月的新文章\n结果却搜到了 [ICRA 2025] From Cognition to Precognition: A Future-Aware Framework for Social Navigation，于是误闯天家到了 Awesome Robot Social Navigation 的领域。","1-从避障到避人再到不扰人#1. 从\u0026quot;避障\u0026quot;到\u0026quot;避人\u0026quot;，再到\u0026quot;不扰人\u0026quot;":"像 SACSoN 这样的工作，其目标已经超越了基础的安全避障，而是追求更高级的社交合规性，希望机器人的存在和行为尽可能不改变人类的自然行为。","1-基础模型在社交导航中的应用#1. 基础模型在社交导航中的应用":"相关论文： VLM-Social-Nav, OLiVia-Nav, Social-LLaVA, CoNVOI, BehAV\n轻量化部署： 研究如何将大型**视觉语言模型（VLM）和大语言模型（LLM）**蒸馏或微调到适合机器人实时部署的规模 多模态融合： 探索视觉、语言、传感器数据的深度融合，提升对复杂社交场景的理解 在线终身学习： 开发能够持续适应新场景和人类行为的在线学习框架","1-融合基础模型#1. 融合基础模型":"这是一个明显的趋势。探索如何利用大型语言模型（LLM）和视觉语言模型（VLM），让机器人能够理解和遵从复杂、抽象的社会规则（例如，**“在拥挤处耐心跟随”**而不仅仅是\"避开人群”），或者更好地解读人类的行为意图。","2-学习与规划的关键预测与上下文理解#2. 学习与规划的关键：预测与上下文理解":"许多研究致力于让机器人更好地预测未来（如行人轨迹）和理解环境上下文（如社交规则）。Exploiting Proximity-Aware Tasks 和 Social NCE 都是通过不同的方式让模型内化对潜在危险和社交规范的理解。","2-提升仿真环境的真实性#2. 提升仿真环境的真实性":"开发更先进的仿真平台（如持续更新的 Arena 系列），模拟更复杂的人类行为（如突然驻足、群体交谈、协作避让），这对于在低成本前提下验证算法的鲁棒性至关重要。","2-轨迹预测与场景理解#2. 轨迹预测与场景理解":"相关论文： Social LSTM, STGAT, From Cognition to Precognition, Following the Human Thread\n实时轨迹预测： 提升对人类未来移动路径的预测精度和实时性 群体行为建模： 研究多智能体协同、群体动态（如群体分裂与合并）的建模方法 上下文理解： 开发更强大的场景表征能力，理解复杂的社会规则和社交动态","3-基础模型与终身学习成为新风向#3. 基础模型与终身学习成为新风向":"OLiVia-Nav 和 GOAT 等论文清晰地展示了如何利用视觉语言模型（VLM）的先验知识进行社交推理，并强调通过终身学习使机器人能够适应不断变化的环境和新遇到的社交场景。","3-增强算法的可解释性与信任度#3. 增强算法的可解释性与信任度":"研究如何让机器人的导航决策过程对人类而言更透明、更容易理解。例如，生成机器人为何选择某条路径的**“因果解释”**，这能极大地增强人类对机器人的信任，促进人机共处。","3-强化学习与混合方法#3. 强化学习与混合方法":"相关论文： SACSoN, SELFI, DR-MPC, Hybrid Approaches\n奖励函数设计： 探索如何将社交规范、舒适度等抽象概念量化为强化学习的奖励信号 混合方法： 结合模型预测控制（MPC）、采样规划等传统方法与深度强化学习 样本效率： 提升强化学习在社交导航任务中的样本效率，减少真实世界训练成本","4-可解释性与信任#4. 可解释性与信任":"相关论文： Generating Causal Explanations, Explainability and Trust\n因果解释： 生成机器人导航决策的因果解释，增强人类对机器人的信任 透明度： 研究如何让机器人的决策过程对人类更透明、更容易理解","4-对仿真数据与评估的持续投入#4. 对仿真、数据与评估的持续投入":"高质量的仿真平台（Habitat 3.0, GRUtopia）、大规模数据集（RoboSense）和专门的评估基准（SocialNav-SUB, EmbodiedEval, SocialEval）是推动领域发展的关键基础设施，这些工作为训练、测试和公平比较不同算法提供了坚实基础。","4-深化人机交互研究#4. 深化人机交互研究":"关注机器人的导航行为如何影响人类的感受和效率。通过用户研究，量化什么是让人感到**“舒适”、“自然”**的机器人行为，并将这些发现转化为算法设计的指导原则。","5-应对极端与复杂场景#5. 应对极端与复杂场景":"专注于解决更具挑战性的场景，例如重度遮挡（在人群中\"看不见\"部分行人）、对**“不可预测\"行人的识别与避让**，以及在密集人群中如何寻找安全路径。","5-数据集与评估基准#5. 数据集与评估基准":"相关论文： SCAND, MuSoHu, SocNavBench, Arena系列, SocialNav-SUB\n真实世界数据集： 构建大规模、多模态的真实人机交互数据集 仿真平台： 开发更真实的仿真环境（如Arena 4.0, Habitat 3.0），支持复杂人类行为模拟 评估指标： 设计更全面的评估体系，包括人类赋权、个人空间合规性（PSC）、碰撞率（H-Coll）等","6-用户研究与伦理#6. 用户研究与伦理":"相关论文： Social Momentum, How Do Robot Experts Measure Success, Overlapping Social Navigation Principles\n用户研究： 通过用户研究量化什么是\"舒适”、“自然\"的机器人行为 伦理考量： 关注算法的公平性、透明度、隐私保护及对人类舒适度的影响","huggingface-上的热门研究#HuggingFace 上的热门研究":"在 huggingface 上按 trending 搜索 social navigation 的结果如下：\n论文标题 核心工作摘要 SACSoN 通过最小化机器人对行人行为的**“反事实扰动”，学习一种不打扰人类的导航策略**。其关键在于使用大量真实人机交互数据进行训练。 Exploiting Proximity-Aware Tasks 提出邻近感知任务，通过让策略理解即时和未来的碰撞危险，为强化学习导航策略注入常识性社交行为。 SELFI 提出一种在线自学习方法，在预训练策略的基础上，利用在线模型无关的强化学习进行快速微调，使机器人能根据实际经验持续改进社交导航行为。 SocialNav-SUB 引入了首个用于评估视觉语言模型（VLM）在社交导航场景中理解能力的基准，发现当前 VLM 在空间、时空和社交推理方面仍有明显不足。 OLiVia-Nav 将视觉语言模型与在线终身学习框架结合，通过独特的蒸馏方法让轻量级 VLM 直接理解社交和环境上下文，并规划符合社交规范的轨迹。 Habitat 3.0 推出了一个支持人、虚拟化身和机器人协同的模拟平台，用于研究社交导航等协作任务，并提供了人类在环的基础设施。 GOAT 提出了一个通用导航系统，能够处理多模态目标，并通过持续构建实例感知的语义记忆，实现终身学习和跨平台部署。 GRUtopia 构建了一个大规模的模拟交互式3D社会，包含多样化的场景和由 LLM 驱动的虚拟角色，用于支持社交移动导航等具身AI任务的训练与评估。 RoboSense 提出了一个大规模的以自我为中心的多模态数据集，专注于拥挤和非结构化环境中的感知与导航，为近场场景理解提供丰富标注。 Social NCE 通过对比学习来提升运动表示的社交感知能力，显式地建模危险负样本，以此降低轨迹预测和行为克隆中的碰撞率。 DriVLMe 探索了基于视频语言模型的自动驾驶智能体，通过模拟环境和真实人类对话进行训练，旨在实现与人类的自然有效沟通。 EPO 提出显式策略优化方法，利用多轮强化学习和自我博弈来提升大语言模型在社交对话等任务中的战略推理能力。 EmbodiedEval 提出了一个统一的、交互式的基准，用于全面评估多模态大模型在具身任务（如导航、社交交互）中的能力。 SocialEval 提出了一个评估大语言模型社交智能的双语基准，通过叙事脚本从结果和过程两个维度评估模型的人际交往能力。","icra-2025-研讨会#ICRA 2025 研讨会":"除了竞争激烈的比赛，ICRA 的\"Advances in Social Robot Navigation\"研讨会则是深入了解该领域学术研究和前沿发展的绝佳平台。\n活动形式： 这是一个学术研讨会，会邀请领域内的专家进行讲座和专题讨论。同时，它也主办 Arena 4.0 挑战赛，旨在为不同的社交导航策略建立基准和评测体系。\n核心议题： 研讨会关注如何使机器人的导航行为更易于理解、更符合社交场景。探讨的技术方向包括运动任务规划、基础模型的应用、人机交互策略等。","robosense-挑战赛#RoboSense 挑战赛":"RoboSense挑战赛 是 IROS 2025 的官方认证竞赛，它设置了专门的社交导航赛道，旨在解决机器人在真实动态环境中的导航问题。\n任务目标： 参赛者需要开发一个基于 RGB-D 输入的移动机器人导航模型。该模型的核心任务是让机器人在不影响周围人类行为的前提下完成导航，并使其行为符合人类的社会规范，例如主动避让、保持合适的社交距离等。\n挑战与评测： 除了衡量导航成功率和路径效率，比赛还特别引入了个人空间合规性（PSC）和人机碰撞次数（H-Coll）等指标，专门用于量化机器人行为的“社交友好度”。\n前沿技术： 该赛道推荐的基线模型（Baseline）是 Falcon，这是一个由港科广和港科大联合提出的新算法，它通过将轨迹预测算法融入强化学习框架，让机器人能够预测行人未来的移动路径，从而实现更超前、更安全的规划。","什么是-social-navigation#什么是 Social Navigation？":"Social Navigation（社会导航） 的核心思想是 “以人为本”。它要求机器人不仅仅把人类当作需要避开的障碍物，而是能够理解并尊重人类的社会规范与个人空间，最终实现自然、和谐、无感知压迫的共同空间使用。例如，在走廊中与人迎面相遇时，机器人会像人一样靠右行驶；当需要穿过一群人时，它会寻找合适的时机和路径，而不是生硬地\"切开\"人群。","基于awesome系列的具体研究方向#基于Awesome系列的具体研究方向":"根据Awesome系列的详细梳理，以下是从方法、数据集、评估等多个维度总结的具体研究方向：","学术社区与行业洞察#学术社区与行业洞察":"然后去学术社区（迫真）上搜索了一下，这里 seven17 这位大佬也在2025年11月16-17给出了自己作为人形公司 SLAM 面试官对业界人形机器人在研究的算法的一些经验，非常有参考意义。\n有一说一小红书真的比很多像是CSDN之类的更好的学术交流平台\n我就很赞同这里在小红书的某个 Ask Me Anything 上看到的港科广的梁老师的话：","我的研究计划#我的研究计划":"我打算接下来的核心往 Social Navigation 上面靠，这里很符合以人为本的设计特点，而 LOVON 也确实面临这一困境。也如梁老师所言，这是个容易入门具身的领域。可惜这个比赛在这个时候已经结束了，下面计划的第一步是：\n研读 Falcon 这个 baseline（也就是上面提到的 ICRA 2025 中稿文章） 使用 Robosense 提供的 GitHub 代码和数据集去复现基线 参考排行榜的改进去思考参赛者解决的问题集中在哪里，又是如何进行的","技术对比social-navigation-vs-lovon-vs-vln#技术对比：Social Navigation vs LOVON vs VLN":"特性维度 Social Navigation (社会导航) LOVON (腿部开放词汇物体导航) VLN (视觉语言导航) 核心目标 安全、舒适、符合社会规范地在人类共享空间中导航 在开放世界中，根据物体名称，自主搜索并导航到指定物体 根据自然语言指令，在环境中执行导航任务 (如\"去厨房拿杯水\") 环境特点 动态、拥挤的人类环境，充满不确定性 非结构化的开放环境，地形复杂，目标物体可能被遮挡或距离遥远 通常基于仿真器（如Habitat, AI2-THOR），环境可以是静态的，也引入动态人类 关键输入 人类的位置、运动轨迹、群体行为、社会规范 目标物体的文本名称 (如 “chair”)、机器人视觉传感器数据 详尽的自然语言指令、机器人视觉传感器数据 技术侧重点 行人轨迹预测、社交力模型、强化学习策略、舒适度与安全性评估 开放词汇目标检测、大语言模型任务分解、腿部机器人运动控制、抗运动模糊 视觉-语言对齐、指令理解、跨模态推理、路径规划 典型输出/动作 避让、保持社交距离、绕行、调整速度、非语言沟通 朝向目标物体的运动控制命令 (如速度、方向)，处理复杂地形 导航动作 (如\"左转\"、“前进1米”、“停止”) 核心挑战 对人类意图的预测、复杂社会规则的建模与量化、安全性、舒适感 长时序任务规划、动态模糊下的稳定感知、复杂地形下的稳定移动、开放词汇识别泛化能力 指令与环境的关联、未知环境泛化、长指令理解、跨模态表示学习","未来方向#未来方向":"而在 awesome系列 里，我们可以看到以下几个重要方向：","核心挑战与思考#核心挑战与思考":"社会导航的终极目标是实现安全、舒适、符合社会规范的人机共存与协作。它关注的是导航行为的**“社交智能\"和\"礼仪”。相比之下，许多视觉语言导航（VLN）或其变体（如 LOVON）更侧重于理解指令、识别物体或地点，并完成具身的导航任务，其核心是“完成任务\"的准确性**。\n最大的难点在于，它需要让机器人理解并量化人类社会中那些不言自明、动态变化的社交潜规则。例如，如何定义并计算**“个人空间”？如何判断什么样的路径是“优雅\"而非\"冒犯\"的**？这与开放词汇任务中要求模型识别未曾见过的物体类别（如 LOVON）相比，是不同类型和层次的挑战。开放词汇扩展了机器人的\"知识面”，而社会导航则是在塑造机器人的\"情商\"和行为方式。\n比如说 Track2 的工作，核心任务是让机器人学会在充满动态行人的室内环境中（如办公楼、商场），实现安全、高效且符合社会规范的导航。不仅要求机器人成功到达目的地（成功率 SR），还要求其行为**“像个有礼貌的人”，比如主动保持舒适的社交距离（个人空间合规性 PSC）、避免碰撞（人类碰撞率 H-Coll），并规划出高效的路径（路径长度加权成功率 SPL）。赛事提供的基线模型是基于 Falcon 框架，它通过融入对行人未来轨迹的预测**，来让机器人实现更具前瞻性的导航决策。","相关竞赛与研讨会#相关竞赛与研讨会":"活动名称 主要关联会议 活动形式 核心侧重点 RoboSense机器感知挑战赛 IROS 2025 (官方认证竞赛) 竞赛 在动态人群环境中，使机器人的导航行为符合人类的社会规范。 Advances in Social Robot Navigation研讨会 ICRA 2025 研讨会 探讨社交机器人导航在规划、人机交互等领域的最新进展，并包含基准测试挑战。","相关资源#相关资源":"Resource Link Description GitHub Repository https://github.com/robosense2025/track2 Baseline code and setup instructions Dataset HuggingFace Dataset Dataset with training and test splits Baseline Model Pre-Trained Model Weights of the baseline model Registration Google Form (Closed on August 15th) Team registration for the challenge Evaluation Server EvalAI Platform Online evaluation platform","研究方向与未来工作规划#研究方向与未来工作规划":"基于 Awesome Robot Social Navigation 的梳理，当前研究主要集中在以下几个方向：\n研究方向 具体未来工作规划 来源论文 模型泛化与适应性 开发轻量化VLM便于机器人部署；探索多模态融合（视觉、语言、传感器）；研究在线/终身学习框架以适应新场景；提升对动态场景和长时序任务的理解与规划能力。 VLM-Social-Nav, OLiVia-Nav, Following the Human Thread 场景理解与交互 研究人类轨迹预测与社交动态的实时、精准推断；探索多智能体协同与群体行为建模；开发更强大的场景表征与上下文理解能力，以处理复杂的社会规则。 Following the Human Thread, DiPCAN 评估体系与伦理 建立更全面的评估指标（如引入\"人类赋权\"概念）；设计标准化基准测试与仿真环境；关注算法的公平性、透明度、隐私保护及人类舒适度等社会伦理影响。 In Search of a Lost Metric, Frontiers Research Topic 注： benchmark一般貌似都要自己提出一个，这样能增大工作量说是，像TrackVLA就是这样提出了一个EVTbench开源使用","研究计划建议#研究计划建议":"基于以上分析，可以从以下几个方面思考研究计划：\n关注新兴的评估范式： 像“人类赋权”这类新指标方兴未艾，如何量化、验证并将其有效融入强化学习奖励函数或模型预测控制的代价函数中，是一个很有潜力的方向。\n探索基础模型的高效应用： 研究如何蒸馏或微调大型VLM/LMM，在保持其社交推理能力的同时，满足机器人平台对低延迟和低功耗的严苛要求。\n致力于弥合仿真与现实差距： 开发更好的**领域自适应（Domain Adaptation）技术或元学习（Meta-Learning）**策略，让模型在离开仿真环境后能快速适应真实世界的复杂性。\n挑战更复杂的社交场景： 可以专注于研究机器人在密集人群、群组交互（如穿越一个正在交谈的群体）或长程、多目标导航任务中的表现，这些场景对现有技术提出了更高要求。\n构建自己的评估基准： 参考TrackVLA提出EVTbench的做法，开发针对特定场景或问题的标准化评估基准，这不仅能增加研究工作量，还能为领域提供有价值的工具。","研究趋势分析#研究趋势分析":"从这些论文可以看出，社交导航领域的研究呈现出一些明显的趋势和重点方向："},"title":"社会意识的导航模型"},"/blog/2025/social-navigation-idea/":{"data":{"":"","after#After":"读完这 7 篇论文后，可以清晰地看到当前社会导航领域的图景。目前正在见证一场**“分裂”**：一边在优化几何强化学习范式（从深度传感器和 PPO 中榨取性能），另一边则倡导结构化/混合方法（地图、规则和 VLM）来绕过纯强化学习的低效。\n这个文献图谱的中心节点是 Falcon，它通过将范式从反应式避障转向预测式轨迹规划，建立了一个强基线。\n**Falcon（基础）**提出了\"未来感知\"框架。它不再仅仅对当前人类位置做出反应，而是使用辅助任务显式预测人类轨迹（未来 $H$ 步），并惩罚机器人阻塞这些未来路径。关键指标是在 Social-HM3D 上达到了 55% 的成功率。但它的弱点是依赖\"盲目\"的端到端强化学习，需要大量训练（约 2400 GPU 小时），并且将人类视为简单的移动障碍物，缺乏语义上下文。\n有三篇论文接受 Falcon 的架构，但认为其训练方法有缺陷。它们旨在解决样本效率和奖励稀疏性问题。\n数据效率（“好学生\"方法）：PER-Falcon（Rank 1）发现 Falcon 通过平等对待所有训练回合而浪费数据。它引入了正样本回放（PER）机制，缓存\"高价值\"回合（成功导航）并定期回放给策略网络。结果是在成功率上比基线提升了约 12%，证明了课程/数据质量比模型大小更重要。\n信号密度（“直觉\"方法）：主动风险感知（Rank 2）认为 Falcon 的碰撞惩罚太\"稀疏”（只有在撞到人时才受到惩罚）。它添加了一个模块来基于距离预测连续的风险分数，让机器人在碰撞发生前很久就能\"感知危险”。结果获得了第 2 名，证明了密集监督有助于强化学习收敛。\n超参数调优（“暴力\"方法）：混合参数优化认为 Falcon 的奖励权重（平衡效率与社会合规性）不是最优的。它使用网格搜索和耦合调优来找到参数的\"帕累托前沿”。结果仅通过调参就实现了 15% 的成功率提升，凸显了强化学习基线的脆弱性。\n有两篇论文挑战端到端强化学习的主导地位，认为结构和心理学是比试错更好的老师。\n“零样本\"反叛：SocialNav-Map 是一个关键的挑战者。它认为强化学习\"不透明\"且\"难以泛化”。它实时构建动态占据地图，预测人类轨迹（使用历史+朝向）并将其\"绘制\"到地图上作为临时障碍物，然后使用经典路径规划器（快速行进方法）。洞察是它在没有训练的情况下击败了基于强化学习的 Falcon（后者需要 2396 GPU 小时训练），这表明显式世界建模可能优于隐式强化学习记忆。\n“以人为中心\"的混合：RLSLM 认为机器人不应该只是\"猜测\"社会规范。它将从心理学实验推导出的社会运动模型（SLM）直接整合到奖励函数中，创建了一个\"非对称舒适场”（人类讨厌从前方接近，而不是从后方）。验证是通过 VR 让人类评价机器人，证明它比标准基于规则的方法\"更有礼貌\"。\n最后两篇论文虽然不严格属于\"社会导航\"论文，但它们提供了解决社会导航中\"语义鸿沟\"的\"新锤子\"（技术）。\nSeeGround 解决了 3D 视觉定位（通过文本找到物体）而无需 3D 训练数据。技术是使用\"视角适应模块\"（模拟相机看向物体）从 3D 场景渲染 2D 图像，并将其输入到 2D VLM。相关性在于它证明了如果正确格式化数据（渲染图像+空间提示），可以使用冻结的 2D VLM 来理解 3D 空间。\nWhere to Fuse 是一篇综述，分类了如何将 VLM/LLM 知识注入导航。框架将融合分为感知（识别物体）、预测（猜测关系）和规划（边界选择）。相关性在于它明确将\"社会交互导航\"称为未来，指出当前方法\"将人类简化为移动障碍物\"。\n基于这些论文，可以绘制出研究路线图。\nNovelty Tree（技术演进）：\nLevel 1：几何反应（过去） → ORCA、Social Force Level 2：几何预测（Falcon 时代） → Falcon、PER-Falcon（预测轨迹） Level 3：结构化混合（当前 SOTA） → SocialNav-Map（显式映射动态风险） Level 4：语义社会智能（空白） → 理解上下文（例如：“那两个人正在交谈，不要从中间走过”，或\"那个人在赶路，让路\"） Challenge-Insight Tree（工具箱）：\n社会导航中的挑战 当前解决方案（“旧\"方法） 提出的洞察（使用文献） 数据效率 训练 1000 万步（Falcon） 回放缓冲区：优先\"正样本回合” 奖励稀疏性 稀疏碰撞惩罚 密集风险：预测连续\"风险分数\" 泛化性 在新地图上微调 零样本映射：使用动态占据地图 社会规范 希望强化学习\"学会\"它们 显式建模：注入心理学规则（SLM） 语义盲区 仅深度输入（看不到\"活动\"） VLM 注入：使用 SeeGround 的\"视角渲染\"来分类社会情境 文献清楚地表明，几何问题已经解决（SocialNav-Map 证明了可以零样本完成）。下一个前沿是语义。\n机会：“语义社会地图”\n空白：Falcon 和 SocialNav-Map 将人类视为动态圆柱体。RLSLM 将它们视为磁场。它们都不知道人类在做什么。\n新锤子：SeeGround 证明了可以使用 VLM 来\"看\"特定的 3D 坐标并理解它。\n提出的想法：“VLM 驱动的社会可供性地图”\n感知：使用 VLM（如 SeeGround）分析来自 RGB 相机的人类裁剪图像。分类它们的状态：“交互中”、“等待中”、“看手机”、“匆忙”。 映射：不仅仅是\"占据地图\"（如 SocialNav-Map），而是构建\"社会规范地图\"。 示例：如果两个人正在\"交互\"，在它们之间创建\"禁止通行区\"。 示例：如果一个人正在\"看手机\"，扩大其风险半径（他们分心了）。 规划：在这个新的语义地图上使用 SocialNav-Map 规划器（FMM）。 这结合了 SocialNav-Map 的结构和 SeeGround 的语义能力，解决了 Where to Fuse 中识别的局限性。\nGoogle Search 验证建议：你可以搜一下 “Foundation model for social navigation” 或 “Language-guided social navigation”，看看Level 4/5目前是否已经有人在用VLM/DepthAnything的Feature做SocialNav了。如果没有，这就是Blue Ocean。","literature-review#Literature Review":"在 Google Scholar 里检索引用了 Falcon 的工作，整理如下：\n序号 论文标题 中稿状态 主要功能 核心内容 1 Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding 已中稿 CVPR 2025 解决零样本开放词汇的 3D 视觉定位问题，即根据自然语言描述在 3D 场景中找到特定物体 提出 SeeGround 方法，无需针对 3D 数据进行专门训练。核心思想是将 3D 场景转化为 2D VLM 可理解的格式，通过渲染 3D 场景图像并使用视觉提示技术建立 2D 图像与 3D 空间信息的对应关系，利用预训练的 2D VLM 理解场景并定位物体 2 Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation IROS 2025 RoboSense Challenge (Track 4) 技术报告，Top-2 解决跨模态无人机导航中的图像检索问题，即根据自然语言描述从大规模数据库中检索对应的无人机视角图像 提出 CGRS 两阶段检索增强框架：第一阶段使用基线模型进行粗略排序；第二阶段利用 VLM 为候选图像生成详细描述，计算查询文本与生成描述的相似度进行精细重排序，显著提高检索精度 3 Zero-Shot 3D Visual Grounding from Vision-Language Models arXiv Preprint (arXiv:2505.22429) 零样本 3D 视觉定位 从作者和题目看，极大概率是 Seeground (CVPR 2025) 的预印本或其前身，内容与 Seeground 一致，探讨如何利用 VLM 实现零样本 3D 视觉定位 4 Learning to Navigate Socially Through Proactive Risk Perception IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告，第 2 名 解决社会导航问题，让机器人在人群密集的动态环境中安全、合乎社会规范地导航 基于 Falcon 模型改进，增加主动风险感知模块，能够预测周围行人的基于距离的碰撞风险分数，让机器人具备更强的空间感知能力，主动采取避障行为并保持合适的社交距离 5 Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration arXiv Preprint 解决多楼层环境下的零样本物体目标导航 提出 ASCENT 框架，结合多楼层空间抽象和基于 LLM 的由粗到细的边界探索，利用 LLM 的常识推理能力（如\"瑜伽垫可能在健身房，而健身房在楼下\"）指导机器人跨楼层搜索 6 SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation arXiv Preprint 零样本社会导航 提出 SocialNav-Map 框架，结合动态人类轨迹预测和占据栅格地图。不需要针对特定环境训练，使用两种互补方法预测人类轨迹（基于历史路径和基于朝向），将预测结果作为动态障碍物整合进地图，使机器人能预见人的移动并提前规划路径 7 View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs arXiv Preprint 零样本 3D 视觉定位 提出 VoG 方法，不同于直接把图像喂给模型，该方法将 3D 场景构建为多模态、多层级的场景图。VLM 被设计为主动代理，在图上进行遍历和推理，逐步搜索并定位目标物体，这种结构化方式降低了推理难度，提高了可解释性 8 RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms arXiv Preprint 社会导航，强调符合人类社会规范的舒适度 提出 RLSLM 混合强化学习框架，将心理学实验推导出的规则基社会运动模型整合到 RL 的奖励函数中，让 RL 智能体在学习导航策略时天生倾向于遵守人类的社交舒适区，实现规则可解释性与 RL 适应性的结合 9 Comfort-Aware Trajectory Optimization for Immersive Human-Robot Interaction 已发表于 IEEE Open Journal on Immersive Displays (2025) 针对沉浸式环境（如 VR）中的人机交互，优化机器人运动轨迹 提出轨迹预测与优化框架，专门针对舒适度和路径合理性进行优化。在 VR 环境中进行用户研究，证明该方法生成的轨迹比传统方法更自然、更让用户感到舒适 10 Where to Fuse in the VLM Era: A Survey on Integrating Knowledge into Object Goal Navigation Workshop Paper，发表于 HEAI Workshop 综述 探讨在物体目标导航任务中应该在\"哪里\"融合 VLM/LLM 的知识。借鉴自动驾驶的感知-预测-规划范式，将现有工作分类为在感知层融合、在预测层融合或在规划层融合，并分析各类优缺点 11 Layout-Robust LiDAR 3D Object Detection via Multi-Representation Fusion IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本 解决跨不同车辆平台的 LiDAR 3D 目标检测问题 针对不同车辆上 LiDAR 传感器布局（位置、数量、角度）不同导致模型泛化能力差的问题，提出统一表示框架，包含多视图融合模块（通过点-体素注意力机制学习统一视图不变表示）和运动引导的时空融合模块 12 Enhancing Multi-View Driving VLMs via Pseudo-Label Pretraining and Long-Tail Balancing IROS 2025 RoboSense Challenge (Track 1) 技术报告/预印本 提升视觉语言模型在自动驾驶场景中的理解能力（感知、预测、规划） 基于 InternVL3-8B 模型提出两阶段优化框架：第一阶段利用伪标签预训练，结合思维链推理，将多视角图像按固定序列拼接；第二阶段针对长尾数据进行平衡处理，结合官方数据与合成数据进行混合微调，通过模型集成提升鲁棒性 13 Robust 3D Object Detection under Sensor Placement Variability IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本 增强 3D 目标检测模型对传感器安装位置变化的鲁棒性 针对不同车型 LiDAR 安装位置差异大导致模型失效的问题，提出三种策略集成：时序增强（聚合连续 LiDAR 扫描帧以丰富几何信息）、混合位置训练（在训练中模拟多种传感器配置）、推理时增强。在 Track 5 基准测试中表现出色 14 Enhancing VLMs for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning IROS 2025 RoboSense Challenge (Track 1) 技术报告 解决 VLM 在自动驾驶中空间推理弱和多任务干扰的问题 提出系统解决方案，核心是任务特定的提示。Prompt Routing：根据问题类型（感知、预测、规划等）将问题路由到专门的 expert prompt。空间推理增强：显式定义多视图坐标系和领域约束（如\"后视摄像头的物体一定在车后\"），帮助 VLM 理解空间关系。在 Track 1 的 Phase-1 和 Phase-2 中均取得很高准确率（70%+） 15 Towards Socially Compliant Navigation: Hybrid Parameter Optimization for Falcon in Dynamic Environments IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告/预印本 优化社会导航模型 Falcon 的参数，使其更符合社会规范 针对 Falcon 模型在平衡\"任务效率\"和\"遵守社会规范\"之间的矛盾，提出混合参数优化策略，结合比例约束的参数耦合和网格搜索，解决奖励函数参数过多导致的维度爆炸问题，更有效地找到让机器人既跑得快又懂礼貌的参数组合 16 HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Cross-Modal Drone Navigation IROS 2025 RoboSense Challenge (Track 4) 技术报告/预印本 解决无人机跨模态导航中的定位匹配问题 针对无人机视角（俯视、广角）与文本描述之间的巨大差异，提出 HCCM 框架，核心在于分层和跨粒度。不仅做整体图像匹配，还将图像和文本分解为不同层级（如全局场景 vs. 局部地标），在不同粒度上进行对比学习和匹配，提高在大范围航拍图像中定位具体目标的准确性 17 Unsupervised Domain Adaptation for 3D Object Detection via Adversarial Learning IROS 2025 RoboSense Challenge (Track 5) 技术报告 解决跨平台 3D 目标检测的域适应问题 针对源域和目标域 LiDAR 配置不同导致的数据分布差异，采用无监督域适应方法，核心引入对抗学习。通过训练域判别器区分特征来自哪个平台，同时强制特征提取器欺骗判别器，提取平台无关特征，使模型能泛化到新车型上 18 Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling IROS 2025 RoboSense Challenge (Track 5) 获奖方案 高效的跨平台 3D 检测 基于强力的 PVRCNN++ 基线模型，使用两项关键技术弥补域差异：强数据增强（在数据层面模拟不同传感器噪声和几何变换）和伪标签（使用模型在未标注目标域数据上生成的置信度高的预测结果作为伪标签进行自我训练，逐步适应新环境） 19 Task Aware Prompt Routing and CoT Augmented Fine Tuning for Driving VQA IROS 2025 RoboSense Challenge (Track 1) 技术报告 提升自动驾驶 VLM 处理复杂问答（感知、预测、规划）的能力 提出任务感知提示路由：不使用通用提示，先判断问题属于哪类任务（如\"前方有车吗？“属感知，“它会左转吗？“属预测），然后路由到专门优化的 Prompt 模板。思维链增强微调：在微调过程中加入推理步骤，强迫模型在给出结论前先生成推理过程，显著提升复杂逻辑问题的回答准确率 20 Driving Robustly through Corruptions: Multi-Source LoRA Fine-Tuning of Driving VLMs for Multi-View Reasoning IROS 2025 RoboSense Challenge (Track 1) 技术报告 增强 VLM 对图像腐蚀/干扰的鲁棒性 针对雨雪雾、传感器噪声等恶劣视觉条件，采用 Multi-Source LoRA 微调策略。在训练时故意引入多种类型的图像腐蚀数据作为多源输入，通过轻量级 LoRA 模块让大模型快速适应这些低质量输入，保证在视觉条件退化时仍能安全推理 21 SegSy3D: Segmentation-Guided Self-Training and Model Synergy for Cross-Platform 3D Detection IROS 2025 RoboSense Challenge (Track 5) 技术报告 利用语义信息辅助跨平台 3D 检测 分割引导：认为仅仅做检测不够，利用点云的语义分割任务作为辅助，帮助模型更好地理解物体形状和背景，从而提升检测器的特征质量。模型协同：涉及多个模型（如分割模型和检测模型）之间的互助学习或集成，以克服单一模型的偏差 22 Towards Generalizable 3D Object Detection Across Sensor Placements IROS 2025 RoboSense Challenge (Track 5) 技术报告 解决 LiDAR 安装位置变化带来的检测失效问题 重点研究当 LiDAR 安装高度、俯仰角发生变化时点云分布的改变，提出通用检测框架，可能包含几何校正模块，或在特征空间进行视角对齐，确保无论雷达装在车顶还是车头，提取出的车辆特征是一致的 23 PlaceRecover: A Transformer-based Point Cloud Recovery Network with Implicit Neural Representations for Robust LiDAR Placement Adaptation IROS 2025 RoboSense Challenge (Track 5) 技术报告 通过重建/恢复点云来解决传感器位置差异问题 独特思路：不同于调整检测器，试图直接调整数据。PlaceRecover 是基于 Transformer 的网络，结合隐式神经表示，目标是将不同位置采集的畸变点云恢复/重构为标准视角下的点云，这样后续检测模型不需要修改，直接在恢复后的标准点云上运行即可 24 A Parameter-Efficient MoE Framework for Cross-Modal Drone Navigation IROS 2025 RoboSense Challenge (Track 4) 冠军方案 高效、高精度的无人机跨模态检索与导航 引入混合专家模型架构，MoE 允许模型拥有巨大参数量但推理计算量很小（每次只激活部分专家）。在无人机导航任务中，不同专家可能分别负责处理文本理解、视觉特征提取或地理空间推理。参数高效通常意味着使用 Adapter 或 LoRA 等技术，使模型在有限算力下快速适应新任务 25 Robust 3D Object Detection via Physical-Aware Augmentation and Class-Specific Model Ensembling IROS 2025 RoboSense Challenge (Track 5) 技术报告 通过物理感知增强和集成学习提升检测鲁棒性 物理感知增强：传统复制粘贴增强可能把车放在天上或穿墙，该方法设计符合物理规律的增强策略（如贴地、防碰撞），生成更逼真的训练样本。类别特定模型集成：针对不同类别（如车、人、骑行者）训练专门检测器，最后进行集成，利用不同模型在不同类别上的优势，最大化整体分数 除了上述文献，RoboSense 2025 Track 2 的两篇冠亚军方案也值得关注。它们的改进思路截然不同：一个改了模型架构，另一个改了训练策略。这种对比很有意思，展现了同一个问题可以从不同角度切入。\n亚军方案来自小米的工作，核心思路是**“预知不够，还需要风险评估”**。这个洞察很有意思。\nFalcon 虽然能预测人类未来的轨迹，但它对危险的感知是滞后的。Falcon 主要靠碰撞后的惩罚来学习，这导致智能体知道人要去哪，但不知道**“离得近有多危险”**。就像一个人能预测另一个人会走到哪里，但不知道保持多远的距离才安全。\n为了解决这个问题，研究者在 Falcon 的架构上增加了一个主动风险感知模块。这是一个轻量级的神经网络，利用共享的隐层状态，显式地预测周围每个人类的碰撞风险分数。它把风险分成了三个等级：Safe、Warning、Danger。","paper-list#Paper List":"基于上述分析，可以梳理出两类值得深入阅读的论文。第一类是直接处理社会导航核心问题的论文，第二类是可以作为方法论迁移的\"新锤子\"。\n直接相关的必读论文主要解决如何在人群中导航的核心问题。SocialNav-Map 是一个很好的基线或对比对象，它尝试把\"预测\"显式地画在地图上（Occupancy Map），而 Falcon 是隐式编码在特征里。对比这两者的优劣是很好的讨论点。\nRLSLM 试图将基于规则的社会力模型的可解释性融合进强化学习。这直接关联到 Falcon 缺乏显式社会规范的问题。\nTowards Socially Compliant Navigation 是针对 Falcon 的超参数调优。虽然技术含量可能不高，但它揭示了奖励函数设计的敏感性。\nComfort-Aware Trajectory Optimization 关注\"舒适度\"指标。如果目标是 Level 5（社会智能导航），这篇论文定义的指标可能比单纯的成功率更有用。\n方法论迁移的选读论文虽然不在 SocialNav 领域，但其中的技术（VLM、Visual Grounding）正是需要的\"新锤子\"。\nSeeground / Zero-Shot 3D Visual Grounding 是极其重要的\"新锤子\"来源。如果想做语义社会导航，需要这篇论文的方法：如何把 3D 场景转化为 2D VLM 能理解的 Prompt。可以把它的\"物体定位\"任务替换为\"社会规范定位\"任务。\nWhere to Fuse in the VLM Era 是一本\"操作手册\"。当决定引入 VLM 时，这篇综述告诉应该把它放在感知层（用来理解人）、预测层（用来预测意图）还是规划层（用来写代码）。\nEnhancing VLMs for Autonomous Driving 虽然是自动驾驶（室外），但它处理思维链（Chain of Thought）和空间推理的 Prompt Engineering 技巧，完全可以迁移到室内 SocialNav。例如：“那个人在看手机 -\u003e 所以他不会让路 -\u003e 我应该从左边绕”。\nTop 2 的方法依然在几何空间（距离、坐标）和RL 优化（Loss、Replay）里打转。它们都没有解决语义理解（Semantic Understanding）和显式博弈（Explicit Negotiation）。这正是未来研究的机会所在。","自问自答#自问自答":"在接触 LOVON 这类开放词汇物体导航工作时，发现它们对语义的理解停留在 YOLO 层面，追目标时避障效果很差。当时考虑过用边走边建图的 3D 重建方式，但看到 social navigation 后就搁置了。这里有两个问题值得深入思考。","这个模块的贡献在于将隐式的避障逻辑变成了显式的风险监督信号智能体在还没撞上之前就学会了这种距离是不舒服的从而更早地进行微调这种从隐式到显式的转变让模型的行为更加可解释也更容易调试#这个模块的贡献在于将\u003cstrong\u003e隐式的避障逻辑变成了显式的风险监督信号\u003c/strong\u003e。智能体在还没撞上之前，就学会了\u0026#34;这种距离是不舒服的”，从而更早地进行微调。这种从隐式到显式的转变，让模型的行为更加可解释，也更容易调试。":"冠军方案来自 Zhang 等人的 PER-Falcon，核心思路是**“模型不需要动，是数据利用效率太低”**。这个角度很独特，大多数工作都在改模型，但这个方法选择改训练策略。\n在强化学习训练社会导航时，大部分回合都是失败的，要么撞人，要么超时。成功的回合非常稀缺且珍贵，包含了完美的绕行和避让操作。Falcon 在训练时对所有数据一视同仁，导致智能体学了一堆\"怎么死”，却没学够\"怎么活”。\nPER-Falcon 引入了正样本回放机制。这是一种数据为中心的方法：把那些回报大于 10 的回合（即成功到达且避障良好）存到一个专门的缓冲区里。每隔一段时间，把这些\"满分作业\"拿出来让智能体再复习一遍，通过辅助的 PPO 更新来强化这些好的行为。\n这个方法的贡献在于证明了在社会导航中，强化好的行为比单纯修补坏的行为更有效。这其实是一个训练技巧，但效果极好，提升了 7 个百分点。有时候，简单的方法反而最有效，关键是要找到问题的本质。\n特征 Rank 2 (Risk Perception) Rank 1 (PER-Falcon) 改进维度 Perception / Loss Design (感知/损失函数) Data Efficiency / Optimization (数据效率/优化) 核心逻辑 把\"危险\"显式量化，作为辅助监督信号 把\"成功经验\"加权，避免被噪音数据淹没 新增参数量 极小 (两层 MLP) 0 (仅改变训练流程) 性能 SR 0.656, H-Coll 0.33 SR 0.660, H-Coll 0.32 借鉴点 引入 Dense Signal 辅助 RL 训练 在 World Model 训练中进行数据筛选 (Data Curation)","问题一social-navigation-需要-3d-重建吗#问题一：Social Navigation 需要 3D 重建吗？":"在 Social Navigation 领域，做显式 3D 重建是一条歧路。这个结论需要从 ObjectNav 和 SocialNav 的本质区别说起。\nObjectNav 的核心难点是记忆。机器人需要记住\"我去过厨房没？那个杯子在哪？“这类问题。这里用 3D 建图（如语义地图）是有用的，因为它解决的是静态环境的探索与回溯。LOVON 避障差，通常是因为它是模块化的（YOLO 指方向，然后 Planner 走），尽管利用了 LLM 进行 NER 分解，这也算是一种传统 Planner，对近距离动态避障很弱。\n但 SocialNav 的核心难点是动态性。人是会动的。如果做 3D 重建（TSDF Fusion、NeRF-SLAM 等），会面临严重的鬼影问题。一个人从左走到右，3D 地图上会留下一串残影，不仅不能辅助导航，反而会变成一堆不存在的障碍物墙，把路堵死。\nFalcon、Rank 1、Rank 2 的成功证明了：处理动态环境，需要的是第一视角感知（Egocentric Perception）加上时序记忆（RNN/Transformer），而不是全局静态地图。\n关于\"避障不行\"的问题，之前遇到的 YOLO 检测到了但还是撞上去的情况，正是端到端强化学习（SocialNav）试图解决的。关键是把\"识别\"和\"运动控制\"分开。SocialNav 的方法（如 Falcon）是把深度/RGB 直接映射到动作。如果智能体撞了，它会直接受到惩罚。这比\"YOLO 告诉 Planner 有人，Planner 计算路径\"的链路反应更快，且更能处理复杂交互。","问题二depth-anything-v3-是合适的新锤子吗#问题二：Depth Anything V3 是合适的\u0026quot;新锤子\u0026quot;吗？":"用目标驱动研究的标准来评估这个\"锤子”，会发现两种截然不同的使用方式。\n工程思维（不推荐）：把 Falcon 输入端的深度传感器换成 Depth Anything V3 生成的深度。问题在于，在仿真器（如 Habitat）里，已经有了完美深度（Ground Truth Depth）。Depth Anything V3 生成的深度再好，也不可能比仿真器自带的完美深度更好。结果可能是性能下降（因为引入了推理延迟和误差），且没有任何创新性。\n科研思维（推荐）：Depth Anything V3 的真正价值在于\"仿真到现实的鸿沟\"或\"语义感知\"。\n第一个方向是鲁棒性/仿真到现实。SocialNav 在仿真里跑得好，是因为仿真里的深度是完美的。真实世界的深度传感器（Realsense/LiDAR）有噪声、有盲区（玻璃、强光）。Depth Anything V3 是一个鲁棒感知探针，它是从大量真实图片训练出来的，对真实世界的噪声有极强的鲁棒性。可以提出一个框架，证明在仿真里使用 Depth Anything V3 提取的特征（而非原始深度），能够让智能体在零样本迁移到真实世界时表现得更好，因为它学到的是通用的深度特征，而不是仿真特定的几何特征。\n第二个方向是隐式语义引导（更高级的）。Depth Anything 不仅仅是深度，它其实是基础模型。为了估计深度，它必须\"理解\"物体是什么（比如理解这块平滑的像素是墙而不是空洞）。利用 Depth Anything V3 的编码器特征作为 SocialNav 的额外输入，它的特征里不仅包含距离，还隐式包含了物体语义。这可能解决 Falcon\"把人当圆柱体\"的问题，让智能体能区分\"人\"和\"人形雕塑\"，或者区分\"柔软的窗帘\"和\"坚硬的墙\"。\nFalcon（基线）和 Rank 2 都依赖几何信息（位置、速度、距离）来判断风险。但有些情况仅靠深度和坐标无法区分：一个人站着不动是在玩手机（不会突然动），还是在等人（可能会突然拥抱）？一个人跑过来是冲着我来的（攻击性），还是只是路过（中性）？\n科学的做法是：不要用 Depth Anything V3 做深度，用它（或者 VLM 如 CLIP/SigLIP）做视觉语义特征提取。参考 Rank 2 的架构，加一个辅助模块，但不是预测\"风险分数\"，而是预测\"社会意图\"或\"语义状态\"。\n为什么这样做有效？Rank 1 证明了数据重要，可以利用基础模型里的海量数据。Rank 2 证明了显式预测隐变量重要，可以预测比风险更高级的意图。"},"title":"Social Navigation Idea"},"/blog/2025/socialnav-map-1/":{"data":{"":"接下来复现 SocialNav-Map\nsource /etc/network_turbo conda install -n base -c conda-forge mamba ~/miniconda3/bin/mamba install habitat-sim=0.3.1 withbullet headless -c conda-forge -c aihabitat cd SocialNav-Map/Falcon pip install -e habitat-lab -i https://pypi.tuna.tsinghua.edu.cn/simple pip install -e habitat-baselines -i https://pypi.tuna.tsinghua.edu.cn/simple pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 然后是麻烦的数据这一块，在 autodl 一定要放在 autodl-fs 里\n# 在 autodl-fs 里创建一个专门存放这个项目数据的文件夹 mkdir -p /root/autodl-fs/SocialNavData # 建立软链接：把 autodl-fs 的文件夹映射到当前目录的 data ln -s /root/autodl-fs/SocialNavData data # 验证一下 ls -l data # 输出应该显示 data -\u003e /root/autodl-fs/SocialNavData # 现在，往 Falcon/data 里下载的任何东西，实际上都会存进那是 200GB 的 autodl-fs 里，不用担心爆盘 然后经典 libEGL.so.1 缺失，habitat-sim 启动前找不到 EGL\nTraceback (most recent call last): File \"/root/miniconda3/envs/falcon/lib/python3.9/runpy.py\", line 188, in _run_module_as_main mod_name, mod_spec, code = _get_module_details(mod_name, _Error) File \"/root/miniconda3/envs/falcon/lib/python3.9/runpy.py\", line 111, in _get_module_details __import__(pkg_name) File \"/root/miniconda3/envs/falcon/lib/python3.9/site-packages/habitat_sim-0.3.1-py3.9-linux-x86_64.egg/habitat_sim/__init__.py\", line 13, in \u003cmodule\u003e import habitat_sim._ext.habitat_sim_bindings ImportError: libEGL.so.1: cannot open shared object file: No such file or directory 虽然 nvidia-smi 显示驱动正常（这是内核层面的），但 Python 环境或系统缺少用户空间的 EGL 接口库 (libEGL.so.1)。\nlibEGL.so.1 通常是一个分发器（Dispatcher）。它的工作是指挥程序去调用真正的后端驱动（在你的情况下是 NVIDIA 驱动）。如果没有这个文件，程序就不知道如何去\"握手\"并调用显卡。\n为了避免 fallback 到纯软件渲染，我们可以通过安装与厂商无关的调度库 (GLVND) 来解决。这不是 Mesa 软件渲染，而是让系统能正确识别并调用 NVIDIA 硬件的标准接口。\napt update \u0026\u0026 apt-get install -y libgl1 libglvnd0 libglx0 libegl1 安装 libglvnd（GL Vendor-Neutral Dispatch） 提供 libEGL.so.1，并会自动检测并使用 NVIDIA 驱动，不会强制变为 Mesa 软件渲染。此时我们再全局查找 find / -name \"libEGL.so*\" 2\u003e/dev/null 就有了：\n/usr/lib/x86_64-linux-gnu/libEGL.so.1 /usr/lib/x86_64-linux-gnu/libEGL.so.1.1.0 然后用 python -m habitat_sim.utils.datasets_download --list 查看数据源：\nNo data-path provided, defaults to: ./data. Use '--data-path' to specify another location.\rNote, ./data is a symbolic link that points to /autodl-fs/data/SocialNavData.\r====================================\rCurrently available datasources are:\r------------------------------------\rhssd-hab\rhab3-episodes\rhssd-raw\rhssd-hab_internal\rhssd-hab_objectnav_dataset\rai2thor-hab\rprocthor-hab_objectnav_dataset\rhabitat_test_scenes\rhabitat_test_pointnav_dataset\rhabitat_example_objects\rlocobot_merged\rmp3d_example_scene\rcoda_scene\rwebxr_hand_demo\rreplica_cad_dataset\rreplica_cad_baked_lighting\rycb\rfranka_panda\rhab_spot_arm\rhab_stretch\rhab_fetch\rhabitat_humanoids\rrearrange_pick_dataset_v0\rrearrange_dataset_v1\rhab2_bench_assets\rhab3_bench_assets\rhm3d_minival_glb_v0.1\rhm3d_minival_glb_v0.2\rhm3d_minival_habitat_v0.1\rhm3d_minival_habitat_v0.2\rhm3d_minival_configs_v0.1\rhm3d_minival_configs_v0.2\rhm3d_train_glb_v0.1\rhm3d_train_glb_v0.2\rhm3d_train_habitat_v0.1\rhm3d_train_habitat_v0.2\rhm3d_train_configs_v0.1\rhm3d_train_configs_v0.2\rhm3d_val_glb_v0.1\rhm3d_val_glb_v0.2\rhm3d_val_habitat_v0.1\rhm3d_val_habitat_v0.2\rhm3d_val_configs_v0.1\rhm3d_val_configs_v0.2\rhm3d_example_glb\rhm3d_example_habitat\rhm3d_example_configs\rhm3d_minival_semantic_annots_v0.1\rhm3d_minival_semantic_annots_v0.2\rhm3d_minival_semantic_configs_v0.1\rhm3d_minival_semantic_configs_v0.2\rhm3d_train_semantic_annots_v0.1\rhm3d_train_semantic_annots_v0.2\rhm3d_train_semantic_configs_v0.1\rhm3d_train_semantic_configs_v0.2\rhm3d_val_semantic_annots_v0.1\rhm3d_val_semantic_annots_v0.2\rhm3d_val_semantic_configs_v0.1\rhm3d_val_semantic_configs_v0.2\rhm3d_example_semantic_annots\rhm3d_example_semantic_configs\r====================================\rCurrently available datagroups are:\r------------------------------------\r('ci_test_assets', ['habitat_test_scenes', 'habitat_test_pointnav_dataset', 'habitat_example_objects', 'locobot_merged', 'mp3d_example_scene', 'coda_scene', 'replica_cad_dataset', 'hab_fetch', 'hab_stretch', 'hab_spot_arm', 'hm3d_example', 'hm3d_example_habitat', 'hm3d_example_configs', 'hm3d_example_semantic_annots', 'hm3d_example_semantic_configs'])\r('rearrange_task_assets', ['replica_cad_dataset', 'hab_fetch', 'ycb', 'rearrange_pick_dataset_v0', 'rearrange_dataset_v1'])\r('hm3d_example', ['hm3d_example_habitat', 'hm3d_example_configs', 'hm3d_example_semantic_annots', 'hm3d_example_semantic_configs'])\r('hm3d_val_v0.1', ['hm3d_val_habitat_v0.1', 'hm3d_val_configs_v0.1', 'hm3d_val_semantic_annots_v0.1', 'hm3d_val_semantic_configs_v0.1'])\r('hm3d_train_v0.1', ['hm3d_train_habitat_v0.1', 'hm3d_train_configs_v0.1', 'hm3d_train_semantic_annots_v0.1', 'hm3d_train_semantic_configs_v0.1'])\r('hm3d_minival_v0.1', ['hm3d_minival_habitat_v0.1', 'hm3d_minival_configs_v0.1', 'hm3d_minival_semantic_annots_v0.1', 'hm3d_minival_semantic_configs_v0.1'])\r('hm3d_semantics_v0.1', ['hm3d_example_semantic_annots_v0.1', 'hm3d_example_semantic_configs_v0.1', 'hm3d_val_semantic_annots_v0.1', 'hm3d_val_semantic_configs_v0.1', 'hm3d_train_semantic_annots_v0.1', 'hm3d_train_semantic_configs_v0.1', 'hm3d_minival_semantic_annots_v0.1', 'hm3d_minival_semantic_configs_v0.1'])\r('hm3d_val_v0.2', ['hm3d_val_habitat_v0.2', 'hm3d_val_configs_v0.2', 'hm3d_val_semantic_annots_v0.2', 'hm3d_val_semantic_configs_v0.2'])\r('hm3d_train_v0.2', ['hm3d_train_habitat_v0.2', 'hm3d_train_configs_v0.2', 'hm3d_train_semantic_annots_v0.2', 'hm3d_train_semantic_configs_v0.2'])\r('hm3d_minival_v0.2', ['hm3d_minival_habitat_v0.2', 'hm3d_minival_configs_v0.2', 'hm3d_minival_semantic_annots_v0.2', 'hm3d_minival_semantic_configs_v0.2'])\r('hm3d_semantics_v0.2', ['hm3d_example_semantic_annots_v0.2', 'hm3d_example_semantic_configs_v0.2', 'hm3d_val_semantic_annots_v0.2', 'hm3d_val_semantic_configs_v0.2', 'hm3d_train_semantic_annots_v0.2', 'hm3d_train_semantic_configs_v0.2', 'hm3d_minival_semantic_annots_v0.2', 'hm3d_minival_semantic_configs_v0.2'])\r('hm3d_v0.1', ['hm3d_val_habitat_v0.1', 'hm3d_val_configs_v0.1', 'hm3d_val_semantic_annots_v0.1', 'hm3d_val_semantic_configs_v0.1', 'hm3d_train_habitat_v0.1', 'hm3d_train_configs_v0.1', 'hm3d_train_semantic_annots_v0.1', 'hm3d_train_semantic_configs_v0.1', 'hm3d_minival_habitat_v0.1', 'hm3d_minival_configs_v0.1', 'hm3d_minival_semantic_annots_v0.1', 'hm3d_minival_semantic_configs_v0.1'])\r('hm3d_full', ['hm3d_minival_glb_v0.2', 'hm3d_minival_habitat_v0.2', 'hm3d_minival_configs_v0.2', 'hm3d_train_glb_v0.2', 'hm3d_train_habitat_v0.2', 'hm3d_train_configs_v0.2', 'hm3d_val_glb_v0.2', 'hm3d_val_habitat_v0.2', 'hm3d_val_configs_v0.2', 'hm3d_example_glb', 'hm3d_example_habitat', 'hm3d_example_configs', 'hm3d_minival_semantic_annots_v0.2', 'hm3d_minival_semantic_configs_v0.2', 'hm3d_train_semantic_annots_v0.2', 'hm3d_train_semantic_configs_v0.2', 'hm3d_val_semantic_annots_v0.2', 'hm3d_val_semantic_configs_v0.2', 'hm3d_example_semantic_annots', 'hm3d_example_semantic_configs'])\r('hm3d_train_full', ['hm3d_train_glb_v0.2', 'hm3d_train_habitat_v0.2', 'hm3d_train_configs_v0.2', 'hm3d_train_semantic_annots_v0.2', 'hm3d_train_semantic_configs_v0.2'])\r('hm3d_val_full', ['hm3d_val_glb_v0.2', 'hm3d_val_habitat_v0.2', 'hm3d_val_configs_v0.2', 'hm3d_val_semantic_annots_v0.2', 'hm3d_val_semantic_configs_v0.2'])\r('hm3d_minival_full', ['hm3d_minival_glb_v0.2', 'hm3d_minival_habitat_v0.2', 'hm3d_minival_configs_v0.2', 'hm3d_minival_semantic_annots_v0.2', 'hm3d_minival_semantic_configs_v0.2'])\r('hm3d_example_full', ['hm3d_example_glb', 'hm3d_example_habitat', 'hm3d_example_configs', 'hm3d_example_semantic_annots', 'hm3d_example_semantic_configs'])\r('hm3d', ['hm3d_val_habitat_v0.2', 'hm3d_val_configs_v0.2', 'hm3d_val_semantic_annots_v0.2', 'hm3d_val_semantic_configs_v0.2', 'hm3d_train_habitat_v0.2', 'hm3d_train_configs_v0.2', 'hm3d_train_semantic_annots_v0.2', 'hm3d_train_semantic_configs_v0.2', 'hm3d_minival_habitat_v0.2', 'hm3d_minival_configs_v0.2', 'hm3d_minival_semantic_annots_v0.2', 'hm3d_minival_semantic_configs_v0.2'])\r==================================== 列表中其实只有两种东西：原子包（Parts） 和 组合包（Groups）。\n当你指定 uid = hm3d_minival_v0.2 时，脚本会自动帮你下载该组内的 4 个原子包。","downloading-hm3d-v02#Downloading HM3D v0.2":"File Name Data Set Format Link Size hm3d-minival-glb-v0.2.tar minival glb https://api.matterport.com/resources/habitat/hm3d-minival-glb-v0.2.tar 464M hm3d-minival-habitat-v0.2.tar minival habitat https://api.matterport.com/resources/habitat/hm3d-minival-habitat-v0.2.tar 390M hm3d-minival-semantic-annots-v0.2.tar minival semantic-annots https://api.matterport.com/resources/habitat/hm3d-minival-semantic-annots-v0.2.tar 240.6M hm3d-minival-semantic-configs-v0.2.tar minival semantic-configs https://api.matterport.com/resources/habitat/hm3d-minival-semantic-configs-v0.2.tar 30K hm3d-train-glb-v0.2.tar train glb https://api.matterport.com/resources/habitat/hm3d-train-glb-v0.2.tar 32G hm3d-train-habitat-v0.2.tar train habitat https://api.matterport.com/resources/habitat/hm3d-train-habitat-v0.2.tar 27G hm3d-train-semantic-annots-v0.2.tar train semantic-annots https://api.matterport.com/resources/habitat/hm3d-train-semantic-annots-v0.2.tar 8.1G hm3d-train-semantic-configs-v0.2.tar train semantic-configs https://api.matterport.com/resources/habitat/hm3d-train-semantic-configs-v0.2.tar 50K hm3d-val-glb-v0.2.tar val glb https://api.matterport.com/resources/habitat/hm3d-val-glb-v0.2.tar 4G hm3d-val-habitat-v0.2.tar val habitat https://api.matterport.com/resources/habitat/hm3d-val-habitat-v0.2.tar 3.3G hm3d-val-semantic-annots-v0.2.tar val semantic-annots https://api.matterport.com/resources/habitat/hm3d-val-semantic-annots-v0.2.tar 2.0G hm3d-val-semantic-configs-v0.2.tar val semantic-configs https://api.matterport.com/resources/habitat/hm3d-val-semantic-configs-v0.2.tar 40K hm3d-example-glb-v0.2.tar example glb hm3d-example-glb-v0.2.tar 186M hm3d-example-habitat-v0.2.tar example habitat hm3d-example-habitat-v0.2.tar 154M hm3d-example-semantic-annots-v0.2.tar example semantic-annots hm3d-example-semantic-annots-v0.2.tar 60M hm3d-example-semantic-configs-v0.2.tar example semantic-configs hm3d-example-semantic-configs-v0.2.tar 30K 类别 (Split) 用途 包含场景数 总大小 (解压前) 详细组成 (核心文件) Minival 代码调试/快速验证 20个 ~1.1 GB 已下载 (GLB 464M + Nav 390M + Annots 240M) Val 标准验证/评估模型 100个 ~9.3 GB GLB: 4G\nHabitat: 3.3G\nSemantics: 2.0G Train 大规模模型训练 800个 ~67.1 GB GLB: 32G\nHabitat: 27G\nSemantics: 8.1G 解压后加起来应该在 150GB 那样子（按文档所说）\npython -m habitat_sim.utils.datasets_download \\ --username XXX \\ --password XXX \\ --uids hm3d_minival_v0.2 这玩意下太慢了，对于 val 的话 python 单线程我得下 5天。而 Autodl 的带宽就算用了 aria2 也不够，只能先在自己电脑上下（用 Github 给的直链），然后传到 Autodl 上（它的入站带宽不小）。\n这里先把 minival 下下来。\n对于 MP3D，我终于理解了，我向 TUM 申请的那个 download_mp.py（只能用 python 2.7 跑的）是 entire Matterport3D dataset，那个是 1.3TB 的，而针对 Habitat 仿真器的版本（即 --task habitat）大小就只有 15 GB，因为它剔除了巨大的原始 RGB-D 视频帧，只保留了重建好的 3D 模型。那个 1.3TB 的是做 3D 重建、超分、深度估计等底层视觉任务用的原始数据。\n这里得给 TUM 发邮件得到他们那个 download_mp.py 才可以，这里我因为早就发过了所以就跳过了，同样也是本地下好（3MB/s 左右）然后再传上去。","下载-episode-datasets-socialnav-任务数据#下载 Episode Datasets (SocialNav 任务数据)":"作者给我们的链接有一个 2.24GB 的 social-hm3d.zip，一个 1.96GB 的 social-mp3d.zip\n# 在 AutoDL 上下载 Google Drive 文件 pip install gdown cd data gdown --folder https://drive.google.com/drive/folders/1V0a8PYeMZimFcHgoJGMMTkvscLhZeKzD # 这个因为 Google Drive 的文件夹抓取受到很多反爬虫限制，而且当文件夹权限设置为“仅查看”时，gdown 经常无法列出目录 # 这里以 social-mp3d.zip 为例，先拿到它的File ID：https://drive.google.com/file/d/16KGr9cae1z3ypfgeeDSd5wDnJTfu2HhK/view?usp=drive_link，即‘16KGr9cae1z3ypfgeeDSd5wDnJTfu2HhK’ gdown 16KGr9cae1z3ypfgeeDSd5wDnJTfu2HhK -O social-mp3d.zip gdown 两种方法都下不下来，主要原因还是在于 Permission 本身，建议在本地电脑下载好，然后通过 AutoDL 的 JupyterLab 网页端或者 FileZilla 上传到 data/ 目录。\n接着按照 README 的结构解压：\n# 确保目录存在 mkdir -p datasets/pointnav # 解压 social-hm3d.zip 到目标目录 unzip social-hm3d.zip -d datasets/pointnav/ # 解压 social-mp3d.zip 到目标目录 unzip social-mp3d.zip -d datasets/pointnav/ # 验证目录结构（可选） ls -la datasets/pointnav/ # 应该看到 social-hm3d/ 和 social-mp3d/ 两个目录 # 清理 zip 文件（可选，节省空间） # rm social-hm3d.zip social-mp3d.zip 最后检查一下路径，惊觉自己居然把代码放在了系统盘，于是赶紧迁移到数据盘去：\nmv ~/SocialNav-Map /root/autodl-tmp/ 好在之前的软链接用的是绝对路径，移动 SocialNav-Map 文件夹时，里面的 data 这个\"快捷方式\"文件被一起移动了，但它指向的目标地址（绝对路径）是写死在它里面的不会变。\n然而以可编辑模式安装的包仍指向旧路径，导致导入失败，所以需要重新安装：\nhabitat-lab 已重新安装到新路径：/root/autodl-tmp/SocialNav-Map/Falcon/habitat-lab habitat-baselines 已重新安装到新路径：/root/autodl-tmp/SocialNav-Map/Falcon/habitat-baselines 问题：ModuleNotFoundError: No module named 'habitat_baselines.il.data.nav_data'\n解决：手动把官方库里的 nav_data.py 创建过来。\n这之后又来一个 ModuleNotFoundError: No module named 'habitat_baselines.il.data.data'，同样把这里的 data.py 挪过来。","下载-leg-animation-腿部动作数据#下载 Leg animation (腿部动作数据)":"mkdir -p data/robots/spot_data # 都是在~/SocialNav-Map/Falcon路径下 wget https://github.com/facebookresearch/habitat-lab/files/12502177/spot_walking_trajectory.csv -O data/robots/spot_data/spot_walking_trajectory.csv","下载-multi-agent-necessary-data#下载 Multi-agent necessary data":"sudo apt install git-lfs python -m habitat_sim.utils.datasets_download \\ --username XXX \\ --password XXX \\ --uids hab3-episodes habitat_humanoids hab3_bench_assets hab_spot_arm 遇到 huggingface.co 443 超时的话就 source /etc/network_turbo 一下","下载-scene-datasets-场景数据#下载 Scene Datasets (场景数据)":""},"title":"复现 SocialNav-Map"},"/blog/2025/vlm-think-with-images/":{"data":{"":"","1-绪论模态鸿沟与系统-2视觉推理的崛起#1. 绪论：模态鸿沟与\u0026quot;系统 2\u0026quot;视觉推理的崛起":"","11-纯文本思维链在多模态语境下的局限性#1.1 纯文本思维链在多模态语境下的局限性":"传统的视觉语言模型（如 LLaVA 系列、GPT-4V）主要采用**“编码器-解码器\"架构**：视觉编码器（如 ViT）将图像压缩为特征向量，随后投影到大语言模型（LLM）的词嵌入空间。一旦进入 LLM，视觉信息便被视为某种\"外语”，推理过程完全由预训练的语言概率分布主导。\n这种架构在图像描述（Captioning）等**“系统 1\"直觉任务**上表现出色，但在需要多步逻辑的复杂任务中面临显著瓶颈：\n信息有损压缩（Information Bottleneck）：视觉编码器通常将高分辨率图像压缩为有限数量的 token（例如 256 或 576 个），导致高频细节（如微小文字、物体精确坐标）在推理开始前即丢失。\n视觉-语言非同构性（Isomorphism Deficit）：语言是离散、符号化且高度抽象的，而视觉是连续、密集且具象的。强行用文本 CoT 描述复杂的空间拓扑（如\"左边第三个红球稍微偏上一点”）会导致语义精度的急剧下降，模型往往因此退化为依赖语言先验而非视觉事实进行猜想，即产生**“幻觉”**。\n缺乏回溯机制（Lack of Retracing）：人类在解决视觉难题时会反复观察图像（Visual Re-scanning），而标准 VLM 往往是**“看一眼，然后闭眼推理”**，缺乏在推理中途重新审视视觉输入的能力。","12-thinking-with-images-的定义与新分类体系#1.2 \u0026ldquo;Thinking with Images\u0026rdquo; 的定义与新分类体系":"“Thinking with Images” 指的是模型在推理过程中，不仅生成文本，还显式或隐式地操作视觉信息，将其作为推理链条中不可或缺的一环。这对应于认知科学中的**“心智意象”（Mental Imagery）**理论，即人类在思考空间问题时，会在大脑中构建视觉模拟。\n基于对现有文献的系统梳理，我们将这一领域的解决方案扩展为以下五大范式：\n范式 I：工具中介与程序化视觉推理（Tool-Mediated \u0026 Programmatic Reasoning） —— 借用外部引擎的\"手\"来操作视觉。 范式 II：显式生成意象与心智模拟（Explicit Generative Imagery \u0026 Mental Simulation） —— 利用生成模型的\"想象力\"进行预演。 范式 III：潜在空间视觉推理（Latent Visual Reasoning） —— 在高维特征空间进行高效的\"内隐视觉思考”。 范式 IV：主动感知与强化视觉搜索（Active Perception \u0026 Agentic Grounding） —— 像人类眼动一样主动\"寻找\"视觉证据。 范式 V：结构化与组合式视觉推理（Compositional \u0026 Structured Grounding） —— 将图像解构为图谱或掩码进行逻辑运算。","2-范式-i工具中介与程序化视觉推理#2. 范式 I：工具中介与程序化视觉推理":"这一范式代表了**“符号主义\"与\"联结主义”的结合。其核心假设是：神经网络在精确计算（如计数、几何测量）和逻辑执行上存在先天不足，应当将这些任务“外包”给擅长此道的外部工具（如 Python 解释器、OpenCV 库或绘图 API**）。VLM 在此扮演**“控制器”的角色，负责理解意图、编写程序并解析执行结果。这也就是用户查询中提到的“OpenAI-O3 范式”**的典型体现。","21-核心机制思维的外化#2.1 核心机制：思维的外化":"在该范式中，中间推理步骤被显式地转化为可执行的代码或可视化操作。这种**“外化”**不仅提高了推理的准确性，还赋予了模型极强的可解释性。","22-里程碑文献深度解析#2.2 里程碑文献深度解析":"","221-visual-sketchpad-neurips-2024#2.2.1 Visual Sketchpad (NeurIPS 2024)":"论文标题：Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models\n核心问题：传统的 CoT 仅在文本层面分解问题，但对于几何题或地图导航，纯文本描述极其低效且易错。\n方法论：作者提出了一种**“视觉草稿本\"机制**。模型在推理过程中，可以生成代码来调用绘图 API（如 Matplotlib），在原图上绘制辅助线、标记框或圈出关键区域。这些绘制了标记的新图像被重新输入模型，作为下一步推理的视觉上下文。\n创新点：首次将**“草图绘制”（Sketching）**引入 VLM 推理链。这模拟了人类做几何题时画辅助线的行为。实验表明，这种视觉辅助能显著提升数学几何和视觉逻辑任务的准确率，证明了视觉符号不仅是输出，更是推理的支架。","222-vipergpt-iccv-2023#2.2.2 ViperGPT (ICCV 2023)":"论文标题：ViperGPT: Visual Inference via Python Execution for Reasoning\n核心问题：**端到端（End-to-End）**模型不仅是黑盒，而且经常在简单的逻辑组合上失败（例如\"红帽子的人左边是不是有一辆车”）。\n方法论：ViperGPT 彻底摒弃了端到端的视觉问答模式。它利用专门针对代码微调的 LLM（如 Codex）将自然语言问题转化为 Python 程序。该程序调用一系列视觉 API（如对象检测、深度估计模型）。程序执行的结果即为最终答案。\n创新点：提出了**“以代码为策略”（Code as Policy）的视觉推理极致形态。它不需要训练多模态模型，而是通过组合现有的视觉专家模型（Vision Experts）**来解决问题，实现了极高的可解释性和组合泛化能力。","223-visual-program-distillation-vpd-cvpr-2024-oral#2.2.3 Visual Program Distillation (VPD) (CVPR 2024 Oral)":"论文标题：Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models\n核心问题：像 ViperGPT 这样的工具调用方法虽然准确，但推理延迟高、计算开销大，且依赖外部 API 的稳定性。\n方法论：VPD 提出了一种**“蒸馏\"策略**。它首先利用大型模型生成成千上万条高质量的\"视觉程序\"轨迹（即问题-程序-答案三元组），然后用这些数据微调一个较小的端到端 VLM（如 PaLM-E 或 LLaVA）。\n创新点：成功将工具调用的逻辑推理能力**“内化”**到模型权重中。微调后的模型不再需要外部 API，而是直接预测出类似于程序执行步骤的推理结果，兼顾了工具方法的逻辑严密性和端到端模型的高效性。","224-task-navigator-cvpr-2024#2.2.4 Task Navigator (CVPR 2024)":"论文标题：Task Navigator: Decomposing Complex Tasks for Multimodal Large Language Models\n核心问题：面对极其复杂的视觉任务（如\"分析这张监控截图中所有异常行为”），模型往往不知从何下手，因为它的注意力机制无法一次性处理所有信息。\n方法论：引入了一个**导航器（Navigator）**模块，负责将宏观任务分解为一系列子查询（Sub-queries）。系统根据子查询的需要，动态选择调用特定的视觉工具（如 OCR、检测器、知识库检索），并根据工具返回的结果规划下一步。\n创新点：强调了**“规划”（Planning）在视觉推理中的核心地位。证明了系统 2 推理**的关键在于将复杂问题降维，并通过工具迭代式地获取信息。","225-deepsketcher-arxiv-2025--iclr-context#2.2.5 DeepSketcher (ArXiv 2025 / ICLR Context)":"论文标题：DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning\n核心问题：外部绘图工具（如 Visual Sketchpad）的操作是离散且不可微的，阻断了梯度回传，限制了模型的学习能力。\n方法论：DeepSketcher 设计了一个**“图像嵌入编辑模块”（Image Embedding Editing Module）**。它虽然受到代码渲染图像的监督，但在推理时，模型是直接在图像的特征空间（Embedding Space）中进行\"涂抹\"和\"高亮\"，模拟绘图操作。\n创新点：实现了工具操作的**“软化\"和\"可微化”**。它不仅保留了绘图辅助推理的直观优势，还允许模型通过梯度下降端到端地学习\"该在哪里画线\"，是范式 I 向范式 III（潜在推理）演进的过渡形态。","3-范式-ii显式生成意象与心智模拟#3. 范式 II：显式生成意象与心智模拟":"这一范式深受认知心理学启发。人类在回答\"大象能不能装进冰箱\"这个问题时，会在脑海中生成大象和冰箱的视觉意象并进行比对。同样，该范式赋予 VLM 调用生成模型（如 Stable Diffusion）的能力，通过生成像素级的图像来辅助推理，特别是针对空间预测、反事实推理和未来预测任务。","31-核心机制视觉想象循环#3.1 核心机制：视觉想象循环":"推理过程不再是单向的（图像 -\u003e 文本），而是闭环的：文本/图像 -\u003e 生成新图像 -\u003e 视觉感知 -\u003e 文本结论。生成的图像充当了推理的**“视觉草稿”**。","32-里程碑文献深度解析#3.2 里程碑文献深度解析":"","321-multimodal-visualization-of-thought-mvot-icml-2025#3.2.1 Multimodal Visualization-of-Thought (MVoT) (ICML 2025)":"论文标题：Imagine While Reasoning in Space: Multimodal Visualization-of-Thought\n核心问题：现有的多模态思维链（Multimodal CoT）大多只是文本 CoT 加上静态图像输入，缺乏真正的\"视觉思考\"过程。\n方法论：MVoT 提出了一种交错式的推理模式，模型可以像输出词语一样输出图像。为了实现这一点，作者设计了**“Token Discrepancy Loss”**，解决了 LLM 文本 Token 与图像生成器（如 VQ-VAE）离散 Codebook 之间的分布差异问题。模型在推理过程中会生成一系列中间图像（Visual Thoughts），展示其空间变换的构思过程。\n创新点：将图像生成内化为 LLM 的原生能力，实现了真正的**“图文交错思维流”**。实验证明，这种显式的视觉化过程显著提升了空间旋转、物体拼接等任务的性能。","322-imaginenav-iclr-2024#3.2.2 ImagineNav (ICLR 2024)":"论文标题：ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination\n核心问题：在具身智能导航中，代理（Agent）受限于视野，无法看到墙后或拐角处的物体，导致规划短视。\n方法论：ImagineNav 利用 VLM 结合新视角合成技术（Novel View Synthesis），根据当前观测\"想象\"出未知区域的景象。模型基于这些生成的幻觉图像来评估路径的可行性。\n创新点：将生成式视觉 CoT 应用于决策规划。证明了**“合理的幻觉”**（基于先验的预测）是智能体在非结构化环境中生存的关键能力。","323-perspective-aware-reasoning-apc-iccv-2025#3.2.3 Perspective-Aware Reasoning (APC) (ICCV 2025)":"论文标题：Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation\n核心问题：VLM 存在严重的**“自我中心偏差”（Egocentric Bias）**，难以理解\"如果我们换个角度看这个物体会怎样\"这类问题。\n方法论：APC 框架模拟了人类的**心智旋转（Mental Rotation）**能力。它首先利用视觉基础模型从输入图像中重建一个粗糙的 3D 抽象场景，然后将该 3D 场景旋转到目标视角，并重新投影为 2D 图像输入 VLM 进行回答。\n创新点：引入了 3D 抽象作为中间推理模态。它表明，解决复杂的视觉关系问题需要超越 2D 像素，进入 3D 语义空间的模拟。","324-spatialdreamer-arxiv-2025--top-venue-context#3.2.4 SpatialDreamer (ArXiv 2025 / Top Venue Context)":"论文标题：Incentivizing Spatial Reasoning via Active Mental Imagery\n核心问题：仅仅生成图像是不够的，模型需要知道\"生成什么图像\"对解题最有帮助。\n方法论：SpatialDreamer 引入了强化学习（RL）来优化生成策略。它训练模型主动进行“视觉做梦”（Dreaming），并通过**世界模型（World Model）**验证这些梦境的物理一致性。奖励机制鼓励模型生成那些能最大程度减少不确定性的视角或状态。\n创新点：将生成式 CoT 与强化学习结合，使视觉想象具有了目的性（Goal-oriented Imagination），这是向自主智能迈进的重要一步。","325-self-imagine-arxiv-2024#3.2.5 Self-Imagine (ArXiv 2024)":"论文标题：Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination\n核心问题：即使是纯文本的逻辑题，人类也往往需要画图辅助理解，而 LLM 缺乏这种能力。\n方法论：该方法无需训练，通过**提示工程（Prompting）**让 VLM 将抽象的文本问题转化为结构化的 HTML 或 SVG 代码，然后渲染成图像。模型再次读取这张自己生成的图表来回答问题。\n创新点：揭示了**“跨模态转换”**本身就是一种强大的推理增强手段。将文本转化为视觉结构，能够利用 VLM 强大的视觉模式识别能力来破解复杂的逻辑谜题。","4-范式-iii潜在空间视觉推理#4. 范式 III：潜在空间视觉推理":"这一范式是目前效率最高、最具理论深度的方向。它认为，显式地生成像素图像（如范式 II）虽然直观但计算极其昂贵，且容易受到生成伪影的干扰。真正的\"视觉思维\"应当发生在紧凑、高维的**潜在空间（Latent Space）**中，类似于人类大脑处理视觉信号时并不需要在视网膜上重新成像。","41-核心机制连续视觉思维链#4.1 核心机制：连续视觉思维链":"模型在推理过程中生成特殊的**“视觉 Token”或“思维向量”。这些向量不对应任何具体的单词，也不必解码为像素，而是保留了连续的梯度信息**，能够承载比离散文本丰富得多的感知细节（如纹理、深度、精确坐标）。","42-里程碑文献深度解析#4.2 里程碑文献深度解析":"","421-chain-of-visual-thought-covt--vchain-iccv-2025--arxiv-2025#4.2.1 Chain-of-Visual-Thought (CoVT / VChain) (ICCV 2025 / ArXiv 2025)":"论文标题：Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens\n核心问题：语言是高度压缩的符号系统，用语言描述视觉细节（如\"这个不规则物体的边缘\"）会造成巨大的信息丢失。\n方法论：作者提出了一组**“连续视觉 Token”（Continuous Visual Tokens）**。模型经过训练，可以在推理步骤中输出这些连续向量。这些向量在训练时通过重构任务（如重构深度图、分割掩码）进行监督，确保其包含物理意义，但在推理时直接作为后续层的输入。\n创新点：重新定义了 CoT 的载体。思维链不再局限于离散的文本，而是变成了**“文本-视觉向量\"混合流**。实验表明，这种方法在细粒度感知任务上大幅超越了纯文本 CoT。","422-mirage-arxiv-2025--cvpr-context#4.2.2 Mirage (ArXiv 2025 / CVPR Context)":"论文标题：Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens\n核心问题：如何在不引入沉重的图像生成解码器的情况下，赋予模型**“心智意象”**能力。\n方法论：Mirage 提出了**“潜在意象”（Latent Imagery）**。它利用轻量级的投影层将视觉编码器的特征映射到 LLM 的嵌入空间。通过两阶段训练（先对齐感知，再强化推理），模型学会了在遇到视觉难题时\"调用\"潜在视觉记忆，并在多轮对话中保持这些视觉状态。\n创新点：实现了高效的**“长上下文视觉推理”**。由于潜在 Token 占用的显存远小于生成图像，Mirage 能够支持更长的推理步骤和更复杂的视觉逻辑操作。","423-latent-visual-reasoning-lvr-cvpreccv-2025-context#4.2.3 Latent Visual Reasoning (LVR) (CVPR/ECCV 2025 Context)":"论文标题：Latent Visual Reasoning\n核心问题：现有的 VLM 往往在特征提取阶段就丢失了与问题相关的细微特征。\n方法论：LVR 引入了**“潜在重放”（Latent Replay）机制。利用强化学习（GRPO），模型在推理过程中学会\"回溯\"并重新激活与当前推理步骤最相关的视觉潜在特征。它实际上是在潜在空间中进行了一种“注意力重分配”**。\n创新点：提出了 **VLPO（Visual-Latent Policy Optimization）**算法，证明了可以通过强化学习直接优化潜在空间的推理路径，而无需显式的监督信号。","424-slot-vlm-neurips-2024#4.2.4 Slot-VLM (NeurIPS 2024)":"论文标题：Slot-VLM: Object-Centric Learning with Slot Attention\n核心问题：标准 Transformer 的注意力机制是全局的、纠缠的，难以分离独立的物体概念，导致数数或关系判断出错。\n方法论：引入了**“Slot Attention\"机制**。图像特征被强制分解为一组独立的**“Slot\"向量**，每个 Slot 代表一个物体或实体。推理过程变成了对这些 Slot 的操作（如比较 Slot A 和 Slot B 的属性）。\n创新点：将**“以物体为中心”（Object-Centric）**的归纳偏置引入 VLM。这使得模型在物理推理、物体计数等任务上具有了类似于符号系统的鲁棒性，同时保留了神经网络的灵活性。","425-coconut-colm-2025--arxiv-2024#4.2.5 Coconut (COLM 2025 / ArXiv 2024)":"论文标题：Training Large Language Models to Reason in a Continuous Latent Space\n核心问题：语言空间的推理往往会过早塌缩（Collapse）到一个确定的路径，限制了**广度优先搜索（BFS）**的能力。\n方法论：Coconut 提出将 LLM 的最后一个**隐藏状态（Hidden State）直接作为下一个时间步的输入，而不是解码为离散的词。这种“连续思维”**允许模型在潜在空间中同时探索多个推理分支（Superposition of thoughts）。\n创新点：虽然最初针对 LLM 提出，但其理论框架是 2025 年多个视觉潜在推理工作（如 VChain）的基石。它从理论上证明了连续空间推理比离散空间推理具有更高的表达能力上限。","5-范式-iv主动感知与强化视觉搜索#5. 范式 IV：主动感知与强化视觉搜索":"这一范式将 VLM 从被动的观察者转变为主动的视觉智能体（Visual Agent）。人类在观察复杂场景时，眼球会不断进行扫视（Saccade）和注视（Fixation），通过主动改变关注点来获取信息。该范式试图在 VLM 中复现这一机制，通过\"动作\"来弥补\"分辨率\"和\"注意力\"的不足。","51-核心机制感知即行动#5.1 核心机制：感知即行动":"推理过程被建模为一个马尔可夫决策过程（MDP）。模型输出的不仅仅是答案，还有一系列感知动作指令：, , , 。这些动作改变了模型的输入，从而形成了动态的推理链。","52-里程碑文献深度解析#5.2 里程碑文献深度解析":"","521-deepeyes-arxiv-2025--likely-cvprneurips-2025#5.2.1 DeepEyes (ArXiv 2025 / Likely CVPR/NeurIPS 2025)":"论文标题：DeepEyes: Incentivizing “Thinking with Images” via Reinforcement Learning\n核心问题：现有的主动感知模型往往需要大量标注数据（如\"先看这里，再看那里”），不仅昂贵且难以覆盖所有场景。\n方法论：DeepEyes 采用了端到端的强化学习（RL），具体使用了 GRPO 算法。模型没有被教导如何看，而是仅在最终答案正确时获得奖励。在数万次训练迭代中，模型**自发涌现（Emerged）**出了类似人类的视觉策略：面对小物体会主动\"放大”，面对大场景会\"扫视”。\n创新点：它是视觉领域的**“DeepSeek-R1 时刻”**。证明了复杂的视觉推理策略（System 2）可以通过简单的结果奖励从零训练出来，而不需要模仿人类的中间步骤。","522-visual-cot-neurips-2024-spotlight#5.2.2 Visual CoT (NeurIPS 2024 Spotlight)":"论文标题：Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning\n核心问题：缺乏一个标准化的基准来衡量模型\"看图说话\"过程中的中间对齐能力。\n方法论：作者构建了一个大规模数据集，其中的推理链不仅包含文本，还包含边界框（Bounding Boxes）。模型被训练执行**“多轮聚焦\"策略**：Step 1 预测感兴趣区域（RoI）-\u003e Step 2 裁剪该区域 -\u003e Step 3 基于裁剪图回答。\n创新点：提出了**“可追踪的视觉推理”（Traceable Visual Reasoning）**。通过强制模型每一步都输出坐标，极大地减少了幻觉，并为错误分析提供了精确的依据。","523-ferret-iclr-2024#5.2.3 Ferret (ICLR 2024)":"论文标题：Ferret: Refer and Ground Anything Anywhere at Any Granularity\n核心问题：传统的 Bounding Box 过于粗糙，无法处理不规则形状（如蛇、线缆）或极细小的物体。\n方法论：Ferret 引入了**“混合区域编码器”（Hybrid Region Encoder）。它允许模型接受点、框、不规则形状（Sketch）作为输入，并输出精细的区域掩码。这赋予了模型“指哪打哪”**的精细感知能力。\n创新点：将**“视觉定位”（Grounding）**的分辨率提升到了一个新的层级，是主动感知范式中处理细节信息的基石技术。","524-shikra-iccv-2023--arxiv-2023#5.2.4 Shikra (ICCV 2023 / ArXiv 2023)":"论文标题：Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic\n核心问题：在 2023 年之前，VLM 的定位能力和对话能力往往是分离的。\n方法论：Shikra 是最早将空间坐标 [x, y] 离散化为自然语言 Token 并参与自回归生成的模型之一。它证明了模型可以像学习外语一样学习**“位置语言”**，从而在对话中流畅地进行指代（Referential Dialogue）。\n创新点：奠定了**“统一建模”**的基础。后来的 Visual CoT 和 DeepEyes 的坐标输出机制很大程度上继承了 Shikra 的设计哲学。","525-look-twice-before-you-answer-cvpr-2024-context#5.2.5 Look Twice Before You Answer (CVPR 2024 Context)":"论文标题：Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation\n核心问题：VLM 的**“遗忘\"现象**——随着文本生成越来越长，模型逐渐忘记了最初看到的图像，开始胡编乱造。\n方法论：提出了一种**“视觉回溯”（Visual Retracing）机制。在生成文本的过程中，模型会动态计算当前文本 Token 与原始图像特征的注意力权重。如果发现注意力发散，模型会强制“回头看”**，重新加权图像特征。\n创新点：将主动感知应用到了**“时间/记忆\"维度**。它不是空间上的移动，而是注意力在时间轴上的回溯，是解决长文本幻觉的关键技术。","6-范式-v结构化与组合式视觉推理#6. 范式 V：结构化与组合式视觉推理":"这一范式认为，视觉推理的本质是结构化（Structure）和组合性（Compositionality）。图像不应被视为一堆像素或 Token，而应被解析为对象、属性和关系的集合。该范式致力于在推理过程中显式地构建或利用这种结构化表征（如场景图、布局树）。","61-核心机制从像素到图谱#6.1 核心机制：从像素到图谱":"推理过程包含显式的结构化步骤：Image -\u003e Scene Graph Generation -\u003e Symbolic Reasoning -\u003e Answer。这种方法将**感知（Perception）与推理（Reasoning）**解耦，使得推理过程更加鲁棒和可控。","62-里程碑文献深度解析#6.2 里程碑文献深度解析":"","621-compositional-chain-of-thought-ccot-cvpr-2024#6.2.1 Compositional Chain-of-Thought (CCoT) (CVPR 2024)":"论文标题：Compositional Chain-of-Thought Prompting for Large Multimodal Models\n核心问题：VLM 经常犯**“属性绑定错误”（Attribute Binding Error）**，例如把穿红衣服的人看成穿蓝衣服，或者混淆两个物体的动作。\n方法论：CCoT 强制模型分两步走：首先生成一个场景图（Scene Graph），明确列出所有对象节点（Nodes）及其属性和关系边（Edges）；然后基于这个场景图生成答案。\n创新点：将结构化数据作为 CoT 的中间模态。实验表明，显式的结构化描述迫使模型理清对象关系，大幅减少了组合性错误。","622-pixellm-cvpr-2024#6.2.2 PixelLM (CVPR 2024)":"论文标题：PixelLM: Pixel Reasoning with Large Multimodal Model\n核心问题：许多推理任务需要像素级的理解（例如\"这两个重叠的物体谁在上面？\"），传统的 Box 无法表达这种遮挡关系。\n方法论：PixelLM 在 LLM 输出端挂载了一个轻量级的像素解码器（Pixel Decoder）。LLM 能够输出特定的**“分割 Token”**，这些 Token 解码后形成精细的物体掩码（Masks）。推理过程基于这些掩码的拓扑关系进行。\n创新点：实现了**“像素级思维”**。它证明了 VLM 的推理粒度可以下沉到像素级别，为处理复杂的物理接触和遮挡关系提供了可能。","623-osprey-cvpr-2024#6.2.3 Osprey (CVPR 2024)":"论文标题：Osprey: Pixel Understanding with Visual Instruction Tuning\n核心问题：如何让用户对图像中任意不规则区域进行提问？\n方法论：Osprey 提出了一种**“掩码感知视觉提取器”（Mask-Aware Visual Extractor）**。它不仅接受图像，还接受一个掩码作为输入，能够提取该掩码覆盖区域的精细视觉特征。这使得推理可以针对图像的任何局部细节进行。\n创新点：实现了细粒度的**“交互式推理”**。它不仅是模型看图，更是用户通过 Point/Mask 与模型进行精准的视觉对话。","624-sphinx-neurips-2024-context#6.2.4 Sphinx (NeurIPS 2024 Context)":"论文标题：Sphinx / ReasonBench Context\n核心问题：抽象视觉推理（如瑞文智商测试、图表逻辑）是 VLM 的短板。\n方法论：Sphinx 通过混合**高分辨率主动缩放（Active Scaling）**和多样化的视觉任务训练，增强了模型对抽象几何结构的感知能力。配合 ReasonBench 基准，它展示了结构化数据训练对提升逻辑推理的重要性。\n创新点：探索了 VLM 在纯抽象视觉逻辑上的边界，证明了通过丰富的数据结构（如合成图表、几何题）可以提升模型的通用推理智商。","625-argus-cvpr-2025#6.2.5 Argus (CVPR 2025)":"论文标题：Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought\n核心问题：**视觉定位（Grounding）与文本推理（Reasoning）**往往是割裂的。\n方法论：Argus 提出了一种以视觉为中心的推理框架，强制模型在生成每一个推理步骤时，都要同步输出对应的视觉证据区域。这不仅仅是 Visual CoT 的应用，更是一种由于架构设计带来的强对齐约束。\n创新点：强调了**“对齐即推理”**。只有当模型能够准确指出\"我为什么这么说\"的视觉依据时，我们才能认为它真正具备了视觉推理能力。","7-综合对比与未来展望#7. 综合对比与未来展望":"","71-五大范式横向对比表#7.1 五大范式横向对比表":"范式维度 范式 I：工具中介 范式 II：显式生成 范式 III：潜在推理 范式 IV：主动感知 范式 V：结构化组合 核心隐喻 外包给计算器 脑海中的草稿纸 直觉与内隐思考 眼动与探索 逻辑大纲与图谱 中间模态 代码 (Python)、API 像素图像 (Images) 连续向量 (Vectors) 动作指令 (Actions) 场景图、掩码 (Graphs) 优势 逻辑严密，可验证，计算精准 处理空间变换、反事实推理极强 信息密度最高，推理效率高 模拟人类行为，适应大图/视频 解决复杂关系，鲁棒性高 劣势 依赖工具库，非端到端，慢 计算开销极大，生成误差累积 黑盒不可解释，调试困难 训练难度大 (RL)，收敛慢 依赖解析器，灵活性受限 代表作 Visual Sketchpad, ViperGPT MVoT, ImagineNav CoVT, DeepSketcher DeepEyes, Ferret CCoT, PixelLM 适用场景 数学几何、精确计数、测量 导航规划、物理预测、拼图 通用视觉问答、细粒度感知 极高分辨率图像、监控视频 复杂场景理解、关系推理","72-技术融合的趋势迈向多模态-agi#7.2 技术融合的趋势：迈向多模态 AGI":"通过对 2025 年最新文献的分析，我们可以清晰地看到不同范式正在发生融合：\nRL 的全面渗透：DeepEyes 和 SpatialDreamer 的成功表明，强化学习正在成为训练 System 2 视觉推理的标准范式。未来的 VLM 将不再仅仅依靠监督微调（SFT），而是通过 RL 自我探索出最优的\"看图策略”。\n潜在与显式的结合：DeepSketcher 展示了可以在潜在空间中进行\"显式\"的操作（如编辑 Embedding）。未来的模型可能会在潜在空间中进行高效推理，仅在需要验证时才\"解码\"为像素图像。\n从感知到行动：随着**具身智能（Embodied AI）**的兴起，Paradigm IV（主动感知）将变得越来越重要。视觉推理将不再局限于静态图像，而是延伸到对环境的主动探索和交互中。","8-结语#8. 结语":"从**“Thinking about Images”到“Thinking with Images”**的转变，标志着多模态大模型正在跨越感知的门槛，迈向认知的殿堂。无论是通过代码外化思维、通过生成模拟未来、通过向量内隐推演，还是通过动作主动探索，这些新兴范式都在试图弥合语言与视觉之间的鸿沟。\n本次调研表明，单纯的文本 CoT 已不足以支撑下一代 VLM 的发展。未来的多模态模型必将是混合架构的——它拥有类似 System 1 的快速感知编码器，同时也拥有由 RL 训练而成的 System 2 推理引擎，能够灵活地调用工具、生成意象、在潜在空间深思熟虑，并像人类一样主动去\"看\"清这个世界。","o3-范式-think-with-images#O3 范式 Think with images":"一种方法是 OpenAI-O3 范式的 Think with images，是指在推理过程中通过视觉工具（如放大、裁剪、旋转、绘制辅助线、草图）来进行辅助思考，从而将视觉操作和操作后图像融入思维链，目的是为了让模型可以更深入地理解图像内容。\n但从这样的角度，本质并没有变，思维链还是纯文本驱动，也就是得到视觉信息不是模型生成的，只是借助工具得到的。模态鸿沟仍然存在，即将视觉信息落地为文本后再进行推理，阻碍了模型对视觉特征的精准捕捉。","thinking-with-images#Thinking with Images":"**多模态大语言模型（Multimodal Large Language Models, MLLMs）正处于从“感知智能\"向\"推理智能”跃迁的关键转折点。尽管早期的视觉-语言模型（VLMs）如 CLIP 或 LLaVA 成功实现了图像与文本的语义对齐，但它们在本质上仍遵循“Thinking about Images”的范式——即迅速将视觉信号转化为文本特征，随后完全依赖语言模型的文本思维链（Text-based Chain-of-Thought, CoT）**进行推理。\n这种**“模态早融合”与“推理纯文本化”的架构，导致了严重的模态鸿沟（Modality Gap）**：在处理需要空间几何感知、细粒度视觉验证或多步视觉逻辑推演的任务时，模型往往因丢失视觉细节而产生幻觉。\n为了克服这一局限，学术界与工业界正在积极探索**“Thinking with Images”**的新范式，即让视觉模态深度参与推理的中间过程，甚至主导推理链条。本报告旨在打破现有的二元分类局限（即简单的\"工具调用\"与\"潜在推理\"之分），基于对 2023 年至 2025 年间发表于 CVPR、ICCV、NeurIPS、ICLR、ICML 等顶级会议的 50 余篇里程碑文献的详尽调研，构建了一个包含五大范式的全新分类体系：\n工具中介与程序化视觉推理 显式生成意象与心智模拟 潜在空间视觉推理与连续思维 主动感知与强化视觉搜索 结构化与组合式视觉推理 本报告将深入剖析每一范式的核心机制、技术演进路径及代表性工作，揭示 MLLMs 如何通过引入视觉中间态（Visual Intermediates）——无论是代码、像素、潜在向量、动作序列还是结构化图谱——来模拟人类的**“系统 2\"慢思考能力**，从而实现真正的视觉通用智能。","vlm-think-with-images-必读论文#VLM-Think with images 必读论文":"VLM-Think with images必读论文\nVLM 尽管具备多模态输入的能力，但在推理过程中完全依赖纯文本的形式进行思考，无论是对视觉内容进行描述，还是输出语言化的推理依据，其内部推理路径始终局限于文本上。然而仅通过文本进行多模态推理，并不总是最有效的策略，尤其对于那些高度依赖视觉信息的任务。","学习的视觉-token-表示有以下几种#学习的视觉 token 表示有以下几种":"VIT 特征 VIT 投影特征 模型中间特征 VQVAE 中间离散 token","总结使用工具的方法#总结使用工具的方法":"设置工作流固定使用工具\n固定的一步，调用固定工具。 模型自主决定使用什么工具\n在哪一步，是直接输出答案，还是调用工具。如果使用工具，自主决定使用什么工具。 固定的工具代码\n代码是设置好的，模型只需要预测输入参数。所支持的操作空间相对受限，也依赖精确的参数输入。 模型生成工具代码\n代码由模型生成，支持广泛的视觉操作。","潜在视觉推理latent-visual-cot#潜在视觉推理（Latent Visual CoT）":"所以另一种方法是潜在视觉推理，模型不再调用外部工具得到视觉信息，而是将视觉信息内化、表征化，也就是模型直接生成视觉 token。模型需要学会运用视觉 token 进行推理，而这些视觉 token 通常包括与分割、深度、边缘、特征等视觉线索。\n训练模型生成视觉 token，就需要加入视觉重建任务。","相关论文#相关论文":"Visual SKETCHPAD: Sketching as a Visual Chain of Thought for Multimodal Language Models V-Thinker: Interactive Thinking with Images DeepEyes: Incentivizing “Thinking with Images” via Reinforcement Learning Thyme: Think Beyond Images Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning From Illusion to Intention- Visual Rationale Learning for Vision-Language Reasoning","相关论文-1#相关论文":"Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens Perception tokens enhance visual reasoning in multimodal language models DeepSketcher- Internalizing Visual Manipulation for Multimodal Reasoning Machine mental imagery: Empower multimodal reasoning with latent visual tokens Latent Visual Reasoning","视觉-token-重建的-label-有以下几种#视觉 token 重建的 label 有以下几种":"引入辅助模型 引入辅助图像 引入原图 ROI 边界框"},"title":"通过图像推理 VLM"},"/blog/2025/vln-matrix/":{"data":{"vln-正交分析法寻找创新点#VLN 正交分析法寻找创新点":"VLN 正交分析法寻找创新点","框架一表征-推理矩阵-representation-reasoning-matrix#框架一：【表征-推理】矩阵 (Representation-Reasoning Matrix)":"核心逻辑：解决\"机器人怎么看世界\"和\"机器人怎么做决策\"的匹配问题。\n纵轴：推理范式 (Reasoning) \\ 横轴：环境表征 (Representation)\n推理范式 \\ 环境表征 A. 纯视觉流 B. 2D 语义地图 C. 3D 场景图 D. 拓扑/文本图 1. End-to-End RL / IL 已拥挤 常见 较少 较少 2. Modular + LLM Prompting 难点 拥挤 热门 热门 3. System 1 + System 2 空白/机会 少见 少见 空白/机会 4. World Model / Generative 前沿 空白/机会 空白/机会 空白 潜在创新点挖掘：\nGap 1 (A-3): 目前 End-to-End 模型（如 NaVid）反应快但缺乏长程逻辑，而 LLM 反应慢。能否设计一个机制，平时用小模型看视频流走路（System 1），遇到\"迷路\"或\"歧义\"时，动态唤醒 LLM 分析当前视频帧（System 2）？\nGap 2 (C-4): 现在的 Scene Graph 都是用来做当前状态的 Prompt。能否基于 Scene Graph 做\"世界模型\"？ 即：让 LLM 预测\"如果我向左走，场景图会变成什么样？\"，从而在图空间里做 Model-Based Planning，而不是由 LLM 直接瞎猜。","框架三多模态融合-时空矩阵-fusion-spatiotemporal-matrix#框架三：【多模态融合-时空】矩阵 (Fusion-Spatiotemporal Matrix)":"核心逻辑：针对 CoRL/ICRA 等机器人会议，关注\"具体怎么融合特征\"。\n纵轴：融合阶段 (Fusion Stage) \\ 横轴：时间维处理 (Temporal)\n融合阶段 \\ 时间维处理 A. Frame-wise (单帧) B. Feature Buffer C. Explicit Map D. Neural Memory 1. Early Fusion 基础 计算量大 VLMaps 少见 2. Late Fusion CLIP-Nav NaVid 常见 IVLN 3. LLM-in-the-loop GPT-4V Nav 少见 UniGoal 空白/机会 4. Cross-Attention (Query-based) 传统 Transformer 常见 少见 空白/机会 潜在创新点挖掘：\nGap 5 (D-3): 目前的 LLM 导航要么看单张图，要么看 3D 场景图。很少有结合 Mamba 或 SSM (State Space Models) 的工作。 创新点：利用 Mamba 这种长序列处理能力极强的架构，作为 VLN 的\"隐式记忆体\"，替代显式的地图构建，实现无图但有长记忆的导航（Mamba-VLN）。","框架二反馈-修正矩阵-feedback-correction-matrix#框架二：【反馈-修正】矩阵 (Feedback-Correction Matrix)":"核心逻辑：针对 2024 年后的趋势——从\"如何走对\"转向\"走错了如何修正\"。\n纵轴：修正机制 (Correction) \\ 横轴：错误源 (Source of Error)\n修正机制 \\ 错误源 A. 感知幻觉 B. 空间迷失 C. 指令歧义 D. 动态障碍/变化 1. Passive (被动重规划) 传统方法 传统方法 无解 传统 DWA/TEB 2. Active Perception (主动探索) 少见 少见 N/A 常见 3. Dialogue / Interaction 空白/机会 空白 已拥挤 空白 4. Self-Reflexion (自省) 热门 空白/机会 少见 空白 潜在创新点挖掘：\nGap 3 (B-4): 现在的 Self-Reflexion 大多是在想\"我是不是理解错指令了\"。很少有工作做\"空间自省\"——即 LLM 结合历史轨迹图，反思\"我现在的视觉观测和我记忆中的地图不一致，我是不是已经走到错误的房间了？\"（Spatial Consistency Check via LLM）。\nGap 4 (A-3): 当 VLM 觉得前面是\"椅子\"但又不确定时（Confidence score 低），目前的做法是硬着头皮走。创新点可以是：主动发起一轮对话确认，或者主动移动相机去验证（Active Perception for VLM uncertainty）。"},"title":"VLN 正交分析法寻找创新点"},"/blog/2025/vln-survey/":{"data":{"history-and-memory#History and Memory":"与视觉问答（Visual Question Answering, VQA）、视觉蕴含（Visual Entailment）等其他视觉-语言任务不同，VLN 智能体需将过去动作与观测的历史信息融入当前步骤的输入中以决策动作，而非仅依赖单一步骤的图像与文本。在 VLN 中应用基础模型之前，研究者通常采用 LSTM 隐藏状态作为支持智能体导航决策的隐式记忆，并进一步设计不同的注意力机制或辅助任务，以提升编码历史与指令的对齐程度。\n目前已有多种基于基础模型的导航历史编码技术，核心可分为两类：\n（1）基于令牌更新或序列建模的编码\n多模态 Transformer 初始化：以基于域内指令-轨迹数据预训练的模型（如 Prevalent）为基础，构建多模态 Transformer，将编码后的指令与导航历史作为输入以实现决策。\n循环状态令牌编码：部分方法通过循环更新的状态令牌编码导航历史。例如，利用上一步的单个 [CLS] 令牌编码历史信息；或设计变长记忆框架，将过去步骤的多个动作激活值存储在记忆库中，作为历史编码。但这类方法需逐步骤更新令牌，难以高效检索导航轨迹中任意步骤的历史编码，限制了预训练的可扩展性。\n全景与历史分层编码：另一类方法直接通过多模态 Transformer 将导航历史编码为序列。例如，对轨迹中每一步的单视角图像进行编码；或进一步提出 “全景编码器 + 历史编码器” 的分层设计 —— 全景编码器处理每一时间步的全景视觉观测，历史编码器则编码所有过往观测。这种设计可分离全景视图中的空间关系与导航历史中跨全景的时间动态性，且无需依赖循环更新的状态令牌，便于基于指令-路径对进行高效、大规模的预训练。后续研究分别用 “图像均值池化” 或 “前视图像编码” 替代全景编码器，均保持了良好的导航性能。\n（2）基于 LLM 的文本化历史编码\n随着基于 LLM 的导航智能体兴起，“将视觉环境转换为文本描述” 成为主流趋势。此时导航历史被编码为 “图像描述序列 + 相对空间信息”（如朝向、高度、距离）的组合。例如，HELPER 设计了 “语言-程序对” 的外部记忆，通过检索增强的 LLM 提示，将人类与机器人的自由形式对话解析为动作程序。\n另一类研究通过融入图信息增强导航历史建模，核心思路是利用结构化图表征环境几何与空间关系：\n拓扑图与结构化编码：部分方法采用结构化 Transformer 编码器捕捉环境中的几何线索。除编码中使用的拓扑图外，许多研究还将俯视图信息（如网格图、语义图、局部度量图）与局部邻域图纳入导航过程中的观测历史建模。\nLLM 与图的结合：近期基于 LLM 的导航智能体在记忆构建中引入了创新性的图应用。例如，提出一种基于地图引导的 GPT 智能体，利用语言化形式的地图存储和管理拓扑图信息；MC-GPT 则将拓扑图作为记忆结构，记录视角、物体及其空间关系的信息。","human-model-interpreting-and-communicating-with-humans#Human Model: Interpreting and Communicating with Humans":"除学习和建模世界外，VLN 智能体还需一个 “人类模型” —— 该模型能根据具体场景理解人类提供的自然语言指令，从而完成导航任务。这一过程主要面临两大挑战：一是解决指令的模糊性，二是实现 “接地指令” 在不同视觉环境中的泛化。","survey#Survey":"研发能够与人类及其周边环境进行交互的具身智能体（embodied agents），是人工智能（AI）领域长期以来的核心目标之一。这类 AI 系统在现实世界中具有巨大的应用潜力，可作为日常生活中的多功能助手，例如家用机器人、自动驾驶汽车以及个人助手。推动这一研究方向的一个正式问题设定是视觉-语言导航（Vision-and-Language Navigation, VLN）—— 这是一项多模态协作任务，要求智能体遵循人类指令、探索三维（3D）环境，并在存在各类歧义的场景下开展情境化通信。多年来，研究者已在照片级真实感模拟器和真实环境中对 VLN 展开探索，由此形成了一系列基准数据集，每个数据集的问题表述略有不同。\n人类（Human）：给出指令 “穿过客厅区域进入走廊。右转，然后再右转并进入房间”；在智能体询问 “左边的房间还是前面的房间？” 时回复 “左边”。 物理环境（Physical Environment）：智能体感知的视觉场景。 VLN 智能体（VLN Agent）：接收指令后进行 “接地与推理”（Grounding \u0026 Reasoning）、“规划”（Planning）、“对话”（Dialogue），执行 “导航动作”（Navigation Execution），并生成语言响应（Language Response）；过程中可能产生疑问，如 “…… 进入房间。左边？右边？““左边的房间还是前面的房间？\"。 核心模块：世界模型（World Model）、人类模型（Human Model），分别支撑智能体的环境理解与人类意图解读。 其仓库提到了一些工作内容，但是不全。","vln-agent-learning-an-embodied-agent-for-reasoning-and-planning#VLN Agent: Learning an Embodied Agent for Reasoning and Planning":"尽管世界模型与人类模型为智能体赋予了视觉与语言理解能力，但 VLN 智能体仍需培养具身推理（embodied reasoning）与规划能力，以支撑自身决策。从这一角度出发，我们将探讨两大挑战：接地与推理、规划；同时还将研究 “直接以基础模型作为 VLN 智能体核心骨干” 的方法。","vln-系列#VLN 系列":"VLN 系列从 Poing Navigation 到 Object Navigation，这也太难了，找Idea真的太难了。\n然后秋冬学期的一半，也就是大四上的一半已经过去了，马上就要寒假了，寒假做什么，实习还是论文？真能憋出论文吗？","world-model-learning-and-representing-the-visual-environments#World Model: Learning and Representing the Visual Environments":"世界模型能够帮助 VLN 智能体理解周边环境、预测自身动作对世界状态的改变，并使自身感知与动作与语言指令对齐。现有研究中，学习世界模型主要面临两大挑战：一是将当前任务段内的视觉观测历史编码为记忆，二是实现对未见过环境的泛化。","三大解决方案#三大解决方案":"","世界模型从二维2d到三维3d#世界模型：从二维（2D）到三维（3D）":"构建有效的世界表征是具身感知、推理与规划领域的核心研究主题。VLN 本质上是一项 3D 任务 —— 智能体需以 3D 形式感知真实世界环境。尽管当前研究已能通过强大的通用 2D 表征描述世界，但这类表征无法充分支持 3D 场景下的空间语言理解。\n以往研究已提出多种显式 3D 表征方式，包括各类语义同步定位与地图构建（semantic SLAM）、体素表征、深度信息、鸟瞰图（Bird’s-Eye-View）表征（如网格图）及局部度量图。但这些表征存在局限：它们将物体集合限定为 “封闭集合”，无法适配自然语言对应的 “开放词汇场景”。\n部分研究尝试构建 “可查询的地图/场景表征”，例如将 CLIP 提取的多视角图像特征整合到 3D 体素网格或俯视特征图中，或利用场景图表征空间关系。然而，“如何将大规模数据中学习到的 3D 表征适配于 VLN 智能体，以提升其 3D 环境感知能力” 仍是待探索的问题。近期兴起的 3D 基础模型 —— 包括 3D 重建模型 与 3D 多模态表征模型 —— 有望为 VLN 领域提供关键支撑。","人类模型从指令到对话#人类模型：从指令到对话":"以往研究多采用 “说话者-倾听者范式” 或 “受限问答对话” —— 这类方式仅允许智能体主动请求帮助。近年来，涌现出一批以 “开放式对话指令” 为核心的新基准数据集，支持智能体在模糊或困惑场景下进行完全自由形式的通信，包括提问、提议、解释、建议、澄清与协商。\n然而，当前方法仍依赖 “基于规则的对话模板” 应对上述复杂场景，即便部分方法包含基础模型组件，也未充分发挥其能力。通过 “人类对话数据 + 仿真导航视频” 对视频-语言模型进行对话调优，使模型在导航过程中具备更强的对话生成能力。未来研究需重点关注两方面：一是将基础模型融入 “情境化任务导向对话管理”；二是探索现有基础模型在 “任务导向对话” 中的应用潜力。","仓库论文链接#仓库论文链接":"说明：本表格按分类和子分类组织所有 VLN 相关论文，便于浏览和筛选。分类包括：Survey（综述）、World Model（世界模型）、Human Model（人类模型）、VLN Agent（VLN 智能体）、Behavior Analysis（行为分析）。\n分类 子分类 标题 会议 年份 代码 Survey - Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions ACL 2022 Github - Visual language navigation: A survey and open challenges - 2023 - - Vision-Language Navigation: A Survey and Taxonomy - 2021 - World Model - MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation ACL 2025 - - VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation AAAI 2024 - - Volumetric Environment Representation for Vision-Language Navigation CVPR 2024 Github - Vision Language Navigation with Knowledge-driven Environmental Dreamer IJCAI 2023 - - Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation NeurIPS 2023 Github - Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation NeurIPS 2023 Github - Simple and Effective Synthesis of Indoor 3D Scenes AAAI 2023 Github - Learning Navigational Visual Representations with Semantic Map Supervision ICCV 2023 - - Learning vision-and-language navigation from youtube videos ICCV 2023 Github - GridMM: Grid Memory Map for Vision-and-Language Navigation ICCV 2023 Github - BEVBert: Multimodal Map Pre-training for Language-guided Navigation ICCV 2023 Github - Scaling Data Generation in Vision-and-Language Navigation ICCV 2023 Github - A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning CVPR 2023 Github - EnvEdit: Environment Editing for Vision-and-Language Navigation CVPR 2022 Github - Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation ECCV 2022 Github - How Much Can CLIP Benefit Vision-and-Language Tasks? ICLR 2022 Github - Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation CVPR 2022 Github - History Aware Multimodal Transformer for Vision-and-Language Navigation NeurIPS 2021 Github - Pathdreamer: A World Model for Indoor Navigation ICCV 2021 - - Episodic Transformer for Vision-and-Language Navigation ICCV 2021 - - Airbert: In-domain Pretraining for Vision-and-Language Navigation ICCV 2021 Github - Vision-Language Navigation with Random Environmental Mixup ICCV 2021 Github Human Model - Scene Map-based Prompt Tuning for Navigation Instruction Generation CVPR 2025 - - NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM ACL 2025 Github - Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel ICLR 2025 Github - Navigation Instruction Generation with BEV Perception and Large Language Models ECCV 2024 Github - Controllable Navigation Instruction Generation with Chain of Thought Prompting ECCV 2024 Github - Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation ACL 2024 Github - Correctable Landmark Discovery via Large Models for Vision-Language Navigation TPAMI 2024 Github - NavHint: Vision and Language Navigation Agent with a Hint Generator EACL 2024 Github - Learning to Follow and Generate Instructions for Language-Capable Navigation TPAMI 2023 - - Lana: A Language-Capable Navigator for Instruction Following and Generation CVPR 2023 Github - KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation CVPR 2023 Github - PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation MM 2023 - - CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation - 2023 - - VLN-Trans: Translator for the Vision and Language Navigation Agent ACL 2023 Github - Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration ACL 2022 Github - Less is More: Generating Grounded Navigation Instructions from Landmarks CVPR 2022 Github - On the Evaluation of Vision-and-Language Navigation Instructions EACL 2021 - - Do As I Can, Not As I Say:Grounding Language in Robotic Affordances - - Github VLN Agent - SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts ICCV 2025 Github - MiniVLN: Efficient Vision-and-Language Navigation byProgressive Knowledge Distillation ICRA 2024 - - Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation AAAI 2023 - - Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation ICCV 2023 Github - Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation ICCV 2023 Github - Bird’s-Eye-View Scene Graph for Vision-Language Navigation ICCV 2023 - - Masked Path Modeling for Vision-and-Language Navigation EMNLP Findings 2023 - - Improving Vision-and-Language Navigation by Generating Future-View Image Semantics CVPR 2023 Github - HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation TPAMI 2023 - - Target-Driven Structured Transformer Planner for Vision-Language Navigation MM 2022 Github - HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation CVPR 2022 Github - LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation COLING 2022 Github - Scene-Intuitive Agent for Remote Embodied Visual Grounding CVPR 2021 - - SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation NeurIPS 2021 - - The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation ICCV 2021 Github - VLN BERT: A Recurrent Vision-and-Language BERT for Navigation CVPR 2021 Github - Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training CVPR 2020 Github VLN-CE MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming ICCV 2025 - JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation Arxiv 2025 Github NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments ICCV 2025 Github Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation AAAI 2025 - Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation CVPR 2024 Github ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments PAMI 2024 Github Narrowing the Gap between Vision and Action in Navigation MM 2024 - Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation CVPR 2022 Github Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments ECCV 2020 Github LLM/VLM (Zero-shot) LLM as Copilot for Coarse-grained Vision-and-Language Navigation ECCV 2024 - Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions ICRA 2024 Github MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation ACL 2024 Github MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains - 2024 - InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment - 2024 Github NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models AAAI 2024 Github March in Chat: Interactive Prompting for Remote Embodied Referring Expression ICCV 2023 Github Vision and Language Navigation in the Real World via Online Visual Language Mapping - 2023 - A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models NeurIPS Workshop 2023 - CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation - 2022 - LLM/VLM (Fine-tuning) EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation Arxiv 2025 Github LangNav: Language as a Perceptual Representation for Navigation NACCL Findings 2024 Github NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning - 2024 Github Towards Learning a Generalist Model for Embodied Navigation CVPR 2024 Github NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models ECCV 2024 Github NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation RSS 2024 Github Behavior Analysis - Do Visual Imaginations Improve Vision-and-Language Navigation Agents? CVPR 2025 - Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation EMNLP Findings 2024 Github Behavioral Analysis of Vision-and-Language Navigation Agents CVPR 2023 Github Diagnosing Vision-and-Language Navigation: What Really Matters NACCL 2022 Github","作为-vln-智能体的基础模型#作为 VLN 智能体的基础模型":"主流方案以 “单流 VL 模型” 作为 VLN 智能体的核心结构：这类模型在每个时间步同时处理 “语言、视觉、历史令牌（token）” 输入，通过对跨模态令牌的自注意力运算捕捉 “文本-视觉对应关系”，进而推断动作概率。\n在零样本 VLN 场景中，CLIP-NAV 利用 CLIP 获取 “描述目标物体的自然语言指称表达式”，实现序贯导航决策。\n此外，VLN-CE（连续环境 VLN）智能体与 VLN-DE（离散环境 VLN）智能体的核心差异在于动作空间：前者在连续环境中执行低层控制，而非后者 “基于图的高层视角选择动作”。尽管早期研究采用 LSTM 推断低层动作，但 “航点预测器”（waypoint predictor）的引入实现了 “从 DE 到 CE 的方法迁移” —— 所有这些方法均通过航点预测器获取 “局部可导航性图”，使 DE 场景中的基础模型能适配连续环境。具体而言，航点检测过程主要通过 “视觉观测”（如全景 RGBD 图像），从智能体当前位置预测 “可导航的相邻候选航点” 作为潜在目标，再由智能体选择其一作为当前目的地。\nLLM 具备强大的推理能力与世界语义抽象能力，且在 “未知大规模环境” 中表现出优异的泛化性 —— 因此，近期 VLN 研究开始直接将 LLM 作为智能体执行导航任务。其核心流程为：将视觉观测转换为文本描述，与指令一同输入 LLM，由 LLM 完成动作预测。\n代表性创新方案包括：\nNavGPT 与 MapGPT：验证了零样本导航的可行性 —— NavGPT 利用 GPT-4 自主生成动作，MapGPT 将拓扑图转换为全局探索提示\nDiscussNav：拓展上述思路，部署 “多领域专用 VLN 专家”（包括指令分析专家、视觉感知专家、完成度估计专家、决策测试专家），减少导航任务中的人工参与：通过将任务分配给专用智能体，减轻单一模型负担，实现 “任务专属优化处理”，并借助多大型模型的协同优势提升鲁棒性、透明度与整体性能\nMC-GPT：利用 “记忆拓扑图” 与 “人类导航示例” 丰富策略多样性\nInstructNav：将导航拆解为子任务，并结合 “多源价值图” 实现高效执行\n与 “零样本使用” 不同，部分研究通过 “微调 LLM”，使其能更有效地处理具身导航任务。另有研究融入 “思维链”（Chain-of-Thought, CoT）推理机制提升推理过程，例如 Nav-CoT 将 LLM 转化为 “世界模型与导航推理智能体”，通过模拟未来环境简化决策 —— 这一方案证实了 “微调语言模型” 在仿真与真实场景中的灵活性与实用潜力，较传统应用实现了显著突破。","后续工作#后续工作":"这里夸奖一下Gemini3和qwen的deep Research，真的救我狗命。\n重点精读部分就看下面Gemini3提供的一份经过深度调研、严格筛选的 2023–2025 年间顶会（CVPR, ICCV, ECCV, ICLR, NeurIPS, CoRL, RSS, ICRA, IROS）已接收 (Accepted) 且 已公开代码 的 VLN / ObjectNav / Zero-Shot / LLM-assisted Navigation 相关论文列表。\n会议 年份 标题 简介 代码 关键词 CVPR 2025 UniGoal: Towards Universal Zero-shot Goal-oriented Navigation 提出了基于场景图（Scene Graph）和 LLM 的通用导航框架，统一了 Object, Image, Text 三种目标导航任务，解决 Zero-Shot 问题 GitHub Zero-Shot, Scene Graph, LLM, Universal Goal 2025 RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics 专注于提升 VLM 的空间理解能力，通过构建空间感知的指令微调数据集，大幅提升了机器人在 3D 环境中的导航和操作能力 GitHub Spatial Reasoning, VLM, Robotics 2024 Vision-and-Language Navigation via Causal Learning (VLN-GOAT) 引入因果推断（Causal Inference）消除数据偏差，提升 VLN 模型的泛化性 GitHub Causal Learning, Deconfounding 2024 NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation 首个基于视频的大模型（Video-based VLM）端到端导航器，无需构建显式地图，直接从视频流规划动作 GitHub Video VLM, Mapless, End-to-End 2024 AIGeN: An Adversarial Approach for Instruction Generation in VLN 利用对抗生成网络生成高质量的导航指令，用于数据增强 GitHub - 2023 Iterative Vision-and-Language Navigation (IVLN) 提出了\"迭代式导航\"新基准，要求机器人在同一环境中持续执行多条指令，考察记忆能力 GitHub Continuous Navigation, Memory 2023 Improving Vision-and-Language Navigation by Generating Future-View Image Semantics (VLN-SIG) 通过生成未来视角的语义图像来辅助当前决策 GitHub - ICCV 2025 CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs 模拟人类认知过程（感知-推理-决策），利用 LLM 进行常识推理和空间推理，解决 ObjectNav 问题 Project Page \u0026 Code Cognitive Modeling, LLM, ObjectNav 2023 Learning Vision-and-Language Navigation from YouTube Videos (YouTube-VLN) 利用大规模 YouTube 房屋导览视频进行预训练，学习真实世界先验 GitHub - 2023 DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation 引入\"心理规划\"机制，在执行前在潜在空间预演路径 GitHub - ECCV 2024 VLN-Copilot: LLM as Copilot for Coarse-grained Vision-and-Language Navigation 提出\"副驾驶\"概念，当导航智能体困惑时，LLM 提供详细的指导和推理辅助 GitHub LLM Agent, Coarse-grained VLN 2024 NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models 通过微调适配，激发通用多模态大模型（VLM）的导航推理能力 GitHub - NeurIPS 2025 Vision-Language Navigation with Energy-Based Policy (ENP) 提出基于能量的模型（Energy-Based Model）来建模导航策略，更好地模拟专家轨迹分布 NeurIPS Page/GitHub - 2024 SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation 构建在线 3D 场景图作为 Prompt，实现无需训练的 Zero-Shot 导航 GitHub - 2024 InstructNav: Zero-shot Vision-and-Language Navigation with Instruction Tuning 这是一个通用的导航大模型框架，统一了 VLN 和 ObjectNav (查看作者 Hao Dong 的 GitHub 或 Project Page) - 2023 PanoGen: Text-Conditioned Panoramic Environment Generation for VLN 使用生成式模型根据文本生成全景环境，用于 VLN 的数据增强和训练 GitHub - CoRL 2025 GC-VLN: Graph-Constrained Vision-and-Language Navigation UniGoal 团队新作，将导航建模为图约束优化问题，无需训练即可在连续环境中导航 GitHub Training-free, Graph Constraints 2024 LeLaN: Learning a Language-Conditioned Navigation Policy from In-the-Wild Video 直接从野外（In-the-Wild）视频数据中学习语言条件的导航策略 Project Page - 2024 OpenVLA: An Open-Source Vision-Language-Action Model 虽然主要针对操作（Manipulation），但其架构和预训练模型被大量用于导航任务的底座 GitHub - 2024 VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding 利用 VLM 进行零样本 3D 视觉定位，是导航的关键前置任务 GitHub - 2023 OVSG: Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs 基于开放词汇 3D 场景图的实体定位与导航 GitHub - ICRA 2025 Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation with Open-Source LLMs 探索使用 Llama 等开源模型替代 GPT-4 进行 Zero-Shot 导航，提出时空思维链 GitHub - 2025 MonoTransmotion 涉及单目视觉下的运动规划与导航 GitHub - 2024 VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation 结合 CLIP 和前沿点（Frontier）地图，指导机器人探索语义目标 GitHub - 2023 VLMaps: Visual Language Maps for Robot Navigation 将 VLM 特征融合进 3D 地图，允许使用自然语言索引地图位置 GitHub - IROS 2024 LLM3: Large Language Model-based Task and Motion Planning 结合 LLM 进行任务和运动规划，虽然偏 TAMP，但也包含导航组件 GitHub - RSS 2025 Unified Video Action Model 统一的视频动作模型，涵盖导航和操作 Project Page/Code - 2024 Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation 雖然偏向操作，但其 Policy 蒸馏方法正被用于加速导航策略 GitHub - ICLR 2024 AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials 利用 Web 教程合成智能体轨迹，辅助导航和任务执行 GitHub -","基准数据集数据与任务的局限#基准数据集：数据与任务的局限":"当前 VLN 数据集在质量、多样性、偏差及可扩展性方面存在明显局限。例如，在 R2R 数据集中，“指令-轨迹对” 偏向于最短路径，无法准确反映现实世界的导航场景。下文将探讨 VLN 基准数据集的改进趋势与建议方向：\n统一且贴近现实的任务与平台：构建可靠的基准数据集并确保结果可复现，是评估真实场景下 VLN 性能的关键。现实世界的复杂性要求基准数据集需全面覆盖各类导航挑战，因此需要一个通用的 “仿真到现实” 评估平台（如 OVMM），以实现仿真与真实场景下的标准化测试。此外，任务与活动设计需贴近现实且源于人类需求，例如 BEHAVIOR-1K 基准数据集，在虚拟、交互式且具生态性的环境中构建日常家庭活动场景，以满足对 “多样性” 与 “真实性” 的需求。\n动态环境：现实世界环境本质上具有复杂性与动态性 —— 移动物体、行人，以及光照、天气等环境变化，均可能引发突发情况。这些因素会干扰导航系统的视觉感知，使其难以维持稳定性能。近期部分研究（如 HAZARD、Habitat 3.0、HA-VLN）已开始关注动态环境，为后续研究提供了良好起点。\n从室内到室外：适用于室外环境的 VLN 智能体（如自动驾驶车辆、无人机）正逐渐获得更多关注，相关语言引导数据集也已陆续开发。早期研究尝试将 LLM 融入室外 VLN 任务，具体方式包括提示工程，或通过微调 LLM 实现 “预测下一步动作” 与 “规划未来轨迹”。为使现成的 VLN 模型适配室外导航场景，研究者利用真实驾驶视频、仿真驾驶数据或两者结合进行指令微调，使基础模型能够学习预测未来的油门与转向角度。此外，研究者还在基于基础模型的驾驶智能体中集成了额外的推理与规划模块。关于室外 VLN 的详细综述，建议读者参考相关综述文献与立场论文。","挑战与未来方向#挑战与未来方向":"尽管基础模型为视觉-语言导航（VLN）提供了创新性解决方案，但仍有若干局限尚未得到充分探索，同时新的挑战也随之出现。在本节中，我们将从基准数据集、世界模型、人类模型、智能体模型及真实机器人部署五个维度，梳理 VLN 领域的挑战与未来研究方向。","接地与推理#接地与推理":"视觉-语言领域的其他任务（如视觉问答（VQA）、图像描述生成）主要聚焦于 “图像与对应文本描述之间的静态对齐”，而 VLN 智能体则需基于自身动作，对 “指令与环境中的时空动态信息” 进行推理。具体而言，智能体需考虑过往动作、识别待执行的子指令片段，并将文本与视觉环境进行接地（grounding），从而执行相应动作。\n传统方法主要通过 “显式语义建模” 或 “辅助任务设计” 获取上述能力；但随着基础模型的兴起，“基于特定设计任务的预训练” 已成为主流方案。\n传统研究通过 “视觉与语言模态的显式语义建模” 提升智能体的显式接地能力，具体方向包括：\n建模动作与地标 利用指令中的句法信息 建模空间关系 目前，“基于基础模型实现 VLN 智能体显式接地” 的研究仍较少。例如，提出 “动作原子概念学习”，并将视觉观测映射为多模态对齐特征，以辅助接地。\n除显式语义建模外，传统研究还通过 “辅助推理任务” 提升智能体的接地能力。但在基于基础模型的 VLN 智能体中，这类方法较少被探索 —— 因为基础模型的预训练过程已使其在导航前就具备了对 “时空语义” 的通用理解。\n现有研究通过设计特定预训练任务，进一步提升智能体的接地能力，代表性工作包括：\n设计专门针对 “场景与物体接地” 的预训练任务\nLOViS：提出两项专项预训练任务，分别增强智能体的 “方向感知” 与 “视觉信息理解”\nHOP：提出 “历史与顺序感知预训练范式”，重点强调历史信息与轨迹顺序\n证实 “增强智能体的未来视角语义预测能力” 有助于提升其在长路径导航中的性能\n设计 “掩码路径建模目标” —— 给定随机掩码的子路径，重建原始完整路径\n提出 “实体感知预训练”，通过预测接地实体并将其与文本对齐实现接地能力提升","接地指令的泛化#接地指令的泛化":"导航数据在规模与多样性上的局限，是影响 VLN 智能体 “理解多样语言表达、有效遵循指令” 的另一重要问题 —— 在未见过的导航环境中该问题尤为突出。尽管语言风格本身在 “见过与未见过的环境” 中具备良好泛化能力，但受限于训练指令的规模，“如何将指令与未见过的环境进行接地” 仍是一项难题。基础模型通过 “预训练表征” 与 “指令生成数据增强” 两种方式，为解决这些问题提供了支持。\n在基础模型出现前，多数研究依赖 LSTM 等文本编码器表征文本指令。而基础模型通过预训练表征，显著提升了 VLN 智能体的语言泛化能力，具体案例包括：\nPRESS 方法：对预训练语言模型 BERT 进行微调，获得对 “未见过指令” 泛化性更强的文本表征。\n多模态 Transformer：为 VLN-BERT、PREVALENT 等方法提供支撑 —— 这些方法通过在 “从网络收集的大规模图文对” 上预训练，获得更通用的视觉-语言表征。\nAirbert 模型：训练一个类 ViLBERT 架构，从 “互联网收集的图像-标题对” 中学习文本表征。\nCLEAR 方法：学习 “跨语言语言表征”，捕捉指令背后的视觉概念。\nProbES 方法：通过采样轨迹实现环境自探索，并利用 CLIP 检测到的 “动作与物体短语” 填充指令模板，自动构建对应指令；同时借助 “基于提示的学习”，实现语言嵌入的快速适配。\nNavGPT-2 模型：探索利用 “预训练 VLMs”（如结合 Flan-T5 或 Vicuna 的 InstructBLIP）的视觉-语言表征，提升导航策略学习与导航推理能力。\n提升智能体泛化能力的另一方法是 “合成更多指令”。相关研究可分为两类：\n（1）离线指令生成\n早期研究采用 “说话者-跟随者（Speaker-Follower）框架”：利用人工标注的 “指令-轨迹对” 训练一个 “离线说话者（指令生成器）”，再让其基于 “给定轨迹上的全景序列” 生成新指令。但发现这类方法生成的指令质量较低，在人类寻路评估中表现不佳。\n后续改进方法包括：\nMarky 模型：采用 “多语言 T5 模型的多模态扩展版本”，结合 “文本对齐的视觉地标对应关系”，在未见过环境的 R2R 风格路径上生成 “接近人类质量” 的指令。\nPASTS 模型：引入 “进度感知的时空 Transformer 说话者”，更好地利用 “有序的多视觉与动作特征”。\nSAS 方法：利用环境的 “语义与结构线索”，生成包含丰富空间信息的指令。\nSRDF 方法：通过 “迭代自训练” 构建一个性能强劲的指令生成器。\n（2）导航中实时指令生成\n部分近期研究不再训练离线指令生成器，而是在导航过程中实时生成指令。例如，LANA 模型提出一种 “具备语言能力的导航智能体” —— 该智能体不仅能执行导航指令，还可生成路线描述。","智能体模型基础模型在-vln-中的适配#智能体模型：基础模型在 VLN 中的适配":"尽管基础模型具有强大的泛化能力，但将其融入导航任务仍面临挑战：LLM 本质上缺乏对真实环境的视觉感知能力，且易产生 “幻觉”；下文还将探讨 LLM 在规划与推理方面的能力局限。\n缺乏具身经验：这一局限可能导致 LLM 在任务规划与推理中仅依赖预设常识，无法满足真实场景的特定需求。部分研究通过 “将视觉观测转换为文本描述，作为 LLM 的提示” 解决该问题，但这种方式可能丢失关键视觉语义。与 LLM 相比，VLM（视觉-语言模型）智能体虽展现出 “感知视觉世界与规划” 的潜力，但其训练数据主要源于互联网，缺乏具身经验，需通过微调实现稳健的智能体决策。未来需进一步研究 “如何将基础模型智能体中的常识知识迁移到具身场景中”。近期提出的 “具身基础模型”（如 EmbodieGPT、PaLM-E、Octopus）为解决该问题提供了可行方向：这些模型通过在多类具身任务上微调基础模型，缩小智能体在 “视觉-语言-具身动作” 理解上的差距，提升其基于多模态输入的理解与执行能力。\n幻觉问题：LLM 与 VLM 可能生成 “不存在的物体”，导致信息失真。例如，LLM 在任务规划时可能生成 “向前走并在沙发处左转” 的指令，即便房间内并无沙发。这种偏差可能导致智能体执行错误或无法完成的动作。\nLLM 在规划与推理中的能力局限：已有文献针对 LLM 的零样本推理与规划能力展开评估（尤其是结合 PlanBench 与 CogEval），结果表明 LLM 在复杂规划任务中存在明显局限。这些研究在 “规划生成、最优性、稳健性、推理” 等挑战性场景下评估 LLM，发现其不仅易产生幻觉，还可能无法理解复杂规划问题背后的关系结构。\n在 VLN 场景中，由于室内环境固定且导航动作集合有限，动作空间与规划需求相对受限。这种 “有界场景” 使 LLM 能够生成 “粗粒度方向的分步指令” —— 已有研究证实该方式的有效性。需强调的是，在 VLN 任务中，LLM 并非主导整个规划过程，而是通过 “结构化拆解指令” 提供辅助；智能体的实际决策仍主要依赖感知、运动控制等其他组件。因此，LLM 的规划功能更多是 “补充性指导”，而非唯一决策依据。","模糊指令#模糊指令":"模糊指令主要出现在单轮导航场景中：智能体仅遵循初始指令执行任务，无法通过进一步人类交互获取澄清。这类指令缺乏灵活性，难以训练智能体根据动态环境调整自身的语言理解与视觉感知能力。例如，指令中可能包含 “当前视角不可见的地标”，或 “从多个视角观察均难以区分的地标”。\n在基础模型应用于 VLN 之前，模糊指令问题几乎未得到有效解决。尽管 LEO 模型尝试通过整合 “从不同视角描述同一轨迹的多条指令” 来缓解该问题，但仍依赖人工标注的指令。而基础模型所具备的 “全面感知上下文” 与 “常识知识”，使智能体既能利用外部知识解读模糊指令，也能向其他 “人类模型” 寻求协助。\nCLIP 等大规模跨模态预训练模型具备视觉语义与文本的匹配能力，这使得 VLN 智能体可利用 “当前感知到的视觉物体及其状态” 来解决指令模糊性问题，在单轮导航场景中尤为有效。具体案例包括：\nVLN-Trans 模型：通过 CLIP 提取 “可见且具有辨识度的物体”，构建易于遵循的子指令；并预训练一个 “转换器”（Translator），将原始模糊指令转换为易于理解的子指令表征。\nLANA+ 模型：利用 CLIP，以视觉全景观测为输入，查询 “地标语义标签文本列表”，并选取排名靠前的检索文本线索作为 “待跟随显著地标的表征”。\nKERM 模型：提出一种 “知识增强推理模型”，可检索 “以语言描述形式存储的导航视角相关知识事实”。\nNavHint 方法：构建一个提示数据集，提供详细的视觉描述，帮助 VLN 智能体全面理解视觉环境，而非仅聚焦于指令中提及的物体。\n另一方面，LLM 的常识推理能力可用于 “澄清或修正指令中的模糊地标”，并将指令拆解为可执行步骤。例如：\n利用 LLM 提供 “开放世界中地标共现的常识”，并结合 CLIP 实现地标探测。\nSayCan 方法：将指令拆解为 “预定义可执行动作的排序列表”，并结合一个 “效用函数” —— 该函数对当前场景中出现的物体赋予更高权重。\n尽管可通过视觉感知与场景上下文解决模糊指令问题，但更直接的方法是向 “通信伙伴”（即生成指令的人类）寻求帮助。这类研究主要面临三大核心挑战：\n判断 “何时请求帮助” 生成 “信息寻求问题”（如询问下一步动作、物体位置、方向等） 设计 “信息提供方”（oracle）—— 可为真实人类、规则与模板或神经模型 LLM 与 VLM 在该框架中可承担两种角色：一是 “信息寻求模型”，二是 “人类助手的代理” 或 “信息提供模型”。已有初步研究探索将 LLM 用作信息寻求模型，解决 “何时问” 与 “问什么” 的问题 —— 这需借助 “保形预测”（conformal prediction, CP）或 “上下文学习”（in-context learning, ICL）等技术实现。\n对于 “信息提供” 角色，基础模型需扮演 “掌握信息提供方专属信息的助手” —— 例如知晓目标位置、环境地图等任务执行者无法获取的信息。近期相关研究包括：\nVLN-Copilot 方法：使智能体在遇到困惑时主动寻求协助，其中 LLM 扮演 “副驾驶” 角色，为导航提供支持。\n证实 GPT-3 可逐步拆解训练数据中的真实响应，这有助于利用预训练的 SwinBert 视频-语言模型训练信息提供方模型；同时，mPLUG-Owl 等大型视觉-语言模型可作为 “现成的强零样本信息提供方”。\n自驱动通信智能体：通过学习 “信息提供方给出肯定答案的置信度” 实现，可采用 “自我问答” 模式，在推理阶段无需依赖信息提供方。","背景与任务基础#背景与任务基础":"人类及其他具备导航能力的动物，很早就展现出对环境导航的理解与策略。例如，加利斯特尔（Gallistel）提出了两种基础机制：其一为引导法（piloting），即利用环境地标计算距离与角度；其二为路径积分（path integration），即通过自运动感知计算位移与方向变化。理解空间导航的核心是认知地图假说（cognitive map hypothesis）—— 该假说认为，大脑会形成统一的空间表征，以支持记忆存储并指导导航行为。例如，托尔曼（Tolman）观察到，当熟悉的路径被阻断且地标消失时，大鼠仍能选择正确的新路径。神经科学家还发现了海马体位置细胞（hippocampal place cells），这表明存在一种以异中心视角（allocentrically）编码地标与目标的空间坐标系。\n传统上，“遵循自然语言导航指令”的任务多采用地图等符号化世界表征（symbolic world representations）进行建模。然而，本综述聚焦于采用视觉环境的模型，重点探讨多模态理解与接地（grounding）的相关挑战。与此相对，关于视觉导航和移动机器人导航的综述文献已十分丰富，这类综述主要关注视觉感知与物理具身性，但若涉及 “语言在导航任务中的作用”，则讨论较为简略，建议读者参考这些文献以获取相关背景。\n接地（Grounding）指将抽象的语言符号与具体的物理世界或感知数据建立对应关系的过程。在 VLN 中，接地是将自然语言指令映射到视觉场景中的具体位置、物体或动作。\n尽管在讨论 VLN 时，我们难免会将范围拓展到导航之外的领域（如移动操作、对话），但本综述的核心焦点仍是导航任务，并将针对该任务提供详细的文献梳理。此外，以往的 VLN 综述多采用 “自下而上” 的总结方式，聚焦于基准数据集与建模创新；而本综述则采用 “自上而下” 的视角，并以基础模型的角色为核心，将现有研究成果从 “世界模型”、“人类模型”、“VLN 智能体” 三个维度，归类为三大核心挑战。\n典型的 VLN 智能体会在指定位置接收人类指令者给出的（一系列）语言指令。该智能体以自我为中心的视觉视角（egocentric visual perspective）在环境中导航，其核心任务是遵循指令生成轨迹 —— 轨迹可基于一系列离散视角，也可基于低层级动作与控制指令（例如 “前进 0.25 米”），最终抵达目标终点。若智能体最终位置与目标终点的距离在指定范围内（例如 3 米），则判定为导航成功。此外，智能体在导航过程中可与指令者交互：既可以请求帮助，也可进行自由形式的语言沟通。近年来，研究者对 VLN 智能体的期望进一步提升，要求其在导航的同时整合附加任务，例如操作任务与目标检测任务。\n如上表，现有（2024）VLN 基准数据集可分为以下四类：\n导航发生的 “世界”：包括领域（室内或室外）与具体环境（如模拟器或真实场景） 人类交互类型：包括交互轮次（单轮或多轮）、通信格式（自由对话、受限对话或多指令）、语言粒度（动作导向或目标导向） VLN 智能体属性：包括智能体类型（如家用机器人、自动驾驶车辆、自主无人机）、动作空间（图基、离散或连续）、附加任务（操作与目标检测） 数据集收集方式：包括文本收集（人类生成或模板生成）与路线演示（人类执行或规划器生成） 研究者主要采用三类指标评估 VLN 智能体的导航寻路性能：\n导航误差（Navigation Error, NE）：智能体最终位置与目标终点之间最短路径距离的平均值 成功率（Success Rate, SR）：最终位置足够接近目标终点的任务占比 路径长度加权成功率（Success Rate Weighted Path Length, SPL）：通过轨迹长度对成功率进行归一化，平衡 “抵达正确终点的成功率” 与 “路径效率” 两大指标 此外，还有一类指标用于衡量 “指令遵循的忠实度” 与 “预测轨迹和真实轨迹的一致性”，例如： 4. 长度加权覆盖得分（Coverage Weighted by Length Score, CLS）：衡量智能体轨迹与参考路径的贴合程度，通过 “参考路径覆盖范围” 与 “轨迹长度效率” 两个维度平衡智能体性能 5. 归一化动态时间规整（Normalized Dynamic Time Warping, nDTW）：对偏离真实轨迹的行为进行惩罚 6. 成功率加权归一化动态时间规整（Normalized Dynamic Time Warping Weighted by Success Rate, sDTW）：在惩罚轨迹偏离的同时，还会结合导航成功率综合评估\n该图反映的是：\n核心模块关联：世界模型中讨论 “历史与记忆”，人类模型中讨论 “模糊指令”，两者均涉及 “泛化能力”；VLN 智能体中讨论 “接地与推理”、“规划”、“基础模型适配为智能体” 三大方法\n基础模型角色：根据基础模型承担的功能，将方法分为四类 —— 数据与知识处理（预处理 / 增强 / 合成数据、利用预训练常识）、表征学习（通用文本 / 视觉表征、历史记忆处理）、决策制定（导航规划器、信息寻求对话管理器、通用决策智能体）、任务学习（具身推理、语言接地、少样本 / 上下文 / 微调学习具身任务）\n交互示例：人类给出指令 “穿过客厅区域进入走廊。右转，然后再右转并进入房间\"“去卫生间”；智能体通过提问（“走廊在哪里？““哪个房间？\"）寻求信息，人类回复（“左边的房间还是前面的房间？““左边”）后，智能体执行动作（“前进\"“左转”）并生成轨迹\n挑战与未来方向：基准数据集（数据与任务局限）、世界模型（从 2D 世界到 3D 世界）、人类模型（从指令到对话）、智能体模型（LLM 与 VLM 适配）、部署（从仿真到真实机器人）","规划#规划":"动态规划能让 VLN 智能体实时适应环境变化、优化导航策略。目前，规划方法主要分为两类：一类是 “利用全局图信息增强局部动作空间” 的图基规划器；另一类是随基础模型（尤其是 LLM）兴起的 LLM 基规划器 —— 这类规划器借助 LLM 的海量常识与先进推理能力，生成动态规划方案，提升决策效果。\n近期 VLN 研究的核心方向之一，是通过 “全局图信息” 增强导航智能体的规划能力，代表性工作包括：\n利用 “已访问节点的图边界” 中的全局动作步骤，增强局部导航动作空间，以实现更优全局规划\n通过 “高层规划（区域选择）+ 低层规划（节点选择）” 的分层策略，进一步优化导航决策\n在 “基于图边界的全局与局部动作空间” 中融入 “网格级动作”，提升动作预测精度\n在连续环境中，规划方法进一步演进：\n采用分层规划思路 —— 通过 “从预测的局部可导航性图中选择局部航点”，用高层动作空间替代低层动作空间\nCM2：通过 “在局部地图中实现指令接地”，辅助轨迹规划\n拓展上述策略，构建全局拓扑图或网格图，支持 “基于地图的全局规划”\n利用 “视频预测模型” 或 “神经辐射表征模型” 预测多个未来航点，并基于 “预测候选航点的长期影响” 规划最优动作\n与此同时，部分研究借助 LLM 的常识知识生成 “基于文本的规划方案”，代表性工作包括：\nLLM-Planner：生成由 “子目标” 构成的详细规划，并根据预定义程序模式整合检测到的物体，实时动态调整规划\nMic 与 A²Nav：专注于将导航任务拆解为详细文本指令 —— Mic 从静态与动态双视角生成分步规划，A²Nav 则利用 GPT-3 将指令解析为可执行子任务\nThinkBot：采用 “思维链推理”（Chain-of-Thought Reasoning），生成 “与交互物体相关的缺失动作”\nVL-Map：基于 “代码化 LLM”（遵循 Code-as-Policy 框架），将导航指令拆解为 “代码格式的时序化目标相关函数”，并利用 “动态构建的可查询地图” 指导目标执行\nSayNav：构建 “已探索环境的 3D 场景图”，将其作为 LLM 输入，为导航器生成 “可行且符合上下文的高层规划”","跨环境泛化#跨环境泛化":"VLN 的核心挑战之一是：如何从有限的可用环境中学习，并泛化到新的、未见过的环境中。现有研究表明，以下方法可提升智能体对未见过环境的泛化性能：学习语义分割特征、利用训练过程中环境的 dropout 信息、最大化不同环境中语义对齐图像对的相似度。\n类别 方法 描述 3.2.1 预训练视觉表征 传统视觉编码器 多数研究采用在 ImageNet 上预训练的 ResNet 提取视觉表征 基于 VL 基础模型的表征 用 CLIP 视觉编码器替代 ResNet——CLIP 通过图文对的对比损失预训练，可自然实现图像与指令的更好对齐，显著提升 VLN 性能 视频预训练表征 探索将从视频数据中学习的视觉表征迁移到 VLN 任务中，证实视频中的时间信息对导航至关重要 3.2.2 环境增强 静态环境修改 EnvEdit、EnvMix、KED 与 FDA 通过修改 Matterport3D 中的现有环境生成合成数据，具体手段包括混合不同环境的房间、改变环境外观与风格、对环境高频特征进行插值 动态环境合成 Pathdreamer 与 SE3DS 进一步实现 “基于当前观测合成未来步骤环境”，并探索将合成视图作为 VLN 训练的增强数据 3.2.3 学习范式的转变 前基础模型时代 多数研究直接用自动收集的新环境增强训练环境，并微调基于 LSTM 的 VLN 智能体 基础模型时代 预训练被证实对基础模型至关重要，因此 “在预训练阶段从收集的环境中学习” 成为 VLN 的标准做法。基于增强域内数据的大规模预训练，已成为缩小智能体与人类性能差距的关键；且域内预训练的多模态 Transformer，被证实比从 VLMs（如 Oscar、LXMERT）初始化的多模态 Transformer 更有效","部署从仿真到真实机器人#部署：从仿真到真实机器人":"仿真环境往往缺乏真实世界的复杂性与多样性，且低质量渲染图像会进一步加剧这一问题。具体而言，当前部署面临三大瓶颈：\n感知差距：仿真与真实场景的视觉差异导致智能体性能与精度下降，因此需构建更稳健的感知系统。例如，尝试利用语义地图与 3D 特征场为单目机器人提供全景感知，显著提升了性能。\n具身差距与数据稀缺：仿真环境的物理规则与真实机器人的具身特性不匹配，且真实场景下 VLN 数据收集成本高、规模有限。\n数据规模化解决方案：机器人远程操控的兴起为解决数据稀缺提供了新思路 —— 通过人类远程控制机器人，可在真实人机交互场景中规模化收集 VLN 数据，为基础模型训练提供支撑。"},"title":"VLN 综述以及后续文献"},"/blog/2025/world-model-comparison/":{"data":{"world-model-as-cognition-prof-lecun#World Model as Cognition: Prof. Lecun":"等待更新中……","world-model-as-interface-marble#World Model as Interface: Marble":"","world-model-as-simulator-geniesima-2#World Model as Simulator: Genie/SIMA 2":"","世界模型的三种路线#世界模型的三种路线":"世界模型的三种路线这则推文给了我很大的启发，事实上，选择这条道路的一个原因就是 Embodied AI 这条道路并未收敛，而真正落地的成果，以 AI 领域的卷度读个博的4、5年时间以内应该就能吃上红利。而且这个时候恰如上个世纪的物理学界，哪怕一个三流的物理学家在那个年代也能做出一流的发现，我也打算依靠这股浪潮。\n好了，接下来进入正文，引用这篇推文的三个问题：\n什么是世界模型： 是一个供人类观看的东西，一个供智能体训练的场所，还是一个图标内部的黑箱——是系统其他部分需要咨询的实际内部模型？ 它的输出是静态资产、实时帧、还是主要驱动和预测和控制的潜在状态？ 如果撞倒一个虚拟花瓶，系统中的任何部分是否会记住——并利用该记忆来更新其未来的预期——持续超过一帧？\n所以得从实际应用出发，从具体的原始论文来熟悉这3类工作。"},"title":"世界模型的三种路线"},"/daily/":{"data":{"":"这里是日常学习和工作的记录。"},"title":"日常日志"},"/daily/2025/11-dec-2025/":{"data":{"":"","1-ѧϰ-vla-#1. \ufffd\ufffd\ufffd\ufffdѧϰ·\ufffd\ufffd (VLA \ufffd\ufffd\ufffd\ufffd)":"���߽�����ѧϰ·����Ϊ��������ģ�飬ǿ�����۽��ϴ���ʵ����\n������ѧ������\n�ص㣺�����˶�ѧ�������궨���������ۣ�Screw Theory�� ��Դ��B վ����������ѧ�γ̣������˶�ѧ�½ڣ�����ѧУ�Ļ�е��ʵ���� ����ģ�ͻ�����\n�ص㣺Diffusion Model��Flow Matching ��Դ��MIT 6.S184��ǿ���Ƽ������߽�������������ҵ�������� Flow Matching ���������� ģ��ѧϰ (Imitation Learning)��\n�ص㣺ACT��Diffusion Policy ��Դ���Ķ��������ģ�**������ CS285** �γ̣�VAE ���ֻὲ ACT�� ǿ��ѧϰ (RL)��\n�ص㣺�����ڻ���ͨ ��Դ�������� CS285�����߻���������ѧ�꣬ǿ��Ҫд��ҵ�� ��ģ�ͻ�����\n��Դ��CS336����ͷ�ִ�һ����ģ�ͣ�","2-ʵս#2. ʵս\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd":"����ģ�ͣ��� OpenVLA ������ģ�����֣��������� ����ϵ�У����龫�� �� ϵ�е��������������루**��0 / ��0.5 / RTC** �ȣ� ʵ�����ڣ��ܷ��棨Simulation��+ ����ʵ�٣�Real Robot�� ע������������˵������ʵ�پ�����\"����\"��","3-ҵȥо-research-directions#3. \ufffd\ufffdҵ\ufffd\ufffd\ufffd\ufffdȤ\ufffd\ufffd\ufffdо\ufffd\ufffd\ufffd\ufffd\ufffd (Research Directions)":"����ͨ�����Ծ����ܽ����ĸ����ŷ�����\nTactile + VLA������ + VLA������������������Offer �϶� VLA + RL�������������������Ϻܶ๫˾���� 3D VLA������Ǳ���ķ�����Promising�� World Model����Ϊ VLA ֮������һ������·�ߣ����๫˾���о�","4-ߵʱο-9-#4. \ufffd\ufffd\ufffdߵ\ufffdʱ\ufffd\ufffd\ufffd߲ο\ufffd (9 \ufffd\ufffd\ufffd\ufffd)":"1 �� - 5 �£�����ѧϰ��RL������ģ�͡���ģ�ͣ� 5 �� - 9 �£�����Ŀ / ���У�ʵս�׶Σ� 9 �� - 10 �£�����","5-ϣ#5. \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd߼\ufffdֵ\ufffd\ufffdϢ\ufffd\ufffd\ufffd\ufffd":"��������Ҫ�ԣ�����ѯ���Ƿ�������������������ȷ�ظ�**\"�����Ǳ���\"** ����ʵϰ������û��ȫְ���飨NG������������ʵϰ��ʵϰֻҪ������֪ʶ���ɣ����������������������Ĳ��� ���弼���㣺 ��ץȡ��Grasping�������� Learning��ѧϰ�������Ͽ��������Ǵ���ͳ�Ŀ��� VLM���Ӿ�����ģ�ͣ��Զ����� MIT 6.S184 Flow matching and diffusion model","heading#\ufffd\ufffd\ufffd\ufffd\ufffdܽ\ufffd":"���̽��д򼤹� �����ۿ����� \u003e ����ʵϰ�����ǿ��������������������ͻغ������ճ�ʵϰ ������֬ ��ʳ���� ���ؼҵ����� �������� 2k �������Ȼ���ظ�΢������ע����ѧ����λ��ʦ ���Ʊ�����Ŀ���� ��Ҫ�����ǣ�������Щͬѧ�Ľ�����������Ϊ�����о���֮ǰ���ǲ���ѡ�������ģ��������о��������Լ���һ�� taste������������������˵������ taste �ǿ���ͨ��������ι�����������ġ�����������һ��̫���������ˡ�\n�ҳ����ҿ�����һ�㸡���ˣ�����Ҳ�Ǹ�ͬѧ�����Ľ����������������ķ�ʽ����һ�� AP ��������Ҳ����������һ������Ϊ����ͬ���кܶ�ѡ�Ӳ�����¿��ܰ�����ȥ�����в��������ǿ�����ʱ�����Ե�ѵ����Ŀ���ڳ�����С�����г���ʵϰ��ѧ��֪ʶ���Ӷ���ѩ�򡣸�����ְ��ѧ����Ϣ��**������������Ŀǰ���Ǻ�ȱ�˵ģ���������Ҳû���������ܵ��ھͿ��ԣ������˴������� job description ȥ��Ϥһ�±��� OpenVLA��Isaac Lab ��Щ��base ���������õ�**��\n��������һ�����Ե��뷨���ǣ���׷�󱾿Ʊ���һ������һ���ܷ������ɹ��Ĺ����ˣ��������ϸп�һ�� locomotion �ֲ���ͼ��SLAM��+ Social Navigation��PointNav����\n��Ҫ�㶮��һ�������ǣ�**�ֲ���ͼ**�������ܺã����Ƿ�����������һ��ȫ�ֵĵ�ͼ��������ô֤���Լ��ڷ����Ļ������� environmental awareness to avoid occlusion �أ�\n��������û�ж������µ��������ݿⰡ����Gemini �������׵Ĳ�ѯ���������������������ĺ�����Ч�ʺܵͰ�����\nObject Navigation �о�ȥ׷�Ӿ���Ϣ����������Ӧ��ʵ�壬�����Ŵʻ��������е��ô��ֵ�һ�����µ㣬Ȼ��ˮ�Ƚ�����һ���˿϶���������\n�������룺DA3��","llm-基础与微调#LLM 基础与微调":"没有A100，4090如何搞llm的微调 [知乎] 吹得神乎其神的从零搭建LLM [GitHub] 无大算力时，作为学生，LLM 还有哪些值得做的研究 [知乎] LangChain入门 [飞书] 阿里云百炼 通义千问Plus最高级的入口 [阿里云]","llm-驱动的导航#LLM 驱动的导航":"NeurIPS2024 | SG-Nav：无需RL训练，基于LLM实现零样本物体巡航 [知乎] ICCV-2025 | 大模型驱动的认知导航框架！CogNav：面向目标导航的大型语言模型驱动的认知过程建模 [知乎] CVPR2025 | UniGoal：通用零样本目标导航，Navigate to Any Goal! [知乎]","llmvlmvla-相关#LLM/VLM/VLA 相关":"","vla-vision-language-action-相关#VLA (Vision-Language-Action) 相关":"从零起步：如何成为一名VLA算法工程师 [小红书] 当我读了一堆VLA+RL的paper之后 [小红书] VLM-R1 - 当年误闯天家看到的第一个VLM工作 [GitHub] 博士想读具身智能/智能机器人应该怎么规划自己的科研 - 主要是这个知乎有个人PO了一张论文泛读，VLA啥的 [知乎]","ʼ#\ufffd\ufffd\ufffd\ufffd\ufffdʼ\ufffd":"","оʵϰ#\ufffdо\ufffd\ufffd\ufffdʵϰ\ufffd\ufffd":"�о���ʵϰ��\n��̫�Ѷ��ˣ������Ǽ��������������⣬û�� paper �Ҳ����о���ʵϰ��û���о���ʵϰ���ѷ� paper��","ҹѧϰ#\ufffd\ufffd\ufffd\ufffd\ufffdҹ\ufffdѧϰ·\ufffd\ufffd":"�����ҹ�ѧϰ·��","ռƻ#\ufffd\ufffd\ufffdռƻ\ufffd":"","个人形象与穿搭#个人形象与穿搭":"建模不是太出众的男生该怎么找一个女朋友？ [知乎] 男士打点自己 [知乎] 男士穿搭 [知乎] 为什么中国就很少有人穿三件套西服？——— 论定制西服与衣装的可能性 - 我在想要不要定制一套衣服穿 [知乎]","今日总结#今日总结":"主要因素是暂存一下浏览器打开的窗口（\n不然一直占我内存挺难受的。","具身智能机器人导航#具身智能/机器人/导航":"具身导航 VLN 最新论文汇总 [CSDN] Perceptive Locomotion 感控一体论文及仓库 [小红书] Unitree Go2 SDK开发指南 [Unitree]","学术ppt#学术PPT":"东京大学PhD申请 - 东京大学博士申请经验分享视频 [Bilibili]","学术相关#学术相关":"","开发技术#开发技术":"万字长文入门SpringBoot [知乎] Java VS Go [腾讯云] Nestjs入门 [掘金] MrBird各种技术栈都有分享的博客 - 比如CI/CD、DevOps、Canvas等 [MrBird]","技术学习资源#技术学习资源":"","技术笔记#技术笔记":"","招聘信息#招聘信息":"","明日计划#明日计划":"","浙大系具身智能初创公司算法实习生招募#浙大系具身智能初创公司算法实习生招募":"地点：杭州钱江世纪城\n薪资：300-600/天\n工作内容：\n聚焦轮式双臂机器人平台，动手实践前沿软件框架（ROS 2, NVIDIA Isaac Sim等）与VLM / VLA模型（Pi-0, OpenVLA等），复现、调优并评估指定模型在复杂导航与灵巧抓取任务中的性能。 探索具身智能新范式（结合强化学习、VLN 等）； 撰写技术报告 / 论文； 职位要求：\n本科及以上学历，机器人学、计算机科学、人工智能、自动化、电子工程或相关专业背景；具身智能相关项目经验者优先； 技能与经验 (任一，包括但不局限于以下领域)： 使用VR/动捕采集机械臂数据；机器人相关数据（图像/视频/状态/动作）的清洗、标注与管理; 机器人模仿学习(IL) / 视觉强化学习(Visual RL)训练调优；大模型(LLM / VLM / VLA)微调技术实践(SFT / RLHF)；动作表示/空间设计/对齐的理解或实践。 使用Isaac Sim / Mujoco进行仿真验证/训练；数字孪生平台经验；理解或应用域随机化来解决sim2real问题。 熟悉定位(SLAM) / 路径规划 / 运动控制算法概念或有相关项目经验。 其余具身智能领域相关的技能与经验。 有顶会顶刊论文、开源项目贡献、知名比赛经验、相关实习背景者优先。 能够提出创新性观点，搭建实验验证方案可行性，有一定英文论文写作能力 投递邮箱：wuhangyu@zenobot.ai\n浙大系具身智能初创公司算法实习生招募 - 聚焦轮式双臂机器人平台，动手实践前沿软件框架（ROS 2, NVIDIA Isaac Sim等）与VLM / VLA模型 [小红书]","深度学习计算机视觉#深度学习/计算机视觉":"最全面的 深度学习CV/NLP 笔记 [GitHub]","生活相关#生活相关":"","职业面试#职业/面试":"阿秀的学习笔记 互联网开发从八股到面经一个比较全面的网站 快速刷完67道剑指offer [InterviewGuide] 互联网校招技术岗开发者学习路线图 - 与 developer-roadmap 仓库不同，本 repo 更专注于计算机初学者，也就是普通本科\u0026研究生参加校招以及打算从生化环材等专业转行到计算机的童鞋 [GitHub] 据说谷歌的colab可以用高级计算卡，和Autodl平价位，这里先mark下来","随想#随想":"鼠辈耗尽英雄气，英雄尽是鼠辈出\n英雄的视野落入鼠辈之手，而鼠辈又把自己打扮成英雄。真正卑鄙的人拿着高尚招摇撞骗，真正高尚的人却总是忘记自己做过的坏事，甚至于被墓志铭上称为卑鄙"},"title":"12-11"},"/daily/2025/13-dec-2025/":{"data":{"":"","今日总结#今日总结":"今天打算进行分野，一半空余时间拿来学习，一半拿来研究。\n然后这里暂时备忘录写一下，都是明年 2026 三四月要做的：\n办理香港签证 申请校友邮箱 延长校网以支持 cc98 访问 根据这个帖子，入门 VLA 主要需要做的是这几点：\n继续完善赵老师的强化学习理论学习，并补足其未涉及的 PPO、GRPO 等算法 这里楼主推荐的是 HuggingFace 强化学习实践，而不是 Hands-on-RL，我想都一样 整合学习 VLM（CLIP 或 LLaVA）然后扩展到 VLA（添加 Action 输出） 理解 VLA 的核心内容：基于预训练的 VLM 通过 finetune 预测机器人动作，挑战包括数据稀缺、实时性和 sim2real 一些关键概念：action chunking、flow matching 和分层架构（高水平规划 + 低水平执行） 阅读论文：RT-2、OpenVLA、π0（Physical Intelligence）、Gemini Robotics VLA 算法的复现，对于 ACT、DP 等基础算法的公式的一个推导深入理解 对于机器臂，首推用 lerobot 的 so101 做一个实体机器人的算法的复现，方便后续进行一个其他算法的拓展，实现自己改造的 VLA 算法 还可以找自动驾驶的项目啥的，但是 lz 不熟 在这里就可以引出新关注到的几位老师了：\n一个是陳林，自动驾驶领域高手，目前主要更新的内容是【每天思考一个强化学习问题】，比较有意思 一个是彭思达，是我们浙大软件学院百人计划研究员，研究方向是面向空间智能体的闭环仿真训练，而阅读它的科研入门文档给了我很大裨益，当然有点多还没有看完，其 Github 主页的公开仓库也值得关注 一个是无敌正义雷欧奥特曼，主要是通过其 26fall 的秋招面试去反推这些企业都有哪些，需要什么技术栈，截止 12 月 13 号已经有 92 面了 还有这个 GAiR 的星河 AI 研究所，算是一个比较大型的民间付费社群，我在 B 站上看到的 3 类世界模型的视频就是他们的。\n又有一位西工大在读博士，研究兴趣为三维视觉的 Gavin Sun 非常厉害，在推荐其日常读到的论文，重点是是用笔记的形式写的，和我的方向也有交叉！\n还有就是 tabris，属于是人形分享器，据说多数是 X 推给他的，少数是师友分享的，对我来说也非常有用，像是 spatial intelligence 这种。","技术笔记#技术笔记":"","明日计划#明日计划":""},"title":"12-13"},"/daily/2025/14-dec-2025/":{"data":{"":"","今日总结#今日总结":"另外又一个优秀的东南大学博主 momo，其主要的笔记是一些 SLAMer 拓展技术栈 Day n 系列，我觉得这里的三维重建很有意思。同时他还是智元实习生！\n还有这位 laumy 的学习笔记，非常牛逼，包括了一系列比如 ROS2 实践、ACT 算法解析的文章博客。\n记录一下 XLAB 里面北美 CS PhD 对于 agent 的学术讨论，我看到这种交流真的是热泪盈眶：\n再过两年现在的 agent 是不是都会废了？\n模型自己就可以\n我觉得 agent 的定义本身就还在演变，很多功能以前的 agent 需要，现在可能就不需要了。但 agent 本身要做的事情应该和模型是区分开的。\n现在多模态模型的未来预期是像人脑一样，能接受多模态的输入，能给出思考结果。这个结果不局限于文本，可能是 action（VLA），可以是语音，图像（omni model），也可能是一个指令。\n而 agent 更多侧重于 action，拿到指令怎么去执行。\n过往的 agent 其实不少功能，比如长期记忆、规划、自我反思这些功能其实是之前大模型这几块能力不足的时候的一种过渡方案，之前的大模型不会思考，那我自己搭一个 agent 专门让模型只做思考等等，大模型的长期记忆不行，我要装个记忆模块，我要用 RAG 等等。但最近大模型的思考和记忆都在飞速提升，现在的 agent 也不是都需要一个记忆模块了，模型本身能力够用了。不过真的要执行指令，比如说工具调用、图像标注等都需要进一步的操作，这件事大模型本身可能并不会去做。后续 agent 的定义可能也在演变。\n我觉得未来的大模型本身可能是个很强的大脑，他的泛化能力很强，是个全才，但不至于说什么都能做的很好。你要写代码，我要用专门的 code agent，比如 Cursor、Claude Code。你要用到具身智能里，要调用机械臂操控抓取，你要让大模型、VLA 输出 action。agent 的概念感觉更偏下游，至少我觉得模型本身再全能，应该也不至于全能到通杀所有领域，只要下游有需求就有专门为下游搭建一个 agent 框架的可能，你模型代码能力再强，能一口气输出再完美的代码，也还是要套个 agent 的壳子才能用的。\n比如说现在很多技术解决上下文不足的问题，要是之后真的可以把整个代码仓库塞进去，那这些记忆管理之类的 trick 还有意义吗？\n现在模型的上下文长度已经很夸张了（context_length 每年大概翻 30 倍）。\n但大多 paper 证明自己上下文能力大多就大海捞针说自己几百万上下文还能记住中间的某几个词。\n但实际应用上还是效果说话吧，记住了不等于效果好。\n比如现在大模型上下文很长，但很明显在对话一段时间后模型回复的质量会有所下降，如果一些记忆 trick 做出来的效果比没用 trick 好就很有意义。\nClaude Code 本身也有一个超长下文对话后会主动对上下文做压缩的阶段。\n未来考虑到运行几天的 agent 模式，无限上下文是发展趋势甚至说必然结果的话，记忆压缩大概率也还是有必要的。\n用人脑对比，人能记住一大堆东西，但为了效率可能不如就记住今天要考试的关键几页内容，别的都忘了，检索效率和回答效果都可能更好。\n更好的记忆管理，或者压缩手段哪怕在无限上下文实现了我觉得也还是有意义的，节省成本和提升效果能做到一个就 ok。\n如果把 LLM 视为人类的下位替代的话，那所有人类遇到的记忆问题大模型最终也无法避免？\n对于普通文本任务上下文压缩有效，但是目前来说遇到代码的话就失灵了……因为具体 code 没法被\"总结\"。\n这个倒可以稍微引申一下，比如代码这种任务往往希望记忆里存储的是完整的代码信息，这个很难压缩，但除了代码领域同样也有很多需要高精度记忆的任务，这样的类 code 领域的记忆压缩就有很多问题。至少现在 Gemini CLI 的做法还是挺相对粗暴的，先对历史进行分割，调用大模型分块压缩，summary 总结。压缩用精心设计的 prompt。但这种方法直觉上说想一种暂时的妥协。\n但是我觉得所谓的 agent 的职能被慢慢统一到上游是可以遇见的吧。\n这个我觉得还是具体问题具体分析吧，我喜欢对比人脑单纯是便于我自己理解现在大模型能做什么不能做什么，然后给一种可能的解释，然后从人的思维角度思考有没什么解决方案。\n实际上很多任务 LLM 都是人类是上位了。单论记忆问题，LLM 可以一分钟记住数万字信息还能复述，人就不太行了。\n嗯，我觉得可能 agent 的功能可能也随着 LLM 增强而改变，agent 本身这个概念可能也在迭代。之前 LLM 不能做的要 agent 做的可能现在 LLM 能做了 agent 就不做了，比如说之前 agent 往往有专门的思考模块。\n那我觉得要你从学术角度看还是从 application 角度看了。\n而且我觉得所谓的 agent 也不过就是一种加 human prior 的方式。\n所以最后还是要看工程实践。\n数学/概念上等价的东西不代表实践中是等价的。\n那肯定是的。\n而且涉及到真的取不取代你还得看发展的多快，旧东西会不会变成历史遗留问题。\n我理解 Agent 是 LLM + tools / MCP；私有的 tools/MCP 不可能完全被大模型集成，所以 agent 大概率还是会一直存在的。\nagent 和 LLM 定义上最大的区别就是和环境的交互吧，目前而言应该也没有必要让 LLM 往直接绕过 tool 之类的东西直接和环境交互方向发展。\nGUI agent 算环境交互吗？\nGUI agent 其实不太涉及到本地环境的操作，更多的是对打开的页面 / 手机的操作。\n我们组现在在搞的就是 GUI agent。\n目前实测下来基本不用自定义 tools / MCP。\n以后会不会出现面向 AI 的 UI 设计？\nGUI agent 这个东西其实和爬虫差不多，鉴于目前已有的各种反爬措施，大概率不会有专门面向 GUI agent 的服务。\n更可能的是反 GUI agent。\n这你让我想起了造机器人是因为现在环境是为人设计的这个 story。\n我完全不 buy 这个 story。\n合理。\n前两天豆包那个就被反了。\n一本浓厚 CS PhD 气息。\n这个 buy story 太典了。\n感觉 GUI agent 有点多此一举的味道。\nUI 本来就是机器适应人类用的。","技术笔记#技术笔记":"见 2025-12-14-habitat-common-sense","明日计划#明日计划":""},"title":"12-14"},"/daily/2025/15-dec-2025/":{"data":{"":"","今日总结#今日总结":"小红书浙大二院点阵激光","技术笔记#技术笔记":"见 2025-12-15-social-navigation-idea","明日计划#明日计划":""},"title":"12-15"},"/daily/2025/16-dec-2025/":{"data":{"":"","今日总结#今日总结":"AutoDL 使用指南","技术笔记#技术笔记":"","明日计划#明日计划":""},"title":"12-16"},"/daily/2025/17-dec-2025/":{"data":{"":"","今日总结#今日总结":"昨天摆大烂了，主要是昨天熬夜熬到5点，然后12点起床，就感觉注意力完全没法集中，哪怕到了实验室也只能发呆，于是开始看《鬼灭之刃》，别说还真让我找回一点《地错》热血漫的感觉，在我25岁之前，王道剧情来多少我吃多少。\n然后是关于睡眠这件事情本身，我个人感觉原因在于，宿舍的床（ZJU玉泉25舍都是上床下桌）已经变成了一种别样的私人空间，而不是一个睡觉的地方，在这个私人空间里可以进行网上娱乐项目，自然就停不下来一直到一点、两点……\n回忆一下，初中住校的时候没有这种烦恼，因为没有电子设备，床就是床，私人空间是床边的桌子，每天早起是带着怒气（针对于大喇叭高分贝把人吓醒的怒气）的所以能做到这样一种\"自律\"，而等到自己走读的时候就不妙了。\n还有一种换位的情况可以作证这一点，那就是在不熟悉的床上入睡，这个时候会有一种不安感，那种私人空间的感觉被剥夺了，自然不会一直刷平板。\n啊啊平板也是一个重要因素，如果我只有手机我断然不会刷太久，可是平板实在太适合在床上使用了。\n然后今天港中深的CS给我发offer了，我也是醉了——它属于我审的最早的一批学校，但是却在我交完港中文本部 Msc AI 的留位费之后一个月才来，而且12月17号发邮件，12月18号就截止。如果来的顺序交换一下的话，我就选择这个港中深的CS了。\n学制方面，港中深有 24 个月（对比CUHK的Msc CS只有1年太紧张了），需完成 36 专业学分（3 门必修 + 9 门选修，每门 3 学分）\n原先对于不了解的人来说，港中深处于深圳肯定是比港中文要方便实习的，但是实际从小红书上来看，这是不切实际的，因为港中深在郊区，值得实习的企业在深圳市区，所以不如港中文来得快。这里我也额外去做了一下调研：\n对比维度 港中文本部（沙田） 港中深（龙岗） 备注 到福田中心区 50-60 分钟 50-60 分钟 打平手。港中深得益于14号线快线，去福田已不比香港本部慢。 到南山科技园 约 70-80 分钟 约 80-90 分钟 本部略优。从深圳湾口岸过关去南山更近；港中深去南山横跨整个深圳。 单程交通成本 高 (约 25-30 RMB) 低 (约 7-9 RMB) 香港段东铁线过境费贵，即便有学生优惠也远高于深圳地铁。 通勤不确定性 高 (海关排队风险) 低 (仅受地铁早高峰影响) 跨境通勤受节假日、海关拥堵影响极大；港中深通勤时间可控。 实习建议 适合：福田、南山、香港本地 适合：福田、罗湖、龙岗本地 如果去腾讯/字节（南山），两者都远，但本部住深圳湾附近会更方便。 对于**“具身智能/AI\"类实习**，这类岗位多集中在 南山粤海街道（腾讯、大疆、创业公司）或 河套深港科技创新合作区。\n港中深（CUHK-SZ）：去河套（福田口岸附近）约 1 小时，尚可接受；去南山（大疆等）非常痛苦，每日往返近 3 小时，基本不可行。\n港中文本部（CUHK）：\n住 深圳福田口岸/皇岗口岸：去河套实习是\"步行级\"距离；去南山约 40 分钟。 去香港本部上课：约 40-50 分钟。 结论：采用**“住深圳口岸、去香港上课”**的模式，对实习最有利。\n香港沙田单间租金约 6000-8000 HKD，而深圳口岸附近（如皇御苑等）约 3000-4000 RMB 且环境更好。所以港中文本部学生住深圳节省成本。\n尽管有一个港中深的**“地铁14号线（东部快线）”的信息差存在（也是有地铁直连了！），小红书上所说的也不算完全刻意放大了港中深的通勤劣势，但对港中文本部的通勤流畅度的美化肯定存在（忽略了通关排队和高铁换乘成本**）。\n如果在福田实习，港中深和港中文本部通勤时间几乎一样，但港中深更便宜、更稳定。 如果在南山实习，港中文本部（配合住深圳湾/福田口岸）具有明显优势。 如果看重具身智能/机器人产业，深圳机会远多于香港，“拿香港学籍，住深圳口岸，在深圳实习” 是目前最具性价比的方案。","技术笔记#技术笔记":"2025-12-17-socialnav-map-1","明日计划#明日计划":""},"title":"12-17"},"/daily/2025/18-dec-2025/":{"data":{"":"","今日总结#今日总结":"见识到真正的 Latex 项目了：浙江大学学位论文 LaTeX 模板，在 1月12-16 之前要把开题报告、文献翻译、文献综述+PPT给搞出来，最好在这之前就做出来先留几版迭代。\nlatexmk 直接这样本地编译。已经做好了一版，但我不太满意，主要还是文章看少了，实践也不够。\n后续的话在这几个方向去改：\n内容本身的更新 图/表 PPT","具身智能入门项目#具身智能入门项目":"具身智能入门超简单开源项目\n该项目使用 isaacsim + isaaclab 实现了一套强化学习的 agent。平台选择：isaacsim4.5 + isaaclab2.0 镜像环境可快速复现和研究此项目，通过此入门级项目的研究可以掌握通过 isaaclab 框架进行机器人强化学习的完整框架和思路，非常适合具身智能的入门学习。","参考资料#参考资料":"对PhD一年级新生有什么建议\n后端圣经：【求职广场】不理解为什么还有小登学开发？","技术笔记#技术笔记":"2025-12-18-vlm-think-with-images.md","明日计划#明日计划":""},"title":"12-18"},"/daily/2025/19-dec-2025/":{"data":{"":"","pnp-机器人#PNP 机器人":"棒读，不太喜欢，因为本来的那个 CEO 在国外，换了个人来讲，我感觉她不是很懂技术（一种姜萍讲高树的感觉），但是内容还行应该是行内人做的，主要是机器臂这一块。\n给老爸的礼物 给老妈的礼物\n有一个籍籍无名的开源项目 上面描述了在 Go2 的 NVIDIA Jetson Orin NX (16GB) 上部署 Qwen2.5-VL-3B-instruct AWQ-int4，而现在 Qwen 到 3 了，可以再升一下级；还有一个 说自己是 real time，但是也没提到 VLM 的型号，本身也是去年的，同样籍籍无名。","今日总结#今日总结":"LeadeRobot 第六届中国机器人年会，有点失算了，它在 12.18 是有活动的，但是我拿到的海报里没有，只有 12.19 的活动，好在有录播回放，主要在\"机器人大讲堂“视频号查看。\n打开微信，搜索并关注”机器人大讲堂“视频号。 进入视频号主页，在\"直播回放\"或发布的视频中查找本届年会内容（目前会议仍在进行，回放将在会后逐步上架）。 也可关注机器人大讲堂官网（leaderobot.com）及官方公众号，获取后续回放链接。 但是四场里面只线下参加了一场，好在也是来了。\n收获了一本描述 2025 中国机器人的行业的年刊报告（只对高校报名的人免费，普通观众要花 100 多块） 收获了【机器人大讲堂】这一公众号（里面有机器人的回放。。。）","优宝特#优宝特":"腿足式机器人的关节驱动仿生机构的电机与控制，硬件这一块我不太感冒。可能也是因为我不太有这方面的经验的原因，这个如果换一个控制学院的同学来可能会挺津津有味的，NMPC 啥的控制算法。后面就开始上强度了，强化学习世界模型之类的，有点应接不暇，而短短几句话介绍完全不够我去理解。","卓益得#卓益得":"人形机器人，这个 PPT 看着更像创赛了，前面那一家后面技术部分的 PPT 开始潦草起来。\n哦我收回我的话，这个 PPT 做的好糟糕，小字非常小，类似我大一大二做的那种 PPT。为什么是法务部/战略发展部总监来介绍而不是他们企业的 CEO/CTO？还欢迎女性科学家。","原力无限#原力无限":"Hyper-VLA \u0026 WM。确实我认同机器人这个概念已经被大大拓展，我其实不太喜欢 Robotics，我喜欢的是物理感知空间智能的 Agent。他们的汇报风格就很类似那种创赛，先上一堆什么首席科学家联合创始人等博士博导大佬背书。前者是 real2sim2real 持续强化学习；后者是因果（语言引导的感知推理？）WM。听起来挺泛泛而谈。","技术笔记#技术笔记":"见 2025-12-19-ai-work","投稿规划#投稿规划":"如果我要投稿的话，是不是现在就要开始选会议开始规划 DDL 了？\n会议等级 会议名称 预估截稿日期 (DDL) 特点与建议 机器人顶会 IROS 2026 2026年 3月 1日左右 最推荐。社交导航（SocialNav） 在 IROS 有专门的 Track，重视工程实现和真机部署（你的 Go2 实验很加分）。 计算机视觉顶会 ECCV 2026 2026年 3月初 偏重视觉表征。如果你的 SocialNav-map 在视觉建图上有重大改进（比如融合了语义或长程感知），可以考虑。 机器学习顶会 NeurIPS 2026 2026年 5月中旬 偏重算法逻辑、强化学习和理论。如果你的 SocialNav 涉及复杂的 RL 策略或端到端训练方案，可以投。 具身智能顶会 CoRL 2026 2026年 6月左右 具身智能的顶级会议。非常看重”Learning for Control\"。时间最宽裕，但竞争极度激烈。 PhD 生存指南\n太长了兄弟，根本看不完。","明日计划#明日计划":"","智澄#智澄":"先讲了一波物理智能世界模型（Genie3、JEPA 等），然后推销自己京东自营的人形机器人。","赛感#赛感":"做的是电子皮肤，说什么 VLA → VTLA 这种概念，突出自己的触觉这一块。他们在深圳，来源于南科大的孵化貌似。他们可能比较喜欢 BME 那个柔性电子材料的那个实验室，本身我没看到像上面那样的无论是控制还是 AI 相关的内容。"},"title":"12-19"},"/daily/2025/20-dec-2025/":{"data":{"":"","今日总结#今日总结":"【组会分享】Isaac Lab代码讲解与开发经验分享\n从0开始找VLA实习\n大家是怎么阅读 arXiv 论文的\n导师给了一个比较复杂的github项目让我学习，请教各位大神应该按什么顺序开始啃呢\ndeepwiki、zread","技术笔记#技术笔记":"","明日计划#明日计划":""},"title":"12-20"},"/daily/2025/21-dec-2025/":{"data":{"":"","今日总结#今日总结":"","技术笔记#技术笔记":"","明日计划#明日计划":""},"title":"12-21"},"/daily/2025/22-dec-2025/":{"data":{"":"","今日总结#今日总结":"","技术笔记#技术笔记":"","明日计划#明日计划":""},"title":"12-22"},"/docs/":{"data":{"":"欢迎来到文档中心。这里包含了各种学习笔记、项目文档和研究资料。"},"title":"文档"},"/docs/research/":{"data":{"":"研究相关的笔记和资料。"},"title":"研究"},"/docs/research/ai4science/":{"data":{"":"AI for Science 相关研究。"},"title":"AI4Science"},"/docs/research/ai4science/ed-gat/":{"data":{"1-引言与背景#1. 引言与背景":"想象一下，我们需要设计一种新的超导材料。传统的做法是什么？科学家们会使用第一性原理计算，特别是密度泛函理论（DFT），从量子力学的基本原理出发，计算每个原子的相互作用。这听起来很美好，但现实是残酷的：\n对于一个包含几百个原子的材料体系，DFT计算可能需要运行几天甚至几周。而如果我们想要筛选成千上万种候选材料来找到最优的超导体，这种方法就变得完全不现实了。\n这就是AI4Science要解决的核心问题：能否用人工智能来学习物理规律，从而大幅度加速科学计算？","2-理论基础从对称性原理到深度学习架构#2. 理论基础：从对称性原理到深度学习架构":"","21-物理系统的对称性困境#2.1 物理系统的对称性困境":"让我们从一个看似简单但极具挑战性的问题开始。假设我们有一个双原子分子，想用神经网络预测其势能面。传统的做法是将原子坐标[x1, y1, z1, x2, y2, z2]直接输入全连接网络。\n然而，这种方法存在一个根本性的缺陷：破坏了物理系统的内在对称性。当我们将整个分子在空间中旋转时，坐标数值会发生剧烈变化，但从物理学角度，分子的内在性质（如键长、键角、势能）应该保持不变。这种不一致性迫使网络需要学习所有可能的旋转变换，导致数据需求的组合爆炸。\n更深层的问题在于，传统神经网络无法理解物理量的张量性质。力常数不是简单的标量，而是二阶张量，其在坐标变换下遵循特定的变换规律。忽视这一点等于抛弃了物理学几百年来积累的对称性智慧。","22-群论与等变性数学的美学与物理的必然#2.2 群论与等变性：数学的美学与物理的必然":"要解决这个问题，我们需要回到数学的基础——群论。在物理系统中，对称性变换形成一个数学群，其中最重要的是三维旋转群。","23-不可约表示理论物理量的分类学#2.3 不可约表示理论：物理量的分类学":"SO(3)群的不可约表示为我们提供了理解和构造等变网络的数学框架。每个不可约表示对应一种特定的物理量类型。","24-图神经网络拓扑结构与几何信息的统一#2.4 图神经网络：拓扑结构与几何信息的统一":"有了群论的数学基础，我们现在面临第二个挑战：如何在保持等变性的同时处理复杂的晶体拓扑结构？","25-注意力机制的等变推广#2.5 注意力机制的等变推广":"传统的注意力机制 $\\text{Attention}(Q,K,V) = \\text{softmax}(QK^T/\\sqrt{d})V$ 假设查询、键、值都是向量，这在几何等变的语境下是不充分的。","26-径向函数与角度编码的统一#2.6 径向函数与角度编码的统一":"等变图神经网络需要同时处理标量距离信息和矢量方向信息。我们采用径向-角度分离的策略：","27-自动微分从能量函数到力常数的数学机器#2.7 自动微分：从能量函数到力常数的数学机器":"最后一个核心技术涉及计算物理学的基本原理：如何从单一的能量函数推导出所有相关的物理量。","3-数据处理管道从原始数据到训练就绪#3. 数据处理管道：从原始数据到训练就绪":"","31-我们的数据来源理解phonopy的输出#3.1 我们的数据来源：理解Phonopy的输出":"在开始构建神经网络之前，我们需要理解我们的训练数据是什么样的。","32-从晶体到图一步步构建数据表示#3.2 从晶体到图：一步步构建数据表示":"","33-力常数数据的组织#3.3 力常数数据的组织":"","34-数据增强教会网络物理对称性#3.4 数据增强：教会网络物理对称性":"虽然我们的网络在架构上具有等变性，但数据增强仍然是有用的。通过人为地生成更多符合物理对称性的训练样本，我们可以让网络更快地学会这些规律。","35-批处理让训练高效进行#3.5 批处理：让训练高效进行":"最后一个重要的技术细节是批处理。对于图数据，批处理比普通数据更复杂，因为不同的图可能有不同的大小。","4-模型架构详解从概念验证到产品级实现#4. 模型架构详解：从概念验证到产品级实现":"","41-架构演进的思考历程#4.1 架构演进的思考历程":"在深度学习项目中，模型架构往往不是一蹴而就的。我们的项目展现了从初步概念到成熟实现的典型演进路径，这种迭代过程体现了科学研究的本质——不断试错、改进和优化。","42-graphattentiontransformer_dx架构深度解析#4.2 GraphAttentionTransformer_dx架构深度解析":"","43-关键技术决策的物理动机#4.3 关键技术决策的物理动机":"","5-训练策略与损失函数监督学习的物理化设计#5. 训练策略与损失函数：监督学习的物理化设计":"","51-监督学习范式为什么不选择pinn#5.1 监督学习范式：为什么不选择PINN？":"在开始详细讨论训练策略之前，我们需要理解一个重要的方法论选择：为什么采用监督学习而不是物理信息神经网络（PINN）？","52-损失函数的精细设计#5.2 损失函数的精细设计":"","53-优化策略的系统化设计#5.3 优化策略的系统化设计":"","6-自动微分与物理验证从数学到物理的桥梁#6. 自动微分与物理验证：从数学到物理的桥梁":"","61-三种计算策略的技术对比#6.1 三种计算策略的技术对比":"我们的项目实现了三种不同的Hessian矩阵计算方法，每种方法都有其独特的技术特点和适用场景。","611-train_one_epoch_dx精确自动微分#6.1.1 train_one_epoch_dx：精确自动微分":"这是我们的主要方法，基于PyTorch的自动微分机制：\ndef train_one_epoch_dx(model, criterion, data_loader, optimizer, device, epoch): for step, data in enumerate(data_loader): data.pos.requires_grad_(True) # 逐样本处理确保梯度隔离 for sample_idx in data.batch.unique(): sample_mask = data.batch == sample_idx sample_pos = data.pos[sample_mask] # 第一步：能量预测 energy = model(batch=data.batch[sample_mask], ...).sum() # 第二步：力计算（一阶导数） forces = -torch.autograd.grad( energy, sample_pos, create_graph=True )[0] # 第三步：力常数计算（二阶导数） force_constants_list = [] for i in range(n): for alpha in range(3): grad = torch.autograd.grad( forces[i, alpha], sample_pos, retain_graph=True, create_graph=True )[0] force_constants_list.append(grad) 技术优势：\n数学精确性：精度仅受浮点数表示限制 自动一致性：力与力常数的微分关系自动满足 内存效率：动态计算图，只保存必要的中间结果 计算复杂度分析：\n时间复杂度：$O(N^2 \\cdot C)$，其中$N$是原子数，$C$是模型计算复杂度 空间复杂度：$O(N^2 \\cdot P)$，其中$P$是模型参数数量","612-train_one_epoch_fd有限差分方法#6.1.2 train_one_epoch_fd：有限差分方法":"作为对比基线，我们实现了有限差分版本：\ndef compute_hessian_finite_difference(model, data, epsilon=1e-6): pos = data.pos.clone().detach() n = pos.size(0) hessian = torch.zeros((n, 3, n, 3)) for i in range(n): for alpha in range(3): # 正向扰动 pos_plus = pos.clone() pos_plus[i, alpha] += epsilon energy_plus = model(..., pos=pos_plus) # 负向扰动 pos_minus = pos.clone() pos_minus[i, alpha] -= epsilon energy_minus = model(..., pos=pos_minus) # 中心差分 for j in range(n): for beta in range(3): pos_plus_plus = pos_plus.clone() pos_plus_plus[j, beta] += epsilon energy_plus_plus = model(..., pos=pos_plus_plus) # ... (其他三个点的计算) hessian[i, alpha, j, beta] = ( energy_plus_plus - energy_plus_minus - energy_minus_plus + energy_minus_minus ) / (4 * epsilon**2) 数值分析考虑：\n截断误差：$O(\\epsilon^2)$对于中心差分 舍入误差：$O(\\frac{\\text{machine_eps}}{\\epsilon^2})$ 最优步长：$\\epsilon_{\\text{opt}} = (\\text{machine_eps})^{1/4}$","613-train_one_epoch_kfackronecker分解近似#6.1.3 train_one_epoch_kfac：Kronecker分解近似":"K-FAC（Kronecker-factored Approximate Curvature）方法基于以下近似：\ndef kfac_hessian_approximation(model_pred_fn, sample_pos, damping=1e-2): # 计算雅可比矩阵 jacobian = torch.autograd.functional.jacobian(model_pred_fn, sample_pos) # K-FAC近似：假设Hessian可以分解为Kronecker积 # H ≈ A ⊗ B，其中A和B是低维矩阵 # 对于我们的应用，使用对角近似 n = sample_pos.size(0) diag_approx = torch.zeros(n, 3, n, 3) for i in range(n): for alpha in range(3): # 只计算对角块 diag_approx[i, alpha, i, alpha] = ( jacobian[i, alpha] * jacobian[i, alpha] + damping ) return diag_approx 理论基础： K-FAC基于Gauss-Newton近似： H≈JTJ+λI\\mathbf{H} \\approx \\mathbf{J}^T \\mathbf{J} + \\lambda \\mathbf{I}H≈JTJ+λI其中$\\mathbf{J}$是雅可比矩阵，$\\lambda$是阻尼参数。","62-evaluate_dx物理一致性的实时验证#6.2 evaluate_dx：物理一致性的实时验证":"评估函数不仅计算预测精度，更重要的是验证物理一致性：\ndef evaluate_dx(model, data_loader, device, amp_autocast=None, logger=None): model.eval() for step, data in enumerate(data_loader): data.pos.requires_grad_(True) with torch.no_grad(): # 评估时不需要梯度 for sample_idx in data.batch.unique(): sample_pos = data.pos[sample_mask] sample_pos.requires_grad_(True) # 使用函数式接口计算Hessian def model_pred(pos): return model(..., pos=pos).sum() hessian = torch.autograd.functional.hessian( model_pred, sample_pos ) # 物理一致性检查 symmetry_error = check_symmetry(hessian) positivity_error = check_positive_definiteness(hessian) logger.info(f\"Symmetry error: {symmetry_error:.6f}\") logger.info(f\"Positivity violation: {positivity_error:.6f}\")","63-计算效率的系统化优化#6.3 计算效率的系统化优化":"","64-数值稳定性的深层考虑#6.4 数值稳定性的深层考虑":"","7-技术挑战与解决方案从理论到实践的鸿沟#7. 技术挑战与解决方案：从理论到实践的鸿沟":"","71-内存爆炸自动微分的阿喀琉斯之踵#7.1 内存爆炸：自动微分的阿喀琉斯之踵":"","72-数值稳定性双精度的必要性#7.2 数值稳定性：双精度的必要性":"","73-批处理的几何挑战#7.3 批处理的几何挑战":"","74-等变性验证理论与实践的差距#7.4 等变性验证：理论与实践的差距":"","75-超参数敏感性炼金术到科学#7.5 超参数敏感性：炼金术到科学":"","8-实验结果与性能分析#8. 实验结果与性能分析":"","81-核心性能指标#8.1 核心性能指标":"我们的最终模型在力常数预测任务上取得了显著的性能提升：\n关键结果：\nMAE (Mean Absolute Error)：从初始的 0.3 eV/Å² 降低到 0.15 eV/Å²，提升了50% 物理一致性：对称性误差控制在 1e-6 量级 计算效率：相比传统DFT计算，速度提升 1000倍 性能演进路径：\nGraphTransformer (baseline): MAE = 0.45 eV/Å²\rGraphAttentionTransformer: MAE = 0.32 eV/Å² GraphAttentionTransformer_dx_v1: MAE = 0.28 eV/Å²\rGraphAttentionTransformer_dx_v3: MAE = 0.18 eV/Å²\rGraphAttentionTransformer_dx_v4: MAE = 0.15 eV/Å² (最终版本) 每个版本的改进都解决了特定的技术问题：v1引入自动微分、v3优化内存管理、v4实现数值稳定性。","9-未来展望与改进方向#9. 未来展望与改进方向":"","91-方法论的进一步发展#9.1 方法论的进一步发展":"","92-pinn集成的第二次尝试#9.2 PINN集成的第二次尝试":"","93-基础科学问题的探索#9.3 基础科学问题的探索":"","adamw修正的adam优化器#AdamW：修正的Adam优化器":"我们选择AdamW而不是标准Adam，主要考虑：\noptimizer = torch.optim.AdamW( model.parameters(), lr=5e-5, weight_decay=5e-3, betas=(0.9, 0.999), eps=1e-8 ) AdamW的技术优势：\n解耦权重衰减：$\\theta_{t+1} = \\theta_t - \\eta(\\frac{m_t}{\\sqrt{v_t} + \\epsilon} + \\lambda\\theta_t)$ 更好的泛化性能：避免了Adam中权重衰减与梯度的耦合 稳定的收敛性：在大规模模型上表现更稳定","clebsch-gordan系数与张量积#Clebsch-Gordan系数与张量积":"等变网络的核心操作是张量积，它描述了不同阶表示的耦合： Yl1m1⊗Yl2m2=∑l=∣l1−l2∣l1+l2∑m=−llCl1m1,l2m2lmYlmY_{l_1}^{m_1} \\otimes Y_{l_2}^{m_2} = \\sum_{l=|l_1-l_2|}^{l_1+l_2} \\sum_{m=-l}^{l} C_{l_1 m_1, l_2 m_2}^{l m} Y_l^mYl1​m1​​⊗Yl2​m2​​=l=∣l1​−l2​∣∑l1​+l2​​m=−l∑l​Cl1​m1​,l2​m2​lm​Ylm​其中$C_{l_1 m_1, l_2 m_2}^{l m}$是Clebsch-Gordan系数，它们确保了张量积的等变性。这不仅是数学上的优雅，更是物理上的必然——量子力学中的角动量耦合遵循完全相同的规律。","eigenloss频谱信息的物理洞察#EigenLoss：频谱信息的物理洞察":"class EigenLoss(nn.Module): def forward(self, input, target): loss_1 = torch.mean((input[:, :3] - target[:, :3]).pow(2)) loss_2 = torch.mean(sum( (torch.norm(input[:, 3+3*i:6+3*i], dim=1) - torch.norm(target[:, 3+3*i:6+3*i], dim=1)).pow(2) for i in range(3) )) return loss_1 + loss_2 这个损失函数基于力常数矩阵的特征值分解：\nΦ=QΛQT\\Phi = Q \\Lambda Q^TΦ=QΛQT其中$\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\lambda_3)$是特征值矩阵。\n物理动机：\n特征值对应正常模式的频率：$\\omega_i = \\sqrt{\\lambda_i/m}$ 特征向量对应振动模式的形状 这种分解直接关联到声子谱的计算 损失函数的两项分别处理：\nloss_1：特征值的准确性（频率匹配） loss_2：特征向量方向的准确性（模式形状匹配）","ema指数移动平均的统计力学解释#EMA：指数移动平均的统计力学解释":"model_ema = ModelEmaV2( model, decay=0.9999, device='cpu' if args.model_ema_force_cpu else None ) EMA可以从统计力学的角度理解： θema(t)=αθema(t−1)+(1−α)θ(t)\\theta_{\\text{ema}}^{(t)} = \\alpha \\theta_{\\text{ema}}^{(t-1)} + (1-\\alpha) \\theta^{(t)}θema(t)​=αθema(t−1)​+(1−α)θ(t)这相当于在参数空间中维护一个热平衡态的系综平均：\n减少随机梯度的噪声影响 提供更稳定的模型性能 类似于分子动力学中的温度控制器","frobeniusnormloss矩阵范数的几何直觉#FrobeniusNormLoss：矩阵范数的几何直觉":"class FrobeniusNormLoss(nn.Module): def forward(self, input, target): return torch.mean((torch.norm(input, p='fro', dim=1) - torch.norm(target, p='fro', dim=1))**2) 这个损失函数的设计体现了深刻的几何直觉。Frobenius范数：\n∥A∥F=∑i,jAij2\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}∥A∥F​=i,j∑​Aij2​​在力常数矩阵的语境下具有清晰的物理意义：\n总相互作用强度：$|\\Phi|_F$衡量原子间总的耦合强度 旋转不变性：Frobenius范数在正交变换下不变 尺度感知性：能够感知不同原子对贡献的差异","hamilton力学与梯度关系#Hamilton力学与梯度关系":"在经典力学中，系统的动力学完全由Hamilton函数（总能量）决定。对于我们的晶体系统，关键的物理量通过以下微分关系联系：\nFi=−∂E∂ri\\mathbf{F}_i = -\\frac{\\partial E}{\\partial \\mathbf{r}_i}Fi​=−∂ri​∂E​ Φiα,jβ=∂2E∂riα∂rjβ=−∂Fiα∂rjβ\\Phi_{i\\alpha,j\\beta} = \\frac{\\partial^2 E}{\\partial r_{i\\alpha} \\partial r_{j\\beta}} = -\\frac{\\partial F_{i\\alpha}}{\\partial r_{j\\beta}}Φiα,jβ​=∂riα​∂rjβ​∂2E​=−∂rjβ​∂Fiα​​其中$E({\\mathbf{r}_i})$是系统总能量，$\\mathbf{F}i$是作用在原子$i$上的力，$\\Phi{i\\alpha,j\\beta}$是力常数矩阵元素。","hessian矩阵的高效计算#Hessian矩阵的高效计算":"力常数矩阵本质上是能量函数的Hessian矩阵。对于包含$N$个原子的系统，这是一个$3N \\times 3N$的矩阵，包含$O(N^2)$个元素。\n朴素的计算方法需要$O(N^2)$次反向传播，计算复杂度为$O(N^3)$。我们采用更高效的策略：\n逐原子计算：对每个原子$i$，计算$\\frac{\\partial \\mathbf{F}_i}{\\partial \\mathbf{r}_j}$ 稀疏性利用：利用力常数矩阵的稀疏结构减少计算 批处理优化：并行计算多个原子对的贡献","irreps配置的物理意义#irreps配置的物理意义":"irreps_node_embedding='128x0e+64x1e+32x2e' 这个配置体现了物理量的层次结构：\n128x0e：大量标量特征，捕获化学环境、电子密度等 64x1e：中等数量的向量特征，处理偶极矩、局部电场等 32x2e：少量张量特征，直接关联力常数、应力等 数量比例128:64:32反映了一个重要的物理直觉：标量信息最基础，张量信息最稀缺但最重要。","phonopy声子计算的瑞士军刀#Phonopy：声子计算的瑞士军刀":"Phonopy是材料科学中计算声子（晶格振动）性质的标准工具。它的工作流程是这样的：\n输入结构：给定一个晶体的原胞结构 构建超胞：创建一个更大的重复单元 微小扰动：系统地微移每个原子 DFT计算：对每种扰动配置计算能量和力 有限差分：通过数值求导得到力常数 我们的训练数据就是这个过程的输出。","pinn方法的理论吸引力#PINN方法的理论吸引力":"物理信息神经网络的核心思想是将物理定律直接编码到损失函数中：\nLPINN=Ldata+λLphysicsL_{\\text{PINN}} = L_{\\text{data}} + \\lambda L_{\\text{physics}}LPINN​=Ldata​+λLphysics​其中物理损失可能包括：\n力的一致性：$L_{\\text{force}} = |\\mathbf{F}{\\text{pred}} + \\nabla{\\mathbf{r}} E_{\\text{pred}}|^2$ 动力学方程：$L_{\\text{dynamics}} = |m\\ddot{\\mathbf{r}} - \\mathbf{F}|^2$ 对称性约束：$L_{\\text{symmetry}} = |E(g \\cdot \\mathbf{r}) - E(\\mathbf{r})|^2$","pytorch-geometric的解决方案#PyTorch Geometric的解决方案":"PyTorch Geometric库提供了一个巧妙的解决方案：\n图合并：将批次中的多个图合并成一个大图 索引调整：自动调整边的索引，避免冲突 批次标记：用一个额外的向量记录每个节点属于哪个原始图 # 自动批处理 dataloader = DataLoader(dataset, batch_size=32, shuffle=True) for batch in dataloader: # batch.x: 所有图的节点特征拼接 # batch.edge_index: 调整后的边索引 # batch.batch: 节点到图的映射 output = model(batch) 这种设计让我们能够高效地并行处理多个晶体结构，大大提高训练速度。\n通过这三章，我们已经建立了从基本概念到实际实现的完整理解。我们看到了：\n为什么需要专门的技术来处理物理问题 如何将抽象的数学概念转化为具体的算法 怎样将真实的物理数据转换为可训练的格式","为什么选择标量输出#为什么选择标量输出？":"这个决策背后有深刻的物理原理：\n热力学第一定律：能量是状态函数，只依赖于系统的配置 变分原理：平衡态对应能量极值，所有其他量都是能量的函数 微分几何：力常数是能量函数的Hessian矩阵，数学上完全确定","从晶体学到图论天然的数学对应#从晶体学到图论：天然的数学对应":"晶体结构在数学上可以自然地表示为赋权图 $G = (V, E, W)$：\n顶点集 $V$：原子位置 ${r_i}_{i=1}^N$ 边集 $E$：原子间相互作用关系 权重函数 $W$：编码距离和方向信息的边特征 这种表示的数学优势在于：\n尺寸不变性：图可以自然处理不同大小的晶体超胞 局部性原理：符合物理中的\"近程相互作用占主导\"假设 置换等变性：图结构天然具有对原子标记重排的不变性","从欧几里得群到特殊正交群#从欧几里得群到特殊正交群":"三维空间中的等距变换构成欧几里得群E(3)，它包含旋转和平移两种基本操作。对于晶体系统，我们主要关注特殊正交群SO(3)——所有保持原点不动的旋转变换。\nSO(3)群的数学表示为： SO(3)={R∈R3×3:RTR=I,det⁡(R)=1}SO(3) = \\{R \\in \\mathbb{R}^{3 \\times 3} : R^T R = I, \\det(R) = 1\\}SO(3)={R∈R3×3:RTR=I,det(R)=1}这个群的重要性在于，物理定律在SO(3)变换下保持不变。这不仅是一个数学抽象，更是自然界的基本原理——诺特定理告诉我们，对称性与守恒律之间存在深刻的联系。","从稀疏到稠密矩阵重构#从稀疏到稠密：矩阵重构":"FORCE_CONSTANTS文件通常只包含非零的矩阵元素。我们需要重构完整的矩阵：\ndef reconstruct_force_constants_matrix(self): # 初始化零矩阵 force_constants_all = torch.zeros((num_atoms, num_atoms, 3, 3)) # 填充非零元素 for atom_pair, matrix_3x3 in self.force_constants.items(): i, j = atom_pair force_constants_all[i-1, j-1] = matrix_3x3 # 注意索引从0开始 return force_constants_all","传统方法的困境一致性问题#传统方法的困境：一致性问题":"传统的机器学习方法分别训练不同的模型来预测$E$、$\\mathbf{F}$和$\\Phi$。这种策略存在根本性缺陷：\n热力学不一致性：独立训练的模型无法保证$\\mathbf{F} = -\\nabla E$ 动力学不一致性：力常数与力场之间缺乏微分关系 能量守恒违背：在分子动力学模拟中导致非物理的能量漂移","余弦退火学习率模拟物理退火过程#余弦退火学习率：模拟物理退火过程":"scheduler = torch.optim.lr_scheduler.CosineAnnealingLR( optimizer, T_max=epochs, eta_min=1e-6 ) 余弦退火策略模拟了物理退火过程： ηt=ηmin+12(ηmax−ηmin)(1+cos⁡(tπT))\\eta_t = \\eta_{\\text{min}} + \\frac{1}{2}(\\eta_{\\text{max}} - \\eta_{\\text{min}})(1 + \\cos(\\frac{t\\pi}{T}))ηt​=ηmin​+21​(ηmax​−ηmin​)(1+cos(Ttπ​))物理类比：\n高温阶段（大学习率）：允许大幅参数调整，跳出局部极值 降温阶段（学习率衰减）：逐渐细化，寻找精确解 低温阶段（小学习率）：微调参数，达到最终平衡","内存管理策略#内存管理策略":"自动微分的内存消耗可能很大，我们采用了多种优化策略：\n1. 分块计算：\n# 对于大系统，分块计算Hessian chunk_size = 64 # 根据GPU内存调整 for chunk_start in range(0, total_size, chunk_size): chunk_end = min(chunk_start + chunk_size, total_size) chunk_hessian = compute_hessian_chunk(chunk_start, chunk_end) full_hessian[chunk_start:chunk_end] = chunk_hessian 2. 梯度检查点：\nfrom torch.utils.checkpoint import checkpoint # 使用梯度检查点减少内存使用 def forward_with_checkpoint(model, inputs): return checkpoint(model, inputs, use_reentrant=False) 3. 稀疏性利用：\n# 利用力常数矩阵的稀疏性 def sparse_hessian_computation(model, pos, cutoff_distance=8.0): # 只计算距离小于cutoff的原子对 distances = torch.cdist(pos, pos) active_pairs = (distances \u003c cutoff_distance).nonzero() sparse_hessian = torch.sparse_coo_tensor( active_pairs.T, compute_partial_hessian(active_pairs), size=(3*len(pos), 3*len(pos)) ) return sparse_hessian","力常数#力常数":"在所有需要计算的物理量中，力常数特别重要。你可以把它想象成材料的\"弹性指纹\"——它描述了当你轻微移动一个原子时，其他原子会如何响应。更准确地说，力常数告诉我们原子间的相互作用力如何随位置变化。\n力常数不仅决定了材料的机械性质（硬度、弹性模量），还影响热学性质（热导率、比热容）和动力学性质（声音传播速度）。可以说，掌握了力常数，就掌握了材料行为的关键。","变长图的批处理困境#变长图的批处理困境":"与图像或文本不同，晶体图没有固定的大小。一个硅晶体可能有32个原子，而一个蛋白质晶体可能有几百个原子。传统的批处理方法无法直接应用。\nPyTorch Geometric的解决方案是图拼接：\n# 多个图合并成一个大图 batch_1: nodes=[1,2,3], edges=[(1,2), (2,3)] batch_2: nodes=[4,5], edges=[(4,5)] # 合并后: nodes=[1,2,3,4,5], edges=[(1,2), (2,3), (4,5)] 但这种方法在自动微分中引入了新问题：梯度串扰。","多尺度建模的集成#多尺度建模的集成":"目前的方法主要处理原子尺度的相互作用，但真实材料的性质往往涉及多个尺度：\n# 未来的多尺度架构概念 class MultiScaleEquivariantNetwork: def __init__(self): self.atomic_scale = E3NN_AtomicLevel() # 1-10 Å self.mesoscale = E3NN_GrainLevel() # 10-1000 Å self.macroscale = ContinuumModel() # \u003e 1 μm def forward(self, structure): atomic_features = self.atomic_scale(structure) meso_features = self.mesoscale(atomic_features) macro_properties = self.macroscale(meso_features) return macro_properties","多版本迭代的技术细节#多版本迭代的技术细节":"我们的代码中存在四个版本的dx模型，每个版本都解决了特定的技术问题：\nGraphAttentionTransformer_dx（v1）：\n基础的自动微分实现 直接从边特征预测能量 GraphAttentionTransformer_dx_v2：\n改进了聚合机制，使用scatter_mean替代简单求和 更好的批处理支持 GraphAttentionTransformer_dx_v3：\n优化了梯度流，添加了trace_grad_fn调试工具 改进了内存管理 GraphAttentionTransformer_dx_v4：\n最终的产品版本，集成了所有改进 最优的计算效率和数值稳定性","多精度计算策略#多精度计算策略":"对于特别敏感的计算，我们实现了混合精度策略：\ndef mixed_precision_hessian(model, pos): # 前向传播使用半精度 with torch.autocast('cuda', dtype=torch.float16): energy = model(pos) # 梯度计算使用单精度 with torch.autocast('cuda', enabled=False): forces = torch.autograd.grad(energy, pos, create_graph=True)[0] # Hessian计算使用双精度 pos_double = pos.double() forces_double = forces.double() hessian = torch.autograd.functional.hessian( lambda x: model(x.float()).double(), pos_double ) return hessian.float() 这种策略在保持数值精度的同时，尽可能减少了计算时间和内存使用。","学习率的临界调节#学习率的临界调节":"我们发现力常数预测对学习率极其敏感：\n# 学习率扫描实验 lr_results = { 1e-3: \"发散，梯度爆炸\", 5e-4: \"震荡，无法收敛\", 1e-4: \"缓慢收敛，性能中等\", 5e-5: \"稳定收敛，最佳性能\", # 最终选择 1e-5: \"过慢收敛，欠拟合\", 1e-6: \"几乎无学习\" } 物理解释：力常数作为二阶导数，对参数变化极其敏感。过大的学习率会导致能量面的剧烈振荡。","实现细节pytorch中的二阶导数#实现细节：PyTorch中的二阶导数":"在我们的代码中，核心计算流程为：\n# 第一步：计算能量 energy = model(pos, batch, ...) # 形状: [batch_size] # 第二步：计算力 (一阶导数) forces = -torch.autograd.grad( energy.sum(), pos, create_graph=True, retain_graph=True )[0] # 形状: [num_atoms, 3] # 第三步：计算力常数 (二阶导数) force_constants = [] for i in range(num_atoms): for alpha in range(3): grad = torch.autograd.grad( forces[i, alpha], pos, retain_graph=True, create_graph=True )[0] # 形状: [num_atoms, 3] force_constants.append(grad) 这种方法的关键优势是数学保证的一致性：所有物理量都来自同一个能量函数，自动满足热力学和动力学约束。","径向基函数展开#径向基函数展开":"距离信息通过径向基函数编码： R(r)=∑k=1Kwkϕk(r)R(r) = \\sum_{k=1}^{K} w_k \\phi_k(r)R(r)=k=1∑K​wk​ϕk​(r)在我们的实现中，使用高斯径向基函数： ϕk(r)=exp⁡(−γ(r−μkσk)2)\\phi_k(r) = \\exp\\left(-\\gamma \\left(\\frac{r - \\mu_k}{\\sigma_k}\\right)^2\\right)ϕk​(r)=exp(−γ(σk​r−μk​​)2)这种选择的物理动机来自于原子相互作用的局域性——大多数相互作用在特定距离范围内最强。","微分几何的深度应用#微分几何的深度应用":"力常数矩阵的几何性质远比我们目前利用的更丰富：\nRiemann几何视角： Φij=∂2E∂ri∂rj\\Phi_{ij} = \\frac{\\partial^2 E}{\\partial r_i \\partial r_j}Φij​=∂ri​∂rj​∂2E​可以视为配置空间上的度量张量，定义了能量景观的曲率。\n拓扑不变量： 某些材料性质与拓扑保护的量子态相关，如拓扑绝缘体的表面态。未来的模型需要能够识别和利用这些拓扑特征。","我们的目标#我们的目标":"本项目的目标很明确：构建一个深度学习系统，能够在几秒钟内预测任意晶体结构的力常数，准确度接近传统DFT计算，但速度快几个数量级。\n为了实现这个目标，我们需要解决三个核心挑战：\n如何让神经网络理解三维空间中的物理对称性？ 如何表示复杂的晶体结构？ 如何确保预测结果符合物理定律？","我们选择监督学习的实际考量#我们选择监督学习的实际考量":"尽管PINN在理论上很吸引人，我们最终选择了监督学习方案，主要基于以下考虑：\n1. 数据质量与可用性 我们拥有高质量的DFT计算数据，包含：\n精确的晶体结构 可靠的力常数矩阵 一致的计算设置 在这种情况下，利用高质量监督信号比强加物理约束更有效。\n2. 等变架构的隐式物理约束 我们的E3NN架构本身就编码了重要的物理定律：\n对称性：通过群论自动满足 局部性：通过图结构体现 微分关系：通过自动微分保证 3. 训练稳定性 PINN的训练往往面临困难：\n多个损失项的权重平衡 不同物理约束之间的冲突 收敛性的不确定性","批处理优化#批处理优化":"为了提高训练效率，我们设计了专门的批处理策略：\ndef efficient_batch_processing(data_loader): for batch in data_loader: # 按原子数量对样本分组 samples_by_size = group_by_atom_count(batch) for size, samples in samples_by_size.items(): # 相同大小的样本可以真正并行处理 if len(samples) \u003e 1: parallel_hessian_computation(samples) else: sequential_hessian_computation(samples[0]) 这种策略显著提高了训练效率，特别是对于包含不同大小晶体的数据集。","挑战的本质#挑战的本质":"在实际实现中，我们遇到的最大挑战是内存消耗的指数级增长。对于包含$N$个原子的系统，计算完整的Hessian矩阵需要存储$3N \\times 3N$的二阶导数信息。当$N=50$时，这已经是一个$150 \\times 150 = 22,500$元素的矩阵。\n更严重的是，PyTorch的自动微分机制需要维护完整的计算图：\n# 问题代码：会导致内存爆炸 for i in range(n): for alpha in range(3): grad = torch.autograd.grad( forces[i, alpha], sample_pos, retain_graph=True, create_graph=True # 保持完整计算图！ )[0] 每次retain_graph=True调用都会在内存中保留从输入到输出的全部中间计算结果。对于深度为6层的等变网络，这种累积效应是灾难性的。","损失函数选择的实验验证#损失函数选择的实验验证":"在我们的实验中，我们比较了不同损失函数的效果：\n损失函数 训练稳定性 物理一致性 计算效率 MSE 高 中 高 FrobeniusNorm 中 高 中 EigenLoss 中 最高 低 组合损失 最高 高 中 最终我们采用了自适应组合策略： L=αLMSE+βLFrobenius+γLEigenL = \\alpha L_{\\text{MSE}} + \\beta L_{\\text{Frobenius}} + \\gamma L_{\\text{Eigen}}L=αLMSE​+βLFrobenius​+γLEigen​其中权重$\\alpha, \\beta, \\gamma$在训练过程中动态调整。","数值精度的级联失效#数值精度的级联失效":"# 问题示例：单精度的累积误差 energy = model(pos).float() # ~1e-7相对误差 forces = -torch.autograd.grad(energy, pos)[0] # ~1e-6相对误差 hessian = compute_hessian(forces, pos) # ~1e-4相对误差！ 在单精度下，每次微分操作都会放大数值误差。二阶导数的相对误差可能达到$10^{-4}$量级，这对于精确的物理预测是不可接受的。","新物理现象的发现#新物理现象的发现":"训练好的模型可能揭示人类尚未发现的物理规律：\nclass PhysicsDiscovery: def __init__(self, model): self.model = model def find_scaling_laws(self, material_database): # 在大规模材料数据库中寻找标度律 predictions = self.model(material_database) # 寻找意外的相关性 correlations = self.analyze_correlations(predictions) # 提取可能的新物理定律 candidate_laws = self.extract_scaling_relations(correlations) return candidate_laws","旋转增强#旋转增强":"def rotate_crystal_data(pos, force_constants): # 生成随机旋转矩阵 rotation_matrix = generate_random_rotation() # 旋转原子坐标 pos_rotated = torch.matmul(pos, rotation_matrix.T) # 相应地变换力常数 fc_rotated = transform_force_constants(force_constants, rotation_matrix) return pos_rotated, fc_rotated 这个过程确保网络见到同一个物理体系的多种方位，强化等变性的学习。","机器学习的物理可解释性#机器学习的物理可解释性":"深层问题：为什么E3NN能够学习到正确的物理规律？是否存在更深层的数学结构？\n研究方向：\n表示学习理论：等变特征空间的几何结构 泛化界理论：物理对称性如何改善泛化性能 信息论分析：物理约束对信息容量的影响","核心组件架构#核心组件架构":"1. NodeEmbeddingNetwork：原子特征的等变编码\nself.atom_embed = NodeEmbeddingNetwork( self.irreps_node_embedding, _MAX_ATOM_TYPE ) 这个模块将86种化学元素的one-hot编码转换为等变特征表示。关键在于它不仅仅是简单的嵌入层，而是等变嵌入：\nhi(0)=Embedding(atom_typei)\\mathbf{h}_i^{(0)} = \\text{Embedding}(\\text{atom\\_type}_i)hi(0)​=Embedding(atom_typei​)其中$\\mathbf{h}_i^{(0)}$是原子$i$的标量特征。\n2. EdgeDegreeEmbeddingNetwork：边信息的几何编码\nself.edge_deg_embed = EdgeDegreeEmbeddingNetwork( self.irreps_node_embedding, self.irreps_edge_attr, self.fc_neurons, _AVG_DEGREE ) 这个组件处理边的几何信息，将距离和方向分别编码：\n径向信息：通过高斯径向基函数$\\phi_k(r)$处理 角度信息：通过球谐函数$Y_l^m(\\hat{\\mathbf{r}})$处理 3. TransBlock：等变注意力的核心\n每个TransBlock实现了等变消息传递：\nmij(l)=∑pathWpath⋅(hi(l1)⊗Yl(r^ij)⊗R(rij))path(l)\\mathbf{m}_{ij}^{(l)} = \\sum_{\\text{path}} W_{\\text{path}} \\cdot (\\mathbf{h}_i^{(l_1)} \\otimes \\mathbf{Y}_l(\\hat{\\mathbf{r}}_{ij}) \\otimes R(r_{ij}))_{\\text{path}}^{(l)}mij(l)​=path∑​Wpath​⋅(hi(l1​)​⊗Yl​(r^ij​)⊗R(rij​))path(l)​其中张量积$\\otimes$通过Clebsch-Gordan系数实现。","梯度裁剪防止数值不稳定#梯度裁剪：防止数值不稳定":"if clip_grad is not None: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=clip_grad ) 在物理系统的深度学习中，梯度爆炸是常见问题，特别是涉及：\n长程相互作用：导致梯度在空间上快速传播 多尺度动力学：不同时间尺度的耦合 临界现象：接近相变点时的敏感性 梯度裁剪提供了数值稳定性的保障。","梯度隔离的工程实现#梯度隔离的工程实现":"def isolated_gradient_computation(model, batch_data): batch_hessians = [] # 逐样本处理，确保梯度完全隔离 for sample_idx in batch_data.batch.unique(): sample_mask = batch_data.batch == sample_idx # 创建独立的子图 sample_pos = batch_data.pos[sample_mask].detach().clone() sample_pos.requires_grad_(True) # 独立计算，无梯度干扰 sample_hessian = compute_single_hessian(model, sample_pos) batch_hessians.append(sample_hessian) return torch.cat(batch_hessians, dim=0) 这种方法牺牲了部分并行性，但保证了每个样本的梯度计算完全独立，避免了微妙的数值错误。","正则化策略的选择#正则化策略的选择":"norm_layer='layer' alpha_drop=0.2 proj_drop=0.0 out_drop=0.0 这种配置的考虑：\nLayerNorm vs BatchNorm：LayerNorm在小批量下更稳定，适合昂贵的物理计算 注意力dropout vs 特征dropout：只在注意力权重上应用dropout，保持特征的等变性 输出层不dropout：确保能量预测的确定性","消息传递框架信息几何的观点#消息传递框架：信息几何的观点":"现代图神经网络基于**消息传递神经网络(MPNN)**框架。在数学上，一次消息传递可以表示为：\nmij(t)=ϕm(t)(hi(t),hj(t),eij)\\mathbf{m}_{ij}^{(t)} = \\phi_m^{(t)}(\\mathbf{h}_i^{(t)}, \\mathbf{h}_j^{(t)}, \\mathbf{e}_{ij})mij(t)​=ϕm(t)​(hi(t)​,hj(t)​,eij​) hi(t+1)=ϕh(t)(hi(t),⨁j∈N(i)mij(t))\\mathbf{h}_i^{(t+1)} = \\phi_h^{(t)}\\left(\\mathbf{h}_i^{(t)}, \\bigoplus_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{ij}^{(t)}\\right)hi(t+1)​=ϕh(t)​​hi(t)​,j∈N(i)⨁​mij(t)​​其中$\\phi_m$和$\\phi_h$是可学习的函数，$\\bigoplus$是置换不变的聚合函数（如求和或平均）。\n然而，标准MPNN存在致命缺陷：破坏了几何等变性。函数$\\phi_m$和$\\phi_h$通常由多层感知机实现，无法保持坐标变换的等变性。","深度学习加速晶体力常数预测基于等变图神经网络与自动微分的ai4science实践#深度学习加速晶体力常数预测：基于等变图神经网络与自动微分的AI4Science实践":"深度学习加速晶体力常数预测：基于等变图神经网络与自动微分的AI4Science实践","深度学习加速晶体力常数预测模型架构训练策略与物理验证续#深度学习加速晶体力常数预测：模型架构、训练策略与物理验证（续）":"","混合精度的精妙平衡#混合精度的精妙平衡":"我们开发了一套自适应混合精度策略：\ndef adaptive_precision_hessian(model, pos): # 前向传播：半精度（性能优先） with torch.autocast('cuda', dtype=torch.float16): energy = model(pos) # 检查数值稳定性 if torch.isnan(energy) or torch.isinf(energy): # 回退到单精度 with torch.autocast('cuda', dtype=torch.float32): energy = model(pos) # 梯度计算：根据系统大小选择精度 if pos.size(0) \u003e 30: # 大系统，使用双精度 pos_double = pos.double() energy_double = energy.double() hessian = torch.autograd.functional.hessian( lambda x: model(x.float()).double(), pos_double ) return hessian.float() else: # 小系统，单精度足够 return torch.autograd.functional.hessian( lambda x: model(x), pos )","物理一致性检查的具体实现#物理一致性检查的具体实现":"1. 对称性验证： 力常数矩阵必须满足$\\Phi_{ij} = \\Phi_{ji}^T$\ndef check_symmetry(hessian): n = hessian.size(0) // 3 reshaped = hessian.view(n, 3, n, 3) symmetry_error = torch.mean( (reshaped - reshaped.transpose(0, 2).transpose(1, 3))**2 ) return symmetry_error.item() 2. 正定性检查： 对于稳定的晶体，力常数矩阵应该是正半定的\ndef check_positive_definiteness(hessian): eigenvals = torch.linalg.eigvals(hessian) negative_eigenvals = eigenvals[eigenvals \u003c -1e-6] return len(negative_eigenvals)","球谐函数作为so3的不可约表示#球谐函数作为SO(3)的不可约表示":"球谐函数$Y_l^m(\\theta, \\phi)$构成SO(3)群的完备不可约表示： Ylm:S2→CY_l^m: S^2 \\to \\mathbb{C}Ylm​:S2→C其中：\n$l = 0, 1, 2, \\ldots$ 称为阶数或角动量量子数 $m = -l, -l+1, \\ldots, l-1, l$ 是磁量子数 每个$l$对应一个$(2l+1)$维的不可约表示空间 物理意义的对应关系：\n$l=0$：标量场（如密度、温度、势能） $l=1$：向量场（如位移、力、电场） $l=2$：二阶张量场（如应力、应变、力常数） 在我们的代码中，irreps_node_embedding='128x0e+64x1e+32x2e'precisely体现了这种分类：\n128x0e：128个标量特征通道 64x1e：64个向量特征通道 32x2e：32个二阶张量特征通道","球谐函数角度编码#球谐函数角度编码":"方向信息通过球谐函数的实形式编码： Yl(r^)=[Yl−l(r^),…,Yll(r^)]T\\mathbf{Y}_l(\\hat{\\mathbf{r}}) = [Y_l^{-l}(\\hat{\\mathbf{r}}), \\ldots, Y_l^l(\\hat{\\mathbf{r}})]^TYl​(r^)=[Yl−l​(r^),…,Yll​(r^)]T其中$\\hat{\\mathbf{r}} = \\mathbf{r}/|\\mathbf{r}|$是单位方向向量。\n张量积耦合将径向和角度信息结合： fij(l)=R(rij)⊗Yl(r^ij)\\mathbf{f}_{ij}^{(l)} = R(r_{ij}) \\otimes \\mathbf{Y}_l(\\hat{\\mathbf{r}}_{ij})fij(l)​=R(rij​)⊗Yl​(r^ij​)这种分离策略不仅数学上优雅，更重要的是它反映了物理相互作用的内在结构。","理解力常数矩阵的结构#理解力常数矩阵的结构":"力常数矩阵有一个特殊的结构。对于N个原子的体系，完整的力常数矩阵是3N×3N的（每个原子有x,y,z三个方向）。\n但在实际存储中，我们通常将其组织为N×N个3×3的子矩阵，其中每个子矩阵描述原子对(i,j)之间的相互作用。","理解数据格式#理解数据格式":"让我们看看实际的数据长什么样：\nphonopy.yaml文件（描述晶体结构）：\nsupercell: lattice: - [8.123, 0.000, 0.000] # 晶格向量 - [0.000, 8.123, 0.000] - [0.000, 0.000, 8.123] points: - symbol: Si # 硅原子 coordinates: [0.0, 0.0, 0.0] mass: 28.0855 - symbol: Si coordinates: [0.25, 0.25, 0.25] mass: 28.0855 FORCE_CONSTANTS文件（力常数矩阵）：\n1 1 # 原子对 (1,1)\r123.45 0.00 0.00 # 3x3 力常数矩阵\r0.00 123.45 0.00\r0.00 0.00 123.45\r1 2 # 原子对 (1,2)\r-23.45 0.00 0.00\r0.00 -23.45 0.00\r0.00 0.00 -23.45","病态矩阵的处理#病态矩阵的处理":"在实际计算中，我们经常遇到条件数很大的系统：\ndef stabilized_hessian_computation(model, pos, regularization=1e-6): hessian = compute_hessian(model, pos) # 检查条件数 condition_number = torch.linalg.cond(hessian) if condition_number \u003e 1e12: # 添加正则化项 n = hessian.size(0) hessian += regularization * torch.eye(n, device=hessian.device) logger.warning(f\"High condition number {condition_number:.2e}, \" f\"added regularization {regularization}\") return hessian","病态系统的普遍性#病态系统的普遍性":"在晶体系统中，病态矩阵（条件数很大的矩阵）是常态而非例外。这源于：\n多尺度相互作用：强共价键($\\sim 10^2$ eV/Å²)与弱范德华力($\\sim 10^{-2}$ eV/Å²)共存 几何敏感性：原子位置的微小变化可能导致相互作用的剧烈变化 对称性破缺：结构缺陷附近的力常数急剧变化","矩阵的重塑适应网络输入#矩阵的重塑：适应网络输入":"最后，我们需要将3×3的子矩阵重塑为向量形式，以便输入神经网络：\n# 将 (N, N, 3, 3) 重塑为 (N*N, 9) force_constants_reshaped = force_constants_all.view(-1, 9) 这个步骤看似技术性，但它反映了深度学习中一个重要的原则：将复杂的数学对象转换为向量，让神经网络能够处理。","离散化误差的累积#离散化误差的累积":"尽管E3NN在理论上严格等变，但在实际计算中，离散化误差会逐步累积：\ndef test_equivariance_violation(): # 测试旋转等变性 rotation_matrix = o3.rand_matrix() # 随机旋转 # 原始预测 energy_original = model(pos, batch, ...) # 旋转后预测 pos_rotated = torch.matmul(pos, rotation_matrix.T) energy_rotated = model(pos_rotated, batch, ...) # 理论上应该相等，但实际存在小误差 violation = torch.abs(energy_original - energy_rotated) print(f\"Equivariance violation: {violation.max().item():.2e}\") # 典型输出：1e-6 到 1e-4","第一步读取晶体结构#第一步：读取晶体结构":"我们首先需要将文本格式的数据转换为程序能理解的形式：\ndef read_phonopy_file(self): with open(self.phonopy_file_path, 'r') as file: yaml_content = yaml.safe_load(file) # 提取原子信息 supercell_points = yaml_content['supercell']['points'] # 提取晶格信息 supercell_lattice = yaml_content['supercell']['lattice'] return supercell_points, supercell_lattice 这一步看似简单，但它建立了从外部数据到内部表示的桥梁。","第一阶段概念验证graphtransformer#第一阶段：概念验证（GraphTransformer）":"最初，我们从最直观的想法开始：能否用传统的图卷积网络加上Transformer来处理晶体结构？\n# module.py中的初代模型 class GraphTransformer(torch.nn.Module): def __init__(self, node_features, num_layers, heads): super(GraphTransformer, self).__init__() self.conv1 = GCNConv(node_features, 256) self.conv2 = GCNConv(256, 256) self.transformer_encoder_layer = TransformerEncoderLayer(d_model=256, nhead=heads) self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layer, num_layers) 这个初代模型有着明显的局限性：\n缺乏几何等变性：标准的GCN和Transformer无法处理3D旋转对称性 特征表示简单：只使用标量特征，忽略了张量性质 物理意义模糊：输出直接预测9维力常数向量，缺乏物理约束 从训练脚本中可以看到，最初我们使用的是：\n# scripts/train_1.sh 中被注释的旧版本 # python -u main.py \\ # --model-name 'graph_attention_transformer_nonlinear_l2_e3_noNorm' \\","第三步构建图的连接关系#第三步：构建图的连接关系":"现在到了关键步骤：决定哪些原子之间应该有边。我们使用一个混合策略：\nthreshold = 8.0 # 8埃的距离截断 max_neighbors = 32 # 最多32个邻居 for i in range(num_atoms): # 找到距离最近的邻居 distances_to_i = dist_matrix[i] nearest_neighbors = distances_to_i.argsort()[1:max_neighbors+1] for j in nearest_neighbors: if distances_to_i[j] \u003c= threshold: # 添加边 i -\u003e j edge_list.append((i, j)) 这个策略平衡了两个考虑：\n物理合理性：距离太远的原子相互作用很弱 计算效率：限制邻居数量避免图过于稠密","第三阶段自动微分革命graphattentiontransformer_dx系列#第三阶段：自动微分革命（GraphAttentionTransformer_dx系列）":"关键的突破来自于物理一致性的考虑。我们意识到应该从能量函数出发，通过自动微分计算力常数：\n# scripts/train_1.sh 中的新版本 python -u main_re.py \\ --model-name 'graph_attention_transformer_nonlinear_l2_e3_noNorm_dx' \\ 注意这里的关键变化：\nmain.py → main_re.py：重构了主训练脚本 模型名加上_dx后缀：标志着自动微分版本","第二步坐标变换#第二步：坐标变换":"晶体学中通常使用\"分数坐标\"（相对于晶格向量的坐标），但我们的神经网络需要\"笛卡尔坐标\"（普通的x,y,z坐标）：\n# 分数坐标 → 笛卡尔坐标 lattice = torch.tensor(supercell_lattice, dtype=torch.float) fractional_coords = torch.tensor([p.coordinates for p in atoms]) cartesian_coords = torch.matmul(fractional_coords, lattice) 这个矩阵乘法完成了坐标系的转换，让我们能够正确计算原子间距离。","第二阶段等变化改造graphattentiontransformer#第二阶段：等变化改造（GraphAttentionTransformer）":"认识到几何对称性的重要性后，我们开始引入E3NN框架，构建真正的等变图神经网络：\nclass GraphAttentionTransformer(torch.nn.Module): def __init__(self, irreps_in='86x0e', irreps_out='1x0e+1x1o+1x2e', # 输出包含多种阶数 irreps_node_embedding='128x0e+64x1e+32x2e', irreps_sh='1x0e+1x1e+1x2e'): 这个版本的关键改进：\n等变架构：基于E3NN框架，保持SO(3)对称性 多阶表示：使用标量、向量、张量的组合表示 物理输出：输出irreps包含不同阶的物理量 然而，这个版本仍然存在问题：我们直接预测力常数，而不是从能量推导。","第四步边特征的计算#第四步：边特征的计算":"对于每条边，我们需要计算描述原子对关系的特征：\nfor edge in edge_list: i, j = edge # 边向量（包含方向和距离信息） edge_vector = pos[j] - pos[i] # 距离（标量） distance = torch.norm(edge_vector) edge_vectors.append(edge_vector) edge_distances.append(distance) 这些边特征将输入到我们的等变图神经网络中。","等变多头注意力#等变多头注意力":"我们需要将注意力机制推广到等变设定。关键洞察是：注意力权重必须是标量以保持等变性。我们的方法是：\n标量注意力计算： αij=softmax((qi(0))Tkj(0)d0)\\alpha_{ij} = \\text{softmax}\\left(\\frac{(\\mathbf{q}_i^{(0)})^T \\mathbf{k}_j^{(0)}}{\\sqrt{d_0}}\\right)αij​=softmax(d0​​(qi(0)​)Tkj(0)​​) 其中$\\mathbf{q}_i^{(0)}$和$\\mathbf{k}_j^{(0)}$是标量（$l=0$）分量。\n等变值聚合： oi(l)=∑j∈N(i)αijvj(l)\\mathbf{o}_i^{(l)} = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{v}_j^{(l)}oi(l)​=j∈N(i)∑​αij​vj(l)​ 对每个$l$阶分量分别进行加权聚合。\n这种设计确保了注意力权重的物理可解释性——它们真正反映了原子间相互作用的强度。","等变性保护的软约束#等变性保护的软约束":"我们在训练中添加了软等变性约束：\ndef equivariance_loss(model, pos, batch, lambda_eq=1e-3): # 标准能量损失 energy = model(pos, batch, ...) # 等变性约束 rotation = o3.rand_matrix() pos_rotated = torch.matmul(pos, rotation.T) energy_rotated = model(pos_rotated, batch, ...) equivariance_violation = torch.mean((energy - energy_rotated)**2) return energy, lambda_eq * equivariance_violation 这种方法在实践中将等变性违背控制在$10^{-7}$量级。","等变性的严格定义#等变性的严格定义":"等变性是比不变性更精细的概念。对于函数$f: X \\to Y$和群作用$g$，等变性要求： f(g⋅x)=ρ(g)⋅f(x)f(g \\cdot x) = \\rho(g) \\cdot f(x)f(g⋅x)=ρ(g)⋅f(x)其中$\\rho(g)$是群元素$g$在输出空间的表示。这个公式看似抽象，但它编码了物理量变换的本质：\n当我们旋转输入（晶体结构） 输出（力常数张量）按照张量变换规律相应变化 物理内容保持不变","等变消息传递e3nn的突破#等变消息传递：E3NN的突破":"**E3NN（Euclidean Neural Networks）**框架通过以下创新解决了这个问题：\n1. 几何张量特征：将标量特征扩展为完整的几何张量 hi=⨁l=0Lmaxhi(l)\\mathbf{h}_i = \\bigoplus_{l=0}^{L_{max}} \\mathbf{h}_i^{(l)}hi​=l=0⨁Lmax​​hi(l)​ 其中$\\mathbf{h}_i^{(l)}$是$l$阶球谐函数空间中的特征。\n2. 等变线性变换：替换标准全连接层为等变线性层 h(l)↦∑l′W(l←l′)h(l′)\\mathbf{h}^{(l)} \\mapsto \\sum_{l'} \\mathbf{W}^{(l \\leftarrow l')} \\mathbf{h}^{(l')}h(l)↦l′∑​W(l←l′)h(l′) 其中权重矩阵$\\mathbf{W}^{(l \\leftarrow l’)}$满足特定的等变约束。\n3. 张量积非线性：通过Clebsch-Gordan耦合实现等变非线性 ϕ(h(l1),h(l2))=⨁l(h(l1)⊗h(l2))(l)\\phi(\\mathbf{h}^{(l_1)}, \\mathbf{h}^{(l_2)}) = \\bigoplus_{l} (\\mathbf{h}^{(l_1)} \\otimes \\mathbf{h}^{(l_2)})^{(l)}ϕ(h(l1​),h(l2​))=l⨁​(h(l1​)⊗h(l2​))(l)","自动微分的革命性解决方案#自动微分的革命性解决方案":"自动微分（Automatic Differentiation, AD）提供了一个数学上严格、计算上高效的解决方案。与数值微分不同，AD能够精确计算任意函数的导数，精度仅受浮点数表示限制。\n前向模式AD：对于函数$f: \\mathbb{R}^n \\to \\mathbb{R}$，前向模式计算方向导数： ∂f∂v=lim⁡ϵ→0f(x+ϵv)−f(x)ϵ\\frac{\\partial f}{\\partial \\mathbf{v}} = \\lim_{\\epsilon \\to 0} \\frac{f(\\mathbf{x} + \\epsilon \\mathbf{v}) - f(\\mathbf{x})}{\\epsilon}∂v∂f​=ϵ→0lim​ϵf(x+ϵv)−f(x)​反向模式AD：更适合神经网络的情况，计算梯度： ∇f=(∂f∂x1,…,∂f∂xn)\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)∇f=(∂x1​∂f​,…,∂xn​∂f​)","自适应超参数策略#自适应超参数策略":"class PhysicsAwareLRScheduler: def __init__(self, optimizer, patience=10): self.optimizer = optimizer self.patience = patience self.best_symmetry_error = float('inf') def step(self, mae_loss, symmetry_error): # 同时考虑预测误差和物理一致性 if symmetry_error \u003e 1e-4: # 物理一致性差，降低学习率 for param_group in self.optimizer.param_groups: param_group['lr'] *= 0.5 elif mae_loss \u003c self.best_mae * 0.99: # 性能改善，可以尝试增大学习率 for param_group in self.optimizer.param_groups: param_group['lr'] *= 1.05","解决方案的演进#解决方案的演进":"第一次尝试：梯度检查点\nfrom torch.utils.checkpoint import checkpoint def checkpointed_forward(model, inputs): return checkpoint(model, inputs, use_reentrant=False) 这种方法通过时间换空间——在反向传播时重新计算前向结果，而不是存储所有中间值。内存使用降低了约60%，但计算时间增加了约40%。\n第二次迭代：分块计算\ndef chunked_hessian_computation(model, pos, chunk_size=16): n = pos.size(0) hessian_blocks = [] for i in range(0, n, chunk_size): chunk_end = min(i + chunk_size, n) chunk_hessian = compute_hessian_chunk(model, pos, i, chunk_end) hessian_blocks.append(chunk_hessian) return torch.cat(hessian_blocks, dim=0) 最终解决方案：稀疏性感知计算\ndef sparse_aware_hessian(model, pos, cutoff=8.0): # 只计算物理上有意义的原子对 distances = torch.cdist(pos, pos) active_mask = distances \u003c cutoff # 稀疏计算，内存使用降低90% sparse_hessian = compute_sparse_hessian(model, pos, active_mask) return sparse_hessian","软物理约束的现代实现#软物理约束的现代实现":"虽然我们最初选择了纯监督学习，但随着模型成熟，重新考虑PINN方法变得有意义：\nclass PhysicsInformedE3NN: def __init__(self, base_model): self.base_model = base_model def compute_physics_loss(self, pos, predicted_energy): # 1. 平移不变性 pos_shifted = pos + torch.randn_like(pos) * 0.1 energy_shifted = self.base_model(pos_shifted) translation_loss = torch.abs(predicted_energy - energy_shifted) # 2. 晶格周期性 lattice_vectors = self.extract_lattice(pos) periodic_loss = self.check_periodicity(predicted_energy, lattice_vectors) # 3. 热力学关系 thermodynamic_loss = self.check_maxwell_relations(predicted_energy, pos) return translation_loss + periodic_loss + thermodynamic_loss 优势：在数据稀缺的体系（如新型材料）中，物理约束可以提供强大的正则化效果。","输出的根本性变化#输出的根本性变化":"最关键的架构变化是输出层的设计思路：\nclass GraphAttentionTransformer_dx(torch.nn.Module): def __init__(self, irreps_out='1x0e'): # 只输出标量能量！ 这个看似简单的改变具有深远的意义：\n传统方法：irreps_out='1x0e+1x1o+1x2e' → 直接预测多种物理量 dx方法：irreps_out='1x0e' → 只预测标量能量，其他量通过微分得到","高阶等变网络的探索#高阶等变网络的探索":"当前的E3NN框架主要处理2阶张量（力常数），但许多重要的物理性质需要更高阶的表示：\n三阶张量：非线性光学系数、压电系数 χijkl(3)=∂3Pi∂Ej∂Ek∂El\\chi^{(3)}_{ijkl} = \\frac{\\partial^3 P_i}{\\partial E_j \\partial E_k \\partial E_l}χijkl(3)​=∂Ej​∂Ek​∂El​∂3Pi​​四阶张量：弹性模量、拉曼张量 Cijkl=∂2σij∂ϵklC_{ijkl} = \\frac{\\partial^2 \\sigma_{ij}}{\\partial \\epsilon_{kl}}Cijkl​=∂ϵkl​∂2σij​​技术挑战：高阶张量的Clebsch-Gordan耦合计算复杂度呈指数增长。未来需要开发稀疏张量积算法和近似耦合方法。"},"title":"ed-gat"},"/docs/research/dl/":{"data":{"":"深度学习相关研究。"},"title":"深度学习"},"/docs/research/dl/wavelet-conv/":{"data":{"1-前期调研#1. 前期调研":"在动手实现之前，让我们先了解当前HAR和时序建模领域的研究现状。这将帮助我们理解为什么需要设计新的架构，以及我们的技术选择有何依据。","2-从一个简单的想法开始如何让机器理解人体活动#2. 从一个简单的想法开始：如何让机器理解人体活动？":"在深入复杂的技术细节之前，让我们先思考一个基本问题：机器如何理解人体活动？\n当你在跑步时，智能手表的传感器会产生一系列数据：加速度计记录你的运动加速度，陀螺仪记录旋转角度，磁力计记录方向信息。这些数据本质上是时间序列信号，包含了丰富的运动模式信息。","3-联邦学习在保护隐私的前提下协作训练#3. 联邦学习：在保护隐私的前提下协作训练":"联邦学习的核心思想很简单：不是让数据去找模型，而是让模型去找数据。每个设备在本地训练模型，只分享模型参数而不分享原始数据。","4-技术实现的精妙之处模型分区的艺术#4. 技术实现的精妙之处：模型分区的艺术":"现在让我们深入了解三种联邦学习策略的技术实现细节。这里的关键挑战是：如何将一个完整的深度学习模型巧妙地分割，使得不同部分能在不同设备上运行，同时还能进行端到端的训练？","5-监控系统让分布式训练变得可观测#5. 监控系统：让分布式训练变得可观测":"在分布式系统中，监控和观测是至关重要的。我们需要实时了解每个设备的资源使用情况，网络通信状态，以及训练进展。","51-数据集与实验设置#5.1 数据集与实验设置":"我们的实验涵盖了具有不同特性的多个标准HAR数据集：\n数据集 传感器类型 采样率 活动类别 参与者 数据特点 WISDM 3轴加速度计 20Hz 6类 29人 基础活动，数据相对简单 PAMAP2 3个IMU单元 100Hz 12类 9人 复杂日常活动，多传感器融合 Opportunity 72个传感器 30Hz 17类 4人 高维多模态，复杂环境交互 UCI-HAR 3轴加速度+陀螺仪 50Hz 6类 30人 标准基准，预处理特征","52-核心技术组件验证#5.2 核心技术组件验证":"小波包变换vs传统方法： 我们重点验证了可学习小波包变换的有效性。通过消融实验对比了不同频域分析方法：\n传统FFT方法：固定频域分解，无法适应HAR信号的非平稳特性 标准小波变换：多尺度分析，但频域分辨率固定 可学习小波包变换：自适应多尺度分解，能够学习任务特定的频域特征 实验表明，可学习滤波器在训练过程中确实能够适应HAR任务的频谱特征，相比固定滤波器有显著提升。\n联邦学习架构验证： 我们实现了三种联邦学习策略（TFL、HFL、FedMEC），并在实际的分布式环境中进行了测试。重点验证了：\n模型分区策略的可行性 跨设备通信的效率 不同资源约束下的性能表现 资源监控与性能评估： 通过实际的资源监控系统，我们收集了各策略在训练过程中的关键指标：\nCPU/内存使用情况 网络通信开销 训练收敛速度 模型准确率变化","53-技术挑战与解决方案验证#5.3 技术挑战与解决方案验证":"Non-IID数据处理： 我们使用Dirichlet分布（α=0.3）模拟真实联邦学习场景中的数据异构性，验证了我们的方法在数据分布不均衡情况下的鲁棒性。\n模型分区的通信效率： 通过实际部署测试，验证了不同分区策略在各种网络条件下的表现，证明了动态分区策略的有效性。\n端到端训练的梯度传播： 验证了跨设备梯度传播的正确性，确保分区模型能够进行有效的端到端训练。\n这些实验验证了WPTCN架构设计的合理性和技术路径的可行性，为后续的深入研究和实际应用奠定了基础。","6-技术创新的深度剖析从信号处理到个性化联邦学习#6. 技术创新的深度剖析：从信号处理到个性化联邦学习":"基于我们在时序建模和联邦学习领域的深入研究，WPTCN的技术创新体现在多个层面的突破。","61-频域分析的理论突破#6.1 频域分析的理论突破":"传统的HAR方法主要依赖FFT进行频域分析，但我们发现这种方法在处理非平稳HAR信号时存在固有局限性。通过深入研究小波理论，我们实现了几个关键突破：\n小波包分解的自适应性：与固定的FFT频带划分不同，小波包变换能够根据信号特性自适应地调整频域分辨率。在低频段提供更细的分辨率（适合捕捉缓慢的步行模式），在高频段提供粗糙的分辨率（适合捕捉快速的运动变化）。\n可学习滤波器的理论基础：我们创新性地将小波滤波器参数设为可学习变量。从数学角度，这等价于在小波空间中进行参数优化：\n# 理论框架：在小波域中的优化 def learnable_wavelet_optimization(): \"\"\" 目标函数：min L(f_θ(WT(x)), y) 其中：WT是小波变换，θ是可学习的小波参数 约束：θ必须满足小波的正交性和紧框架条件 \"\"\" # 正交性约束 orthogonality_loss = ||θ_lo * θ_hi|| - 0 # 重构完美性约束 reconstruction_loss = ||x - IWT(WT(x))|| # 总损失 total_loss = classification_loss + λ1*orthogonality_loss + λ2*reconstruction_loss 这种设计确保了滤波器在学习过程中仍然保持小波的数学性质，同时能够适应特定的HAR任务需求。","62-现代时序建模架构的融合创新#6.2 现代时序建模架构的融合创新":"我们的研究深入分析了当前时序建模的三大流派：\nTransformer-based方法的局限性：通过对iTransformer的深入分析，我们发现其在HAR任务中的主要问题是注意力机制对于短时序的HAR数据（通常2-10秒）过于复杂，存在过参数化问题。\nMLP+频域方法的启发：FITS和FreTS的成功给了我们重要启发。这些方法证明了频域特征提取的重要性，但FFT的固定频带划分限制了其在非平稳信号上的表现。\n现代TCN的计算效率：ModernTCN展示了卷积网络在时序任务中的潜力，但其固定的卷积核限制了多尺度特征提取能力。\n我们的WPTCN融合了这些方法的优势：\n采用小波包变换替代FFT，提供自适应频域分析 保持卷积网络的计算效率，避免Transformer的过度复杂性 通过可学习滤波器实现端到端优化","63-联邦学习中的个性化技术深化#6.3 联邦学习中的个性化技术深化":"基于我们在联邦学习方面的研究，我们发现传统FedAvg在HAR任务中面临严重的数据异构性问题。不同用户的行为模式差异巨大，需要更精细的个性化策略。\n分层个性化架构：\nclass PersonalizedFederatedWPTCN: def __init__(self): # 共享特征提取层（小波变换参数全局共享） self.shared_wavelet_params = GloballySharedParams() # 个性化表示层（TCN参数客户端特定） self.personalized_tcn = ClientSpecificParams() # 全局分类器（跨客户端共享） self.global_classifier = GloballySharedParams() def federated_update(self, global_model, local_data): \"\"\"个性化联邦更新策略\"\"\" # 1. 更新共享的小波参数 self.shared_wavelet_params.update_from_global(global_model.wavelet) # 2. 本地训练个性化TCN层 self.personalized_tcn.local_train(local_data) # 3. 聚合全局分类器 classifier_update = self.compute_classifier_gradient(local_data) return classifier_update 元学习增强的快速适应： 受Per-FedAvg启发，我们设计了基于梯度的快速适应机制：\ndef meta_learning_adaptation(self, support_set, query_set): \"\"\" 元学习快速适应新客户端 支持集：少量本地数据用于快速适应 查询集：评估适应后性能 \"\"\" # 一阶梯度适应 adapted_params = self.global_params - α * grad(loss(support_set)) # 二阶优化全局参数 meta_loss = loss(query_set, adapted_params) self.global_params -= β * grad(meta_loss, self.global_params)","64-缺陷传感器环境下的鲁棒性设计#6.4 缺陷传感器环境下的鲁棒性设计":"基于对\"Flawed Wearable Sensor Data\"相关工作的研究，我们在WPTCN中集成了处理传感器缺陷的机制：\n传感器故障检测与补偿：\nclass RobustSensorFusion: def __init__(self, sensor_types=['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']): self.sensor_reliability = nn.Parameter(torch.ones(len(sensor_types))) self.compensation_network = nn.ModuleDict({ sensor: nn.Linear(input_dim, output_dim) for sensor in sensor_types }) def forward(self, sensor_data, sensor_mask): \"\"\" sensor_data: [batch, channels, time] sensor_mask: [batch, channels] - 0表示传感器故障 \"\"\" # 自适应权重调整 reliability_weights = torch.sigmoid(self.sensor_reliability) weighted_data = sensor_data * reliability_weights.unsqueeze(-1) # 缺失传感器补偿 compensated_data = self.compensate_missing_sensors(weighted_data, sensor_mask) return compensated_data","65-跨模态联邦har前沿进展mcarn模型深度解析#6.5 跨模态联邦HAR前沿进展：MCARN模型深度解析":"近期IEEE TPAMI 2024发表的跨模态联邦人体活动识别（CM-FHAR）研究代表了该领域的最新突破。该工作首次系统性地解决了联邦学习场景下不同客户端持有不同模态数据的挑战。","cm-fhar问题定义与挑战#CM-FHAR问题定义与挑战":"传统联邦学习假设所有客户端拥有相同类型的数据，但现实中客户端往往具有异构的模态：某些设备提供运动传感器数据，其他设备则仅有视频数据。这种模态异构性带来三大核心挑战：\n分布式跨模态特征学习：在隐私约束下构建不同模态间的公共特征子空间 模态相关判别特征学习：学习每种模态特有的判别模式 模态不平衡问题：处理某些模态数据稀缺导致的训练偏差","har领域的发展脉络#HAR领域的发展脉络":"人体活动识别经历了从传统机器学习到深度学习的演进过程。早期研究主要依赖手工特征工程，通过统计特征、频域特征等方式来描述人体活动模式。然而，这种方法面临特征设计复杂、泛化能力差等问题。\n深度学习的兴起为HAR带来了新的机遇。卷积神经网络(CNN)能够自动学习时序特征，循环神经网络(RNN)擅长捕获长期依赖关系。但随着应用场景的复杂化，单一的网络架构已经难以满足多样化的需求。","mcarn架构创新#MCARN架构创新":"双编码器设计：\nAltruistic Encoder（利他编码器）：学习模态无关的共享特征表示 Egocentric Encoder（自我中心编码器）：捕获模态特定的判别特征 双分类器机制：\n全局共享分类器：基于模态无关特征进行活动分类 模态私有分类器：利用模态特定特征增强判别能力 关键技术组件：\n对抗模态判别器：通过对抗学习引导编码器产生模态不变特征 分离损失约束：确保模态无关和模态特定特征的正交性 角度边距调整机制： 对主导模态增加较大角度边距，增强类内紧凑性 对稀少模态使用较小边距，提高模态间区分度 关系感知全局-本地校准：约束共享分类器和私有分类器的类级配对关系","wptcn-基于小波包变换的联邦异构人体活动识别模型#WPTCN: 基于小波包变换的联邦异构人体活动识别模型":"WPTCN: 基于小波包变换的联邦异构人体活动识别模型","三种不同的分工策略#三种不同的分工策略":"在实际的HAR联邦学习中，不同设备的计算能力差异很大。智能手机的计算能力强，智能手环的计算能力弱。我们需要根据设备能力来分配计算任务。","从最简单的联邦学习开始#从最简单的联邦学习开始":"让我们先实现一个最基本的联邦学习系统。假设我们有两个用户，Alice和Bob，他们都想训练一个HAR模型，但不愿意分享自己的运动数据。\n传统方式需要他们将数据上传到服务器：\n# 传统集中式训练 - 隐私风险 def centralized_training(): alice_data = alice.upload_data() # 隐私泄露！ bob_data = bob.upload_data() # 隐私泄露！ combined_data = merge(alice_data, bob_data) model = train_model(combined_data) return model 联邦学习的方式则完全不同：\n# 联邦学习 - 保护隐私 def federated_training(): global_model = initialize_model() # 每轮训练 for round in range(num_rounds): # 1. 服务器发送全局模型给客户端 alice.receive_model(global_model) bob.receive_model(global_model) # 2. 客户端本地训练（数据不离开设备） alice_update = alice.local_train() bob_update = bob.local_train() # 3. 服务器聚合模型更新 global_model = aggregate([alice_update, bob_update]) return global_model 这个简单的框架就是联邦学习的基础。但在实际应用中，我们需要考虑更多复杂情况。","处理真实世界的复杂性#处理真实世界的复杂性":"在实验室里，我们可以假设所有设备都有相似的数据分布。但在真实世界中，每个用户的行为模式都不同。Alice可能是一个马拉松跑者，她的数据主要是跑步；Bob可能是上班族，他的数据主要是走路和坐着。\n这种数据分布的不均匀性被称为Non-IID（非独立同分布）问题。我们需要在数据处理阶段就考虑这个问题：\ndef simulate_real_world_distribution(num_clients): \"\"\"模拟真实世界的数据分布不均匀性\"\"\" # 使用Dirichlet分布模拟数据量的不平衡 data_ratios = np.random.dirichlet(np.ones(num_clients) * 0.3) # 不同客户端有不同的窗口大小和重叠率 client_configs = [] for i in range(num_clients): config = { 'window_size': np.random.choice([80, 100, 120]), 'overlap_rate': np.random.choice([0.1, 0.25, 0.5]), 'data_ratio': data_ratios[i] } client_configs.append(config) return client_configs 通过这种方式，我们的实验更贴近真实应用场景，训练出的模型也更加鲁棒。","实验结果与技术验证#实验结果与技术验证":"我们基于项目实际进展进行了系统性的实验验证，重点关注WPTCN架构的核心创新点。","实验验证与性能表现#实验验证与性能表现":"MCARN在多个跨模态数据集上验证了其有效性：\nStanford-ECM数据集：包含传感器和视频数据的复合活动识别 Ego-Exo-AR数据集：第一人称视角与外部视角的跨模态数据 Epic-Kitchens数据集：厨房活动的多模态识别任务 实验结果表明，MCARN在所有测试场景中都显著优于现有方法，特别是在模态不平衡情况下表现尤为突出。","引言为什么我们需要重新思考har#引言：为什么我们需要重新思考HAR？":"想象一下这样的场景：你戴着智能手表跑步，手表需要准确识别你是在慢跑、快跑还是走路。这看似简单的任务，实际上包含了深度学习、信号处理、隐私保护等多个复杂技术问题。更重要的是，传统的解决方案往往要求将你的私人运动数据上传到云端服务器，这既带来了隐私风险，也造成了巨大的网络传输开销（这听起来非常BME，刘清君老师的穿戴式传感设备emmm）。\n本文将从第一性原理出发，逐步构建一个现代化的人体活动识别(HAR)系统。我们将从最简单的概念开始，逐渐添加小波变换、时间卷积网络、联邦学习等高级技术，最终实现一个既保护隐私又高效准确的分布式HAR（Human Activity Recognition，人类活动识别）系统。","我们的技术选择动机#我们的技术选择动机":"基于以上调研，我们的技术选择有了清晰的依据：\n为什么选择小波变换？ 小波变换能够同时提供时域和频域信息，这对于HAR任务至关重要。不同的人体活动在频谱特征上有显著差异，而小波包变换能够提供更细致的频域分解。\n为什么选择时间卷积网络？ 相比于RNN，TCN具有并行计算的优势；相比于Transformer，TCN在移动设备上的计算效率更高。ModernTCN的成功验证了卷积网络在时序建模中的潜力。\n为什么需要联邦学习？ 在隐私保护日益重要的今天，联邦学习提供了在不暴露原始数据的前提下进行协作训练的可能性。对于HAR这样涉及个人隐私的应用场景，联邦学习几乎是唯一可行的大规模训练方案。\n现在，让我们从最基础的概念开始，逐步构建我们的系统。","时序建模的技术演进#时序建模的技术演进":"通过文献调研，我们可以看到时序建模领域近年来出现了许多突破性进展：\nTransformer架构的时序应用：从Informer到Autoformer，研究者们探索了如何将注意力机制应用到长序列预测中。Informer提出了稀疏注意力机制来降低计算复杂度，而Autoformer则引入了序列分解的思想，将趋势和季节性分离处理。\n频域分析的复兴：FEDformer和频域MLP的出现表明，频域分析在时序建模中仍然具有重要价值。通过在频域中进行计算，可以更高效地捕获全局依赖关系。\n现代卷积网络的进展：ModernTCN的研究显示，经过精心设计的卷积网络在许多时序任务中仍能与Transformer媲美，且具有更好的计算效率。\n混合架构的探索：TimeMixer和TimesNet等工作尝试将多种技术融合，通过分解、多尺度建模等方式来处理复杂的时序模式。\n轻量化模型设计：FITS等工作专注于参数高效的模型设计，这对于移动设备部署具有重要意义。","构建资源监控系统#构建资源监控系统":"让我们先构建一个简单的资源监控器：\nclass ResourceMonitor: \"\"\"监控设备资源的小助手\"\"\" def __init__(self, sampling_interval=1.0): self.sampling_interval = sampling_interval self.is_monitoring = False self.metrics = { 'cpu_usage': [], 'memory_usage': [], 'network_io': [] } def start_monitoring(self): \"\"\"开始监控\"\"\" self.is_monitoring = True self.monitoring_thread = threading.Thread(target=self._monitor_loop) self.monitoring_thread.start() def _monitor_loop(self): \"\"\"监控主循环\"\"\" while self.is_monitoring: # 收集CPU使用率 cpu_percent = psutil.cpu_percent() self.metrics['cpu_usage'].append(cpu_percent) # 收集内存使用率 memory = psutil.virtual_memory() self.metrics['memory_usage'].append(memory.percent) # 收集网络IO net_io = psutil.net_io_counters() self.metrics['network_io'].append({ 'bytes_sent': net_io.bytes_sent, 'bytes_recv': net_io.bytes_recv }) time.sleep(self.sampling_interval) 这个监控系统能够实时跟踪设备的关键性能指标。在联邦学习过程中，这些数据帮助我们了解不同策略的资源消耗特点。","模型分区的基本思想#模型分区的基本思想":"想象一下，我们有一个完整的WPTCN模型，它包含几个主要组件：\nclass WPTCN(nn.Module): def __init__(self, ...): super().__init__() self.normalization = CustomNormalization() # 数据标准化 self.initial_conv = nn.Conv1d(...) # 初始特征提取 self.backbone = nn.ModuleList([ # 多个WTTCN块 WTTCNBlock(...) for _ in range(num_layers) ]) self.global_pool = nn.AdaptiveAvgPool1d(1) # 全局池化 self.fc = nn.Linear(...) # 最终分类 现在的问题是：在哪里\"切一刀\"，将模型分成设备端和服务器端两部分？","第一步最简单的分类器#第一步：最简单的分类器":"让我们从最简单的想法开始。假设我们有一个传感器数据序列，最直观的方法是使用一个简单的神经网络：\nclass SimpleHAR(nn.Module): def __init__(self, input_size, num_classes): super().__init__() self.fc = nn.Linear(input_size, num_classes) def forward(self, x): # 简单地将时序数据拍平 x = x.view(x.size(0), -1) return self.fc(x) 这个方法虽然简单，但有明显的问题：它完全忽略了时间信息，把时序数据当作静态特征处理。人体活动的本质是时间序列模式，我们需要保留时间维度的信息。","第三步发现频域的秘密#第三步：发现频域的秘密":"人体活动在频域中有着独特的特征。走路、跑步、跳跃等不同活动具有不同的频率特征。这让我们想到：能否同时在时域和频域中分析信号？\n这就是小波变换的用武之地。小波变换能够提供时频分析，既保留时间信息，又能分析频率成分。\ndef simple_wavelet_transform(signal): \"\"\"简单的小波变换实现概念\"\"\" # 使用低通和高通滤波器分解信号 low_freq = low_pass_filter(signal) # 保留低频成分 high_freq = high_pass_filter(signal) # 保留高频成分 return low_freq, high_freq 但我们的想法更进一步：如果滤波器的参数也能学习呢？这样网络就能自动找到最适合HAR任务的频域分解方式。","第二步引入时间感知#第二步：引入时间感知":"既然需要处理时序信息，自然想到使用卷积神经网络。一维卷积能够捕获局部的时序模式：\nclass TimeCNN(nn.Module): def __init__(self, input_channels, num_classes): super().__init__() self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1) self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1) self.pool = nn.AdaptiveAvgPool1d(1) self.fc = nn.Linear(128, num_classes) def forward(self, x): x = F.relu(self.conv1(x)) x = F.relu(self.conv2(x)) x = self.pool(x).squeeze(-1) return self.fc(x) 这已经是一个相当不错的HAR模型了。但是，我们还可以做得更好。","第五步构建完整的wttcn块#第五步：构建完整的WTTCN块":"现在我们可以将这些想法组合成一个完整的模块。我们的WTTCNBlock不仅包含可学习的小波变换，还加入了深度卷积和前馈网络：\nclass WTTCNBlock(nn.Module): def __init__(self, channels, num_levels=2): super().__init__() self.channels = channels self.num_levels = num_levels # 可学习的小波滤波器 wavelet = pywt.Wavelet('db1') self.dec_lo = nn.Parameter(torch.tensor(wavelet.dec_lo[::-1], dtype=torch.float32)) self.dec_hi = nn.Parameter(torch.tensor(wavelet.dec_hi[::-1], dtype=torch.float32)) # 深度卷积处理各频带 self.dw_conv = nn.Conv1d(channels, channels, kernel_size=3, groups=channels, padding='same') self.bn = nn.BatchNorm1d(channels) # 前馈网络 self.ffn = nn.Sequential( nn.Conv1d(channels, channels * 2, kernel_size=1), nn.GELU(), nn.Conv1d(channels * 2, channels, kernel_size=1) ) 这个设计的精妙之处在于：它从简单的卷积开始，逐步添加了频域分析能力，最终形成了一个既能处理时域特征又能捕获频域特征的强大模块。","第六步面临新的挑战---隐私问题#第六步：面临新的挑战 - 隐私问题":"到现在为止，我们已经有了一个相当强大的HAR模型。但在实际应用中，我们面临一个新的挑战：隐私保护。\n传统的机器学习需要将所有用户数据收集到中央服务器进行训练。但对于HAR这样涉及个人行为模式的敏感数据，用户越来越不愿意将原始数据上传到云端。\n这就引出了我们的下一个核心技术：联邦学习。","第四步可学习的频域分解#第四步：可学习的频域分解":"这就是我们WPTCN的核心创新点。我们让小波滤波器的参数变得可学习：\nclass LearnableWavelet(nn.Module): def __init__(self, channels, wavelet_type='db1'): super().__init__() # 初始化为传统小波滤波器 wavelet = pywt.Wavelet(wavelet_type) self.low_filter = nn.Parameter(torch.tensor(wavelet.dec_lo)) self.high_filter = nn.Parameter(torch.tensor(wavelet.dec_hi)) def forward(self, x): # 使用可学习的滤波器进行分解 low_freq = F.conv1d(x, self.low_filter.view(1, 1, -1), padding='same') high_freq = F.conv1d(x, self.high_filter.view(1, 1, -1), padding='same') return low_freq, high_freq 这个想法很有趣：我们从传统的小波变换开始，但允许网络在训练过程中调整滤波器参数，以找到最适合特定HAR任务的频域分解方式。","策略一传统联邦学习---不切割#策略一：传统联邦学习 - 不切割":"最简单的方法是不切割，每个设备运行完整模型：\nclass TraditionalClient: def __init__(self): # 设备上运行完整模型 self.complete_model = WPTCN() def train_locally(self, data): \"\"\"设备上的完整训练流程\"\"\" predictions = self.complete_model(data) loss = compute_loss(predictions, labels) loss.backward() self.optimizer.step() # 只发送模型参数更新 return self.complete_model.state_dict() 这种方式的好处是每个设备都是独立的，坏处是对设备性能要求很高。","策略一传统联邦学习tfl---大家都是全才#策略一：传统联邦学习(TFL) - 大家都是\u0026quot;全才\u0026quot;":"在TFL模式下，每个设备都运行完整的WPTCN模型：\nclass TraditionalFederatedClient: def __init__(self, device_id, data_loader): self.device_id = device_id self.data_loader = data_loader # 每个客户端都有完整的WPTCN模型 self.model = WPTCN(input_channels=6, num_classes=12) def local_train(self): \"\"\"在本地训练完整模型\"\"\" for data, labels in self.data_loader: predictions = self.model(data) loss = compute_loss(predictions, labels) loss.backward() self.optimizer.step() # 只返回模型参数，不暴露数据 return self.model.state_dict() 这种方式的优点是简单，每个设备都是\"全才\"，能独立完成所有计算。缺点是对设备要求高，不适合资源受限的设备。","策略三动态分区---智能切割#策略三：动态分区 - 智能切割":"最高级的方法是根据实际情况动态决定切割点：\nclass AdaptivePartition: def __init__(self, full_model): self.full_model = full_model self.current_partition = 'tcn_1' # 当前分区点 def adjust_partition(self, device_load, network_speed): \"\"\"根据设备状态动态调整分区点\"\"\" if device_load \u003e 0.8: # 设备负载高 if network_speed \u003e 10: # 网络好 self.current_partition = 'wavelet' # 更多工作给服务器 else: self.current_partition = 'tcn_2' # 平衡分配 elif device_load \u003c 0.3: # 设备负载低 self.current_partition = 'tcn_3' # 设备做更多工作 self._recreate_partitioned_models() def _recreate_partitioned_models(self): \"\"\"根据新的分区点重新创建模型\"\"\" if self.current_partition == 'wavelet': # 在小波变换后分割 self.device_model = self._create_device_model_wavelet() self.server_model = self._create_server_model_wavelet() elif self.current_partition.startswith('tcn_'): # 在指定TCN层后分割 layer_idx = int(self.current_partition.split('_')[1]) self.device_model = self._create_device_model_tcn(layer_idx) self.server_model = self._create_server_model_tcn(layer_idx) 这种动态分区的思想很有趣：系统能够根据实时状况自动调整工作分配，就像一个智能的项目经理，能够根据团队成员的能力和工作负载来分配任务。","策略三边缘计算联邦学习fedmec---动态分工#策略三：边缘计算联邦学习(FedMEC) - \u0026ldquo;动态分工\u0026rdquo;":"FedMEC更进一步，它能够根据网络条件和设备性能动态调整分工点：\nclass AdaptiveFederatedClient: def __init__(self, device_id, data_loader): self.device_id = device_id self.data_loader = data_loader self.full_model = WPTCN() self.current_partition_point = 'tcn_1' # 动态调整 def adapt_partition_point(self): \"\"\"根据设备状态动态调整分区点\"\"\" cpu_usage = get_cpu_usage() network_bandwidth = get_network_bandwidth() if cpu_usage \u003e 80 and network_bandwidth \u003e 10: # 高CPU低带宽 self.current_partition_point = 'wavelet' # 更多计算转移到服务器 elif cpu_usage \u003c 30 and network_bandwidth \u003c 1: # 低CPU低带宽 self.current_partition_point = 'tcn_3' # 更多计算保留在设备 self.update_device_model() 这种自适应的方式使得系统能够根据实际情况优化性能，在不同的网络和设备条件下都能良好工作。","策略二分层联邦学习hfl---专业分工#策略二：分层联邦学习(HFL) - \u0026ldquo;专业分工\u0026rdquo;":"在HFL模式下，我们将模型分成两部分：设备端负责数据预处理和初步特征提取，服务器端负责复杂的推理计算。\nclass HierarchicalClient: def __init__(self, device_id, data_loader): self.device_id = device_id self.data_loader = data_loader # 设备端只运行模型的前半部分 self.device_model = DeviceModel() # 包含标准化和初始卷积 def local_train(self): for data, labels in self.data_loader: # 设备端前向传播 features = self.device_model(data) # 将特征发送到边缘服务器（而不是原始数据） predictions = self.send_to_edge_server(features) # 从服务器获取梯度并更新本地模型 gradients = self.receive_gradients_from_server() self.update_device_model(gradients) 这种方式巧妙地平衡了隐私保护和计算效率。设备只需要传输抽象的特征向量，而不是原始的传感器数据，既保护了隐私，又减少了通信开销。","策略二在小波变换后切割#策略二：在小波变换后切割":"更聪明的做法是在合适的地方切割。我们发现，在小波变换之后切割是一个很好的选择：\nclass PartitionedModel: def __init__(self): # 设备端：轻量级预处理 self.device_part = nn.Sequential( CustomNormalization(), nn.Conv1d(input_channels, hidden_dim, kernel_size=3) ) # 服务器端：计算密集的部分 self.server_part = nn.Sequential( *[WTTCNBlock() for _ in range(num_layers)], # 多个WTTCN块 nn.AdaptiveAvgPool1d(1), nn.Linear(hidden_dim, num_classes) ) def forward_on_device(self, x): \"\"\"设备端前向传播\"\"\" return self.device_part(x) def forward_on_server(self, features): \"\"\"服务器端前向传播\"\"\" return self.server_part(features) 这样分工的逻辑很清楚：设备端负责基础的数据预处理和特征提取，服务器端负责复杂的模式识别和分类。","跨设备训练的技术挑战#跨设备训练的技术挑战":"分区之后的一个关键问题是：如何进行端到端的训练？毕竟梯度需要从服务器端流回设备端。\ndef distributed_training_step(self, data, labels): \"\"\"跨设备的训练步骤\"\"\" # 1. 设备端前向传播 device_output = self.device_model(data) # 2. 将中间特征发送到服务器 server_input = self.send_to_server(device_output) # 3. 服务器端前向传播和损失计算 predictions = self.server_model(server_input) loss = F.cross_entropy(predictions, labels) # 4. 服务器端反向传播 loss.backward() server_gradients = server_input.grad # 5. 将梯度发送回设备 device_gradients = self.receive_from_server(server_gradients) # 6. 设备端反向传播 device_output.backward(device_gradients) self.device_optimizer.step() 这个过程看起来复杂，但逻辑很清晰：前向传播从设备流向服务器，反向传播从服务器流回设备，形成一个完整的训练循环。\n通过这种方式，我们实现了真正的分布式深度学习：不同的网络层运行在不同的设备上，但整个系统仍然能够进行端到端的训练。"},"title":"wavelet-conv"},"/docs/research/llm/":{"data":{"":"大语言模型相关研究。"},"title":"LLM"},"/docs/research/llm/realhitbench/":{"data":{"realhitbench-的核心创新定义并挑战真复杂#RealHiTBench 的核心创新：定义并挑战“真·复杂”":"RealHiTBench 的核心贡献，在于我们对“复杂表格结构”的系统性定义和应用。它不只是数据的堆砌，更是对复杂性的深刻解构。在论文中，我们定义了五种典型的复杂结构，大家可以通过论文中的核心示例图（Figure 1）直观理解：\n层级列头 (Hierarchical Column Header)：这可能是最常见的复杂结构。通过单元格合并，列头被组织成一个树状层级。比如在 Figure 1 中，“Average hours per day” 这个一级表头下，又分出了“Both full time”和“Mother part time \u0026 Father full time”两个二级表头，每个二级表头下还细分了“Mothers”和“Fathers”。这种结构要求模型必须理解列与列之间的从属关系。\n层级行头 (Hierarchical Row Header)：与列头类似，行头也能通过缩进或多列分组来表示层级。在 Figure 1 中，“Household”这一项下面，通过缩进展示了“Housework”“Food”“Lawn”等多个子项，清晰表达了它们之间的包含关系。\n嵌套子表 (Nested Sub-Tables)：有时，一个大表内部会由横跨整个表格宽度的特殊行（通常是标题行）分割成多个语义上独立的子区域。Figure 1 中，“Children under 18”“Children under 6”等橙色背景的行，就将整个表格划分成了针对不同年龄段儿童的独立统计区域。模型需要理解，对“所有儿童”的提问，可能并不意味着对所有数字进行简单加总。\n隐式多表连接 (Implicit Multi-Table Join)：这是一种非常巧妙且在现实中常见的设计。从表面看，它可能只是一个大表，但实际上它并列了多个结构完全相同的子表，目的是实现数据间的直观对比。Figure 1 的左右两侧分别展示了“Children under 18”和“Children 7-12”等不同群组的数据，它们的列结构完全一致，这本身就蕴含了对比的语义。","treethinker从看表格到理解表格结构#TreeThinker：从“看”表格到“理解”表格结构":"在 RealHiTBench 中提出评测难题后，我们还探索了解决思路，设计了一个名为 TreeThinker 的理解增强流水线。这个方案的核心思想很直观：在让 LLM 直接回答问题前，先引导它把表格的层级结构显式地解析成“树”的形式，从而将隐式的结构信息转化为显式的、可供模型直接利用的知识。","任务设计不止于查找更考验深度理解#任务设计：不止于查找，更考验深度理解":"为了全面评估模型的能力，我们为 RealHiTBench 设计了五大类、包含多个子类的复杂任务：\n事实核查 (Fact Checking)：包括需要整合多处信息进行验证的“多跳核查”，以及需要基于表格信息进行逻辑推理的“推理核查”。 数值推理 (Numerical Reasoning)：涵盖了计数、排序、比较和复杂计算等多种需要数学能力的任务。这要求模型不仅要定位数据，还要正确执行数学运算。 数据分析 (Data Analysis)：这是更高阶的任务，要求模型进行初步分析、总结、预测，甚至探索性分析和异常检测。比如，模型需要根据表格数据总结趋势或发现异常值。 图表生成 (Chart Generation)：要求模型根据数据生成可执行的代码来绘制图表，比如条形图、折线图等。这直接考验了模型将结构化数据转换为可视化代码的能力。 结构理解 (Structure Comprehending)：这是我们专为复杂表格设计的创新任务类型。我们会通过修改（比如交换）表格的某些结构部分，然后对新旧两个表格问同一个问题，以此评测模型是否真正理解结构变化对答案的决定性影响。","实验结果揭示的真相#实验结果揭示的真相":"我们用 RealHiTBench 对 25 个当前主流的 LLM 和 MLLM 进行了全面评测，得出的结果很值得思考：\n普遍表现不佳：即便是像 GPT-4o 这样的顶级模型，在多数任务上的表现也远未达到理想状态，精确匹配率（EM）最高没超过 70。这说明 RealHiTBench 确实是一个极具挑战性的基准，也印证了我们构建它的价值。 文本优于图像：在处理复杂表格结构时，基于文本（尤其是 LaTeX 格式）的输入，平均表现要优于图像（PNG）输入。我们推测，这可能是因为文本格式能更无损地保留精细的结构信息。 模型越大，推理越强：拥有 671B MoE 架构的 DeepSeek-R1 在所有模型中表现最出色。这在一定程度上表明，强大的通用推理能力，对于攻克复杂表格结构至关重要。","第一步树生成-tree-generation---绘制结构蓝图#第一步：树生成 (Tree Generation) - 绘制“结构蓝图”":"TreeThinker 首先会引导模型对表格进行结构化的自我解析，具体分为两步：\n编码与解释：我们引导模型将每个表头（无论是行还是列）编码为一个包含四元素的元组 T=(t1, t2, t3, t4)。这个元组能精确描述表头的全部信息：t1 代表类型（行/列）和层级，t2 和 t3 代表它在表格中的起始和结束位置，t4 则是单元格的具体内容。比如 (R0, 1, 2, City) 就表示这是一个0级行头，从第1行跨越到第2行，内容是“City”。\n构建层级树：得到所有表头的元组列表后，模型会根据它们的层级和跨越范围（起始、结束位置）确定父子关系。如果一个低层级的表头，其位置范围被一个高层级表头完全包含，那么它们之间就构成了父子关系。通过这种方式，原本扁平的表头列表会被组织成等级分明的“表头树”。","第二步基于树的推理-tree-based-reasoning---精准定位与思考#第二步：基于树的推理 (Tree-based Reasoning) - 精准定位与思考":"有了这张“结构蓝图”，推理过程会变得更精准高效，具体分为三步：\n分解与对齐：模型首先会将用户的问题分解成一组关键词 K=[k1, k2, ..., km]。之后，它会执行对齐操作，计算每个关键词 k 与表头树中每个节点 T 的匹配度 (Align_LM(T,k))。\n定位子表：我们会设定一个特定阈值，只有匹配度超过阈值的表头节点才会被选中，形成一个与问题高度相关的“关键词-表头子树” (H')。这个子树会指导模型从原始表格中精确抽取出回答问题所需的核心信息。\n迭代求精：最后，TreeThinker 还采用了一种类似 ReAct 的多轮“思考-行动-结果”精炼策略，对抽出的信息进行迭代处理，最终生成答案。\n从实验结果来看，TreeThinker 的效果很显著。比如，在使用该方法后，GPT-4o 在图表生成任务上的 PASS@1 指标提升了惊人的 134.7%。这也印证了我们的判断：显式地理解表格结构，是解锁 LLM 更高阶表格处理能力的关键。","结语与展望#结语与展望":"这个工作给我最大的启发是：我们对 LLM 的评估，需要从“通用能力”转向“特定领域的深度能力”。在表格分析领域，这就意味着我们必须正视并量化“结构复杂性”这一核心挑战。","超越扁平realhitbench-如何为大语言模型llm的表格处理能力设立新标杆#超越扁平：RealHiTBench 如何为大语言模型（LLM）的表格处理能力设立新标杆":"超越扁平：RealHiTBench 如何为大语言模型（LLM）的表格处理能力设立新标杆随着大语言模型（LLMs）的飞速发展，它们在文本生成、摘要和对话方面的能力已经令人印象深刻。然而，当我们把目光从非结构化的文本转向结构化的表格数据时，一个更深层次的挑战浮现了：LLMs 真的能“理解”现实世界中那些复杂的、多层次的表格吗？\n《RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark》里，我们不仅指出了当前评测基准的局限性，还通过构建一个极具挑战性的新基准，揭示了 SOTA 模型在真实复杂表格分析任务中的能力边界。","问题的根源为什么我们需要一个新的表格评测基准#问题的根源：为什么我们需要一个新的表格评测基准？":"长期以来，许多流行的表格问答（TableQA）评测基准，比如 TAT-QA 和 TableBench，大多依赖于结构简单的“扁平表格” (flat tables)。这类表格中，每一行代表一个记录，每一列代表一个属性，结构清晰明了。\n但在经济、科学、社会等多个领域的实际应用中，表格的结构远比这复杂。为了在二维空间内呈现多维信息，人们创造了大量包含层级结构的表格。虽然已有一些工作（比如 HiTab）开始关注层级表格，但它们往往存在领域局限、格式有损（比如预处理成 JSON 破坏了原始结构）、或层级结构过于简单等问题。\n这种评测基准与现实应用之间的脱节，让我们无法准确评估 LLM 在处理真实世界复杂表格时的真正实力。而 RealHiTBench，正是我们为了填补这一空白所开发的工具。"},"title":"realhitbench"},"/docs/research/llm/vllm-optimization/":{"data":{"vllm-优化研究#VLLM 优化研究":"VLLM 优化研究不愿提及"},"title":"vllm-optimization"},"/docs/self-study/":{"data":{"":"自学相关的学习笔记。"},"title":"自学"},"/docs/self-study/ai/":{"data":{"":"人工智能相关学习笔记。"},"title":"AI"},"/docs/self-study/ai/agent/":{"data":{"":"","实习#实习":"https://v2ex.com/member/sskyy https://chat.deepseek.com/share/c29wwvrc986fuudeyn"},"title":"agent"},"/docs/self-study/ai/dl/":{"data":{"":"深度学习相关学习笔记。"},"title":"深度学习"},"/docs/self-study/ai/dl/basic/":{"data":{"11-线性代数---矩阵乘法#1.1 线性代数 - 矩阵乘法":"","111-向量神经网络的基本单位#1.1.1 向量：神经网络的基本单位":"首先，向量就是一串数字。比如一张 28×28 的手写数字图片，可以展平成一个 784 维的向量：\nx=[x1x2⋮x784]x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{784} \\end{bmatrix}x=​x1​x2​⋮x784​​​ 列向量：像上面这样竖着写 行向量：$x^T = [x_1, x_2, \\ldots, x_{784}]$ 横着写 为什么要区分？因为矩阵乘法对顺序有要求！","112-矩阵乘法神经网络的灵魂操作#1.1.2 矩阵乘法：神经网络的灵魂操作":"假设我们有一个权重矩阵 $W$（大小 $m \\times n$）和输入向量 $x$（大小 $n \\times 1$）：\ny=Wxy = Wxy=Wx计算规则：结果 $y$ 的第 $i$ 个元素是：\nyi=∑j=1nWijxjy_i = \\sum_{j=1}^{n} W_{ij} x_jyi​=j=1∑n​Wij​xj​举个例子：\n[1234][56]=[1×5+2×63×5+4×6]=[1739]\\begin{bmatrix} 1 \u0026 2 \\\\ 3 \u0026 4 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 1 \\times 5 + 2 \\times 6 \\\\ 3 \\times 5 + 4 \\times 6 \\end{bmatrix} = \\begin{bmatrix} 17 \\\\ 39 \\end{bmatrix}[13​24​][56​]=[1×5+2×63×5+4×6​]=[1739​]几何意义：矩阵乘法其实是一种线性变换。你可以把 $W$ 想象成一个\"变换器\"，把输入空间的向量 $x$ 变换到输出空间。","113-在神经网络中的应用#1.1.3 在神经网络中的应用":"全连接层（或者叫线性层）的核心操作就是：\nh=Wx+bh = Wx + bh=Wx+b其中 $b$ 是偏置向量。一个神经网络里可能有几十上百层这样的操作！","114-推荐视频#1.1.4 推荐视频":"3Blue1Brown - 线性代数的本质 3Blue1Brown - 微积分的本质 3Blue1Brown - 反向传播的直观理解","12-求导法则#1.2 求导法则":"","121-为什么要求导#1.2.1 为什么要求导？":"训练神经网络的核心就是：找到让损失函数最小的参数。怎么找？靠梯度下降，而梯度就是导数（或者说偏导数）。","122-基本求导规则快速回顾#1.2.2 基本求导规则（快速回顾）":"这些应该在高数课上学过，快速过一遍：\n函数 导数 $f(x) = C$ $f’(x) = 0$ $f(x) = x^n$ $f’(x) = nx^{n-1}$ $f(x) = e^x$ $f’(x) = e^x$ $f(x) = \\ln x$ $f’(x) = \\frac{1}{x}$ $f(x) = \\sin x$ $f’(x) = \\cos x$","123-链式法则chain-rule#1.2.3 链式法则（Chain Rule）":"这个是重中之重！神经网络的反向传播完全依赖链式法则。\n一元函数的链式法则：\n如果 $y = f(u)$ 且 $u = g(x)$，那么：\ndydx=dydu⋅dudx\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}dxdy​=dudy​⋅dxdu​举个例子：\ny=(2x+1)3y = (2x + 1)^3y=(2x+1)3设 $u = 2x + 1$，则 $y = u^3$：\ndydx=dydu⋅dudx=3u2⋅2=6(2x+1)2\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} = 3u^2 \\cdot 2 = 6(2x + 1)^2dxdy​=dudy​⋅dxdu​=3u2⋅2=6(2x+1)2","124-多元函数与偏导数#1.2.4 多元函数与偏导数":"神经网络的损失函数通常是多元函数，比如 $L(W, b)$ 依赖于很多参数。\n偏导数：对某一个变量求导，其他变量当常数。\n∂L∂wi\\frac{\\partial L}{\\partial w_i}∂wi​∂L​表示 $L$ 对 $w_i$ 的偏导数。","125-梯度gradient#1.2.5 梯度（Gradient）":"梯度就是所有偏导数组成的向量：\n∇L=[∂L∂w1∂L∂w2⋮∂L∂wn]\\nabla L = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} \\\\ \\frac{\\partial L}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial L}{\\partial w_n} \\end{bmatrix}∇L=​∂w1​∂L​∂w2​∂L​⋮∂wn​∂L​​​ 梯度的方向：函数值增长最快的方向 梯度下降：我们沿着梯度的反方向走，让损失函数减小","126-多元链式法则#1.2.6 多元链式法则":"假设 $z = f(x, y)$，$x = g(t)$，$y = h(t)$，那么：\ndzdt=∂z∂xdxdt+∂z∂ydydt\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\frac{dy}{dt}dtdz​=∂x∂z​dtdx​+∂y∂z​dtdy​这就是反向传播的数学基础！","21-线性层全连接层--fully-connected-layer#2.1 线性层（全连接层 / Fully Connected Layer）":"","211-从感知机说起#2.1.1 从感知机说起":"1958 年，Rosenblatt 提出了感知机（Perceptron），这是神经网络的鼻祖。它的想法超级简单：\ny=f(WTx+b)y = f(W^T x + b)y=f(WTx+b)其中：\n$x$：输入特征向量 $W$：权重向量 $b$：偏置（bias） $f$：激活函数（比如阶跃函数） 感知机能解决线性可分的问题（比如 AND、OR），但解决不了 XOR 问题。这就引出了多层感知机（MLP）。","212-线性层的数学表达#2.1.2 线性层的数学表达":"一个线性层就是：\ny=Wx+by = Wx + by=Wx+b 输入：$x \\in \\mathbb{R}^n$（$n$ 维向量） 权重矩阵：$W \\in \\mathbb{R}^{m \\times n}$（$m$ 行 $n$ 列） 偏置：$b \\in \\mathbb{R}^m$ 输出：$y \\in \\mathbb{R}^m$","213-前向传播的计算过程#2.1.3 前向传播的计算过程":"举个例子：输入是 3 维，输出是 2 维\ny=[w11w12w13w21w22w23][x1x2x3]+[b1b2]y = \\begin{bmatrix} w_{11} \u0026 w_{12} \u0026 w_{13} \\\\ w_{21} \u0026 w_{22} \u0026 w_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}y=[w11​w21​​w12​w22​​w13​w23​​]​x1​x2​x3​​​+[b1​b2​​] import torch import torch.nn as nn # 定义一个线性层：输入 3 维，输出 2 维 linear = nn.Linear(in_features=3, out_features=2) # 输入一个 batch 的数据（batch_size=4） x = torch.randn(4, 3) # shape: (4, 3) y = linear(x) # shape: (4, 2)","214-线性层的局限性#2.1.4 线性层的局限性":"问题：如果我们把多个线性层叠加起来会怎样？\ny=W2(W1x+b1)+b2y = W_2(W_1 x + b_1) + b_2y=W2​(W1​x+b1​)+b2​展开后发现：\ny=(W2W1)x+(W2b1+b2)=W′x+b′y = (W_2 W_1)x + (W_2 b_1 + b_2) = W'x + b'y=(W2​W1​)x+(W2​b1​+b2​)=W′x+b′结论：无论堆叠多少层线性层，最终还是一个线性变换！\n怎么办？需要引入非线性，也就是激活函数！","22-激活函数#2.2 激活函数":"激活函数是神经网络的\"灵魂\"，没有它，再深的网络也只是个线性回归。","221-为什么需要激活函数#2.2.1 为什么需要激活函数？":"前面我们证明了：多层线性层 = 一层线性层。这意味着没有激活函数的深度网络毫无意义。激活函数引入了非线性，让神经网络能够拟合复杂的函数。","222-常见激活函数#2.2.2 常见激活函数":"常见的激活函数包括：\nSigmoid：$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\nTanh：$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\nReLU：$\\text{ReLU}(x) = \\max(0, x)$\nLeaky ReLU：$\\text{LeakyReLU}(x) = \\max(\\alpha x, x)$，其中 $\\alpha$ 是小的正数（通常取 0.01）\nELU：指数线性单元\nELU(x)={xif x\u003e0α(ex−1)if x≤0\\text{ELU}(x) = \\begin{cases} x \u0026 \\text{if } x \u003e 0 \\\\ \\alpha(e^x - 1) \u0026 \\text{if } x \\leq 0 \\end{cases}ELU(x)={xα(ex−1)​if x\u003e0if x≤0​ SELU：缩放指数线性单元\nSELU(x)=λ{xif x\u003e0α(ex−1)if x≤0\\text{SELU}(x) = \\lambda \\begin{cases} x \u0026 \\text{if } x \u003e 0 \\\\ \\alpha(e^x - 1) \u0026 \\text{if } x \\leq 0 \\end{cases}SELU(x)=λ{xα(ex−1)​if x\u003e0if x≤0​其中 $\\lambda \\approx 1.0507$，$\\alpha \\approx 1.6733$\nSwish：$x \\cdot \\sigma(x)$\nSwish(x)=x⋅11+e−x\\text{Swish}(x) = x \\cdot \\frac{1}{1 + e^{-x}}Swish(x)=x⋅1+e−x1​ Mish：$x \\cdot \\tanh(\\ln(1 + e^x))$\nMish(x)=x⋅tanh⁡(ln⁡(1+ex))\\text{Mish}(x) = x \\cdot \\tanh(\\ln(1 + e^x))Mish(x)=x⋅tanh(ln(1+ex)) Softmax：$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$\n激活函数图像可参考：\n![Softmax](/img/dl/softmax.png)Softmax ![Swish](/img/dl/swish.png)Swish ![ReLU](/img/dl/relu.png)ReLU ![Leaky ReLU](/img/dl/leakyrelu.png)Leaky ReLU ![ELU](/img/dl/elu.png)ELU ![SELU](/img/dl/selu.png)SELU ![Mish](/img/dl/mish.png)Mish ![PReLU](/img/dl/prelu.png)PReLU","31-梯度下降沿着山坡往下走#3.1 梯度下降：沿着山坡往下走":"","311-直观理解#3.1.1 直观理解":"想象你在一座山上蒙着眼睛，想走到山底（最低点）。你会怎么做？\n感受脚下的坡度 朝着最陡的下坡方向走一小步 重复上述过程 这就是梯度下降的思想！","312-数学表达#3.1.2 数学表达":"假设我们要最小化损失函数 $L(w)$：\nwnew=wold−η∇L(wold)w_{\\text{new}} = w_{\\text{old}} - \\eta \\nabla L(w_{\\text{old}})wnew​=wold​−η∇L(wold​)其中：\n$\\nabla L$：损失函数的梯度（指向函数增长最快的方向） $\\eta$：学习率（步长），控制每次走多远 负号：沿着梯度的反方向走（下山而不是上山）","32-三种梯度下降#3.2 三种梯度下降":"","321-批量梯度下降batch-gradient-descent-bgd#3.2.1 批量梯度下降（Batch Gradient Descent, BGD）":"每次更新参数时，用所有训练样本计算梯度：\nw=w−η1N∑i=1N∇Li(w)w = w - \\eta \\frac{1}{N} \\sum_{i=1}^{N} \\nabla L_i(w)w=w−ηN1​i=1∑N​∇Li​(w) 优点：稳定，收敛平滑 缺点：慢！如果数据集很大，每次更新要算 $N$ 个样本的梯度","322-随机梯度下降stochastic-gradient-descent-sgd#3.2.2 随机梯度下降（Stochastic Gradient Descent, SGD）":"每次只用一个样本计算梯度：\nw=w−η∇Li(w)w = w - \\eta \\nabla L_i(w)w=w−η∇Li​(w) 优点：快！每次更新只需要一个样本 缺点：不稳定，损失函数震荡","323-小批量梯度下降mini-batch-gd#3.2.3 小批量梯度下降（Mini-batch GD）":"每次用一小批样本（比如 32、64、128）计算梯度：\nw=w−η1B∑i=1B∇Li(w)w = w - \\eta \\frac{1}{B} \\sum_{i=1}^{B} \\nabla L_i(w)w=w−ηB1​i=1∑B​∇Li​(w)其中 $B$ 是 batch size。\n优点：结合了前两者的优点，既快又稳定 这是现在的标准做法！ 一定要明白，是对数据分布求对应的导数！ 这里可以看强化学习那一部分对这种 SGD 的解释：强化学习章节 6","33-反向传播backpropagation#3.3 反向传播（Backpropagation）":"问题：神经网络有成千上万个参数，怎么计算每个参数的梯度？\n答案：反向传播算法！利用链式法则，从输出层往回算。","331-计算图#3.3.1 计算图":"把神经网络画成一个计算图，每个节点是一个操作。\n举个简单例子：\nL=(w1x1+w2x2+b)2L = (w_1 x_1 + w_2 x_2 + b)^2L=(w1​x1​+w2​x2​+b)2计算图：\nx1 --- [* w1]---\u003e a ---\u003e [+] ---\u003e z ---\u003e [^2] ---\u003e L x2 --- [* w2]---\u003e b -----^ [+ b] ---------------^","332-前向传播forward-pass#3.3.2 前向传播（Forward Pass）":"从输入到输出，计算每一层的值：\na1=w1x1a_1 = w_1 x_1a1​=w1​x1​a2=w2x2a_2 = w_2 x_2a2​=w2​x2​z=a1+a2+bz = a_1 + a_2 + bz=a1​+a2​+bL=z2L = z^2L=z2","333-反向传播backward-pass#3.3.3 反向传播（Backward Pass）":"从输出往回，用链式法则计算梯度：\n∂L∂z=2z\\frac{\\partial L}{\\partial z} = 2z∂z∂L​=2z∂L∂a1=∂L∂z⋅∂z∂a1=2z⋅1=2z\\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial a_1} = 2z \\cdot 1 = 2z∂a1​∂L​=∂z∂L​⋅∂a1​∂z​=2z⋅1=2z∂L∂a2=∂L∂z⋅∂z∂a2=2z⋅1=2z\\frac{\\partial L}{\\partial a_2} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial a_2} = 2z \\cdot 1 = 2z∂a2​∂L​=∂z∂L​⋅∂a2​∂z​=2z⋅1=2z∂L∂w1=∂L∂a1⋅∂a1∂w1=2z⋅x1\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial w_1} = 2z \\cdot x_1∂w1​∂L​=∂a1​∂L​⋅∂w1​∂a1​​=2z⋅x1​∂L∂w2=∂L∂a2⋅∂a2∂w2=2z⋅x2\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial w_2} = 2z \\cdot x_2∂w2​∂L​=∂a2​∂L​⋅∂w2​∂a2​​=2z⋅x2​∂L∂b=∂L∂z⋅∂z∂b=2z⋅1=2z\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = 2z \\cdot 1 = 2z∂b∂L​=∂z∂L​⋅∂b∂z​=2z⋅1=2z关键点：每次算梯度时，都复用前面已经算好的中间结果！","334-两层神经网络的反向传播推导#3.3.4 两层神经网络的反向传播推导":"假设网络结构：\nz[1]=W[1]x+b[1]z^{[1]} = W^{[1]} x + b^{[1]}z[1]=W[1]x+b[1]a[1]=ReLU(z[1])a^{[1]} = \\text{ReLU}(z^{[1]})a[1]=ReLU(z[1])z[2]=W[2]a[1]+b[2]z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}z[2]=W[2]a[1]+b[2]y=softmax(z[2])y = \\text{softmax}(z^{[2]})y=softmax(z[2])L=crossEntropy(y,y^)L = \\text{crossEntropy}(y, \\hat{y})L=crossEntropy(y,y^​)反向传播步骤：\n计算输出层梯度： ∂L∂z[2]=y−y^\\frac{\\partial L}{\\partial z^{[2]}} = y - \\hat{y}∂z[2]∂L​=y−y^​ 计算第二层权重梯度： ∂L∂W[2]=∂L∂z[2](a[1])T\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} (a^{[1]})^T∂W[2]∂L​=∂z[2]∂L​(a[1])T 向前传播梯度： ∂L∂a[1]=(W[2])T∂L∂z[2]\\frac{\\partial L}{\\partial a^{[1]}} = (W^{[2]})^T \\frac{\\partial L}{\\partial z^{[2]}}∂a[1]∂L​=(W[2])T∂z[2]∂L​ 通过 ReLU： ∂L∂z[1]=∂L∂a[1]⊙1z[1]\u003e0\\frac{\\partial L}{\\partial z^{[1]}} = \\frac{\\partial L}{\\partial a^{[1]}} \\odot \\mathbf{1}_{z^{[1]} \u003e 0}∂z[1]∂L​=∂a[1]∂L​⊙1z[1]\u003e0​其中 $\\odot$ 表示逐元素相乘，$\\mathbf{1}_{z^{[1]} \u003e 0}$ 是指示函数。\n计算第一层权重梯度： ∂L∂W[1]=∂L∂z[1]xT\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} x^T∂W[1]∂L​=∂z[1]∂L​xT 推荐视频：3Blue1Brown 梯度下降法\n代码示例（PyTorch 自动求导）：\nimport torch # 定义参数 w = torch.tensor([1.0], requires_grad=True) x = torch.tensor([2.0]) # 前向传播 y = w * x loss = y ** 2 # 反向传播（PyTorch 自动计算梯度！） loss.backward() print(f\"梯度 dL/dw = {w.grad}\") # 输出: 8.0","34-训练相关的术语#3.4 训练相关的术语":"","341-损失函数loss-function#3.4.1 损失函数（Loss Function）":"衡量模型预测和真实标签之间差距的函数。常见的有：\n均方误差（MSE）：回归问题常用（并且 MLE 可以从数学上证明 = MSE） L=1N∑i=1N(yi−y^i)2L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2L=N1​i=1∑N​(yi​−y^​i​)2最大似然估计（Maximum Likelihood Estimation, MLE）：\nMLE=arg⁡max⁡xP[(a1,b1),(a2,b2),…∣x]=arg⁡max⁡xexp⁡(−∥Ax−b∥22σ2)=arg⁡min⁡x∥Ax−b∥2\\text{MLE} = \\arg\\max_x P[(a_1, b_1), (a_2, b_2), \\ldots | x] = \\arg\\max_x \\exp\\left(-\\frac{\\|Ax - b\\|^2}{2\\sigma^2}\\right) = \\arg\\min_x \\|Ax - b\\|^2MLE=argxmax​P[(a1​,b1​),(a2​,b2​),…∣x]=argxmax​exp(−2σ2∥Ax−b∥2​)=argxmin​∥Ax−b∥2MSE = MLE with Gaussian noise assumption\n交叉熵（Cross Entropy）：分类问题常用 L=−∑i=1Nyilog⁡(y^i)L = -\\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)L=−i=1∑N​yi​log(y^​i​)","342-优化器optimizer#3.4.2 优化器（Optimizer）":"用来更新参数的算法。常见的有：\nSGD（Stochastic Gradient Descent）：最基础的 Adam：目前最流行的，自动调整学习率 AdamW：Adam 的改进版，现代大模型常用","343-学习率learning-rate-lr#3.4.3 学习率（Learning Rate, lr）":"控制参数更新的步长。太大容易震荡，太小收敛慢。通常设在 $10^{-3}$ 到 $10^{-4}$ 之间。\nwnew=wold−lr⋅∇Lw_{\\text{new}} = w_{\\text{old}} - \\text{lr} \\cdot \\nabla Lwnew​=wold​−lr⋅∇L","344-epochbatchiteration#3.4.4 Epoch、Batch、Iteration":"Epoch：把整个训练集过一遍叫一个 epoch Batch：一次喂给模型的样本数（比如 32、64、128） Iteration：一次参数更新 举个例子：训练集有 1000 个样本，batch size = 100\n1 个 epoch = 10 个 iteration 训练 5 个 epoch = 50 个 iteration","35-模型泛化相关的术语#3.5 模型泛化相关的术语":"","351-过拟合overfitting与欠拟合underfitting#3.5.1 过拟合（Overfitting）与欠拟合（Underfitting）":"欠拟合：模型太简单，训练集和测试集都表现差（high bias） 过拟合：模型太复杂，训练集表现好但测试集差（high variance）","352-正则化regularization#3.5.2 正则化（Regularization）":"防止过拟合的技巧：\nL2 正则化（Weight Decay）：给损失函数加一项 $\\lambda |w|^2$，让权重不要太大 Ltotal=L+λ∥w∥2L_{\\text{total}} = L + \\lambda \\|w\\|^2Ltotal​=L+λ∥w∥2 Dropout：训练时随机\"关闭\"一些神经元 数据增强（Data Augmentation）：对图片旋转、翻转、裁剪等，增加训练样本多样性","41-mlp多层感知机#4.1 MLP（多层感知机）":"","411-mlp-的结构#4.1.1 MLP 的结构":"一个典型的 MLP 包含：\n输入层：接收原始数据 多个隐藏层：每层 = 线性变换 + 激活函数 输出层：产生最终预测 数学表达：\nh1=σ(W1x+b1)h_1 = \\sigma(W_1 x + b_1)h1​=σ(W1​x+b1​)h2=σ(W2h1+b2)h_2 = \\sigma(W_2 h_1 + b_2)h2​=σ(W2​h1​+b2​)⋮\\vdots⋮y=WLhL−1+bLy = W_L h_{L-1} + b_Ly=WL​hL−1​+bL​","412-万能逼近定理universal-approximation-theorem#4.1.2 万能逼近定理（Universal Approximation Theorem）":"理论上，只要隐藏层足够宽，一个单隐藏层的 MLP 就能逼近任意连续函数！\n但实践中，我们更倾向于用 “深而窄” 的网络，而不是 “浅而宽” 的网络。为什么？\n深层网络能学到层次化的特征表示 参数更少，更容易训练","413-代码示例#4.1.3 代码示例":"import torch import torch.nn as nn class MLP(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(MLP, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.fc3 = nn.Linear(hidden_dim, output_dim) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.fc1(x)) x = self.relu(self.fc2(x)) x = self.fc3(x) # 输出层不加激活函数 return x # 使用示例 model = MLP(input_dim=784, hidden_dim=256, output_dim=10) x = torch.randn(32, 784) # batch_size=32 output = model(x) # shape: (32, 10)","414-mlp-的应用场景#4.1.4 MLP 的应用场景":"表格数据（比如房价预测、信用评分） 简单的分类/回归任务 作为其他网络的组件（比如 Transformer 里的 FFN） 缺点是什么？ 对于图像、文本这种有空间/时间结构的数据，MLP 效果不太好。这就引出了 CNN。","42-cnn卷积神经网络#4.2 CNN（卷积神经网络）":"CNN 是计算机视觉的基石，2012 年 AlexNet 横空出世后，深度学习彻底火了。","421-从连续卷积到离散卷积#4.2.1 从连续卷积到离散卷积":"连续卷积的定义（信号处理中）：\n(f∗g)(t)=∫−∞∞f(τ)g(t−τ)dτ(f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) d\\tau(f∗g)(t)=∫−∞∞​f(τ)g(t−τ)dτ离散卷积（用在图像上）：\n(I∗K)(i,j)=∑m∑nI(i+m,j+n)K(m,n)(I * K)(i, j) = \\sum_{m} \\sum_{n} I(i + m, j + n) K(m, n)(I∗K)(i,j)=m∑​n∑​I(i+m,j+n)K(m,n)其中 $I$ 是图像，$K$ 是卷积核（kernel）。\n实际上，深度学习里用的是互相关（cross-correlation），但大家习惯叫它卷积：\n(I⋆K)(i,j)=∑m∑nI(i+m,j+n)K(m,n)(I \\star K)(i, j) = \\sum_{m} \\sum_{n} I(i + m, j + n) K(m, n)(I⋆K)(i,j)=m∑​n∑​I(i+m,j+n)K(m,n)","422-卷积层的工作原理#4.2.2 卷积层的工作原理":"卷积核（Filter/Kernel）：\n一个卷积核就是一个小矩阵，比如 3×3 或 5×5。它在图像上滑动，每次计算一个位置的加权和。\n举个例子，一个 3×3 的卷积核：\n[−1−1−1000111]\\begin{bmatrix} -1 \u0026 -1 \u0026 -1 \\\\ 0 \u0026 0 \u0026 0 \\\\ 1 \u0026 1 \u0026 1 \\end{bmatrix}​−101​−101​−101​​这个卷积核可以检测水平边缘！\n步长（Stride）：\n卷积核每次移动几个像素。stride=1 表示每次移动 1 个像素，stride=2 表示跳着走。\n填充（Padding）：\n在图像边缘填充 0，让卷积后的特征图尺寸不变。\n输出尺寸计算：\noutput_size=input_size−kernel_size+2×paddingstride+1\\text{output\\_size} = \\frac{\\text{input\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} + 1output_size=strideinput_size−kernel_size+2×padding​+1","423-池化层pooling#4.2.3 池化层（Pooling）":"池化层用来降低特征图的空间分辨率。\nMax Pooling：取局部区域的最大值 MaxPool(R)=max⁡(i,j)∈RI(i,j)\\text{MaxPool}(R) = \\max_{(i,j) \\in R} I(i, j)MaxPool(R)=(i,j)∈Rmax​I(i,j) Average Pooling：取平均值 AvgPool(R)=1∣R∣∑(i,j)∈RI(i,j)\\text{AvgPool}(R) = \\frac{1}{|R|} \\sum_{(i,j) \\in R} I(i, j)AvgPool(R)=∣R∣1​(i,j)∈R∑​I(i,j)其中 $R$ 是池化窗口区域。","424-感受野receptive-field#4.2.4 感受野（Receptive Field）":"感受野是指卷积神经网络中，某一层特征图上的一个点，对应输入图像上的区域大小。感受野越大，看到的原始图像范围就越大。","425-cnn-的三大优势#4.2.5 CNN 的三大优势":"局部连接（Local Connectivity）\n每个神经元只连接输入的一小块区域，而不是全部。大大减少了参数量。\n权值共享（Weight Sharing）\n同一个卷积核在整张图片上共享参数。一个 3×3 卷积核只有 9 个参数！\n平移不变性（Translation Invariance）\n不管猫在图片的左上角还是右下角，CNN 都能识别出来。","426-经典-cnn-架构简介#4.2.6 经典 CNN 架构简介":"","427-代码示例#4.2.7 代码示例":"import torch.nn as nn class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1) self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) self.pool = nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = nn.Linear(64 * 7 * 7, 128) self.fc2 = nn.Linear(128, 10) self.relu = nn.ReLU() def forward(self, x): # x shape: (batch, 1, 28, 28) x = self.relu(self.conv1(x)) # -\u003e (batch, 32, 28, 28) x = self.pool(x) # -\u003e (batch, 32, 14, 14) x = self.relu(self.conv2(x)) # -\u003e (batch, 64, 14, 14) x = self.pool(x) # -\u003e (batch, 64, 7, 7) x = x.view(-1, 64 * 7 * 7) # flatten x = self.relu(self.fc1(x)) x = self.fc2(x) return x","43-rnn-和-lstm#4.3 RNN 和 LSTM":"RNN 是处理序列数据的利器，但它有个致命缺陷：梯度消失。LSTM 就是来解决这个问题的。","431-rnn-的基本结构#4.3.1 RNN 的基本结构":"为什么需要 RNN？\nMLP 和 CNN 都假设输入是固定长度的，但很多数据是序列：\n文本：“今天天气真不错”（长度可变） 语音：一段录音（长度可变） 时间序列：股票价格（长度可变） RNN 的核心思想：维护一个隐藏状态 $h_t$，在处理序列时不断更新\nht=tanh⁡(Whhht−1+Wxhxt+bh)h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)ht​=tanh(Whh​ht−1​+Wxh​xt​+bh​)yt=Whyht+byy_t = W_{hy} h_t + b_yyt​=Why​ht​+by​其中：\n$h_t$：时刻 $t$ 的隐藏状态 $x_t$：时刻 $t$ 的输入 $y_t$：时刻 $t$ 的输出 $W_{hh}$、$W_{xh}$、$W_{hy}$：权重矩阵 $b_h$、$b_y$：偏置向量 时间展开（Unrolling in Time）：\n虽然 RNN 看起来有循环，但我们可以把它\"展开\"成一个很长的前馈网络：\nx1 -\u003e [RNN] -\u003e h1 -\u003e [RNN] -\u003e h2 -\u003e [RNN] -\u003e h3 -\u003e ... ↓ ↓ ↓ y1 y2 y3","432-rnn-的问题梯度消失和梯度爆炸#4.3.2 RNN 的问题：梯度消失和梯度爆炸":"反向传播时，梯度要沿着时间步往回传。假设序列长度是 $T$，梯度会连乘 $T$ 次：\n∂L∂h1=∏t=1T∂ht+1∂ht⋅∂L∂hT\\frac{\\partial L}{\\partial h_1} = \\prod_{t=1}^{T} \\frac{\\partial h_{t+1}}{\\partial h_t} \\cdot \\frac{\\partial L}{\\partial h_T}∂h1​∂L​=t=1∏T​∂ht​∂ht+1​​⋅∂hT​∂L​ 如果每个 $\\frac{\\partial h_{t+1}}{\\partial h_t}$ 小于 1，连乘 $T$ 次后会接近 0（梯度消失） 如果大于 1，会指数爆炸（梯度爆炸） 结果：RNN 很难学到长期依赖（long-term dependencies）","433-lstm-的结构#4.3.3 LSTM 的结构":"LSTM (Long Short-Term Memory) 在 1997 年被提出，专门解决梯度消失问题。\nLSTM 的核心：引入细胞状态 $c_t$，用**门控机制（gate）**来控制信息流动\n三个门：\n遗忘门（Forget Gate）：决定丢弃多少旧信息 ft=σ(Wf⋅[ht−1,xt]+bf)f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)ft​=σ(Wf​⋅[ht−1​,xt​]+bf​)遗忘门决定让那些信息继续通过这个 cell，以上一单元的输出 $h_{t-1}$ 和本单元的输入 $x_t$ 为输入的 sigmoid 函数，为 $C_{t-1}$ 中的每一项产生一个在 [0,1] 内的值，来控制上一单元状态被遗忘的程度。\n输入门（Input Gate）：决定存储多少新信息 it=σ(Wi⋅[ht−1,xt]+bi)i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)it​=σ(Wi​⋅[ht−1​,xt​]+bi​)C~t=tanh⁡(WC⋅[ht−1,xt]+bC)\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)C~t​=tanh(WC​⋅[ht−1​,xt​]+bC​)传入门决定让多少新的信息加入到 cell 状态中来，再把 cell 状态从 $C_{t-1}$ 更新为 $C_t$。实现这个需要包括两个步骤：\n首先 input gate layer 的 sigmoid 层决定哪些信息需要更新；一个 tanh 层生成一个向量，也就是备选的用来更新的内容 $\\tilde{C}_t$。\n把 cell 状态从 $C_{t-1}$ 更新为 $C_t$：首先我们把旧的状态 $C_{t-1}$ 和 $f_t$ 相乘，就是遗忘后保留的部分信息；然后加上 $i_t \\odot \\tilde{C}_t$。这部分信息就是我们要添加的新内容。\n输出门（Output Gate）：决定输出什么 ot=σ(Wo⋅[ht−1,xt]+bo)o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)ot​=σ(Wo​⋅[ht−1​,xt​]+bo​)输出主要是依赖于 cell 的状态 $C_t$ 和经过一个过滤的处理。首先用一个 sigmoid 层的计算结果决定将 $C_t$ 中的哪部分信息输出。再把 $C_t$ 用一个 tanh 层把数值都归到 -1 和 1 之间，最后把 tanh 层的输出和 sigmoid 层计算出来的权重相乘以得到输出结果。\n细胞状态更新：\nCt=ft⊙Ct−1+it⊙C~tC_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_tCt​=ft​⊙Ct−1​+it​⊙C~t​隐藏状态更新：\nht=ot⊙tanh⁡(Ct)h_t = o_t \\odot \\tanh(C_t)ht​=ot​⊙tanh(Ct​)\n为什么 LSTM 能解决梯度消失？\n细胞状态 $c_t$ 是加法更新的，梯度可以直接流过，不会连乘！","434-gru-简介#4.3.4 GRU 简介":"GRU (Gated Recurrent Unit) 是 LSTM 的简化版，只有两个门：\n更新门（Update Gate） 重置门（Reset Gate） 参数更少，训练更快，效果和 LSTM 差不多。它把 LSTM 中的细胞状态和隐藏状态进行了合并，最后模型比标准 LSTM 结构简单。\nGRU 的数学表达：\n重置门（Reset Gate）：\nrt=σ(Wr⋅[ht−1,xt]+br)r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)rt​=σ(Wr​⋅[ht−1​,xt​]+br​)更新门（Update Gate）：\nzt=σ(Wz⋅[ht−1,xt]+bz)z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)zt​=σ(Wz​⋅[ht−1​,xt​]+bz​)候选隐藏状态：\nh~t=tanh⁡(W⋅[rt⊙ht−1,xt]+b)\\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1}, x_t] + b)h~t​=tanh(W⋅[rt​⊙ht−1​,xt​]+b)隐藏状态更新：\nht=(1−zt)⊙ht−1+zt⊙h~th_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_tht​=(1−zt​)⊙ht−1​+zt​⊙h~t​其中，$r_t$ 表示重置门，$z_t$ 表示更新门。重置门决定是否将之前的状态忘记。（作用相当于合并了 LSTM 中的遗忘门和传入门）当 $r_t$ 趋于 0 的时候，前一个时刻的状态信息 $h_{t-1}$ 会被忘掉，隐藏状态 $\\tilde{h}_t$ 会被重置为当前输入的信息。更新门决定是否要将隐藏状态更新为新的状态 $\\tilde{h}_t$（作用相当于 LSTM 中的输出门）。","4341-gru-与-lstm-的对比#4.3.4.1 GRU 与 LSTM 的对比":"与 LSTM 相比：\nGRU 少一个门，同时少了细胞状态 $C_t$\n在 LSTM 中，通过遗忘门和传入门控制信息的保留和传入；GRU 则通过重置门来控制是否要保留原来隐藏状态的信息，但是不再限制当前信息的传入。\n在 LSTM 中，虽然得到了新的细胞状态 $C_t$，但是还不能直接输出，而是需要经过一个过滤的处理：$h_t = o_t \\odot \\tanh(C_t)$；同样，在 GRU 中，虽然我们也得到了新的隐藏状态 $\\tilde{h}t$，但是还不能直接输出，而是通过更新门来控制最后的输出：$h_t = (1 - z_t) \\odot h{t-1} + z_t \\odot \\tilde{h}_t$","435-代码示例#4.3.5 代码示例":"import torch.nn as nn class LSTMModel(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(LSTMModel, self).__init__() self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x): # x shape: (batch, seq_len, input_size) lstm_out, (h_n, c_n) = self.lstm(x) # lstm_out shape: (batch, seq_len, hidden_size) # 取最后一个时间步的输出 out = self.fc(lstm_out[:, -1, :]) return out","44-gnn图神经网络#4.4 GNN（图神经网络）":"**图（Graph）**是一种非常通用的数据结构：社交网络、分子结构、知识图谱都可以用图表示。GNN 就是处理图数据的神经网络。","441-图的基本概念#4.4.1 图的基本概念":"一个图 $G = (V, E)$ 包含：\n节点（Vertices）：$V = {1, 2, \\ldots, n}$ 边（Edges）：$E \\subseteq {(i, j) : i, j \\in V}$ 邻接矩阵（Adjacency Matrix）：\nAij={1if (i,j)∈E0otherwiseA_{ij} = \\begin{cases} 1 \u0026 \\text{if } (i, j) \\in E \\\\ 0 \u0026 \\text{otherwise} \\end{cases}Aij​={10​if (i,j)∈Eotherwise​每个节点可以有特征向量 $x_i \\in \\mathbb{R}^d$，所有节点的特征可以组成特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$。","442-spatial-gnn#4.4.2 Spatial GNN":"Spatial 方法的核心思想：聚合邻居节点的信息\n消息传递框架（Message Passing）：\n消息聚合（Aggregate）： mi=AGGREGATE{hj:j∈N(i)}m_i = \\text{AGGREGATE}\\{h_j : j \\in \\mathcal{N}(i)\\}mi​=AGGREGATE{hj​:j∈N(i)}其中 $\\mathcal{N}(i)$ 表示节点 $i$ 的邻居集合。\n节点更新（Update）： hi(l+1)=UPDATE(hi(l),mi)h_i^{(l+1)} = \\text{UPDATE}(h_i^{(l)}, m_i)hi(l+1)​=UPDATE(hi(l)​,mi​)GCN（Graph Convolutional Network）：\n最经典的 GNN 模型，公式很简洁：\nH(l+1)=σ(D~−12A~D~−12H(l)W(l))H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)})H(l+1)=σ(D~−21​A~D~−21​H(l)W(l))其中 $\\tilde{A} = A + I$（加自环），$\\tilde{D}$ 是度矩阵，$D_{ii} = \\sum_j \\tilde{A}_{ij}$。\nGraphSAGE：\n采样邻居，而不是聚合所有邻居（适合大规模图）\nhi(l+1)=σ(W(l)⋅CONCAT(hi(l),AGGREGATE{hj(l):j∈N(i)}))h_i^{(l+1)} = \\sigma(W^{(l)} \\cdot \\text{CONCAT}(h_i^{(l)}, \\text{AGGREGATE}\\{h_j^{(l)} : j \\in \\mathcal{N}(i)\\}))hi(l+1)​=σ(W(l)⋅CONCAT(hi(l)​,AGGREGATE{hj(l)​:j∈N(i)}))GAT（Graph Attention Network）：\n用注意力机制动态计算邻居的权重\neij=LeakyReLU(aT[Whi∣∣Whj])e_{ij} = \\text{LeakyReLU}(a^T [W h_i || W h_j])eij​=LeakyReLU(aT[Whi​∣∣Whj​])αij=exp⁡(eij)∑k∈N(i)exp⁡(eik)\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}αij​=∑k∈N(i)​exp(eik​)exp(eij​)​hi′=σ(∑j∈N(i)αijWhj)h_i' = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} W h_j\\right)hi′​=σ​j∈N(i)∑​αij​Whj​​其中 $a$ 是注意力参数向量，$||$ 表示拼接操作。","443-spectral-gnn#4.4.3 Spectral GNN":"Spectral 方法基于图信号处理理论，用到一些图论知识。\n图拉普拉斯矩阵（Graph Laplacian）：\nL=D−AL = D - AL=D−A其中 $D$ 是度矩阵，$A$ 是邻接矩阵。\n归一化拉普拉斯矩阵：\nLsym=I−D−12AD−12L_{\\text{sym}} = I - D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}Lsym​=I−D−21​AD−21​谱域卷积的想法：\n对图信号做傅里叶变换（特征分解） 在谱域做卷积（相当于滤波） 逆变换回来 ChebNet 用切比雪夫多项式近似谱卷积，GCN 是 ChebNet 的一阶近似。\nSpatial 与 Spectral 的联系：\nGCN 可以从两个角度推导出来！Spectral 提供了理论基础，Spatial 提供了直观理解。","444-异构图#4.4.4 异构图":"异构图（Heterogeneous Graph）：节点和边有不同的类型\n举个例子：学术网络\n节点类型：论文、作者、会议 边类型：写作、发表、引用 元路径（Meta-path）：\n在异构图上定义的路径模板，比如 “作者-论文-会议”\nHAN（Heterogeneous Attention Network）：对不同类型的邻居用不同的注意力机制\nHGT（Heterogeneous Graph Transformer）：\n结合 Transformer 和异构图，处理大规模异构图","445-代码示例#4.4.5 代码示例":"import torch import torch.nn as nn import torch.nn.functional as F class GCNLayer(nn.Module): def __init__(self, in_features, out_features): super(GCNLayer, self).__init__() self.linear = nn.Linear(in_features, out_features) def forward(self, X, A): # X: (N, in_features), A: (N, N) adjacency matrix # 添加自环并归一化 A_hat = A + torch.eye(A.size(0), device=A.device) D_hat = torch.diag(A_hat.sum(1)) D_inv_sqrt = torch.pow(D_hat, -0.5) D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0. A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt # GCN操作 out = A_norm @ X out = self.linear(out) return F.relu(out) 提示：接下来就该讲 Attention 啦，请移步 ./transformer.md","51-u-net#5.1 U-Net":"U-Net 最初是为医学图像分割设计的，现在已经成为图像分割的标准架构。\nU-Net 采用对称的 Encoder-Decoder 结构，形状像字母 “U”：\nEncoder（下采样路径）：\n不断用卷积+池化降低分辨率 提取高层语义特征 Decoder（上采样路径）：\n用上采样（或转置卷积）恢复分辨率 生成像素级的预测 跳跃连接（Skip Connection）：\n这是 U-Net 的关键！Encoder 的特征直接连接到 Decoder 的对应层。\n好处：结合低层细节（边缘、纹理）和高层语义（物体类别）\n应用：医学图像分割、卫星图像分割、图像修复等","52-vitvision-transformer#5.2 ViT（Vision Transformer）":"2020 年 Google 的 ViT 证明：纯 Transformer 也能在视觉任务上打败 CNN！\n将 Transformer 应用到图像的挑战：\n图像是 2D 的，像素太多（比如 224×224=50176 个像素）。如果每个像素都是一个 token，attention 的计算量是 $O(n^2)$，太大了！\n图像分块（Patch Embedding）：\nViT 的做法：把图像切成小块（patch），比如 16×16 的 patch\nImage(224×224)→patches(14×14)→Flatten to tokens(196)\\text{Image}(224 \\times 224) \\to \\text{patches}(14 \\times 14) \\to \\text{Flatten to tokens}(196)Image(224×224)→patches(14×14)→Flatten to tokens(196)每个 patch 用线性层映射成 embedding，就像 NLP 里的 word embedding。\nViT 的完整流程：\n图像分块 → 196 个 patch tokens 加上位置编码 加一个特殊的 [CLS] token（用于分类） 送入标准的 Transformer Encoder 取 [CLS] token 的输出做分类 ViT vs CNN：\nViT 需要更多数据才能训练好（归纳偏置（inductive bias）更少） 在大规模数据集上，ViT 效果更好 ViT 的计算效率更高（可以并行） ViT 的改进：\nDeiT (Data-efficient Image Transformer)：用知识蒸馏，减少数据需求 Swin Transformer：分层的 Transformer，在局部窗口内做 attention，效率更高","53-dptdense-prediction-transformer#5.3 DPT（Dense Prediction Transformer）":"密集预测任务：每个像素都要输出一个值\n深度估计：预测每个像素的深度 语义分割：预测每个像素的类别 DPT 的想法：用 ViT 做 backbone，加上一个密集预测的 head\n关键设计：重新组装（Reassemble）ViT 的特征，恢复空间分辨率","54-clip#5.4 CLIP":"CLIP (Contrastive Language-Image Pre-training) 是 OpenAI 2021 年的工作，通过对比学习连接视觉和语言。\n对比学习（Contrastive Learning）：\n让相似的样本在特征空间中靠近，不相似的样本远离。\nCLIP 的训练方式： 数据：4 亿个图像-文本对（从互联网爬取） 训练目标： 正样本：匹配的图像-文本对，余弦相似度要高 负样本：不匹配的图像-文本对，余弦相似度要低 损失函数：\nL=−1N∑i=1Nlog⁡exp⁡(sim(Ii,Ti)/τ)∑j=1Nexp⁡(sim(Ii,Tj)/τ)\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(I_i, T_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(I_i, T_j) / \\tau)}L=−N1​i=1∑N​log∑j=1N​exp(sim(Ii​,Tj​)/τ)exp(sim(Ii​,Ti​)/τ)​其中 $\\text{sim}(I, T)$ 是图像和文本特征的余弦相似度，$\\tau$ 是温度参数。\nZero-shot 学习能力：\n训练好的 CLIP 可以不用微调，直接做分类！怎么做？给类别写文本描述，算图像和每个描述的相似度，取最大的。\n应用：图像分类、图像检索、文本到图像生成（DALL-E、Stable Diffusion 都用了 CLIP）","55-equivariant-network等变网络#5.5 Equivariant Network（等变网络）":"等变性（Equivariance）的数学定义：\n如果 $f(T(x)) = T(f(x))$，则 $f$ 对变换 $T$ 是等变的。\nCNN 的平移等变性：\n卷积操作对平移是等变的：先平移再卷积 = 先卷积再平移\nconv(Tshift(I))=Tshift(conv(I))\\text{conv}(T_{\\text{shift}}(I)) = T_{\\text{shift}}(\\text{conv}(I))conv(Tshift​(I))=Tshift​(conv(I))但 CNN 不是旋转等变的！\nViT 的等变性：\n标准 ViT 没有平移等变性（因为位置编码是绝对的），但可以通过相对位置编码实现（参考论文：Relative Position Encoding）\n旋转等变网络：\n在分子、3D 点云等任务中，旋转不变性很重要。\nE(n) Equivariant GNN：保持旋转和平移等变性的图神经网络","61-gan生成对抗网络#6.1 GAN（生成对抗网络）":"GAN 是 2014 年 Ian Goodfellow 提出的，被 Yann LeCun 称为\"过去10年机器学习最有趣的想法\"。\nGAN 的基本思想：\n生成器（Generator）：生成假数据，骗过判别器 判别器（Discriminator）：区分真假数据 两者对抗训练，互相进步！\nGAN 的损失函数：\nmin⁡Gmax⁡DEx∼pdata[log⁡D(x)]+Ez∼pz[log⁡(1−D(G(z)))]\\min_G \\max_D \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]Gmin​Dmax​Ex∼pdata​​[logD(x)]+Ez∼pz​​[log(1−D(G(z)))] $D$ 想最大化这个目标：真数据分对（$\\log D(x)$ 大），假数据分对（$\\log(1 - D(G(z)))$ 大） $G$ 想最小化这个目标：让假数据骗过 $D$ 训练挑战：\n模式崩塌（Mode Collapse）：只生成几种样本 训练不稳定：$G$ 和 $D$ 的平衡很难把握 梯度消失：$D$ 太强时，$G$ 收不到有效梯度 经典 GAN 变体：\nDCGAN：用深度卷积网络，引入 BatchNorm StyleGAN：控制生成图像的风格，质量超高 CycleGAN：无需配对数据的图像翻译（马→斑马）","62-vae变分自编码器#6.2 VAE（变分自编码器）":"自编码器（Autoencoder）：\nEncoder：$z = f_{\\text{enc}}(x)$（压缩） Decoder：$\\hat{x} = f_{\\text{dec}}(z)$（重建） 目标：让 $\\hat{x} \\approx x$ VAE 的改进：让 $z$ 服从某个分布（比如标准正态分布）\nqϕ(z∣x)∼N(μ(x),σ2(x))q_\\phi(z|x) \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))qϕ​(z∣x)∼N(μ(x),σ2(x))重参数化技巧（Reparameterization Trick）：\n不能直接对采样操作求梯度。改写成：\nz=μ(x)+σ(x)⊙ϵ,ϵ∼N(0,I)z = \\mu(x) + \\sigma(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)z=μ(x)+σ(x)⊙ϵ,ϵ∼N(0,I)ELBO（Evidence Lower Bound）：\nL=Eqϕ(z∣x)[log⁡pθ(x∣z)]−DKL(qϕ(z∣x)∣∣p(z))\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x) || p(z))L=Eqϕ​(z∣x)​[logpθ​(x∣z)]−DKL​(qϕ​(z∣x)∣∣p(z))第一项是重建误差，第二项是KL散度（让 $q$ 接近先验 $p$）","63-ddpm--flow-matching#6.3 DDPM \u0026amp; Flow Matching":"扩散模型（Diffusion Model）是目前生成模型的主流 ！ 前向扩散过程（Forward Diffusion）： 逐步给图像加噪声，最终变成纯噪声 q(xt∣xt−1)=N(xt;1−βtxt−1,βtI)q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)q(xt​∣xt−1​)=N(xt​;1−βt​​xt−1​,βt​I) 反向去噪过程（Reverse Denoising）： 训练一个神经网络 (xt , t) 预测噪声 ，逐步去噪 pθ(xt−1∣xt)=N(xt−1;μθ(xt,t),Σθ(xt,t))p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))pθ​(xt−1​∣xt​)=N(xt−1​;μθ​(xt​,t),Σθ​(xt​,t)) DDPM 的训练： 损失函数超简单： L=Et,x0,ϵ∥ϵ−ϵθ(xt,t)∥2\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2L=Et,x0​,ϵ​∥ϵ−ϵθ​(xt​,t)∥2 就是预测噪声的 MSE ！ Flow Matching： 最近的新方法，直接学习从噪声到数据的 ODE 流\n比 DDPM 更快，训练更稳定\n[图片占位符： 扩散模型的前向和反向过程示意图]","64-stable-diffusion#6.4 Stable Diffusion":"Latent Diffusion Model 的思想：\n直接在像素空间做扩散太慢！先用 VAE 压缩到潜空间（latent space）\nStable Diffusion 的架构：\nVAE Encoder：图像 → 潜空间 (512×512 → 64×64) U-Net 去噪网络：在潜空间做扩散 CLIP Text Encoder：文本 → embedding Cross-Attention：把文本 embedding 注入 U-Net VAE Decoder：潜空间 → 图像 为什么叫 Stable Diffusion？\n在 latent space 做，更稳定、更快 开源，人人都能跑（相比 DALL-E）","65-conditional-generation条件生成#6.5 Conditional Generation（条件生成）":"条件生成：根据给定条件生成内容（比如根据文本生成图像）\nCross Attention：\n在扩散模型的 U-Net 中，加入 Cross-Attention 层：\nQuery 来自图像特征 Key 和 Value 来自文本 embedding Attention(Qimage,Ktext,Vtext)\\text{Attention}(Q_{\\text{image}}, K_{\\text{text}}, V_{\\text{text}})Attention(Qimage​,Ktext​,Vtext​)Token Concat：\n直接把条件信息拼接到输入上（简单粗暴）\nControlNet：\n参考论文：ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models\n核心思想：复制预训练模型，加上额外的条件输入（边缘、深度图等）\n训练时： 冻结原模型参数 只训练新加的分支 推理时：两个分支的输出相加 好处：保留预训练模型的生成能力，同时添加精确控制","71-解码方式#7.1 解码方式":"","711-单向解码autoregressive#7.1.1 单向解码（Autoregressive）":"GPT 系列的做法：从左到右生成，每次预测下一个 token\nCausal Masking（因果掩码）：\n在 Self-Attention 中，只能看到左边的 token，看不到右边的\n解码策略：\n贪婪解码（Greedy）：每次选概率最大的 token t=arg⁡max⁡P(xt∣x1,…,xt−1)t = \\arg\\max P(x_t | x_1, \\ldots, x_{t-1})t=argmaxP(xt​∣x1​,…,xt−1​) Beam Search：保留 top-k 个候选序列 采样策略：\nTemperature Sampling：$P(x) \\propto \\exp(\\log p(x) / T)$ Top-k Sampling：只从概率最高的 k 个 token 中采样 Top-p (Nucleus) Sampling：累积概率达到 p 时停止","712-双向解码#7.1.2 双向解码":"BERT 的做法：随机 mask 一些 token，预测被 mask 的内容\nMasked Language Modeling (MLM)：\nlog⁡p(xmask∣xcontext)\\log p(x_{\\text{mask}} | x_{\\text{context}})logp(xmask​∣xcontext​)双向上下文的优势：\n可以同时利用左边和右边的信息，理解能力更强\n适合：文本分类、问答、命名实体识别等理解任务 不适合：文本生成（因为没有自回归的训练） BERT 的预训练+微调范式：\n预训练：在大规模语料上做 MLM 微调：在下游任务上 fine-tune","72-grpo#7.2 GRPO":"GRPO (Group Relative Policy Optimization) 是强化学习在大模型训练中的应用\nRLHF (Reinforcement Learning from Human Feedback)：\n预训练：在大规模文本上训练语言模型 奖励模型：人类标注哪个回答更好，训练 reward model RL 微调：用 PPO 等算法优化模型，让它生成高奖励的回答 GRPO vs PPO：\nGRPO 是 PPO 的改进，用相对奖励而不是绝对奖励\nLGRPO=E[log⁡πθ(y∣x)⋅(r(y)−rˉ(x))]\\mathcal{L}_{\\text{GRPO}} = \\mathbb{E}[\\log \\pi_\\theta(y | x) \\cdot (r(y) - \\bar{r}(x))]LGRPO​=E[logπθ​(y∣x)⋅(r(y)−rˉ(x))]其中 $\\bar{r}(x)$ 是同一个 prompt $x$ 下的一组回答的平均奖励。\n好处：减少奖励模型的方差，训练更稳定","alexnet#AlexNet":"在 2012 年，Alex Krizhevsky 等人提出的 AlexNet 以很大优势获得了 ImageNet 比赛的冠军。这一成果极大的激发了产业界对神经网络的兴趣，开创了使用深度神经网络解决图像问题的途径，随后也在这一领域涌现出越来越多的优秀成果。\nAlexNet 与 LeNet 相比，具有更深的网络结构，包含 5 层卷积和 3 层全连接，同时使用了如下三种方法改进模型的训练过程：\n数据增广：深度学习中常用的一种处理方式，通过对训练随机加一些变化，比如平移、缩放、裁剪、旋转、翻转或者增减亮度等，产生一系列跟原始图片相似但又不完全相同的样本，从而扩大训练数据集。通过这种方式，可以随机改变训练样本，避免模型过度依赖于某些属性，能从一定程度上抑制过拟合。\n使用 Dropout 抑制过拟合\n使用 ReLU 激活函数减少梯度消失现象\nAlexNet 的具体结构如图：\n其中有四个模块：\n第一模块：包含了 11×11 步长为 4 的 96 通道卷积以及一个最大池化\n第二模块：包含了 5×5 步的 256 通道卷积以及一个最大池化\n第三模块：包含了两个 3×3 的 384 通道以及一个 3×3 的 256 通道的卷积，后面加一个最大池化\n第四模块：包含了两个 4096 通道输入的全连接层，每个全连接层后面加一个 Dropout 层来抑制过拟合，以及还有最后一个 1000 通道的全连接层","googlenet#GoogLeNet":"GoogLeNet 是 2014 年 ImageNet 比赛的冠军，它的主要特点是网络不仅有深度，还在横向上具有 “宽度”。由于图像信息在空间尺寸上的巨大差异，如何选择合适的卷积核大小来提取特征就显得比较困难了。空间分布范围更广的图像信息适合用较大的卷积核来提取其特征，而空间分布范围较小的图像信息则适合用较小的卷积核来提取其特征。为了解决这个问题，GoogLeNet 提出了一种被称为 Inception 模块的方案。如下图所示：\n其中（a）是 Inception 模块的设计思想，使用 3 个不同大小的卷积核对输入图片进行卷积操作，并附加最大池化，将这 4 个操作的输出沿着通道这一维度进行拼接，构成的输出特征图将会包含经过不同大小的卷积核提取出来的特征。Inception 模块采用**多通路（multi-path）**的设计形式，每个支路使用不同大小的卷积核，最终输出特征图的通道数是每个支路输出通道数的总和，这将会导致输出通道数变得很大，尤其是使用多个 Inception 模块串联操作的时候，模型参数量会变得非常大。\n为了减小参数量，Inception 模块使用了图(b)中的设计方式，在每个 3×3 和 5×5 的卷积层之前，增加 1×1 的卷积层来控制输出通道数；在最大池化层后面增加 1×1 卷积层减小输出通道数。基于这一设计思想，形成了上图(b)中所示的结构。\nGoogLeNet 的架构如下图所示，在主体卷积部分中使用 5 个模块（block），每个模块之间使用步幅为 2 的 3×3 最大池化层来减小输出高宽。\n其中：\n第一模块：使用一个 64 通道的 7×7 卷积层\n第二模块：使用 2 个卷积层：首先是 64 通道的 1×1 卷积层，然后是将通道增大 3 倍的 3×3 卷积层\n第三模块：串联 2 个完整的 Inception 块\n第四模块：串联了 5 个 Inception 块\n第五模块：串联了 2 个 Inception 块\n第五模块的后面紧跟输出层，使用全局平均池化层来将每个通道的高和宽变成 1，最后接上一个输出个数为标签类别数的全连接层。\n并且：在原作者的论文中添加了图中所示的 softmax1 和 softmax2 两个辅助分类器，如下图所示，训练时将三个分类器的损失函数进行加权求和，以缓解梯度消失现象。这里的程序作了简化，没有加入辅助分类器。","lenet-5#LeNet-5":"LeNet 是最早的卷积神经网络之一。1998年，Yan LeCun 第一次将 LeNet 卷积神经网络应用到图像分类上，在手写数字识别任务中取得了巨大成功。LeNet 通过连续使用卷积和池化层的组合提取图像特征，其架构如所图示，这里展示的是作者论文中的 LeNet-5 模型：\n上图就是 LeNet 网络的结构模型，其中包含：\n第一模块：包含 5×5 的 6 通道卷积和 2×2 的池化。卷积提取图像中包含的特征模式（激活函数使用 sigmoid），图像尺寸从 32 减小到 28。经过池化层可以降低输出特征图对空间位置的敏感性，图像尺寸减到 14。\n第二模块：和第一模块尺寸相同，通道数由 6 增加为 16。卷积操作使图像尺寸减小到 10，经过池化后变成 5。\n第三模块：包含 5×5 的 120 通道卷积。卷积之后的图像尺寸减小到 1，但是通道数增加为 120。将经过第 3 次卷积提取到的特征图输入到全连接层。第一个全连接层的输出神经元的个数是 64，第二个全连接层的输出神经元个数是分类标签的类别数，对于手写数字识别其大小是 10。然后使用 Softmax 激活函数即可计算出每个类别的预测概率。\n虽然 LeNet 网络模型对手写数字的识别取得的效果很明显，因为手写数字的输入图片尺寸仅为 28×28，但是当输入图片的尺寸过大时（224×224），它的效果就不尽人意了。","resnet#ResNet":"ResNet 是 2015 年 ImageNet 比赛的冠军，将图像分类识别错误率降低到了 3.6%，这个结果甚至超出了正常人眼识别的精度。\n通过前面几个经典模型学习，我们可以发现随着深度学习的不断发展，模型的层数越来越多，网络结构也越来越复杂。那么是否加深网络结构，就一定会得到更好的效果呢？从理论上来说，假设新增加的层都是恒等映射，只要原有的层学出跟原模型一样的参数，那么深模型结构就能达到原模型结构的效果。换句话说，原模型的解只是新模型的解的子空间，在新模型解的空间里应该能找到比原模型解对应的子空间更好的结果。但是实践表明，增加网络的层数之后，训练误差往往不降反升。\nKaiming He 等人提出了残差网络 ResNet 来解决上述问题，其基本思想如下图所示。\n(a)：表示增加网络的时候，将 $x$ 映射成 $y$ 输出。\ny=F(x)y = F(x)y=F(x)(b)：对(a)作了改进，输出：\ny=F(x)+xy = F(x) + xy=F(x)+x这时不是直接学习输出特征 $y$ 的表示，而是学习残差 $F(x) = y - x$。如果想学习出原模型的表示，只需将 $F(x)$ 的参数全部设置为 0，则 $y = x$ 是恒等映射。$F(x) = y - x$ 也叫做残差项，如果 $x \\to y$ 的映射接近恒等映射，(b)中通过学习残差项也比(a)学习完整映射形式更加容易。\n(b)的结构是残差网络的基础，这种结构也叫做残差块（residual block）。输入 $x$ 通过跨层连接（skip connection），能更快的向前传播数据，或者向后传播梯度。\n下图表示出了 ResNet-50 的结构，一共包含 49 层卷积和 1 层全连接，所以被称为 ResNet-50。","vgg#VGG":"VGG 是当前最流行的 CNN 模型之一，2014 年由 Simonyan 和 Zisserman 提出，其命名来源于论文作者所在的实验室 Visual Geometry Group。AlexNet 模型通过构造多层网络，取得了较好的效果，但是并没有给出深度神经网络设计的方向。VGG 通过使用一系列大小为 3×3 的小尺寸卷积核和 pooling 层构造深度卷积神经网络，并取得了较好的效果。VGG 模型因为结构简单、应用性极强而广受研究者欢迎，尤其是它的网络结构设计方法，为构建深度神经网络提供了方向。\n下图是 VGG-16 的网络结构示意图，有 13 层卷积和 3 层全连接层。VGG 网络的设计严格使用 3×3 的卷积层和池化层来提取特征，并在网络的最后面使用三层全连接层，将最后一层全连接层的输出作为分类的预测。在 VGG 中每层卷积将使用 ReLU 作为激活函数，在全连接层之后添加 dropout 来抑制过拟合。\n使用小的卷积核能够有效地减少参数的个数，使得训练和测试变得更加有效。比如使用两层 3×3 卷积层，可以得到感受野为 5 的特征图，而比使用 5×5 的卷积层需要更少的参数。由于卷积核比较小，可以堆叠更多的卷积层，加深网络的深度，这对于图像分类任务来说是有利的。VGG 模型的成功证明了增加网络的深度，可以更好的学习图像中的特征模式。","深度学习基础#深度学习基础":"深度学习基础","第1章-数学基础#第1章 数学基础":"别担心，这部分不会特别硬核，只讲神经网络里真正会用到的东西。","第2章-神经网络的结构#第2章 神经网络的结构":"推荐视频：3Blue1Brown - 神经网络的结构","第3章-梯度下降与反向传播#第3章 梯度下降与反向传播":"","第4章-现代的网络结构#第4章 现代的网络结构":"","第5章-现代视觉领域#第5章 现代视觉领域":"","第6章-生成模型#第6章 生成模型":"","第7章-大模型语言视觉#第7章 大模型（语言\u0026amp;视觉）":""},"title":"第1章 Basic Concepts"},"/docs/self-study/ai/dl/transformer/":{"data":{"":"","1-一切的起点为什么要发明attention#1. 一切的起点：为什么要发明Attention？":"在Transformer诞生之前，处理序列数据（比如一句话）的主流模型是RNN（循环神经网络）。","2-数学与代码实现#2. 数学与代码实现":"让我们来看一个最简单的RNN单元在 t 时刻是如何工作的。","3-attention的革命性思想#3. Attention的革命性思想":"打破这种\"链式\"处理！ 让序列中的任何一个词，都能**直接\"看到\"**并连接到序列中的任何其他词，无论它们相距多远。它把信息处理从\"单核单线程\"变成了\"多核并行\"。","31-核心思想从逐步传递到一步直达#3.1 核心思想：从\u0026quot;逐步传递\u0026quot;到\u0026quot;一步直达\u0026quot;":"Attention说：“我不要再玩传话游戏了！在序列的任何一个位置，我都要能直接看到所有其他位置的信息！”","32-数学与代码实现-self-attention#3.2 数学与代码实现 (Self-Attention)":"我们直接来看Transformer里的自注意力机制，它是如何用Q, K, V实现\"一步直达\"的。","4-解决顺序问题位置编码-positional-encoding#4. 解决顺序问题：位置编码 (Positional Encoding)":"刚才的QKV流程有一个大问题：它完全忽略了词的顺序！在他看来，“猫追老鼠\"和\"老鼠追猫\"是一样的，因为词的集合没变。这显然不行。为了让模型理解顺序，我们需要引入位置信息。","41-经典位置编码-absolute-positional-encoding#4.1 经典位置编码 (Absolute Positional Encoding)":"思想： 给每个位置（第1个、第2个、第3个…）创建一个独一无二的、固定不变的\"位置向量”。\n做法： 将这个\"位置向量\"直接加到对应位置的\"词向量\"上。\n比喻： 就像给每个进教室的学生发一个固定的\"座位号\"，学生A + 1号座位 和 学生A + 5号座位 对于模型来说是完全不同的输入。\n我们反复强调，自注意力机制本身是无序的，它就像一个\"装着词的袋子\"。为了让模型理解单词的顺序，我们必须手动将位置信息注入到模型中。","42-sin-和-cos-公式的数学与代码实现#4.2 sin 和 cos 公式的数学与代码实现":"原始 Transformer 论文中使用了一种非常巧妙的 sin 和 cos 函数组合来创建固定的位置编码。","5-完整的transformer架构---不仅仅是attention#5. 完整的Transformer架构 - 不仅仅是Attention":"Attention虽然是灵魂，但一个完整的Transformer模型还需要一些重要的\"辅助组件\"。我们以Encoder（编码器）为例，看看一个标准的模块长什么样。","51-encoder-block编码器模块的构成#5.1 Encoder Block（编码器模块）的构成":"它就像一个乐高积木，可以重复堆叠。每个积木块包含两个核心部分。","52-完整的encoder-decoder架构#5.2 完整的Encoder-Decoder架构":"原始的Transformer是为机器翻译（Seq2Seq任务）设计的，所以它包含一个编码器(Encoder)和一个解码器(Decoder)。\n比喻： 一位精通双语的翻译官","6-为什么需要掩码-mask#6. 为什么需要\u0026quot;掩码\u0026quot; (Mask)？":"我们先回顾一下场景：这个机制专门用在 Transformer 的 Decoder 部分。\nDecoder 的任务是生成，比如从英文翻译成法文。它是一个词一个词地往外\"吐\"的，这个过程叫自回归 (Autoregressive)。","61-问题分析#6.1 问题分析":"问题： 假设我们正在翻译 “I love cats” -\u003e “J’aime les chats”。当 Decoder 准备生成第3个词 “les” 的时候，它只能看到它已经生成的前两个词 “J’aime”。它绝对不能看到答案 “les” 或者更后面的 “chats”。如果看到了，那就是\"作弊\"，模型就学不到真正的预测能力了。\n挑战： 但是，我们之前学的自注意力是全局的 (All-to-All)！它会让 “J’aime”、“les”、“chats” 互相看到对方。\n解决方案： 我们需要一个掩码 (Mask)，像一个\"遮挡板\"，在计算注意力分数时，强行把\"未来\"的信息给遮住。","62-掩码的数学与代码实现#6.2 \u0026ldquo;掩码\u0026quot;的数学与代码实现":"这个\"遮挡\"操作非常巧妙，它发生在计算完分数矩阵 $QK^T$ 之后，但在进行 Softmax 之前。","decoder-翻译和生成阶段#Decoder (翻译和生成阶段)":"它也由一堆Decoder Block堆叠而成 它的任务是一个词一个词地生成目标语言的句子（比如法文：“J’aime les chats”） Decoder的核心区别： 它内部有两个注意力层！\n第一个是\"带掩码的\"多头自注意力 (Masked MHA)： Decoder在生成第3个词时，只能看到第1、2个词，**不能\"偷看\"**后面的答案。这个\"掩码\"就是用来遮挡未来信息的。\n第二个是\"交叉注意力\" (Cross-Attention)： 这是连接Encoder和Decoder的桥梁！在这一层，Q来自于Decoder自身（它想知道下一步该生成什么），而 K和V来自于Encoder的最终输出（即Encoder对源句子的深度理解）。\n这完美地模拟了翻译过程：Decoder一边思考自己已经说出口的法文，一边反复回顾原文（英文）的含义，来决定下一个最合适的法文单词。\n最后一步： Decoder的最终输出会经过一个线性层和Softmax，来预测词典中每个词成为下一个词的概率。","encoder-阅读和理解阶段#Encoder (阅读和理解阶段)":"它由一堆Encoder Block堆叠而成（比如6个） 它的任务是完整地阅读源语言句子（比如英文：“I love cats”） 经过层层处理，它最终输出一套富含上下文信息的特征向量。这份输出可以被看作是Encoder对整个句子的深度理解","rnn的工作方式#RNN的工作方式":"RNN像一个\"单核处理器\"，一个词一个词地读。读完第一个词，把信息压缩一下传给第二个词；读完第二个词，再把包含前两个词的信息压缩一下传给第三个词……","rnn的致命弱点#RNN的致命弱点":"这就像一个**“传话游戏”**。信息每传递一步，就会有一些损失和遗忘。当句子很长时，传到最后，开头的信息可能已经面目全非了。这导致它很难捕捉长距离依赖关系。\n举例说明： 比如这句话：“我在法国长大，……（中间省略50个词）……，所以我能说一口流利的法语。”\nRNN很难把最后的\"法语\"和开头的\"法国\"紧密联系起来。\nRNN的核心是**“循环”**，它试图通过一个循环传递的隐藏状态 (hidden state, h) 来模拟人类的\"短期记忆\"。","代码实现与可视化#代码实现与可视化":"我们来创建一个标准的位置编码模块，并把它可视化，这是理解它最好的方式！\nimport torch import torch.nn as nn import math import matplotlib.pyplot as plt class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() # 创建一个足够长的位置编码矩阵 pe = torch.zeros(max_len, d_model) # 位置索引 (0, 1, ..., max_len-1) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # 维度索引的除数项 (10000^(2i/d_model)) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 偶数维度用 sin pe[:, 0::2] = torch.sin(position * div_term) # 奇数维度用 cos pe[:, 1::2] = torch.cos(position * div_term) # 将 pe 注册为 buffer，它不是模型的参数，但会随模型移动(e.g., to(device)) self.register_buffer('pe', pe) def forward(self, x): # x: (batch_size, seq_len, d_model) # 将位置编码加到输入词向量上 # self.pe 是 (max_len, d_model)，我们只需要取前 seq_len 个 # unsqueeze(0) 是为了匹配 batch 维度 return x + self.pe[:x.size(1), :].unsqueeze(0) # --- 可视化 --- plt.figure(figsize=(10, 5)) pe_module = PositionalEncoding(d_model=128, max_len=100) pe_matrix = pe_module.pe.squeeze(0).numpy() # 获取 (100, 128) 的矩阵 plt.pcolormesh(pe_matrix, cmap='viridis') plt.xlabel('Embedding Dimension (i)') plt.ylabel('Position (pos)') plt.colorbar() plt.title(\"Positional Encoding Matrix Visualization\") plt.show() # --- 使用示例 --- # 创建一个 dummy 输入 dummy_word_embeddings = torch.randn(2, 10, 128) # (batch=2, seq_len=10, d_model=128) # 添加位置编码 embeddings_with_pos = pe_module(dummy_word_embeddings) print(\"\\n--- 位置编码使用示例 ---\") print(\"原始词向量形状:\", dummy_word_embeddings.shape) print(\"加入位置编码后形状:\", embeddings_with_pos.shape) 可视化结果解读： 你会看到一张彩色的图，这张图完美地揭示了位置编码的奥秘：\nY轴是位置，从上到下代表序列的开头到结尾。\nX轴是维度，从左到右代表128维向量的每一维。\n你会清晰地看到平滑变化的正弦/余弦波。左侧（低维）的波纹很宽，右侧（高维）的波纹非常密集。正是这种独特的、跨越不同频率的“模式”，让Transformer能够精确地“感知”到每个词的绝对和相对位置。 旋转位置编码 (RoPE - Rotary Positional Embedding)\n这是现在大模型（如Llama）更青睐的方式，它编码的是相对位置。\n思想：不直接给一个固定的“座位号”，而是给每个词的向量赋予一个随位置变化的“旋转角度”。\n比喻：想象每个词向量都是一个罗盘指针。\n第1个词的指针不旋转。\n第2个词的指针旋转15°。\n第3个词的指针旋转30°…\n好处：当计算两个词（比如第2个和第5个）的相关性时，模型关注的是它们指针之间的夹角（比如 75° - 15° = 60°）。这个相对夹角是固定的，无论这两个词出现在句子的开头还是结尾。这使得模型能更好地泛化到不同长度的句子。\n机制升级：多头注意力 (Multi-Head Attention) 一个注意力机制（一个“头”）可能只能学习到一种关系模式（比如语法关系）。但一句话里可能包含多种关系（语法关系、语义关系、指代关系等）。怎么办？ 思想：大力出奇迹！一次不行，就多来几次。\n比喻：你不是只派一个图书管理员（一个头）去查资料，而是派了一个专家团队。\nHead 1 (语法专家)：他的QKV被训练得专门去寻找主谓宾关系。\nHead 2 (语义专家)：他的QKV专门去寻找同义词、反义词。\nHead 3 (上下文专家)：他的QKV专门去寻找长距离的因果、指代关系。\n做法：\n把原始的词向量切分成N份（比如8个头，就切成8份）。\n每一份都独立地进行一次完整的QKV注意力计算。\n得到N个各自的输出结果。\n将这N个结果拼接起来，再通过一个线性层进行信息融合。\n好处：让模型能够并行地从不同的“子空间”中学习到多种多样的信息，极大地丰富了模型的表达能力。\n为什么一个“头”不够用？ 单个自注意力机制，就像只有一个关注点的人。在阅读一句话时，他可能只学会了关注语法结构（比如动词和主语的关系），但可能会忽略语义上的关联（比如“苹果”和“水果”的关系）。 我们希望模型能更“聪明”，能同时捕捉到多种不同类型的关系。\n核心思想：专家团队并行工作 多头注意力的思想非常暴力且有效：“既然一个头不够，那就同时上八个头！” 比喻：还是那个图书馆查资料的例子。你不再只派一个图书管理员去帮你查，而是派了一个专家委员会，比如一个8人的团队。\nHead 1 (语法专家)：他只负责寻找句子成分关系。\nHead 2 (语义专家)：他只负责寻找近义词、反义词关系。\nHead 3 (指代专家)：他只负责寻找 “it”, “he”, “they” 到底指向谁。\n…\nHead 8 (长距离主题专家)：他负责寻找文章的整体主题关联。\n这8位专家同时出发，并行工作，每个人都独立地进行自己的 Q, K, V 计算，最后带回一份自己的“信息摘要”。\n数学与代码实现 输入: 词向量序列 X∈R N×d_model\n头的数量 h (比如 h=8)\n计算:\n维度划分：首先，我们将模型的总维度 d_model 划分给每个头。例如，d_model=512，有 h=8 个头，那么每个头分到的维度就是 d_k=d_v=d_model/h=512/8=64。\n独立投影：为每一个头 i (i=1,…,h) 都创建一套独立的、可学习的权重矩阵：$W_Q^i, W_K^i, W_V^i$。然后，用它们来生成每个头专属的 Q, K, V。\nQi=XWQi,Ki=XWKi,Vi=XWViQ_i = X W_Q^i, \\quad K_i = X W_K^i, \\quad V_i = X W_V^iQi​=XWQi​,Ki​=XWKi​,Vi​=XWVi​并行计算注意力：每个头都独立地执行我们上节课学的注意力计算。\nheadi=Attention(Qi,Ki,Vi)=softmax(QiKiTdk)Vi\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_iheadi​=Attention(Qi​,Ki​,Vi​)=softmax(dk​​Qi​KiT​​)Vi​拼接与融合：将所有头得到的输出 head_i 拼接起来。\nConcat(head1,head2,…,headh)∈RN×(h×dv)=N×dmodel\\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) \\in \\mathbb{R}^{N \\times (h \\times d_v) = N \\times d_{\\text{model}}}Concat(head1​,head2​,…,headh​)∈RN×(h×dv​)=N×dmodel​最后，将这个拼接后的大矩阵再通过一个最终的线性层 $W_O$ 进行一次信息融合，得到多头注意力的最终输出。\nMultiHead(Q,K,V)=Concat(head1,…,headh)WO\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_OMultiHead(Q,K,V)=Concat(head1​,…,headh​)WO​PyTorch代码示例 (简化版):\nimport torch import torch.nn as nn import torch.nn.functional as F import math class MultiHeadAttention(nn.Module): def __init__(self, d_model=512, num_heads=8): super().__init__() assert d_model % num_heads == 0 self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads # 将所有头的 W_q, W_k, W_v 矩阵合并成一个大的线性层，更高效 self.W_qkv = nn.Linear(d_model, d_model * 3) self.W_o = nn.Linear(d_model, d_model) def forward(self, x): # x: (batch, seq_len, d_model) batch_size, seq_len, _ = x.shape # 1. 独立投影 + 维度划分 # (batch, seq_len, d_model * 3) -\u003e 3 * (batch, seq_len, d_model) qkv = self.W_qkv(x).chunk(3, dim=-1) # 3 * (batch, seq_len, num_heads, d_k) -\u003e 3 * (batch, num_heads, seq_len, d_k) q, k, v = [val.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) for val in qkv] # 2 \u0026 3. 并行计算注意力 (用一个函数封装) scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k) attention_weights = F.softmax(scores, dim=-1) context = torch.matmul(attention_weights, v) # 4. 拼接与融合 # (batch, num_heads, seq_len, d_k) -\u003e (batch, seq_len, num_heads, d_k) -\u003e (batch, seq_len, d_model) context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model) output = self.W_o(context) return output # --- 测试 --- mha = MultiHeadAttention() input_tensor = torch.randn(2, 10, 512) # (batch=2, seq_len=10, d_model=512) output_tensor = mha(input_tensor) print(f\"多头注意力输出形状: {output_tensor.shape}\") # 应为 (2, 10, 512)","代码实现与详解#代码实现与详解":"我们来手动构建这个过程，看看张量在每一步的变化。\nimport torch import torch.nn.functional as F import math # 假设序列长度为 4 seq_len = 4 # --- 1. 创建一个\"未来\"信息的遮挡板 (Mask) --- # torch.triu 创建一个上三角矩阵, diagonal=1 表示不包括对角线 # 值为1的地方，就是我们需要遮挡的\"未来\"位置 mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() print(\"--- Step 1: 创建遮挡 Mask ---\") print(\"Mask (True=遮挡):\") print(mask) # tensor([[False, True, True, True], # [False, False, True, True], # [False, False, False, True], # [False, False, False, False]]) # --- 2. 模拟一个注意力分数矩阵 (Q·K^T 的结果) --- # 假设这是一个已经计算好的分数 scores = torch.randn(seq_len, seq_len) print(\"\\n--- Step 2: 原始注意力分数 (Softmax之前) ---\") print(scores) # --- 3. 应用 Mask --- # 将 mask 中为 True 的位置，在 scores 中用 -infinity 填充 scores.masked_fill_(mask, float('-inf')) print(\"\\n--- Step 3: 应用 Mask 后的分数 ---\") print(\"注意右上角都被替换成了 -inf\") print(scores) # --- 4. 进行 Softmax --- attention_weights = F.softmax(scores, dim=-1) print(\"\\n--- Step 4: Softmax 后的最终注意力权重 ---\") print(\"注意右上角权重都变成了 0.0\") print(attention_weights.round(decimals=2))","代码输出分析关键#代码输出分析（关键！）":"你会看到，最终的 attention_weights 矩阵：\n第1行： 只有第1个位置有权重，它只能关注自己 第2行： 只有前2个位置有权重，它只能关注自己和第1个词 第3行： 只有前3个位置有权重，它能关注自己和前2个词","数学公式#数学公式":"ht=tanh⁡(Whhht−1+Wxhxt+bh)h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)ht​=tanh(Whh​ht−1​+Wxh​xt​+bh​)yt=Whyht+byy_t = W_{hy} h_t + b_yyt​=Why​ht​+by​其中：\n$W_{hh}, W_{xh}, W_{hy}$ 是权重矩阵，是模型需要学习的参数。它们在所有时间步都是共享的 $b_h, b_y$ 是偏置项 $\\tanh$ 是一个将数值压缩到 (-1, 1) 区间的激活函数","数学公式-1#数学公式":"对于位置为 pos、维度为 i 的编码，其计算方式如下：\nPE(pos,2i)=sin⁡(pos100002i/dmodel)PE_{(pos,2i)} = \\sin(\\frac{pos}{10000^{2i/d_{model}}})PE(pos,2i)​=sin(100002i/dmodel​pos​)PE(pos,2i+1)=cos⁡(pos100002i/dmodel)PE_{(pos,2i+1)} = \\cos(\\frac{pos}{10000^{2i/d_{model}}})PE(pos,2i+1)​=cos(100002i/dmodel​pos​)其中：\npos: 词在序列中的位置 (0, 1, 2, …) i: 向量的维度索引 (0, 1, 2, …) d_model: 词向量的总维度","数学原理#数学原理":"Softmax 函数是 $e^x$ 的归一化。指数函数 $e^x$ 有一个特点：当 x 是一个非常大的负数（比如 $-\\infty$）时，$e^x$ 会无限趋近于 0。\n所以，我们只需要把所有想\"遮挡\"的位置的注意力分数，都设置为一个非常大的负数，那么经过 Softmax 之后，这些位置的注意力权重就自然变成 0 了。","梯度消失问题#梯度消失问题":"这个链式结构带来了严重的问题。在训练时，误差需要沿着这条链反向传播回来，以更新权重。\n数学推导（直观理解）： 假设我们要计算损失 $L$ 对最早的隐藏状态 $h_1$ 的梯度 $\\frac{\\partial L}{\\partial h_1}$，根据链式法则，它看起来是这样的：\n∂L∂h1=∂L∂hT∂hT∂hT−1∂hT−2∂hT−1⋯∂h1∂h2\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-2}}{\\partial h_{T-1}} \\cdots \\frac{\\partial h_1}{\\partial h_2}∂h1​∂L​=∂hT​∂L​∂hT−1​∂hT​​∂hT−1​∂hT−2​​⋯∂h2​∂h1​​注意到其中反复出现的项 $\\frac{\\partial h_t}{\\partial h_{t-1}}$ 吗？它约等于权重矩阵 $W_{hh}$。这意味着，为了把误差从序列末尾传到开头，你需要将 $W_{hh}$ 连乘很多次。\n如果 $W_{hh}$ 中的值（严格来说是它的范数）普遍大于1，连乘后梯度会变得超级大，导致梯度爆炸 (Gradient Exploding) 如果 $W_{hh}$ 中的值普遍小于1，连乘后梯度会趋近于0，导致梯度消失 (Gradient Vanishing) 梯度消失是更常见也更致命的。它意味着模型几乎无法从很久之前的输入中学到任何东西，也就是我们说的\"遗忘\"。","直观理解#直观理解":"这个公式的巧妙之处在于，它利用了不同频率的 sin/cos 波：\n在低维部分（i 较小），波长很长，编码值变化很慢 在高维部分（i 较大），波长很短，编码值变化很快 这种组合使得每个位置都有一个独一无二的编码向量，并且模型可以很容易地学习到相对位置关系。","第一部分多头自注意力层-multi-head-self-attention#第一部分：多头自注意力层 (Multi-Head Self-Attention)":"就是我们刚刚讲的，负责让输入序列内部的词元互相\"交流\"，捕捉上下文关系。","第二部分前馈神经网络-feed-forward-network-ffn#第二部分：前馈神经网络 (Feed-Forward Network, FFN)":"这是一个非常简单的两层全连接网络，被独立地作用于序列中的每一个词元上。\n作用： 它为模型提供了非线性变换的能力。如果说注意力层负责\"信息交互与融合\"，那么FFN层就负责对融合后的信息进行\"深度加工和提炼\"。","计算#计算":"更新隐藏状态：将\"当前输入\"和\"过去记忆\"结合起来，通过一个激活函数（通常是 tanh）来生成新的记忆 生成输出：根据新的记忆，生成当前时刻的输出","计算-1#计算":"生成Q, K, V： 用三个不同的、可学习的权重矩阵 $W_Q, W_K, W_V$ 将输入序列 $X$ 投影，得到Q, K, V。\nQ=XWQQ = XW_QQ=XWQ​ K=XWKK = XW_KK=XWK​ V=XWVV = XW_VV=XWV​计算注意力分数：\nScores=QKTScores = QK^TScores=QKT这里 $Q \\in \\mathbb{R}^{N \\times d_k}$，$K^T \\in \\mathbb{R}^{d_k \\times N}$，所以结果 $Scores \\in \\mathbb{R}^{N \\times N}$。这是一个注意力矩阵，它的第 (i, j) 个元素表示第 i 个词对第 j 个词的关注程度。\n缩放、归一化、加权求和：\nAttention(Q,K,V)=softmax(QKTdk)VAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})VAttention(Q,K,V)=softmax(dk​​QKT​)V 除以 $\\sqrt{d_k}$ 是为了缩放，防止点积结果过大导致softmax梯度消失 softmax 对分数的每一行进行操作，确保每一行的权重总和为1 最后乘以 V，将所有词的Value根据权重加权求和 注意力机制的核心：Q, K, V的图书馆比喻 Attention的计算过程听起来很复杂，但它的本质思想非常直观。我们可以用一个“去图书馆查资料”的比喻来理解 Query (Q), Key (K), Value (V) 这三个关键角色。 想象一下，你要写一篇关于“人工智能在医疗领域的应用”的论文。\nQuery (Q) - 你的查询意图 你脑子里的研究主题就是Query。它代表了：“我当前关心的是什么？我需要什么样的信息？”\nKey (K) - 书架上每本书的“关键词标签” 图书馆里成千上万的书，你不可能一本一本地翻。每本书都有一个书名或关键词标签，这就是Key。它高度概括了这本书是关于什么的，用来和你脑中的Query做匹配。\nValue (V) - 书本里真正的“知识内容” 关键词标签只是索引，书里面详实的内容才是你最终想要的知识，这就是Value。Key和Value是一一对应的，同一本书，它的标签是Key，内容是Value。\n注意力计算的三部曲：\n第一步：计算相关性分数 (Score) 你拿着你的Query（研究主题），去和书架上每一本书的Key（关键词标签）进行比对。比对的方式在Transformer里就是点积 (dot product)。匹配度越高，分数就越高。\nScore = Query · Key\n第二步：分数归一化 (Softmax) 你得到了一堆相关性分数，但这些分数大小不一，不方便使用。于是你用 Softmax 函数将它们转换成一个总和为1的权重（或叫“注意力分布”）。\nWeights = Softmax(Scores)\n结果可能是：《AI与影像诊断》这本书分配到70%的注意力。\n《机器学习入门》分配到20%的注意力。\n《古代史》分配到0.01%的注意力。\n第三步：加权求和 (Weighted Sum) 你不会只看一本书。你会根据刚刚得到的注意力权重，去“借阅”所有书的内容（Value），然后组合成一份定制的“信息摘要”。\nOutput = Weights × Value\n最终输出 = 70%的《AI与影像诊断》内容 + 20%的《机器学习入门》内容 + 0.01%的《古代史》内容…\n这样，输出结果就高度集中了与你Query最相关的信息。\n在Transformer的自注意力 (Self-Attention) 中，Q, K, V都来源于同一个输入序列。也就是说，序列中的每一个词，都既是\"查询者\"，又是\"被查询者\"。\nimport torch.nn.functional as F import math # --- 参数定义 --- seq_length = 5 # 序列长度 d_model = 32 # 词向量维度 d_k = 8 # Q, K 的维度 (通常 d_k = d_model / num_heads) # --- 模拟输入数据 --- # (批次大小, 序列长度, 词向量维度) X = torch.randn(1, seq_length, d_model) # --- 模拟线性投影层 --- W_q = nn.Linear(d_model, d_k) W_k = nn.Linear(d_model, d_k) W_v = nn.Linear(d_model, d_k) # V的维度可以不同，这里为了简单设为一样 # 1. 生成Q, K, V Q = W_q(X) # (1, 5, 8) K = W_k(X) # (1, 5, 8) V = W_v(X) # (1, 5, 8) print(\"--- Self-Attention 手动计算过程 ---\") # 2. 计算分数 Q·K^T # K.transpose(-2, -1) 将 (1, 5, 8) 变为 (1, 8, 5) scores = torch.matmul(Q, K.transpose(-2, -1)) # (1, 5, 5) print(f\"注意力分数矩阵形状: {scores.shape}\") # 3. 缩放 scores = scores / math.sqrt(d_k) # 4. Softmax归一化 attention_weights = F.softmax(scores, dim=-1) print(f\"注意力权重矩阵 (第一行加总应为1): \\n{attention_weights[0]}\") print(f\"第一行权重加总: {attention_weights[0][0].sum()}\") # 5. 加权求和 output = torch.matmul(attention_weights, V) # (1, 5, 5) x (1, 5, 8) -\u003e (1, 5, 8) print(f\"最终输出形状: {output.shape}\")","输入#输入":"当前时刻的输入向量：$x_t$ (比如一个词的词向量) 上一时刻的隐藏状态：$h_{t-1}$ (来自过去的\"记忆\")","输入-1#输入":"一个完整的词向量序列 $X \\in \\mathbb{R}^{N \\times d}$ (N是序列长度, d是词向量维度)","重要的辅助组件残差连接-add-和层归一化-norm#重要的辅助组件：残差连接 (Add) 和层归一化 (Norm)":"你会发现，在Attention层和FFN层之后，都跟着一个 Add \u0026 Norm 操作。\nAdd (残差连接)： 就是把该层的输入 X 直接加到该层的输出 Y 上，即 X+Y。\n作用： 这是一个\"高速公路\"，允许信息和梯度直接跳过某些层向前或向后流动。这极大地缓解了深度网络中的梯度消失问题，让我们可以把模型做得非常深。 Norm (层归一化)： 对每个样本的特征进行归一化，使其均值为0，方差为1。\n作用： 让模型的训练过程更稳定、更快速。"},"title":"第2章 Transformer"},"/docs/self-study/ai/gat/":{"data":{"":"","1-新的世界图-graph---万物皆可连#1. 新的世界：图 (Graph) - 万物皆可连":"到目前为止，我们处理的都是序列 (Sequence)，就像一串珍珠，有明确的前后顺序。但现实世界中，更多的数据结构长得像一张网，而不是一条线。","2-为什么不能把大力的transformer直接出奇迹#2. 为什么不能把\u0026quot;大力\u0026quot;的Transformer直接\u0026quot;出奇迹\u0026quot;？":"我们强大的Transformer，能让序列中任何两个词直接对话。那么，我们能不能把图里的所有节点粗暴地\"拉直\"成一个1D序列，然后直接扔给Transformer呢？\n答案是：绝对不行！ 这会引发两个灾难性的问题。","21-灾难一计算的维度爆炸#2.1 灾难一：计算的\u0026quot;维度爆炸\u0026quot;":"Transformer的本质： 是全局注意力 (All-to-All)，它的计算复杂度是 $O(N^2)$，其中N是序列长度 图的尺度： 对于一个包含100个单词的句子，计算 $100^2 = 10000$ 次注意力还可以接受。但对于一个真实的图，节点数可以轻易达到数万、数百万！ 具体例子： 一个中等大小的社交网络可能有10万用户。N=100,000，那么 $N^2 = 100$亿！这会让任何现代GPU都瞬间崩溃 全局注意力在图的世界里，计算成本高到无法承受。","22-灾难二最宝贵信息的丢失#2.2 灾难二：最宝贵信息的丢失":"这是更致命的问题。图的灵魂在于它的结构。\n“邻居\"的概念： 在图中，“节点A和节点B是邻居\"是一个至关重要的信息 拉直操作的破坏性： 当你把一个图（比如上面的分子结构）强行拉成一个1D序列时，原本的邻居关系就被彻底打乱了 [碳原子1, 碳原子2, 氧原子, 氢原子1, ...] 在这个序列里，模型无法知道\"碳原子1\"的直接邻居是\"碳原子2\"和\"氧原子”，而不是\"氢原子1”。它失去了所有的局部结构信息。把图拉直，就像把一张藏宝图剪成碎片再随机排成一行，地图本身已经失去了意义。","3-gat的破局点从全局关注到邻里聚焦#3. GAT的破局点：从\u0026quot;全局关注\u0026quot;到\u0026quot;邻里聚焦\u0026quot;":"面对这两个灾难，图注意力网络 (GAT) 提出了一个极其优雅且直观的解决方案：\n既然全局关注不可行，那我们就只关注重要的！在图里，谁最重要？当然是我的邻居！","31-gat的核心思想#3.1 GAT的核心思想":"注意力计算不再是\"All-to-All\"，而是被图的结构所约束，只发生在有边直接连接的节点之间。","32-两大优势#3.2 两大优势":"","4-edge_indexgat的注意力计算指南#4. edge_index：GAT的\u0026quot;注意力计算指南\u0026quot;":"现在我们解决了\"应该在哪计算注意力\"的问题，但还有一个技术问题：我们如何告诉计算机这个\"计算模板\"呢？\n这就是 edge_index 发挥作用的地方。你问过，“图就算用邻接矩阵都是2D的啊”，这个观察非常敏锐！但邻接矩阵对于稀疏图来说空间效率很低。edge_index 是一种更高效的稀疏表示。","41-edge_index-是什么#4.1 edge_index 是什么？":"它不是一个2D的\"图像\"矩阵，而是一个形状为 [2, num_edges] 的张量。你可以把它理解成一个**“连接清单”**。\n第0行： 所有边的源节点 (source) 索引 第1行： 所有边的目标节点 (target) 索引","42-举个例子#4.2 举个例子":"对于一个图：0 – 1, 1 – 2, 0 – 2 它的 edge_index 就是：\ntensor([[0, 1, 0], # 源节点 [1, 2, 2]]) # 目标节点 (假设边是有向的) [0, 1] 这一列代表一条从节点0指向节点1的边 [1, 2] 这一列代表一条从节点1指向节点2的边 [0, 2] 这一列代表一条从节点0指向节点2的边 GAT就是拿着这份\"连接清单\"，精确地、只为清单上列出的这些节点对计算注意力。edge_index 成为了注意力机制的\"导航地图\"，完美地将图的结构信息注入到了计算流程中。","5-场景设定论文引用网络中的节点分类#5. 场景设定：论文引用网络中的节点分类":"领域交叉： 我们现在是计算机科学家，同时也是图书情报专家。","51-任务描述#5.1 任务描述":"我们有一个包含7篇学术论文的引用网络。每篇论文本身有一个主题（比如\"机器学习\"、“生物学”、“物理学”），这是我们需要模型预测的标签。","52-图的构建#5.2 图的构建":"节点 (Node)： 一篇论文 边 (Edge)： 如果论文A引用了论文B，就有一条从A指向B的边 (A -\u003e B) 节点特征 (Node Feature)： 为了简单起见，我们假设通过论文摘要的关键词提取，为每篇论文生成了一个2维的特征向量","53-gat要解决的问题#5.3 GAT要解决的问题":"一篇论文的学术影响力，不应该由所有引用它的论文平均决定。一篇来自同领域顶级会议的引用，其\"价值\"远高于一篇来自不相关领域的跨界引用。我们希望模型能自动学会，为不同重要性的邻居（引用/被引论文）分配不同的注意力权重。","54-具体实例一个7节点的引用网络#5.4 具体实例：一个7节点的引用网络":"节点 (7篇论文)： P0, P1, P2, P3, P4, P5, P6\n真实类别：\n机器学习 (ML): P0, P1, P2 生物学 (Bio): P3, P4 物理学 (Phy): P5, P6 引用关系 (边)： 如上图所示。注意 P0 是一篇\"明星论文\"，被多篇其他论文引用。\n初始特征 (2维)：\nP0:[1, 1], P1:[1, 2], P2:[2, 1], P3:[8, 7], P4:[7, 8], P5:[1, 8], P6:[2, 7] (我们故意让同类别的论文特征在空间上更接近)","6-数学公式详解以更新节点p0为例#6. 数学公式详解：以更新节点P0为例":"我们的目标是利用其邻居 P1, P2, P3, P5 的信息，来更新 P0 的特征。","61-第零步初始特征变换#6.1 第零步：初始特征变换":"首先，所有节点的特征都会经过一个共享的线性变换（乘以权重矩阵W），将它们投影到一个更高维的特征空间。假设W将2维特征变为4维。\n公式： $h’_i = Wh_i$\n示例： P0的新特征 $h’_{P0} = W \\cdot [1,1]^T$","62-第一步计算注意力系数-e_ij#6.2 第一步：计算注意力系数 $e_{ij}$":"现在，P0要依次\"评估\"它与每个邻居的相关性。我们以邻居P1为例。\n公式： $e_{P0,P1} = LeakyReLU(a^T [h’{P0} || h’{P1}])$\n解释：\n$[h’{P0} || h’{P1}]$: 将P0和P1的4维变换后特征拼接成一个8维向量 $a^T$: 用一个可学习的注意力向量 $a$ (8维) 与这个拼接向量做点积，得到一个标量分数 $LeakyReLU$: 对这个分数进行非线性激活 结果： P0会得到4个分数：$e_{P0,P1}, e_{P0,P2}, e_{P0,P3}, e_{P0,P5}$","63-第二步邻域softmax归一化#6.3 第二步：邻域Softmax归一化":"这是GAT的灵魂！Softmax只在P0的邻居集合 P1, P2, P3, P5 上进行。\n公式: α_P0,P1= exp(e_P0,P1)+exp(e_P0,P2)+exp(e_P0,P3)+exp(e_P0,P5) exp(e_P0,P1) ​\n结果: 我们得到4个注意力权重 α_P0,P1,α_P0,P2,α_P0,P3,α_P0,P5，它们的和为1。","64-第三步加权聚合#6.4 第三步：加权聚合":"P0的新特征是其邻居特征的加权平均，权重就是我们刚刚算出的 α。\n公式: h ′′ ∗P0=ReLU(∑∗j∈P1,P2,P3,P5α_P0,jh ′ _j)\n结果: h ′′ _P0 就是P0更新后的特征向量。它吸收了邻居的信息，并且是“有选择性”地吸收——对它认为更重要的邻居（α值高的），吸收得更多。","7-代码实现与结果分析-pytorch-geometric#7. 代码实现与结果分析 (PyTorch Geometric)":"现在，我们用代码来复现这个过程，并看看模型到底学到了什么。\nimport torch import torch.nn.functional as F from torch_geometric.data import Data from torch_geometric.nn import GATConv import numpy as np # --- 1. 构建我们的7节点引用网络图 --- # 节点特征 (7个节点, 2维特征) x = torch.tensor([ [1, 1], [1, 2], [2, 1], # ML papers (P0, P1, P2) [8, 7], [7, 8], # Bio papers (P3, P4) [1, 8], [2, 7] # Phy papers (P5, P6) ], dtype=torch.float) # 边 (引用关系, 注意是 source -\u003e target) # P1-\u003eP0, P2-\u003eP0, P3-\u003eP0, P5-\u003eP0, P3-\u003eP4, P4-\u003eP3, P5-\u003eP6, P6-\u003eP5 edge_index = torch.tensor([ [1, 2, 3, 5, 3, 4, 5, 6], # 源节点 [0, 0, 0, 0, 4, 3, 6, 5] # 目标节点 ], dtype=torch.long) # 创建图数据对象 data = Data(x=x, edge_index=edge_index) # --- 2. 定义一个简单的GAT模型 --- class SimpleGAT(torch.nn.Module): def __init__(self): super().__init__() # 定义一个GAT层。 # in_channels=2 (原始特征维度) # out_channels=4 (输出特征维度) # heads=2 (使用2个注意力头) # concat=True (拼接2个头的结果，最终输出维度为 4*2=8) self.gat_layer = GATConv(in_channels=2, out_channels=4, heads=2, concat=True) def forward(self, data): # return_attention_weights=True 是关键！ # 它会额外返回一个包含(edge_index, attention_weights)的元组 h, (edge_idx, attention_weights) = self.gat_layer(data.x, data.edge_index, return_attention_weights=True) return h, edge_idx, attention_weights # --- 3. 运行模型并分析结果 --- # 实例化并运行模型 model = SimpleGAT() model.eval() # 使用评估模式，不进行训练 final_embeddings, attn_edge_idx, attn_weights = model(data) print(\"--- 初始节点特征 ---\") print(np.round(data.x.numpy(), 2)) print(\"\\n--- GAT更新后的节点嵌入 (2个头拼接，8维) ---\") print(np.round(final_embeddings.detach().numpy(), 2)) print(\"\\n--- 学到的注意力权重 (2个头) ---\") # 打印每个头为每条边学到的权重 for i, (src, tgt) in enumerate(attn_edge_idx.T): print(f\"Edge {src.item()} -\u003e {tgt.item()}: Head 1 Alpha = {attn_weights[i, 0].item():.3f}, Head 2 Alpha = {attn_weights[i, 1].item():.3f}\") # --- 4. 深入分析：看看节点P0的邻居获得了多少注意力 --- print(\"\\n--- 深入分析：P0的邻居注意力分配 ---\") # P0是目标节点(target=0)的边有 P1-\u003eP0, P2-\u003eP0, P3-\u003eP0, P5-\u003eP0 # 它们在edge_index中的索引分别是 0, 1, 2, 3 p0_neighbors_indices = [0, 1, 2, 3] p0_neighbor_nodes = [1, 2, 3, 5] print(\"目标节点: P0\") for i, node_idx in enumerate(p0_neighbors_indices): src_node = p0_neighbor_nodes[i] head1_alpha = attn_weights[node_idx, 0].item() head2_alpha = attn_weights[node_idx, 1].item() print(f\" - 来自邻居 P{src_node} 的注意力: Head 1 = {head1_alpha:.3f}, Head 2 = {head2_alpha:.3f}\") 结果解读 你的具体数值会因为模型参数的随机初始化而略有不同，但趋势和结论是类似的。 输出示例： --- 深入分析：P0的邻居注意力分配 --- 目标节点: P0 - 来自邻居 P1 的注意力: Head 1 = 0.352, Head 2 = 0.401 - 来自邻居 P2 的注意力: Head 1 = 0.361, Head 2 = 0.388 - 来自邻居 P3 的注意力: Head 1 = 0.135, Head 2 = 0.095 - 来自邻居 P5 的注意力: Head 1 = 0.152, Head 2 = 0.116 分析结论： 同类更重要：模型（即使是未经训练的）也倾向于给特征更相似的邻居更高的权重。P0, P1, P2 都是“机器学习”论文，它们的初始特征很接近。因此，模型给 P1-\u003eP0 和 P2-\u003eP0 的引用分配了非常高的注意力权重（比如 Head 1 中，0.352 和 0.361）。 异类被抑制：P3（生物学）和 P5（物理学）与 P0 的特征差异很大。因此，模型认为这两篇跨领域的引用对于确定 P0 的身份不那么重要，所以分配了较低的注意力权重（比如 Head 1 中，0.135 和 0.152）。 多头学不同：注意 Head 1 和 Head 2 的权重分配并不完全相同。这表明两个头可能在学习不同的“邻里关系”模式。经过训练后，这种差异会更明显。 最终，P0的新特征向量 $h''_{P0}$，就是由 P1, P2, P3, P5 的变换后特征，按照上述这些权重加权求和得到的。它更多地吸收了来自同领域论文 P1 和 P2 的信息，而相对忽略了来自 P3 和 P5 的信息，从而使得更新后的 P0 特征能够更好地代表其\"机器学习\"的本质。 ## 8. Transformer vs GAT 对比总结 | 对比维度 | 标准Transformer (Encoder) | 图注意力网络 (GAT) | |---------|-------------------------|-------------------| | **输入数据结构** | 1D序列 (Sequence)\u003cbr\u003e例如：一句话中的单词序列 | 图结构 (Graph)\u003cbr\u003e例如：分子中的原子（节点）和化学键（边） | | **注意力范围 (Scope)** | 全局 (Global / All-to-All)\u003cbr\u003e序列中每个元素都会关注所有其他元素 | 局部/稀疏 (Local / Sparse)\u003cbr\u003e节点只关注与它有边直接相连的邻居 | | **Softmax归一化范围** | 在整个输入序列上进行归一化 | 只在节点的局部邻域 (Neighborhood) 内进行归一化 | | **结构/位置信息** | 需要外部注入\u003cbr\u003e本身无法感知顺序，必须依赖位置编码 (Positional Encoding) | 天然利用图结构\u003cbr\u003e通过 edge_index 定义的连接关系，天然地利用了图的拓扑结构 | | **计算复杂度** | $O(N^2 \\cdot d)$\u003cbr\u003eN是序列长度，计算量随长度平方增长 | $O(|E| \\cdot d)$\u003cbr\u003e|E|是边的数量，对于稀疏图效率更高 | | **核心应用** | 自然语言处理(NLP)、时间序列预测、计算机视觉(ViT) | 分子建模、社交网络分析、推荐系统、知识图谱 |","什么是图#什么是图？":"节点 (Nodes/Vertices)： 代表实体。可以是社交网络里的用户、分子结构里的原子、论文引用网络里的论文 边 (Edges)： 代表实体之间的关系。可以是用户之间的\"好友关系\"、原子之间的\"化学键\"、论文之间的\"引用关系\" 图结构的数据无处不在。它的核心在于连接性 (Connectivity)——谁和谁相连，以及如何相连，这其中蕴含着海量的信息。","结构的保留#结构的保留":"通过只在邻居间计算注意力，模型天然地就尊重并利用了图的局部结构信息。它不再是一个\"扁平\"的世界，而是一个有远近、有亲疏的结构化世界。","计算的拯救#计算的拯救":"图通常是稀疏的。一个节点平均可能只有几个或几十个邻居，而不是数万个。计算量从 $O(N^2)$ 急剧下降到 $O(|E|)$，其中 $|E|$ 是边的数量，这在计算上是完全可行的。"},"title":"gat"},"/docs/self-study/ai/llm/":{"data":{"":"等我有空了基于LLM zero2hero这个来学习手写LLM，当然作者主页也给了中文翻译的 Hands-On-Large-Language-Models (hands-on-llms)值得一看，但是现在的主线不是这个，每日也分不出多余的时间出来 原理手撕差不多了之后，可以到(https://github.com/Shubhamsaboo/awesome-llm-apps)找点喜欢的项目"},"title":"llm"},"/docs/self-study/ai/nlp/":{"data":{"":"","学习资源#学习资源":"CS224N 斯坦福课程 《自然语言处理综论》 Hugging Face 课程 Papers With Code NLP 任务 本页面内容正在完善中…"},"title":"nlp"},"/docs/self-study/ai/rl/":{"data":{"强化学习资源推荐#强化学习资源推荐":"资源名称 类型 主要特点 语言 链接 动手学强化学习 (OpenI) 开源教程 以Jupyter Notebook形式呈现，从基础概念到主流算法，图文代码并茂。适合系统性地从零开始学习。 中文 https://openi.pcl.ac.cn/kewei/Hands-on-RL/src/branch/main Hands-on Reinforcement Learning (Data Machines) 在线课程 一个循序渐进的强化学习课程，旨在通过Python代码让你从零基础到进阶。 英文 https://datamachines.xyz/the-hands-on-reinforcement-learning-course-page/ 《动手做深度强化学习》 (Maxim Lapan) 书籍及代码 书籍内容全面，覆盖从DQN到AlphaGo Zero等多种现代方法。GitHub提供完整的配套代码。适合希望深入掌握DRL的读者。 中文（译） https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On Easy RL (蘑菇书) 中文教程 对初学者友好。融合了李宏毅等老师课程精华，提供公式推导、习题和面试题。 中文 https://datawhalechina.github.io/easy-rl/","赵世钰深度强化学习#【赵世钰】深度强化学习":"【赵世钰】深度强化学习来入门西湖大学强化学习了，用10h（最短）去理解原理，因此规划每天1h即可，与我而言教材的信息熵优于视频 并不是说10h之后就不看了，实际上每天都在对应看相应的论文"},"title":"强化学习"},"/docs/self-study/ai/rl/chapter-1/":{"data":{"":"","11-state-and-action#1.1 State and Action":"状态是描述智能体相对于环境所处状态的核心概念，在网格世界示例中，状态直接对应智能体（机器人）的位置。由于网格包含 9 个可区分的单元格，因此存在 9 个状态，分别记为 $s_1, s_2, …, s_9$。\n$S={s_1,…,s_9}$表示所有状态的集合，即状态空间。\n针对每个状态，智能体可执行 5 种可能的动作：向上移动（$a_1$）、向右移动（$a_2$）、向下移动（$a_3$）、向左移动（$a_4$）和原地不动（$a_5$）（如图 1.3 (b) 所示），所有动作的集合称为动作空间，用符号 $A = {a_1, …, a_5}$ 表示。\n（这里简化了，比如 $s_1$ 的不能做出 $a_1$、$a_4$ 的动作，但是我们权当所有状态都有全局动作空间一致）","12-state-transittion#1.2 State Transittion":"当智能体在某一状态下执行特定动作时，会从当前状态转移到另一状态，这一过程称为状态转移。以网格世界为例，若智能体处于状态 $s_1$ 并选择动作 $a_2$（向右移动），则会转移到状态 $s_2$，该过程可表示为 $s_1 \\stackrel{a_2}{\\to} s_2$。\n两类特殊情况： 撞上forbidden cell，反弹回原状态 超出网格边界，反弹回原状态 状态转移过程可通过表格直观描述：\n但也可通过条件概率描述：例如在 $s_1$ 执行 $a_2$ 时，转移到 $s_2$ 的概率为 1，转移到其他状态的概率为 0，即 $p(s_2 | s_1, a_2) = 1$，$p(s_i | s_1, a_2) = 0$（$i \\neq 2$）。这种转移概率为 1 或 0 的情况称为确定性转移，而现实中可能存在随机转移（如网格中有阵风时，执行 $a_2$ 可能被吹到 $s_5$ 而非 $s_2$，此时 $p(s_5 | s_1, a_2) \u003e 0$），但这本书举的网格迷宫示例中均简化为确定性转移。","13-policy#1.3 Policy":"策略是指导智能体在每个状态下选择动作的规则。\n从数学角度，策略用条件概率 $\\pi(a | s)$ 描述，其含义是 “在状态s下选择动作a的概率”，且对任意状态s，所有动作的概率和满足 $\\sum_{a \\in A(s)} \\pi(a | s) = 1$。策略分为两种类型：确定性策略和随机性策略。","14-reward#1.4 Reward":"智能体在某一状态下执行动作后，会从环境中获得一个奖励值r，该奖励是状态s和动作a的函数，因此也记为 $r(s, a)$。奖励值可为正、负或零，其核心作用是引导智能体的行为：正奖励鼓励智能体重复对应动作，负奖励则抑制该动作。\n从数学角度，奖励过程可通过条件概率 $p(r | s, a)$ 描述，例如在 $s_1$ 执行 $a_1$ 时，获得 $r = -1$ 的概率为 1，即 $p(r = -1 | s_1, a_1) = 1$，$p(r \\neq -1 | s_1, a_1) = 0$，这属于确定性奖励过程；现实中也存在随机奖励（如学生努力学习后，考试成绩为正奖励，但具体分数不确定），此时 $p(r | s, a)$ 会包含多个正概率的奖励值。","15-trajectories-returns-and-episodes#1.5 Trajectories, Returns, and Episodes":"（1）轨迹：智能体遵循策略与环境交互时，生成的状态 - 动作 - 奖励链式序列：\ns1→a2,r=0s2→a3,r=0s5→a3,r=0s8→a2,r=1s9s_1 \\xrightarrow[a_2, r=0]{} s_2 \\xrightarrow[a_3, r=0]{} s_5 \\xrightarrow[a_3, r=0]{} s_8 \\xrightarrow[a_2, r=1]{} s_9s1​a2​,r=0​s2​a3​,r=0​s5​a3​,r=0​s8​a2​,r=1​s9​（2）回报（又称总奖励、累积奖励）：轨迹中所有奖励的总和，用于定量评估策略的优劣。\nreturn_a = 0+0+0+1 = 1\rreturn_b = 0+(-1)+0+1 = 0 对比两个策略的回报（1 \u003e 0），可直接判断左策略更优。\n上述回报计算适用于有限长度轨迹，而当轨迹为无限长，直接求和会导致回报发散，为此需引入折扣回报的概念，将未来奖励按折扣率 $\\gamma$（$\\gamma \\in (0,1)$）进行衰减：\ndiscounted return=r0+γr1+γ2r2+γ3r3+...\\text{discounted return} = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\gamma^3 r_3 + ...discounted return=r0​+γr1​+γ2r2​+γ3r3​+...（等比数列求和）\n折扣率 $\\gamma$ 的核心作用有两点：一是解决无限轨迹的回报发散问题；二是调整对近 / 远期奖励的重视程度 —— $\\gamma$ 接近 0 时，智能体更关注即时奖励（短视）；$\\gamma$ 接近 1 时，智能体更重视远期奖励（远视，愿意承担短期负奖励以获取长期收益）。\n（3）情节：指包含终止状态的有限轨迹，对应的任务称为 episodic 任务（如网格迷宫中到达 $s_9$ 即视为一次情节结束）。\n若任务无终止状态，轨迹持续无限长，则称为 continuing 任务（如持续运行的机器人导航）。\n两类任务可通过对终止状态的处理实现统一：一种方式是将终止状态设为吸收状态（如 $s_9$ 的动作空间仅为 $a_5$，或所有动作均转移回 $s_9$），智能体进入后不再离开；另一种方式（本书采用）是将终止状态视为普通状态，其动作空间与其他状态一致，智能体可离开但会因 “返回目标可获正奖励” 而逐渐学会停留。需注意，若 continuing 任务中存在持续正奖励，必须通过折扣回报避免求和发散。","16-markov-decision-processes-mdps#1.6 Markov Decision Processes, MDPs":"前面通过网格世界示例引入的状态、动作、转移、策略、奖励等概念，可通过马尔可夫决策过程（MDP）进行形式化统一： （1）核心集合\n状态空间：所有可能状态的集合，记为S； 动作空间：每个状态 $s \\in S$ 对应的可用动作集合，记为 $A(s)$（本书中 $A(s) = A$）； 奖励集：每个状态 - 动作对(s,a)(s, a)(s,a)对应的可能奖励集合，记为R(s,a)R(s, a)R(s,a)。 （2）环境模型（Dynamics）\n状态转移概率：在状态s执行动作a，转移到状态 $s’$ 的概率，记为 $p(s’ | s, a)$，且对任意 $(s, a)$ 满足 $\\sum_{s’ \\in S} p(s’ | s, a) = 1$（所有可能转移的概率和为 1）； 奖励概率：在状态s执行动作a，获得奖励r的概率，记为 $p(r | s, a)$，且对任意 $(s, a)$ 满足 $\\sum_{r \\in R(s, a)} p(r | s, a) = 1$。 （3）策略 $\\pi(a | s)$ 的定义与 1.4 节一致，即状态s下选择动作a的概率，且满足 $\\sum_{a \\in A(s)} \\pi(a | s) = 1$\n（4）马尔可夫性（Markov Property）\n未来状态和奖励仅依赖当前状态与动作，与历史状态 / 动作无关：\np(st+1∣st,at,st−1,at−1,...,s0,a0)=p(st+1∣st,at)p(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = p(s_{t+1} | s_t, a_t)p(st+1​∣st​,at​,st−1​,at−1​,...,s0​,a0​)=p(st+1​∣st​,at​)p(rt+1∣st,at,st−1,at−1,...,s0,a0)=p(rt+1∣st,at)p(r_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = p(r_{t+1} | s_t, a_t)p(rt+1​∣st​,at​,st−1​,at−1​,...,s0​,a0​)=p(rt+1​∣st​,at​) 区分 MDP 与马尔可夫过程（Markov Process, MP）：当 MDP 中的策略 $\\pi$ 固定后，状态转移概率可表示为 $p(s’ | s) = \\sum_{a \\in A(s)} \\pi(a | s) p(s’ | s, a)$，此时 MDP 退化为仅包含状态转移的 MP；若 MP 为离散时间且状态数有限 / 可数，则称为马尔可夫链（Markov Chain）。","17-summary#1.7 Summary":"强化学习的本质是 “智能体 - 环境交互闭环”：智能体（如网格中的机器人）感知当前状态、依据策略选择动作、通过执行器作用于环境；环境则根据动作产生新状态和奖励，智能体通过 “解释器” 接收新状态与奖励，进而更新策略，形成循环。","1basic-concepts#1、Basic Concepts":"以这个网格迷宫为例。"},"title":"第1章 Basic Concepts"},"/docs/self-study/ai/rl/chapter-10/":{"data":{"actor-critic-methods#Actor-Critic Methods":"Actor-Critic Methods从一个角度来看，“Actor-Critic” 指的是一种融合了基于策略（policy-based）和基于价值（value-based）方法的结构：其中，\"演员（Actor）\" 对应策略更新步骤，之所以称为 “演员”，是因为智能体通过遵循策略来执行动作；\"评论家（Critic）\" 对应价值更新步骤，之所以称为 “评论家”，是因为它通过评估 “演员”（策略）的对应价值来对其进行评判。\n从另一个角度来看，Actor-Critic 方法本质上仍是策略梯度算法，可通过对第 9 章介绍的策略梯度算法进行扩展得到。","advantage-actor-critic-a2c#Advantage actor-critic (A2C)":"该算法的核心思想是引入**基线（baseline）**以降低估计方差。","algorithm-description#Algorithm Description":"当 $b(s) = v_\\pi(s)$ 时，式（10.1）中的梯度上升算法可写为：\nθt+1=θt+αE[∇θln⁡π(A∣S,θt)[qπ(S,A)−vπ(S)]]≐θt+αE[∇θln⁡π(A∣S,θt)δπ(S,A)](10.7)\\begin{aligned} \\theta_{t+1} \u0026= \\theta_t + \\alpha \\mathbb{E}\\left[ \\nabla_\\theta \\ln \\pi(A|S, \\theta_t) \\left[ q_\\pi(S, A) - v_\\pi(S) \\right] \\right] \\\\ \u0026\\doteq \\theta_t + \\alpha \\mathbb{E}\\left[ \\nabla_\\theta \\ln \\pi(A|S, \\theta_t) \\delta_\\pi(S, A) \\right] \\tag{10.7} \\end{aligned}θt+1​​=θt​+αE[∇θ​lnπ(A∣S,θt​)[qπ​(S,A)−vπ​(S)]]≐θt​+αE[∇θ​lnπ(A∣S,θt​)δπ​(S,A)]​(10.7)其中，$\\delta_\\pi(S, A) \\doteq q_\\pi(S, A) - v_\\pi(S)$ 被称为优势函数（advantage function），它反映了某一动作相对于其他动作的优势。更具体地说，注意到 $v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_\\pi(s, a)$ 是动作价值的均值：若 $\\delta_\\pi(s, a) \u003e 0$，则说明对应动作的价值高于均值。\n式（10.7）的随机版本为：\nθt+1=θt+α∇θln⁡π(at∣st,θt)[qt(st,at)−vt(st)]=θt+α∇θln⁡π(at∣st,θt)δt(st,at)(10.8)\\begin{aligned} \\theta_{t+1} \u0026= \\theta_t + \\alpha \\nabla_\\theta \\ln \\pi(a_t|s_t, \\theta_t) \\left[ q_t(s_t, a_t) - v_t(s_t) \\right] \\\\ \u0026= \\theta_t + \\alpha \\nabla_\\theta \\ln \\pi(a_t|s_t, \\theta_t) \\delta_t(s_t, a_t) \\tag{10.8} \\end{aligned}θt+1​​=θt​+α∇θ​lnπ(at​∣st​,θt​)[qt​(st​,at​)−vt​(st​)]=θt​+α∇θ​lnπ(at​∣st​,θt​)δt​(st​,at​)​(10.8)其中，$s_t$、$a_t$ 是该时刻状态 $S$、动作 $A$ 的样本；$q_t(s_t, a_t)$ 和 $v_t(s_t)$ 分别是 $q_{\\pi(\\theta_t)}(s_t, a_t)$ 和 $v_{\\pi(\\theta_t)}(s_t)$ 的估计值。\n式（10.8）所示算法的核心特点是：策略更新基于 $q_t$ 相对于 $v_t$ 的相对价值，而非 $q_t$ 的绝对价值。这一设计在直觉上是合理的 —— 当我们在某一状态下选择动作时，真正关心的是 “哪个动作的价值相对更高”，而非动作价值的绝对大小。\n若通过蒙特卡洛学习估计 $q_t(s_t, a_t)$ 和 $v_t(s_t)$，则式（10.8）对应的算法被称为 “带基线的 REINFORCE 算法（REINFORCE with a baseline）\"；若通过时序差分（TD）学习估计这两个值，则该算法通常被称为优势演员 - 评论家（Advantage Actor-Critic，A2C）。\nA2C 的实现流程总结于算法 10.2 中。需要注意的是，该实现中用时序差分误差（TD 误差）近似优势函数，即：\nqt(st,at)−vt(st)≈rt+1+γvt(st+1)−vt(st)q_t(s_t, a_t) - v_t(s_t) \\approx r_{t+1} + \\gamma v_t(s_{t+1}) - v_t(s_t)qt​(st​,at​)−vt​(st​)≈rt+1​+γvt​(st+1​)−vt​(st​)这一近似的合理性可通过以下推导验证：根据 $q_\\pi(s_t, a_t)$ 的定义，有\nqπ(st,at)−vπ(st)=E[Rt+1+γvπ(St+1)−vπ(St)∣St=st,At=at]q_\\pi(s_t, a_t) - v_\\pi(s_t) = \\mathbb{E}\\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) - v_\\pi(S_t) \\mid S_t = s_t, A_t = a_t \\right]qπ​(st​,at​)−vπ​(st​)=E[Rt+1​+γvπ​(St+1​)−vπ​(St​)∣St​=st​,At​=at​]使用 TD 误差近似的一大优势是：仅需一个神经网络即可表示 $v_\\pi(s)$。反之，若直接用 $\\delta_t = q_t(s_t, a_t) - v_t(s_t)$ 计算优势，则需要维护两个独立的网络，分别表示 $v_\\pi(s)$（状态价值）和 $q_\\pi(s, a)$（动作价值）。正因如此，用 TD 误差近似的 A2C 也常被称为 “时序差分演员 - 评论家（TD Actor-Critic）\"。\n此外值得注意的是，策略 $\\pi(\\theta_t)$ 是随机策略，天然具备探索性，因此无需依赖 ε- 贪心（ε-greedy）等策略，即可直接用于生成经验样本。A2C 存在多种变体，例如异步优势演员 - 评论家（Asynchronous Advantage Actor-Critic，A3C）。\nAlgorithm 10.2: Advantage actor-critic (A2C) or TD actor-critic\nInitialization: A policy function $\\pi(a|s, \\theta_0)$ where $\\theta_0$ is the initial parameter. A value function $v(s, w_0)$ where $w_0$ is the initial parameter. $\\alpha_w, \\alpha_\\theta \u003e 0$.\nGoal: Learn an optimal policy to maximize $J(\\theta)$.\nAt time step in each episode, do:\nGenerate $a_t$ following $\\pi(a|s_t, \\theta_t)$ and then observe $r_{t+1}$, $s_{t+1}$. Advantage (TD error): $\\delta_t = r_{t+1} + \\gamma v(s_{t+1}, w_t) - v(s_t, w_t)$ Actor (policy update): $\\theta_{t+1} = \\theta_t + \\alpha_\\theta \\delta_t \\nabla_\\theta \\ln \\pi(a_t|s_t, \\theta_t)$ Critic (value update): $w_{t+1} = w_t + \\alpha_w \\delta_t \\nabla_w v(s_t, w_t)$","algorithm-description-1#Algorithm description":"基于离线策略梯度定理，我们现在可以介绍离线演员-评论家算法。由于离线情况与在线情况非常相似，因此我们仅介绍部分关键步骤。\n首先，离线策略梯度对任意额外基线 $b(s)$ 具有不变性。具体而言，我们有：\n∇θJ(θ)=ES∼ρ,A∼β[π(A∣S,θ)β(A∣S)∇θln⁡π(A∣S,θ)(qπ(S,A)−b(S))]\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{S \\sim \\rho, A \\sim \\beta}\\left[ \\frac{\\pi(A|S, \\theta)}{\\beta(A|S)} \\nabla_{\\theta} \\ln \\pi(A|S, \\theta) \\left( q_{\\pi}(S, A) - b(S) \\right) \\right]∇θ​J(θ)=ES∼ρ,A∼β​[β(A∣S)π(A∣S,θ)​∇θ​lnπ(A∣S,θ)(qπ​(S,A)−b(S))]这是因为 $\\mathbb{E}\\left[ \\frac{\\pi(A|S, \\theta)}{\\beta(A|S)} \\nabla_{\\theta} \\ln \\pi(A|S, \\theta) b(S) \\right] = 0$。\n为降低估计方差，我们可选择状态价值 $b(S) = v_{\\pi}(S)$ 作为基线，此时梯度可表示为：\n∇θJ(θ)=E[π(A∣S,θ)β(A∣S)∇θln⁡π(A∣S,θ)(qπ(S,A)−vπ(S))]\\nabla_{\\theta} J(\\theta) = \\mathbb{E}\\left[ \\frac{\\pi(A|S, \\theta)}{\\beta(A|S)} \\nabla_{\\theta} \\ln \\pi(A|S, \\theta) \\left( q_{\\pi}(S, A) - v_{\\pi}(S) \\right) \\right]∇θ​J(θ)=E[β(A∣S)π(A∣S,θ)​∇θ​lnπ(A∣S,θ)(qπ​(S,A)−vπ​(S))]对应的随机梯度上升算法为：\nθt+1=θt+αθ⋅π(at∣st,θt)β(at∣st)⋅∇θln⁡π(at∣st,θt)⋅(qt(st,at)−vt(st))\\theta_{t+1} = \\theta_t + \\alpha_{\\theta} \\cdot \\frac{\\pi(a_t|s_t, \\theta_t)}{\\beta(a_t|s_t)} \\cdot \\nabla_{\\theta} \\ln \\pi(a_t|s_t, \\theta_t) \\cdot \\left( q_t(s_t, a_t) - v_t(s_t) \\right)θt+1​=θt​+αθ​⋅β(at​∣st​)π(at​∣st​,θt​)​⋅∇θ​lnπ(at​∣st​,θt​)⋅(qt​(st​,at​)−vt​(st​))其中 $\\alpha_{\\theta} \u003e 0$（策略更新学习率）。\n与在线情况类似，优势函数 $q_t(s, a) - v_t(s)$ 可替换为时序差分误差（TD 误差），即：\nqt(st,at)−vt(st)≈rt+1+γvt(st+1)−vt(st)≐δt(st,at)q_t(s_t, a_t) - v_t(s_t) \\approx r_{t+1} + \\gamma v_t(s_{t+1}) - v_t(s_t) \\doteq \\delta_t(s_t, a_t)qt​(st​,at​)−vt​(st​)≈rt+1​+γvt​(st+1​)−vt​(st​)≐δt​(st​,at​)此时，算法可简化为：\nθt+1=θt+αθ⋅π(at∣st,θt)β(at∣st)⋅∇θln⁡π(at∣st,θt)⋅δt(st,at)\\theta_{t+1} = \\theta_t + \\alpha_{\\theta} \\cdot \\frac{\\pi(a_t|s_t, \\theta_t)}{\\beta(a_t|s_t)} \\cdot \\nabla_{\\theta} \\ln \\pi(a_t|s_t, \\theta_t) \\cdot \\delta_t(s_t, a_t)θt+1​=θt​+αθ​⋅β(at​∣st​)π(at​∣st​,θt​)​⋅∇θ​lnπ(at​∣st​,θt​)⋅δt​(st​,at​)离线演员-评论家算法的实现流程总结于算法 10.3。可以看出，该算法与优势演员-评论家（A2C）算法的结构完全一致，唯一区别在于：演员（策略更新）和评论家（价值更新）的步骤中均额外引入了重要性权重。需特别注意的是，除演员外，评论家也通过重要性采样技术从 “在线” 转换为 “离线”。事实上，重要性采样是一种通用技术，既可应用于基于策略（policy-based）的算法，也可应用于基于价值（value-based）的算法。最后，算法 10.3 可通过多种方式扩展，以融入资格迹（eligibility traces）等更多技术。\nAlgorithm 10.3: Off-policy actor-critic based on importance sampling\nInitialization: A given behavior policy $\\beta(a|s)$. A target policy $\\pi(a|s, \\theta_0)$ where $\\theta_0$ is the initial parameter. A value function $v(s, w_0)$ where $w_0$ is the initial parameter. $\\alpha_w, \\alpha_{\\theta} \u003e 0$.\nGoal: Learn an optimal policy to maximize $J(\\theta)$.\nAt time step in each episode, do:\nGenerate $a_t$ following $\\beta(s_t)$ and then observe $r_{t+1}, s_{t+1}$. Advantage (TD error): δt=rt+1+γv(st+1,wt)−v(st,wt) \\delta_t = r_{t+1} + \\gamma v(s_{t+1}, w_t) - v(s_t, w_t) δt​=rt+1​+γv(st+1​,wt​)−v(st​,wt​) Actor (policy update): $\\theta_{t+1} = \\theta_t + \\alpha_{\\theta} \\frac{\\pi(a_t|s_t, \\theta_t)}{\\beta(a_t|s_t)} \\delta_t \\nabla_{\\theta} \\ln \\pi(a_t|s_t, \\theta_t)$ Critic (value update): $w_{t+1} = w_t + \\alpha_w \\frac{\\pi(a_t|s_t, \\theta_t)}{\\beta(a_t|s_t)} \\delta_t \\nabla_w v(s_t, w_t)$","algorithm-description-2#Algorithm description":"基于定理 10.2 给出的梯度，我们可采用梯度上升算法来最大化目标函数 $J(\\theta)$，具体形式如下：\nθt+1=θt+αθES∼η[∇θμ(S)⋅(∇aqμ(S,a))∣a=μ(S)]\\theta_{t+1} = \\theta_t + \\alpha_{\\theta} \\mathbb{E}_{S \\sim \\eta} \\left[ \\nabla_{\\theta} \\mu(S) \\cdot \\left. (\\nabla_{a} q_{\\mu}(S, a)) \\right|_{a=\\mu(S)} \\right]θt+1​=θt​+αθ​ES∼η​[∇θ​μ(S)⋅(∇a​qμ​(S,a))∣a=μ(S)​]对应的随机梯度上升算法为：\nθt+1=θt+αθ∇θμ(st)⋅(∇aqμ(st,a))∣a=μ(st)\\theta_{t+1} = \\theta_t + \\alpha_{\\theta} \\nabla_{\\theta} \\mu(s_t) \\cdot \\left. (\\nabla_{a} q_{\\mu}(s_t, a)) \\right|_{a=\\mu(s_t)}θt+1​=θt​+αθ​∇θ​μ(st​)⋅(∇a​qμ​(st​,a))∣a=μ(st​)​该算法的实现流程总结于算法 10.4。需要注意的是，由于行为策略 $\\beta$ 可能与目标策略 $\\mu$ 不同，因此该算法属于**离线（off-policy）**算法，具体原因如下：\nAlgorithm 10.4: Deterministic policy gradient or deterministic actor-critic\nInitialization: A given behavior policy $\\beta(a|s)$. A deterministic target policy $\\mu(s, \\theta_0)$ where $\\theta_0$ is the initial parameter. A value function $q(s, a, w_0)$ where $w_0$ is the initial parameter. $\\alpha_w, \\alpha_{\\theta} \u003e 0$.\nGoal: Learn an optimal policy to maximize $J(\\theta)$.\nAt time step in each episode, do:\nGenerate $a_t$ following $\\beta$ and then observe $r_{t+1}, s_{t+1}$.\nTD error: δt=rt+1+γq(st+1,μ(st+1,θt),wt)−q(st,at,wt) \\delta_t = r_{t+1} + \\gamma q(s_{t+1}, \\mu(s_{t+1}, \\theta_t), w_t) - q(s_t, a_t, w_t) δt​=rt+1​+γq(st+1​,μ(st+1​,θt​),wt​)−q(st​,at​,wt​) Actor (policy update): $\\theta_{t+1} = \\theta_t + \\alpha_{\\theta}\\nabla_{\\theta}\\mu(s_t, \\theta_t)\\left(\\nabla_a q(s_t, a, w_t)\\right)\\Big|_{\\text{where } a=\\mu(s_t)}$\nCritic (value update): wt+1=wt+αwδt∇wq(st,at,wt) w_{t+1} = w_t + \\alpha_w\\delta_t\\nabla_w q(s_t, a_t, w_t) wt+1​=wt​+αw​δt​∇w​q(st​,at​,wt​) 演员（Actor）是离线的：这一点在介绍定理 10.2 时已说明，核心在于确定性策略梯度无需动作采样，可利用任意策略生成的样本。\n评论家（Critic）也是离线的：需特别注意 “为何评论家是离线的，却无需使用重要性采样技术”。具体而言，评论家所需的经验样本为 $(s_t, a_t, r_{t+1}, s_{t+1}, \\tilde{a}{t+1})$，其中 $\\tilde{a}{t+1} = \\mu(s_{t+1})$。该经验样本的生成涉及两个策略：\n第一个策略：在状态 $s_t$ 生成动作 $a_t$ 的策略，由于 $a_t$ 用于与环境交互，因此该策略是行为策略 $\\beta$； 第二个策略：在状态 $s_{t+1}$ 生成动作 $\\tilde{a}_{t+1}$ 的策略，由于评论家的目标是评估策略 $\\mu$，因此该策略必须是目标策略 $\\mu$。 需注意，$\\tilde{a}_{t+1}$ 不会用于下一步与环境的交互，因此目标策略 $\\mu$ 并非行为策略。综上，评论家属于离线模式。\n如何选择价值函数 $q(s, a, w)$？ 提出确定性策略梯度方法的原始研究采用了线性函数来表示 $q(s, a, w)$，具体形式为：\nq(s,a,w)=ϕT(s,a)wq(s, a, w) = \\phi^T(s, a)wq(s,a,w)=ϕT(s,a)w其中 $\\phi(s, a)$ 是特征向量。目前，更主流的做法是采用神经网络来表示 $q(s, a, w)$，例如深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）。","baseline-invariance#Baseline invariance":"策略梯度有一个有趣的性质：它对额外引入的基线具有不变性。即：\nES∼η,A∼π[∇θln⁡π(A∣S,θt)qπ(S,A)]=ES∼η,A∼π[∇θln⁡π(A∣S,θt)(qπ(S,A)−b(S))](10.3)\\mathbb{E}_{S\\sim \\eta,A\\sim \\pi}\\left[ \\nabla _{\\theta } \\ln \\pi (A|S, \\theta_t)q_{\\pi }(S, A) \\right] = \\mathbb{E}_{S\\sim \\eta,A\\sim \\pi}\\left[ \\nabla _{\\theta } \\ln \\pi (A|S, \\theta_t)\\left(q_{\\pi }(S, A) - b(S)\\right) \\right] \\tag{10.3}ES∼η,A∼π​[∇θ​lnπ(A∣S,θt​)qπ​(S,A)]=ES∼η,A∼π​[∇θ​lnπ(A∣S,θt​)(qπ​(S,A)−b(S))](10.3)其中，额外基线 $b(S)$ 是关于状态 $S$ 的标量函数。\n接下来我们回答关于基线的两个问题。\n第一，式（10.3）为何成立？\n式（10.3）成立的充要条件是：\nES∼η,A∼π[∇θln⁡π(A∣S,θt)b(S)]=0\\mathbb{E}_{S\\sim \\eta,A\\sim \\pi}\\left[ \\nabla _{\\theta } \\ln \\pi (A|S, \\theta_t)b(S) \\right] = 0ES∼η,A∼π​[∇θ​lnπ(A∣S,θt​)b(S)]=0该等式成立的推导过程如下：\nES∼η,A∼π[∇θln⁡π(A∣S,θt)b(S)]=∑s∈Sη(s)∑a∈Aπ(a∣s,θt)∇θln⁡π(a∣s,θt)b(s)=∑s∈Sη(s)∑a∈A∇θπ(a∣s,θt)b(s)=∑s∈Sη(s)b(s)∑a∈A∇θπ(a∣s,θt)=∑s∈Sη(s)b(s)∇θ∑a∈Aπ(a∣s,θt)=∑s∈Sη(s)b(s)∇θ1=0\\begin{aligned} \\mathbb{E}_{S\\sim \\eta,A\\sim \\pi}\\left[ \\nabla _{\\theta } \\ln \\pi (A|S, \\theta_t)b(S) \\right] \u0026= \\sum_{s \\in \\mathcal{S}} \\eta(s) \\sum_{a \\in \\mathcal{A}} \\pi(a|s, \\theta_t) \\nabla_{\\theta} \\ln \\pi(a|s, \\theta_t) b(s) \\\\ \u0026= \\sum_{s \\in \\mathcal{S}} \\eta(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi(a|s, \\theta_t) b(s) \\\\ \u0026= \\sum_{s \\in \\mathcal{S}} \\eta(s) b(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi(a|s, \\theta_t) \\\\ \u0026= \\sum_{s \\in \\mathcal{S}} \\eta(s) b(s) \\nabla_{\\theta} \\sum_{a \\in \\mathcal{A}} \\pi(a|s, \\theta_t) \\\\ \u0026= \\sum_{s \\in \\mathcal{S}} \\eta(s) b(s) \\nabla_{\\theta} 1 = 0 \\end{aligned}ES∼η,A∼π​[∇θ​lnπ(A∣S,θt​)b(S)]​=s∈S∑​η(s)a∈A∑​π(a∣s,θt​)∇θ​lnπ(a∣s,θt​)b(s)=s∈S∑​η(s)a∈A∑​∇θ​π(a∣s,θt​)b(s)=s∈S∑​η(s)b(s)a∈A∑​∇θ​π(a∣s,θt​)=s∈S∑​η(s)b(s)∇θ​a∈A∑​π(a∣s,θt​)=s∈S∑​η(s)b(s)∇θ​1=0​（注：任意状态下所有动作的概率和为 1，即 $\\sum_{a \\in \\mathcal{A}} \\pi(a|s, \\theta_t) = 1$，而常数的梯度为 0，因此最终结果为 0。）\n第二，基线为何有用？\n基线之所以有用，是因为当我们使用样本近似真实梯度时，它能降低近似方差。具体来说，令：\nX(S,A)≐∇θln⁡π(A∣S,θt)[qπ(S,A)−b(S)](10.4)X(S, A) \\doteq \\nabla_{\\theta} \\ln \\pi(A|S, \\theta_t)\\left[q_{\\pi}(S, A) - b(S)\\right] \\tag{10.4}X(S,A)≐∇θ​lnπ(A∣S,θt​)[qπ​(S,A)−b(S)](10.4)此时，真实梯度即为 $\\mathbb{E}[X(S, A)]$。由于我们需要使用随机样本 $x$ 来近似 $\\mathbb{E}[X]$，因此若方差 $\\text{var}(X)$ 较小，会更有利于近似 —— 例如，若 $\\text{var}(X)$ 接近 0，则任意样本 $x$ 都能准确近似 $\\mathbb{E}[X]$；反之，若 $\\text{var}(X)$ 较大，则单个样本的值可能与 $\\mathbb{E}[X]$ 相差甚远。\n尽管 $\\mathbb{E}[X]$ 对基线具有不变性，但方差 $\\text{var}(X)$ 并不具备这一性质。我们的目标是设计一个优良的基线，以最小化 $\\text{var}(X)$。\n在 REINFORCE 算法和 QAC 算法中，我们将基线设为 $b = 0$，但这一设置并不能保证是优良的基线。事实上，能最小化 $\\text{var}(X)$ 的最优基线为：\nb∗(s)=EA∼π[∥∇θln⁡π(A∣s,θt)∥2qπ(s,A)]EA∼π[∥∇θln⁡π(A∣s,θt)∥2],s∈S(10.5)b^*(s) = \\frac{\\mathbb{E}_{A \\sim \\pi}\\left[ \\left\\| \\nabla_{\\theta} \\ln \\pi(A|s, \\theta_t) \\right\\|^2 q_{\\pi}(s, A) \\right]}{\\mathbb{E}_{A \\sim \\pi}\\left[ \\left\\| \\nabla_{\\theta} \\ln \\pi(A|s, \\theta_t) \\right\\|^2 \\right]}, \\quad s \\in \\mathcal{S} \\tag{10.5}b∗(s)=EA∼π​[∥∇θ​lnπ(A∣s,θt​)∥2]EA∼π​[∥∇θ​lnπ(A∣s,θt​)∥2qπ​(s,A)]​,s∈S(10.5)其证明过程直接跳过有缘再见。\n尽管式（10.5）中的基线是最优的，但它过于复杂，难以在实际中应用。若从式（10.5）中移除权重项 $\\left| \\nabla_{\\theta} \\ln \\pi(A|s, \\theta_t) \\right|^2$，可得到一个次优但表达式简洁的基线：\nb†(s)=EA∼π[qπ(s,A)]=vπ(s),s∈Sb^{\\dagger}(s) = \\mathbb{E}_{A \\sim \\pi}\\left[ q_{\\pi}(s, A) \\right] = v_{\\pi}(s), \\quad s \\in \\mathcal{S}b†(s)=EA∼π​[qπ​(s,A)]=vπ​(s),s∈S有趣的是，这个次优基线正是状态价值 $v_{\\pi}(s)$。","deterministic-actor-critic#Deterministic actor-critic":"到目前为止，策略梯度方法中所使用的策略均为随机策略 —— 这是因为其要求对所有（状态-动作）对 $(s, a)$，都满足 $\\pi(a|s, \\theta) \u003e 0$。本节将说明，确定性策略同样可应用于策略梯度方法。此处的 “确定性” 指：对于任意状态，仅单个动作被赋予 1 的概率，而所有其他动作的概率均为 0。研究确定性策略具有重要意义，因为它天然具备**离线（off-policy）**属性，且能有效处理连续动作空间。\n此前，我们一直用 $\\pi(a|s, \\theta)$ 表示通用策略（既可为随机策略，也可为确定性策略）。在本节中，我们将用 $a = \\mu(s, \\theta)$ 专门表示确定性策略。与 $\\pi$（给出动作概率）不同，$\\mu$ 是从状态空间 $\\mathcal{S}$ 到动作空间 $\\mathcal{A}$ 的映射，因此会直接输出动作。例如，这种确定性策略可通过神经网络实现：以状态 $s$ 为输入，动作 $a$ 为输出，$\\theta$ 为网络参数。为简化表述，我们常将 $\\mu(s, \\theta)$ 简记为 $\\mu(s)$。","importance-sampling#Importance sampling":"考虑随机变量 $X \\in \\mathcal{X}$，设 $p_0(X)$ 为某一概率分布，我们的目标是估计期望 $\\mathbb{E}{X \\sim p_0}[X]$。假设我们拥有一组独立同分布（i.i.d.）样本 ${x_i}{i=1}^n$。\n第一种情况：若样本 ${x_i}{i=1}^n$ 是遵循分布 $p_0$ 生成的，则可使用样本均值 $\\bar{x} = \\frac{1}{n}\\sum{i=1}^n x_i$ 来近似 $\\mathbb{E}{X \\sim p_0}[X]$。这是因为 $\\bar{x}$ 是 $\\mathbb{E}{X \\sim p_0}[X]$ 的无偏估计，且当 $n \\to \\infty$ 时，估计方差会收敛到 0。\n第二种情况：考虑另一种场景 —— 样本 ${x_i}{i=1}^n$ 并非由 $p_0$ 生成，而是由另一分布 $p_1$ 生成。此时我们仍能利用这些样本来近似 $\\mathbb{E}{X \\sim p_0}[X]$ 吗？答案是肯定的。但此时不能再用 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ 进行近似，因为 $\\bar{x}$ 近似的是 $\\mathbb{E}{X \\sim p_1}[X]$，而非 $\\mathbb{E}{X \\sim p_0}[X]$。在第二种场景下，可基于重要性采样技术近似 $\\mathbb{E}{X \\sim p_0}[X]$。具体而言，$\\mathbb{E}{X \\sim p_0}[X]$ 满足以下等式：\nEX∼p0[X]=∑x∈Xp0(x)x=∑x∈Xp1(x)p0(x)p1(x)x⏟f(x)=EX∼p1[f(X)](10.9)\\mathbb{E}_{X \\sim p_0}[X] = \\sum_{x \\in \\mathcal{X}} p_0(x)x = \\sum_{x \\in \\mathcal{X}} p_1(x) \\underbrace{\\frac{p_0(x)}{p_1(x)}x}_{f(x)} = \\mathbb{E}_{X \\sim p_1}[f(X)] \\tag{10.9}EX∼p0​​[X]=x∈X∑​p0​(x)x=x∈X∑​p1​(x)f(x)p1​(x)p0​(x)​x​​=EX∼p1​​[f(X)](10.9)由此，估计 $\\mathbb{E}{X \\sim p_0}[X]$ 的问题转化为估计 $\\mathbb{E}{X \\sim p_1}[f(X)]$ 的问题。令 $\\bar{f} \\doteq \\frac{1}{n}\\sum_{i=1}^n f(x_i)$。由于 $\\bar{f}$ 可有效近似 $\\mathbb{E}_{X \\sim p_1}[f(X)]$，结合式（10.9）可得：\nEX∼p0[X]=EX∼p1[f(X)]≈fˉ=1n∑i=1nf(xi)=1n∑i=1np0(xi)p1(xi)⏟重要性权重（importance weight）xi(10.10)\\mathbb{E}_{X \\sim p_0}[X] = \\mathbb{E}_{X \\sim p_1}[f(X)] \\approx \\bar{f} = \\frac{1}{n}\\sum_{i=1}^n f(x_i) = \\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\frac{p_0(x_i)}{p_1(x_i)}}_{\\text{重要性权重（importance weight）}} x_i \\tag{10.10}EX∼p0​​[X]=EX∼p1​​[f(X)]≈fˉ​=n1​i=1∑n​f(xi​)=n1​i=1∑n​重要性权重（importance weight）p1​(xi​)p0​(xi​)​​​xi​(10.10)式（10.10）表明，$\\mathbb{E}_{X \\sim p_0}[X]$ 可通过样本 $x_i$ 的加权平均来近似，其中 $\\frac{p_0(x_i)}{p_1(x_i)}$ 被称为重要性权重。具体特性如下：当 $p_1 = p_0$ 时，重要性权重为 1，此时 $\\bar{f}$ 退化为样本均值 $\\bar{x}$；当 $p_0(x_i) \\geq p_1(x_i)$ 时，样本 $x_i$ 在分布 $p_0$ 中被采样到的概率更高，而在 $p_1$ 中被采样到的概率更低。此时重要性权重大于 1，可 “放大” 该样本的重要性。\n可能有读者会问：既然式（10.10）中仍需用到 $p_0(x)$，为何不直接利用期望的定义 $\\mathbb{E}{X \\sim p_0}[X] = \\sum{x \\in \\mathcal{X}} p_0(x)x$ 来计算呢？答案如下：要直接使用定义计算，需满足以下任一条件 —— 要么知道 $p_0$ 的解析表达式，要么获取所有 $x \\in \\mathcal{X}$ 对应的 $p_0(x)$ 值。但在实际场景中，这两个条件往往难以满足：例如，若分布通过神经网络等方式表示，我们难以获取 $p_0$ 的解析表达式；而当 $\\mathcal{X}$ 的规模较大时，也难以获取所有 $x \\in \\mathcal{X}$ 对应的 $p_0(x)$ 值。相比之下，式（10.10）仅需获取部分样本 $x_i$ 对应的 $p_0(x_i)$ 值，在实际中更易于实现。\n接下来，我们通过一个示例来演示重要性采样技术。考虑随机变量 $X \\in \\mathcal{X}$，其中 $\\mathcal{X}$ 定义为 ${+1, -1}$。假设分布 $p_0$ 是满足以下条件的概率分布：\np0(X=+1)=0.5,p0(X=−1)=0.5p_0(X = +1) = 0.5,\\quad p_0(X = -1) = 0.5p0​(X=+1)=0.5,p0​(X=−1)=0.5则 $X$ 在分布 $p_0$ 下的期望为：\nEX∼p0[X]=(+1)×0.5+(−1)×0.5=0\\mathbb{E}_{X \\sim p_0}[X] = (+1) \\times 0.5 + (-1) \\times 0.5 = 0EX∼p0​​[X]=(+1)×0.5+(−1)×0.5=0假设分布 $p_1$ 是另一个满足以下条件的概率分布：\np1(X=+1)=0.8,p1(X=−1)=0.2p_1(X = +1) = 0.8,\\quad p_1(X = -1) = 0.2p1​(X=+1)=0.8,p1​(X=−1)=0.2则 $X$ 在分布 $p_1$ 下的期望为：\nEX∼p1[X]=(+1)×0.8+(−1)×0.2=0.6\\mathbb{E}_{X \\sim p_1}[X] = (+1) \\times 0.8 + (-1) \\times 0.2 = 0.6EX∼p1​​[X]=(+1)×0.8+(−1)×0.2=0.6假设我们拥有一组从分布 $p_1$ 中抽取的样本 ${x_i}$，目标是利用这些样本来估计 $\\mathbb{E}{X \\sim p_0}[X]$。如图 10.2 所示，样本中取值为 $+1$ 的数量多于取值为 $-1$ 的数量，这是因为 $p_1(X = +1) = 0.8 \u003e p_1(X = -1) = 0.2$。若直接计算样本的均值 $\\sum{i=1}^n x_i / n$，该均值会收敛到 $\\mathbb{E}{X \\sim p_1}[X] = 0.6$（见图 10.2 中的虚线）；与之相反，若按照式（10.10）计算加权均值，该均值则能成功收敛到 $\\mathbb{E}{X \\sim p_0}[X] = 0$（见图 10.2 中的实线）。 图中 $X \\in {+1, -1}$，且 $p_0(X = +1) = p_0(X = -1) = 0.5$。样本根据分布 $p_1$ 生成，其中 $p_1(X = +1) = 0.8$、$p_1(X = -1) = 0.2$。样本的均值收敛到 $\\mathbb{E}{X \\sim p_1}[X] = 0.6$，而通过式（10.10）中重要性采样技术计算的加权均值收敛到 $\\mathbb{E}{X \\sim p_0}[X] = 0$。\n最后，用于生成样本的分布 $p_1$ 必须满足：当 $p_0(x) \\neq 0$ 时，$p_1(x) \\neq 0$。若出现 “$p_0(x) \\neq 0$ 但 $p_1(x) = 0$” 的情况，估计结果可能会出现问题。例如，若：\np1(X=+1)=1,p1(X=−1)=0p_1(X = +1) = 1,\\quad p_1(X = -1) = 0p1​(X=+1)=1,p1​(X=−1)=0则从 $p_1$ 中生成的样本全为正值，即 ${x_i} = {+1, +1, \\dots, +1}$。这些样本无法用于正确估计 $\\mathbb{E}_{X \\sim p_0}[X] = 0$，因为无论样本数量 $n$ 多大，都有：\n1n∑i=1np0(xi)p1(xi)xi=1n∑i=1np0(+1)p1(+1)×1=1n∑i=1n0.51×1≡0.5\\frac{1}{n}\\sum_{i=1}^n \\frac{p_0(x_i)}{p_1(x_i)} x_i = \\frac{1}{n}\\sum_{i=1}^n \\frac{p_0(+1)}{p_1(+1)} \\times 1 = \\frac{1}{n}\\sum_{i=1}^n \\frac{0.5}{1} \\times 1 \\equiv 0.5n1​i=1∑n​p1​(xi​)p0​(xi​)​xi​=n1​i=1∑n​p1​(+1)p0​(+1)​×1=n1​i=1∑n​10.5​×1≡0.5","metric-1-average-value#Metric 1: Average value":"我们首先推导平均价值的梯度：\nJ(θ)=E[vμ(s)]=∑s∈Sd0(s)vμ(s)(10.15)J(\\theta) = \\mathbb{E}\\left[ v_{\\mu}(s) \\right] = \\sum_{s \\in \\mathcal{S}} d_0(s) v_{\\mu}(s) \\tag{10.15}J(θ)=E[vμ​(s)]=s∈S∑​d0​(s)vμ​(s)(10.15)其中，$d_0$ 是状态的概率分布。此处为简化分析，选择 $d_0$ 与（确定性策略）$\\mu$ 相互独立。选择 $d_0$ 存在两种特殊且重要的情况：\n第一种情况：$d_0(s_0) = 1$ 且 $d_0(s \\neq s_0) = 0$（其中 $s_0$ 是某个特定的目标状态）。在此情况下，策略的目标是最大化从 $s_0$ 出发所能获得的折扣回报。 第二种情况：$d_0$ 是某一给定行为策略（与目标策略 $\\mu$ 不同）的分布。 要计算 $J(\\theta)$ 的梯度，需先计算任意状态 $s \\in \\mathcal{S}$ 下 $v_{\\mu}(s)$（策略 $\\mu$ 的状态价值）的梯度。我们考虑折扣因子 $\\gamma \\in (0,1)$ 的折扣情况。\n引理 10.1（$v_{\\mu}(s)$ 的梯度）：在折扣情况下，对任意 $s \\in \\mathcal{S}$，以下等式成立：\n∇θvμ(s)=∑s′∈SPrμ(s′∣s)⋅∇θμ(s′)⋅(∇aqμ(s′,a))∣a=μ(s′)(10.16)\\nabla_{\\theta} v_{\\mu}(s) = \\sum_{s' \\in \\mathcal{S}} \\text{Pr}_{\\mu}(s'|s) \\cdot \\nabla_{\\theta} \\mu(s') \\cdot \\left. (\\nabla_{a} q_{\\mu}(s', a)) \\right|_{a=\\mu(s')} \\tag{10.16}∇θ​vμ​(s)=s′∈S∑​Prμ​(s′∣s)⋅∇θ​μ(s′)⋅(∇a​qμ​(s′,a))∣a=μ(s′)​(10.16)其中，$\\text{Pr}{\\mu}(s’|s) \\doteq \\sum{k=0}^{\\infty} \\gamma^k \\left[ P_{\\mu}^k \\right]{ss’} = \\left[ (I - \\gamma P{\\mu})^{-1} \\right]{ss’}$ 表示在策略 $\\mu$ 下，从状态 $s$ 转移到状态 $s’$ 的折扣总概率。此处 $[\\cdot]{ss’}$ 表示矩阵中第 $s$ 行、第 $s’$ 列的元素。\n有了引理 10.1 的铺垫，我们现在可以推导 $J(\\theta)$ 的梯度。\n定理 10.3（折扣情况下的确定性策略梯度定理）：在折扣因子 $\\gamma \\in (0, 1)$ 的折扣情况下，式（10.15）中 $J(\\theta)$ 的梯度为：\n∇θJ(θ)=∑s∈Sρμ(s)∇θμ(s)⋅(∇aqμ(s,a))∣a=μ(s)=ES∼ρμ[∇θμ(S)⋅(∇aqμ(S,a))∣a=μ(S)]\\begin{aligned} \\nabla_{\\theta} J(\\theta) \u0026= \\sum_{s \\in \\mathcal{S}} \\rho_{\\mu}(s) \\nabla_{\\theta} \\mu(s) \\cdot \\left. (\\nabla_{a} q_{\\mu}(s, a)) \\right|_{a=\\mu(s)} \\\\ \u0026= \\mathbb{E}_{S \\sim \\rho_{\\mu}} \\left[ \\nabla_{\\theta} \\mu(S) \\cdot \\left. (\\nabla_{a} q_{\\mu}(S, a)) \\right|_{a=\\mu(S)} \\right] \\end{aligned}∇θ​J(θ)​=s∈S∑​ρμ​(s)∇θ​μ(s)⋅(∇a​qμ​(s,a))∣a=μ(s)​=ES∼ρμ​​[∇θ​μ(S)⋅(∇a​qμ​(S,a))∣a=μ(S)​]​其中，状态分布 $\\rho_{\\mu}$ 的定义为：\nρμ(s)=∑s′∈Sd0(s′)Prμ(s∣s′),s∈S\\rho_{\\mu}(s) = \\sum_{s' \\in \\mathcal{S}} d_0(s') \\text{Pr}_{\\mu}(s|s'), \\quad s \\in \\mathcal{S}ρμ​(s)=s′∈S∑​d0​(s′)Prμ​(s∣s′),s∈S此处，$\\text{Pr}{\\mu}(s|s’) = \\sum{k=0}^{\\infty} \\gamma^k \\left[ P_{\\mu}^k \\right]{s’s} = \\left[ (I - \\gamma P{\\mu})^{-1} \\right]{s’s}$，表示在策略 $\\mu$ 下从状态 $s’$ 转移到状态 $s$ 的折扣总概率。（注：$\\left[ P{\\mu}^k \\right]{s’s}$ 表示矩阵 $P{\\mu}$ 的 $k$ 次幂中第 $s’$ 行、第 $s$ 列的元素；$I$ 为单位矩阵，$P_{\\mu}$ 为策略 $\\mu$ 对应的状态转移矩阵。）","metric-2-average-reward#Metric 2: Average reward":"接下来，我们推导平均奖励的梯度：\nJ(θ)=rˉμ=∑s∈Sdμ(s)rμ(s)=ES∼dμ[rμ(S)](10.20)\\begin{aligned} J(\\theta) \u0026= \\bar{r}_\\mu = \\sum_{s \\in \\mathcal{S}} d_\\mu(s) r_\\mu(s) \\\\ \u0026= \\mathbb{E}_{S \\sim d_\\mu} \\left[ r_\\mu(S) \\right] \\tag{10.20} \\end{aligned}J(θ)​=rˉμ​=s∈S∑​dμ​(s)rμ​(s)=ES∼dμ​​[rμ​(S)]​(10.20)其中，$r_\\mu(s) = \\mathbb{E}\\left[ R \\mid s, a = \\mu(s) \\right] = \\sum_{r} r p\\left(r \\mid s, a = \\mu(s)\\right)$ 表示即时奖励的期望。关于该指标的更多信息可参考第 9.2 节。$J(\\theta)$ 的梯度由下述定理给出。\n定理 10.4（无折扣情况下的确定性策略梯度定理）：在无折扣情况下，式（10.20）中 $J(\\theta)$ 的梯度为：\n∇θJ(θ)=∑s∈Sdμ(s)∇θμ(s)⋅(∇aqμ(s,a))∣a=μ(s)=ES∼dμ[∇θμ(S)⋅(∇aqμ(S,a))∣a=μ(S)]\\begin{aligned} \\nabla_{\\theta} J(\\theta) \u0026= \\sum_{s \\in \\mathcal{S}} d_\\mu(s) \\nabla_{\\theta} \\mu(s) \\cdot \\left. (\\nabla_{a} q_\\mu(s, a)) \\right|_{a=\\mu(s)} \\\\ \u0026= \\mathbb{E}_{S \\sim d_\\mu} \\left[ \\nabla_{\\theta} \\mu(S) \\cdot \\left. (\\nabla_{a} q_\\mu(S, a)) \\right|_{a=\\mu(S)} \\right] \\end{aligned}∇θ​J(θ)​=s∈S∑​dμ​(s)∇θ​μ(s)⋅(∇a​qμ​(s,a))∣a=μ(s)​=ES∼dμ​​[∇θ​μ(S)⋅(∇a​qμ​(S,a))∣a=μ(S)​]​其中，$d_\\mu$ 是策略 $\\mu$ 下的状态平稳分布。","off-policy-actor-critic#Off-policy actor-critic":"到目前为止，我们研究过的策略梯度方法（包括 REINFORCE、QAC 和 A2C）均为**在线（on-policy）**方法。其原因可从真实梯度的表达式中看出：\n∇θJ(θ)=ES∼η,A∼π[∇θln⁡π(A∣S,θt)(qπ(S,A)−vπ(S))]\\nabla_{\\theta} J(\\theta)=\\mathbb{E}_{S \\sim \\eta, A \\sim \\pi}\\left[\\nabla_{\\theta} \\ln \\pi\\left(A | S, \\theta_{t}\\right)\\left(q_{\\pi}(S, A)-v_{\\pi}(S)\\right)\\right]∇θ​J(θ)=ES∼η,A∼π​[∇θ​lnπ(A∣S,θt​)(qπ​(S,A)−vπ​(S))]若要用样本近似这一真实梯度，必须遵循策略 $\\pi(\\theta)$ 来生成动作样本。因此，$\\pi(\\theta)$ 既是行为策略（生成样本的策略），也是我们希望改进的目标策略，故而这些策略梯度方法属于在线方法。若我们已拥有由某一给定行为策略生成的样本，仍可应用策略梯度方法来利用这些样本。要实现这一点，需引入一种名为**重要性采样（importance sampling）**的技术。值得一提的是，重要性采样并非局限于强化学习领域的技术，它是一种通用方法 —— 可利用从某一概率分布中抽取的样本，来估计另一概率分布下随机变量的期望。","summary#Summary":"本章介绍了**演员-评论家（Actor-Critic）**方法，核心内容总结如下：\n10.1 节：介绍了最简单的演员-评论家算法 —— QAC（Q 演员-评论家）。该算法与上一章介绍的策略梯度算法 REINFORCE 结构相似，唯一区别在于：QAC 通过时序差分（TD）学习估计动作价值 $q$，而 REINFORCE 通过蒙特卡洛（Monte Carlo）方法估计。\n10.2 节：将 QAC 扩展为**优势演员-评论家（A2C）**算法。本节证明了 “策略梯度对任意额外基线具有不变性”，并指出 “最优基线可帮助降低梯度估计的方差”。\n10.3 节：通过引入 “重要性采样” 这一关键技术，将优势演员-评论家算法进一步扩展到**离线（off-policy）**场景，实现了对非目标策略（行为策略）生成样本的利用。\n10.4 节：此前介绍的策略梯度算法均依赖随机策略，而本节证明 “策略可强制设为确定性”，并推导了确定性策略的梯度，最终引入了**确定性策略梯度（即确定性演员-评论家）**算法。\n策略梯度方法与演员-评论家方法在现代强化学习中应用广泛，文献中存在大量基于这些方法的进阶算法，例如软演员-评论家（SAC）、信任域策略优化（TRPO）、近端策略优化（PPO）、**双延迟深度确定性策略梯度（TD3）**等。\n此外，单智能体强化学习的框架还可扩展到多智能体强化学习场景；经验样本也可用于拟合系统模型，从而实现模型基强化学习；分布型强化学习则为强化学习提供了与传统框架截然不同的研究视角；强化学习与控制理论的关联也已在相关文献中展开讨论。本门课程无法涵盖所有这些主题，希望书中奠定的基础能帮助读者在未来更深入地学习这些内容。","the-deterministic-policy-gradient-theorem#The deterministic policy gradient theorem":"上一章介绍的策略梯度定理仅适用于随机策略。当我们要求策略为确定性策略时，需推导新的策略梯度定理。\n定理 10.2（确定性策略梯度定理）：目标函数 $J(\\theta)$ 的梯度为：\n∇θJ(θ)=∑s∈Sη(s)∇θμ(s)⋅(∇aqμ(s,a))∣a=μ(s)=ES∼η[∇θμ(S)⋅(∇aqμ(S,a))∣a=μ(S)](10.14)\\begin{aligned} \\nabla_{\\theta} J(\\theta) \u0026= \\sum_{s \\in \\mathcal{S}} \\eta(s) \\nabla_{\\theta} \\mu(s) \\cdot \\left. (\\nabla_{a} q_{\\mu}(s, a)) \\right|_{a=\\mu(s)} \\\\ \u0026= \\mathbb{E}_{S \\sim \\eta} \\left[ \\nabla_{\\theta} \\mu(S) \\cdot \\left. (\\nabla_{a} q_{\\mu}(S, a)) \\right|_{a=\\mu(S)} \\right] \\tag{10.14} \\end{aligned}∇θ​J(θ)​=s∈S∑​η(s)∇θ​μ(s)⋅(∇a​qμ​(s,a))∣a=μ(s)​=ES∼η​[∇θ​μ(S)⋅(∇a​qμ​(S,a))∣a=μ(S)​]​(10.14)其中，$\\eta$ 为状态分布。定理 10.2 是对定理 10.3 与定理 10.4 所呈现结果的总结 —— 这两个定理中梯度的表达式结构相似。目标函数 $J(\\theta)$ 与状态分布 $\\eta$ 的具体表达式可参考定理 10.3 和定理 10.4。\n与随机策略的情况不同，式（10.14）所示的确定性策略梯度中不包含动作随机变量 $A$。因此，当我们用样本近似真实梯度时，无需对动作进行采样。这也使得确定性策略梯度方法天然属于**离线（off-policy）**方法。\n此外，部分读者可能会疑惑：为何不能将 $\\left. (\\nabla_{a} q_{\\mu}(S, a)) \\right|{a=\\mu(S)}$ 写成看似更简洁的 $\\nabla{a} q_{\\mu}(S, \\mu(S))$？原因很简单：若采用后一种写法，将无法明确 $q_{\\mu}(S, \\mu(S))$ 如何作为动作 $a$ 的函数（进而无法对 $a$ 求梯度）。一种更简洁且不易产生混淆的表述可写为 $\\nabla_{a} q_{\\mu}(S, a = \\mu(S))$。\n在本小节的剩余部分，我们将详细推导定理 10.2。具体而言，我们会针对两种常用指标推导梯度：第一种是平均价值（average value），第二种是平均奖励（average reward）。由于这两种指标已在 9.2 节中详细讨论，因此有时会直接使用其性质而不再赘述其证明过程。\n对多数读者而言，只需掌握定理 10.2 的内容即可，无需深入了解推导细节","the-off-policy-policy-gradient-theorem#The off-policy policy gradient theorem":"借助重要性采样技术，我们现在可以给出离线策略梯度定理。假设 $\\beta$ 为行为策略（behavior policy），我们的目标是利用 $\\beta$ 生成的样本学习一个目标策略（target policy） $\\pi$，以最大化下述指标：\nJ(θ)=∑s∈Sdβ(s)vπ(s)=ES∼dβ[vπ(S)]J(\\theta) = \\sum_{s \\in \\mathcal{S}} d_{\\beta}(s) v_{\\pi}(s) = \\mathbb{E}_{S \\sim d_{\\beta}} \\left[ v_{\\pi}(S) \\right]J(θ)=s∈S∑​dβ​(s)vπ​(s)=ES∼dβ​​[vπ​(S)]其中，$d_{\\beta}$ 是策略 $\\beta$ 下的状态平稳分布（stationary distribution），$v_{\\pi}$ 是策略 $\\pi$ 下的状态价值（state value）。该指标的梯度由下述定理给出。\n定理 10.1（离线策略梯度定理）：在折扣因子 $\\gamma \\in (0, 1)$ 的折扣情况下，$J(\\theta)$ 的梯度为：\n∇θJ(θ)=ES∼ρ,A∼β[π(A∣S,θ)β(A∣S)⏟重要性权重（importance weight）∇θln⁡π(A∣S,θ)qπ(S,A)](10.11)\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{S \\sim \\rho, A \\sim \\beta} \\left[ \\underbrace{\\frac{\\pi(A|S, \\theta)}{\\beta(A|S)}}_{\\text{重要性权重（importance weight）}} \\nabla_{\\theta} \\ln \\pi(A|S, \\theta) q_{\\pi}(S, A) \\right] \\tag{10.11}∇θ​J(θ)=ES∼ρ,A∼β​​重要性权重（importance weight）β(A∣S)π(A∣S,θ)​​​∇θ​lnπ(A∣S,θ)qπ​(S,A)​(10.11)其中，状态分布 $\\rho$ 的定义为：\nρ(s)≐∑s′∈Sdβ(s′)Prπ(s∣s′),s∈S\\rho(s) \\doteq \\sum_{s' \\in \\mathcal{S}} d_{\\beta}(s') \\text{Pr}_{\\pi}(s|s'), \\quad s \\in \\mathcal{S}ρ(s)≐s′∈S∑​dβ​(s′)Prπ​(s∣s′),s∈S而 $\\text{Pr}{\\pi}(s|s’) = \\sum{k=0}^{\\infty} \\gamma^k \\left[ P_{\\pi}^k \\right]{s’s} = \\left[ (I - \\gamma P{\\pi})^{-1} \\right]{s’s}$，表示在策略 $\\pi$ 下，从状态 $s’$ 转移到状态 $s$ 的折扣总概率（discounted total probability）（注：$\\left[ P{\\pi}^k \\right]{s’s}$ 表示矩阵 $P{\\pi}^k$ 中第 $s’$ 行、第 $s$ 列的元素，$I$ 为单位矩阵）。\n式（10.11）中的梯度与第 9 章定理 9.1 中在线情况的梯度类似，但存在两处关键差异：\n新增了重要性权重（即 $\\frac{\\pi(A|S, \\theta)}{\\beta(A|S)}$）； 动作 $A$ 服从行为策略 $\\beta$ 的分布（$A \\sim \\beta$），而非在线情况中服从目标策略 $\\pi$ 的分布（$A \\sim \\pi$）。 因此，我们可以利用遵循行为策略 $\\beta$ 生成的动作样本来近似真实梯度。","the-simplest-actor-critic-algorithm-qac#The simplest actor-critic algorithm (QAC)":"回顾策略梯度方法的核心思想：通过最大化标量指标 $J(\\theta)$ 来寻找最优策略。用于最大化 $J(\\theta)$ 的梯度上升算法为：\nθt+1=θt+α∇θJ(θt)=θt+αES∼η,A∼π[∇θln⁡π(A∣S,θt)qπ(S,A)](10.1) \\begin{aligned} \\theta_{t+1} \u0026= \\theta_{t}+\\alpha \\nabla _{\\theta }J(\\theta _{t}) \\\\ \u0026= \\theta _{t}+\\alpha \\mathbb {E}_{S\\sim \\eta ,A\\sim \\pi }\\left[ \\nabla _{\\theta }\\ln \\pi (A|S,\\theta _{t})q_{\\pi }(S,A) \\right] \\tag{10.1} \\end{aligned} θt+1​​=θt​+α∇θ​J(θt​)=θt​+αES∼η,A∼π​[∇θ​lnπ(A∣S,θt​)qπ​(S,A)]​(10.1)其中，$\\eta$ 表示状态分布（更多信息参见定理 9.1）。\n由于真实梯度无法直接获取，我们可使用随机梯度对其进行近似，得到：\nθt+1=θt+α∇θln⁡π(at∣st,θt)qt(st,at)(10.2) \\begin{aligned} \\theta _{t+1}=\\theta _{t}+\\alpha \\nabla _{\\theta }\\ln \\pi (a_{t}|s_{t},\\theta _{t})q_{t}(s_{t},a_{t}) \\tag{10.2} \\end{aligned} θt+1​=θt​+α∇θ​lnπ(at​∣st​,θt​)qt​(st​,at​)​(10.2)该式与第 9 章中式（9.32）给出的算法一致。\n式（10.2）的重要性在于，它清晰地展示了基于策略和基于价值的方法如何结合：\n一方面，它属于基于策略的算法，因为其直接更新策略参数 $\\theta$； 另一方面，该式需要用到 $q_{t}(s_{t}, a_{t})$（即动作价值 $q_{\\pi}(s_{t}, a_{t})$ 的估计值），因此需要另一个基于价值的算法来生成 $q_{t}(s_{t}, a_{t})$。 截至目前，本书已介绍过两种估计动作价值的方法：\n第一种基于蒙特卡洛（Monte Carlo）学习 第二种基于时序差分（Temporal-Difference，TD）学习 若通过蒙特卡洛学习估计 $q_{t}(s_{t}, a_{t})$，对应的算法称为 REINFORCE 算法（或蒙特卡洛策略梯度算法），该算法已在第 9 章中介绍；若通过 TD 学习估计 $q_{t}(s_{t}, a_{t})$，对应的算法通常称为 “演员-评论家算法\"。\n因此，演员-评论家方法可通过 “将基于 TD 的价值估计融入策略梯度方法” 得到。\nAlgorithm 10.1: The simplest actor-critic algorithm (QAC)\nInitialization: A policy function $\\pi(a|s, \\theta_0)$ where $\\theta_0$ is the initial parameter. A value function $q(s, a, w_0)$ where $w_0$ is the initial parameter. $\\alpha_w, \\alpha_{\\theta} \u003e 0$.\nGoal: Learn an optimal policy to maximize $J(\\theta)$.\nAt time step in each episode, do:\nGenerate $a_t$ following $\\pi(a|s_t, \\theta_t)$, observe $r_{t+1}, s_{t+1}$, and then generate $a_{t+1}$ following $\\pi(a|s_{t+1}, \\theta_t)$. Actor (policy update): θt+1=θt+αθ∇θln⁡π(at∣st,θt)q(st,at,wt) \\theta_{t+1} = \\theta_t + \\alpha_{\\theta}\\nabla_{\\theta} \\ln \\pi(a_t|s_t, \\theta_t)q(s_t, a_t, w_t) θt+1​=θt​+αθ​∇θ​lnπ(at​∣st​,θt​)q(st​,at​,wt​) Critic (value update): wt+1=wt+αw[rt+1+γq(st+1,at+1,wt)−q(st,at,wt)]∇wq(st,at,wt) w_{t+1} = w_t + \\alpha_w \\left[ r_{t+1} + \\gamma q(s_{t+1}, a_{t+1}, w_t) - q(s_t, a_t, w_t) \\right] \\nabla_w q(s_t, a_t, w_t) wt+1​=wt​+αw​[rt+1​+γq(st+1​,at+1​,wt​)−q(st​,at​,wt​)]∇w​q(st​,at​,wt​) 最简单的演员-评论家算法流程总结于算法 10.1 中： 评论家（Critic）：对应通过式（8.35）所示的 Sarsa 算法实现的价值更新步骤，动作价值由参数化函数 $q(s, a, w)$ 表示； 演员（Actor）：对应式（10.2）所示的策略更新步骤。 该演员-评论家算法有时也被称为 Q 演员-评论家（Q Actor-Critic，QAC）。尽管结构简单，但 QAC 揭示了演员-评论家方法的核心思想，后续可基于其扩展出多种更复杂的进阶算法。"},"title":"第10章 Actor-Critic 方法"},"/docs/self-study/ai/rl/chapter-11/":{"data":{"proximal-policy-optimization#Proximal Policy Optimization":"Proximal Policy Optimization感觉无论是王树森老师还是赵世钰老师都没有讲到PPO、SFT、DPO、GRPO这些算法，还得额外找其他地方来学习。再加上学习强化学习打基础主要原因是看Falcon的Methodology大败而归，因此需要额外补一些其他的内容：\n小鱼儿at青岛 五道口纳什 这两位B站博主可以很好的入门强化学习上面所提到的RL算法以及VeRL框架之类的，但是Auxiliary Tasks in RL(Jaderberg et al. (2016) “Reinforcement Learning with Unsupervised Auxiliary Tasks”)这些可能还要我自己再找文章看。"},"title":"第11章 PPO"},"/docs/self-study/ai/rl/chapter-2/":{"data":{"":"","1-计算-state-value状态价值v_pis#1. 计算 State Value（状态价值$v_\\pi(s)$）":"状态价值是 “从该状态出发，按策略行动能拿到的平均奖励总和”。我们逐个计算：\n$v_\\pi(s_4)$：$s_4$是终点，每回合拿 1 颗糖，且一直待在这里。所以回报是：\n1+γ×1+γ2×1+⋯=11−γ=11−0.9=101 + \\gamma \\times 1 + \\gamma^2 \\times 1 + \\cdots = \\frac{1}{1-\\gamma} = \\frac{1}{1-0.9} = 101+γ×1+γ2×1+⋯=1−γ1​=1−0.91​=10即$v_\\pi(s_4) = 10$。\n$v_\\pi(s_2)$：$s_2$只能向下走到$s_4$，即时奖励$r=1$，之后$s_4$的价值是 10。所以：\nvπ(s2)=1+γ×vπ(s4)=1+0.9×10=10v_\\pi(s_2) = 1 + \\gamma \\times v_\\pi(s_4) = 1 + 0.9 \\times 10 = 10vπ​(s2​)=1+γ×vπ​(s4​)=1+0.9×10=10$v_\\pi(s_3)$：$s_3$只能向右走到$s_4$，即时奖励$r=1$，之后$s_4$的价值是 10。所以：\nvπ(s3)=1+γ×vπ(s4)=1+0.9×10=10v_\\pi(s_3) = 1 + \\gamma \\times v_\\pi(s_4) = 1 + 0.9 \\times 10 = 10vπ​(s3​)=1+γ×vπ​(s4​)=1+0.9×10=10$v_\\pi(s_1)$：$s_1$只能向右走到$s_2$，即时奖励$r=-1$（负奖励！），之后$s_2$的价值是 10。所以：\nvπ(s1)=−1+γ×vπ(s2)=−1+0.9×10=8v_\\pi(s_1) = -1 + \\gamma \\times v_\\pi(s_2) = -1 + 0.9 \\times 10 = 8vπ​(s1​)=−1+γ×vπ​(s2​)=−1+0.9×10=8重要观察：虽然$v_\\pi(s_1) = 8$是正数，但明显小于其他状态的价值（都是 10）。这是因为$s_1$的策略导致必须经过一个负奖励（$-1$），降低了整体价值。如果策略更差，State Value 可能变成负数，这表示该策略非常不好！","2-计算-action-value动作价值q_pisa#2. 计算 Action Value（动作价值$q_\\pi(s,a)$）":"动作价值是 “在该状态选该动作后，按策略行动能拿到的平均奖励总和”。我们看每个状态的动作：\n$q_\\pi(s_1, \\text{向右})$：选 “向右” 动作，即时奖励$r=-1$，之后到$s_2$（价值 10）。所以：\nqπ(s1,向右)=−1+0.9×10=8q_\\pi(s_1, \\text{向右}) = -1 + 0.9 \\times 10 = 8qπ​(s1​,向右)=−1+0.9×10=8（和$v_\\pi(s_1)$相等，因为$s_1$只有这一个动作）\n$q_\\pi(s_2, \\text{向下})$：选 “向下” 动作，即时奖励$r=1$，之后到$s_4$（价值 10）。所以：\nqπ(s2,向下)=1+0.9×10=10q_\\pi(s_2, \\text{向下}) = 1 + 0.9 \\times 10 = 10qπ​(s2​,向下)=1+0.9×10=10（和$v_\\pi(s_2)$相等，因为$s_2$只有这一个动作）\n$q_\\pi(s_3, \\text{向右})$：选 “向右” 动作，即时奖励$r=1$，之后到$s_4$（价值 10）。所以：\nqπ(s3,向右)=1+0.9×10=10q_\\pi(s_3, \\text{向右}) = 1 + 0.9 \\times 10 = 10qπ​(s3​,向右)=1+0.9×10=10（和$v_\\pi(s_3)$相等，因为$s_3$只有这一个动作）\n$q_\\pi(s_4, \\text{无动作})$：终点无动作，即时奖励$r=1$，之后还在$s_4$（价值 10）。所以：\nqπ(s4,无动作)=1+0.9×10=10q_\\pi(s_4, \\text{无动作}) = 1 + 0.9 \\times 10 = 10qπ​(s4​,无动作)=1+0.9×10=10","21-significance-of-return#2.1 Significance of Return":"左政策使 $s_1$ 直接避开禁止区域，中政策使 $s_1$ 必然进入禁止区域，右政策使 $s_1$ 有 50% 概率进入禁止区域。\n回报等于轨迹上所有奖励的折扣和。从直觉出发，当然是左政策最优、中政策最差对吧？但需通过数学化的回报验证：\n左政策轨迹：$s_1 \\to s_3 \\to s_4 \\to s_4 \\cdots$，折扣回报为 $return_1 = 0 + \\gamma \\cdot 1 + \\gamma^2 \\cdot 1 + \\cdots = \\frac{\\gamma}{1-\\gamma}$ 中政策轨迹：$s_1 \\to s_2 \\to s_4 \\to s_4 \\cdots$，因进入禁止区域获即时负奖励 $-1$，折扣回报为 $return_2 = -1 + \\frac{\\gamma}{1-\\gamma}$； 右政策（随机）：两种轨迹各占 50% 概率，平均回报（期望）为 $return_3 = 0.5 \\times return_2 + 0.5 \\times return_1 = -0.5 + \\frac{\\gamma}{1-\\gamma}$ 对比可知，对任意 $\\gamma$ 均满足 $return_1 \u003e return_3 \u003e return_2$，数学结论与直觉完全一致。","22-calculate-return#2.2 Calculate Return":"（1）Discounted sum of all rewards\n设 $v_i$ 为从 $s_i$ 出发的回报，则：\nv1=r1+γr2+γ2r3+⋯ ,v2=r2+γr3+γ2r4+⋯ ,v3=r3+γr4+γ2r1+⋯ ,v4=r4+γr1+γ2r2+⋯ .\r\\begin{aligned}\rv_1 \u0026= r_1 + \\gamma r_2 + \\gamma^2 r_3 + \\cdots, \\\\\rv_2 \u0026= r_2 + \\gamma r_3 + \\gamma^2 r_4 + \\cdots, \\\\\rv_3 \u0026= r_3 + \\gamma r_4 + \\gamma^2 r_1 + \\cdots, \\\\\rv_4 \u0026= r_4 + \\gamma r_1 + \\gamma^2 r_2 + \\cdots.\r\\end{aligned}\rv1​v2​v3​v4​​=r1​+γr2​+γ2r3​+⋯,=r2​+γr3​+γ2r4​+⋯,=r3​+γr4​+γ2r1​+⋯,=r4​+γr1​+γ2r2​+⋯.​（2）Bootstrapping based\n观察上述表达式可发现，某状态的回报可拆分为 “即时奖励” 与 “未来状态回报的折扣”。例如 $v_1 = r_1 + \\gamma (r_2 + \\gamma r_3 + \\cdots) = r_1 + \\gamma v_2$，同理可得：\nv1=r1+γv2,v2=r2+γv3,v3=r3+γv4,v4=r4+γv1v_1 = r_1 + \\gamma v_2, \\quad v_2 = r_2 + \\gamma v_3, \\quad v_3 = r_3 + \\gamma v_4, \\quad v_4 = r_4 + \\gamma v_1v1​=r1​+γv2​,v2​=r2​+γv3​,v3​=r3​+γv4​,v4​=r4​+γv1​这种 “用其他未知量表示当前未知量” 的思想即自举（Bootstrapping），看似是 “循环依赖”，实则可转化为线性方程组求解：\n将上述方程组写成矩阵形式：\n[v1v2v3v4]=[r1r2r3r4]+γ[0100001000011000][v1v2v3v4]\\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ v_4 \\end{bmatrix} = \\begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\\\ r_4 \\end{bmatrix} + \\gamma \\begin{bmatrix} 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\\\ 1 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ v_4 \\end{bmatrix}​v1​v2​v3​v4​​​=​r1​r2​r3​r4​​​+γ​0001​1000​0100​0010​​​v1​v2​v3​v4​​​记 $\\mathbf{v} = \\begin{bmatrix} v_1 \\ v_2 \\ v_3 \\ v_4 \\end{bmatrix}$，$\\mathbf{r} = \\begin{bmatrix} r_1 \\ r_2 \\ r_3 \\ r_4 \\end{bmatrix}$，$P = \\begin{bmatrix} 0 \u0026 1 \u0026 0 \u0026 0 \\ 0 \u0026 0 \u0026 1 \u0026 0 \\ 0 \u0026 0 \u0026 0 \u0026 1 \\ 1 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix}$，则上式可简写为：\nv=r+γPv\\mathbf{v} = \\mathbf{r} + \\gamma P \\mathbf{v}v=r+γPv其中转移矩阵 $P$ 表示状态转移概率。进一步简化为： v=r+γPvv = r + \\gamma P vv=r+γPv 移项得到：$v - \\gamma P v = r$ 提取公因子：$(I - \\gamma P) v = r$（其中 $I$ 为单位矩阵） 两边同时左乘逆矩阵：$v = (I - \\gamma P)^{-1} r$ 这就是大名鼎鼎的贝尔曼方程的雏形！\n总结一下，计算回报有两个关键思路：\n直接加：把每一步的奖励按时间打折后加起来（比如现在拿 10 分，下一步拿 10 分，折扣 0.9，总回报就是 10 + 10×0.9 + 10×0.9² + …）； 互相借力（自举）：比如从状态 A 出发的回报 = 状态 A 的即时奖励 + 折扣 × 从状态 B 出发的回报（因为 A 下一步会到 B）。 这就像 “想知道 A 的价值，先看看 B 的价值”，虽然绕，但能简化计算。","23-state-value#2.3 State Value":"State Value（状态价值）是 Return 的 “平均版”—— 因为很多时候，从同一个状态出发可能有不同结果（比如随机选动作），没法确定一个固定回报。\n直观理解： 从状态 $s$ 出发，一半概率拿到 10 分回报，一半概率拿到 20 分回报，那 $s$ 的状态价值就是 $(10+20) \\div 2 = 15$ 分。\n重要性质：\n它只和 “状态” 和 “策略” 有关：同一个状态，用不同策略（比如选动作的规则），价值可能不一样； 但和 “什么时候到这个状态” 无关（今天来和明天来，价值相同）。 数学定义： vπ(s)≐E[Gt∣St=s]v_{\\pi}(s) \\doteq \\mathbb{E}\\left[G_{t} \\mid S_{t}=s\\right]vπ​(s)≐E[Gt​∣St​=s]这个定义告诉我们：\n$v_{\\pi}(s)$ 依赖于状态 $s$：定义中以 “从状态 $s$ 出发” 为条件； $v_{\\pi}(s)$ 依赖于策略 $\\pi$：轨迹由策略 $\\pi$ 生成，不同策略对应不同状态价值； $v_{\\pi}(s)$ 与时间步 $t$ 无关：状态价值由策略决定，与当前时刻无关。","24-bellman-equation#2.4 Bellman Equation":"贝尔曼方程就是把 State Value 和 Bootstrapping 结合起来的简单规则，本质就一句话：\n一个状态的价值 = 现在能拿到的平均奖励 + 未来能拿到的平均价值（打折扣）\n拆成两步理解：\n现在的奖励：在这个状态选动作，能立刻拿到的平均分数（比如选动作 $a_1$ 拿 5 分，选动作 $a_2$ 拿 3 分，策略里 $a_1$ 选 80%、$a_2$ 选 20%，平均就是 $5 \\times 0.8 + 3 \\times 0.2 = 4.6$ 分）； 未来的价值：选完动作会转到下一个状态，下一个状态的价值乘以折扣（比如下一个状态价值 10 分，折扣 $\\gamma = 0.9$，就是 $10 \\times 0.9 = 9$ 分）； 加起来就是当前状态的价值：$4.6 + 9 = 13.6$ 分。 整个逻辑串起来：想算状态价值，不用硬加所有未来奖励，只要知道 “现在能拿多少” 和 “下一步状态值多少”，用贝尔曼方程就能算。","25-action-value#2.5 Action Value":"状态 - 动作对$(s,a)$的动作价值，记为$q_{\\pi}(s,a)$，定义为：\nqπ(s,a)≐E[Gt∣St=s,At=a]q_{\\pi}(s,a) \\doteq \\mathbb{E}\\left[G_t | S_t=s, A_t=a\\right]qπ​(s,a)≐E[Gt​∣St​=s,At​=a]即 “在状态s采取动作a后，遵循策略$\\pi$获得的期望回报”。\n状态价值与动作价值的关系：\n状态价值是动作价值的期望（按策略概率加权）：\nvπ(s)=∑a∈Aπ(a∣s)qπ(s,a)(2.13)v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_{\\pi}(s,a) \\quad (2.13)vπ​(s)=a∈A∑​π(a∣s)qπ​(s,a)(2.13)动作价值依赖即时奖励和未来状态价值：对比式 (2.13) 与贝尔曼方程（式 2.7），可得：\nqπ(s,a)=∑r∈Rp(r∣s,a)r+γ∑s′∈Sp(s′∣s,a)vπ(s′)(2.14)q_{\\pi}(s,a) = \\sum_{r \\in \\mathcal{R}} p(r|s,a) r + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s,a) v_{\\pi}(s') \\quad (2.14)qπ​(s,a)=r∈R∑​p(r∣s,a)r+γs′∈S∑​p(s′∣s,a)vπ​(s′)(2.14)","26-summary#2.6 Summary":"","2bellman-equation#2、Bellman Equation":"来来来看看这是什么Value？State Value！\n有了State Value，我们才能量化评估\"遵循某一策略时，从某状态出发的长期收益期望\"。","3-验证贝尔曼方程#3. 验证贝尔曼方程":"贝尔曼方程的核心是 “当前状态价值 = 所有动作的（即时奖励 + 未来状态价值折扣）的加权平均”。以$s_1$为例：\nvπ(s1)=∑aπ(a∣s1)[∑rp(r∣s1,a)r+γ∑s′p(s′∣s1,a)vπ(s′)]=1×[(−1)+0.9×vπ(s2)]=−1+0.9×10=8\r\\begin{aligned}\rv_\\pi(s_1) \u0026= \\sum_a \\pi(a|s_1) \\left[ \\sum_r p(r|s_1,a)r + \\gamma \\sum_{s'} p(s'|s_1,a) v_\\pi(s') \\right] \\\\\r\u0026= 1 \\times \\left[ (-1) + 0.9 \\times v_\\pi(s_2) \\right] \\\\\r\u0026= -1 + 0.9 \\times 10 = 8\r\\end{aligned}\rvπ​(s1​)​=a∑​π(a∣s1​)[r∑​p(r∣s1​,a)r+γs′∑​p(s′∣s1​,a)vπ​(s′)]=1×[(−1)+0.9×vπ​(s2​)]=−1+0.9×10=8​（因为$s_1$只有一个动作，$\\pi(\\text{向右}|s_1)=1$，$p(r=-1|s_1,\\text{向右})=1$，$p(s_2|s_1,\\text{向右})=1$）","4-不同策略的影响#4. 不同策略的影响":"关键点：不同的策略会导致不同的 State Value！\n假设我们改变策略，让$s_1$可以向左走（如果允许的话），但向左走会导致更大的负奖励$r=-5$。那么：\nvπ′(s1)=−5+γ×vπ′(s2)=−5+0.9×10=4v'_\\pi(s_1) = -5 + \\gamma \\times v'_\\pi(s_2) = -5 + 0.9 \\times 10 = 4vπ′​(s1​)=−5+γ×vπ′​(s2​)=−5+0.9×10=4可以看到，这个新策略的 State Value（4）比原来的策略（8）更差。\n更极端的情况：如果某个策略导致 State Value 为负数，比如：\nvπ′′(s1)=−15+γ×vπ′′(s2)=−15+0.9×10=−6v''_\\pi(s_1) = -15 + \\gamma \\times v''_\\pi(s_2) = -15 + 0.9 \\times 10 = -6vπ′′​(s1​)=−15+γ×vπ′′​(s2​)=−15+0.9×10=−6这表示该策略非常差！从$s_1$出发，按这个策略行动，长期来看会得到负的累积奖励，说明这个策略不应该被采用。\n总结：\nState Value 为正且较大：策略好，能够获得正收益 State Value 为正但较小：策略一般，收益有限 State Value 为负：策略很差，应该避免使用","一state-value状态价值-vs-action-value动作价值#一、State Value（状态价值） vs. Action Value（动作价值）":"想象你在玩一款 “迷宫寻宝游戏”：\nState Value（状态价值） $v_\\pi(s)$：你现在在迷宫的某个路口（状态s），按照策略$\\pi$（比如 “50% 概率走左路，50% 概率走右路” 的选路规则）行动，最终能拿到的 “平均宝藏总和”（把未来找到的宝藏按 “时间折扣” 加起来的平均值）。它回答的是：“这个路口本身有多’值钱’？”\nAction Value（动作价值） $q_\\pi(s,a)$：你现在在迷宫的某个路口（状态s），先选一个具体动作a（比如 “走左路”），之后再按照策略$\\pi$行动，最终能拿到的 “平均宝藏总和”。它回答的是：“在这个路口选这个动作有多’值钱’？”","三矩阵迷宫里的-policy-和-action#三、矩阵迷宫里的 Policy 和 Action":"图中的四个格子是四个状态（$s_1, s_2, s_3, s_4$），绿色箭头是 “策略（Policy）选择的动作（Action）”，两者是 “规则→执行” 的关系，不冲突：\nPolicy（策略）：是 “在每个状态下选什么动作的规则”。比如：\n状态$s_1$的策略：选 “向右移动” 的动作； 状态$s_2$的策略：选 “向下移动” 的动作； 状态$s_3$的策略：选 “向右移动” 的动作； 状态$s_4$的策略：无（因为是终点，用圆圈标记）。 Action（动作）：是策略的具体执行。比如$s_1$的 “向右移动” 就是一个动作，执行后转移到$s_2$；$s_2$的 “向下移动” 是一个动作，执行后转移到$s_4$，以此类推。","二state-value-和-action-value-的-互相推导#二、State Value 和 Action Value 的 \u0026ldquo;互相推导\u0026rdquo;":"它们是 “整体” 和 “部分” 的关系，能通过公式互相计算：\n从 Action Value 算 State Value：\nvπ(s)=∑aπ(a∣s) qπ(s,a)v_\\pi(s) = \\sum_a \\pi(a|s) \\, q_\\pi(s,a)vπ​(s)=a∑​π(a∣s)qπ​(s,a)解释：策略$\\pi$会以概率$\\pi(a|s)$选每个动作a，所以 “状态s的价值” 就是 “所有动作a的价值$q_\\pi(s,a)$按概率加权后的平均值”。\n类比：路口有两条路，左路（概率 60%，价值 10）、右路（概率 40%，价值 8），则状态价值 = $0.6×10 + 0.4×8 = 9.2$。\n从 State Value 算 Action Value：\nqπ(s,a)=∑rp(r∣s,a)r+γ∑s′p(s′∣s,a)vπ(s′)q_\\pi(s,a) = \\sum_r p(r|s,a)r + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s')qπ​(s,a)=r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)解释：“动作a的价值” 由两部分组成：\n选动作a后立刻拿到的平均奖励（比如走左路后立刻捡到 1 个金币）； 选动作a后转移到下一个路口$s’$，下一个路口的价值的折扣平均值（比如下一个路口价值 10，折扣率$\\gamma=0.9$，则这部分价值是$0.9×10=9$）。 所以$q_\\pi$就是 Action Value。","四贝尔曼公式的作用#四、贝尔曼公式的作用":"贝尔曼方程是强化学习的 “核心计算器”，专门用来计算 State Value。它的逻辑很简单：\n当前状态的价值=所有动作的「即时奖励 + 未来状态价值的折扣」的加权平均\\text{当前状态的价值} = \\text{所有动作的「即时奖励 + 未来状态价值的折扣」的加权平均}当前状态的价值=所有动作的「即时奖励 + 未来状态价值的折扣」的加权平均这也是上述公式的核心 —— 通过不断把 “当前状态” 和 “未来状态” 的价值关联起来，就能算出每个状态到底多 “值钱”。\n如果还是觉得抽象，再举个 “点外卖” 的例子：\nState Value：你现在饿了（状态s），按照 “50% 点美团、50% 点饿了么” 的策略，最终吃到饭的 “满足感总和”（美团满足感 8，饿了么 6，状态价值 = $0.5×8 + 0.5×6 = 7$）。\nAction Value：你现在饿了（状态s），选 “点美团”（动作a），之后不管选啥，最终满足感的总和（比如点美团后满足感 8，那动作价值$q_\\pi(s, \\text{美团})=8$）。","基于动作价值的贝尔曼方程#基于动作价值的贝尔曼方程":"将式 (2.13) 代入式 (2.14)，得到动作价值的贝尔曼方程：\nqπ(s,a)=∑r∈Rp(r∣s,a)r+γ∑s′∈Sp(s′∣s,a)∑a′∈A(s′)π(a′∣s′)qπ(s′,a′)q_{\\pi}(s,a) = \\sum_{r \\in \\mathcal{R}} p(r|s,a) r + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s,a) \\sum_{a' \\in \\mathcal{A}(s')} \\pi(a'|s') q_{\\pi}(s',a')qπ​(s,a)=r∈R∑​p(r∣s,a)r+γs′∈S∑​p(s′∣s,a)a′∈A(s′)∑​π(a′∣s′)qπ​(s′,a′)矩阵 - 向量形式：\nqπ=r~+γPΠqπ(2.15)q_{\\pi} = \\tilde{r} + \\gamma P \\Pi q_{\\pi} \\quad (2.15)qπ​=r~+γPΠqπ​(2.15)其中：\n$q_{\\pi}$：动作价值向量（索引为状态 - 动作对）； $\\tilde{r}$：即时奖励向量（$[\\tilde{r}]{(s,a)} = \\sum{r \\in \\mathcal{R}} p(r|s,a) r$）； $P$：转移概率矩阵（$[P]_{(s,a),s’} = p(s’|s,a)$）； $\\Pi$：块对角矩阵（$\\Pi_{(s’,a’)} = \\pi(a’|s’)$，其余元素为 0）。 特点：$\\tilde{r}$和$P$仅由系统模型决定，与策略无关；策略嵌入在$\\Pi$中。该方程是压缩映射，存在唯一解，可通过迭代求解。","实际例子咖啡屋寻宝游戏#实际例子：咖啡屋寻宝游戏":"你在一家咖啡屋玩寻宝游戏，咖啡屋有 4 个区域（对应 4 个状态$s_1, s_2, s_3, s_4$），每个区域有不同的 “移动规则” 和 “奖励”：\n$s_1$（左上角）：只能向右走，奖励$r=-1$（因为走过去要花 1 分钟，是负奖励） $s_2$（右上角）：只能向下走，奖励$r=1$（因为能捡到 1 颗糖） $s_3$（左下角）：只能向右走，奖励$r=1$（因为能捡到 1 颗糖） $s_4$（右下角）：终点，无动作，奖励$r=1$（因为待在这里每回合都能拿 1 颗糖） 折扣率$\\gamma=0.9$（未来的奖励没现在值钱，打 9 折）。\n策略$\\pi$：在每个状态下 “只能选唯一的动作”（比如$s_1$只能选 “向右”，$s_2$只能选 “向下”）。","数学推导过程#数学推导过程":"第一步：将回报 $G_t$ 进行分解\n我们知道回报的定义是： Gt=Rt+1+γRt+2+γ2Rt+3+⋯G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdotsGt​=Rt+1​+γRt+2​+γ2Rt+3​+⋯可以将它分解为： Gt=Rt+1+γRt+2+γ2Rt+3+⋯=Rt+1+γ(Rt+2+γRt+3+⋯ )=Rt+1+γGt+1\r\\begin{aligned}\rG_{t} \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + \\cdots \\\\\r\u0026= R_{t+1} + \\gamma \\left(R_{t+2} + \\gamma R_{t+3} + \\cdots\\right) \\\\\r\u0026= R_{t+1} + \\gamma G_{t+1}\r\\end{aligned}\rGt​​=Rt+1​+γRt+2​+γ2Rt+3​+⋯=Rt+1​+γ(Rt+2​+γRt+3​+⋯)=Rt+1​+γGt+1​​为什么要这样分解？ 因为我们想把 “从 $t$ 时刻开始的回报” 拆成 “$t+1$ 时刻的即时奖励” 和 “从 $t+1$ 时刻开始的未来回报”。这样就能用自举的思想：用 $G_{t+1}$ 来表示 $G_t$。\n第二步：对两边取条件期望\n条件期望 $\\mathbb{E}[X \\mid Y=y]$ 的含义是：在已知 $Y=y$ 的条件下，随机变量 $X$ 的平均值。比如 “在状态 $s$ 的条件下，回报 $G_t$ 的期望值”。\n对第一步的等式两边取条件期望（以 $S_t=s$ 为条件）： vπ(s)=E[Gt∣St=s]=E[Rt+1+γGt+1∣St=s]=E[Rt+1∣St=s]+γE[Gt+1∣St=s](2.4)\r\\begin{aligned}\rv_{\\pi}(s) \u0026= \\mathbb{E}\\left[G_{t} \\mid S_{t}=s\\right] \\\\\r\u0026= \\mathbb{E}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\\\\r\u0026= \\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s\\right]+\\gamma \\mathbb{E}\\left[G_{t+1} \\mid S_{t}=s\\right] \\quad (2.4)\r\\end{aligned}\rvπ​(s)​=E[Gt​∣St​=s]=E[Rt+1​+γGt+1​∣St​=s]=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s](2.4)​为什么期望可以拆开？ 因为期望的线性性质：$\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y]$，即使有条件也成立。\n现在我们需要分别计算这两项：$\\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s\\right]$ 和 $\\mathbb{E}\\left[G_{t+1} \\mid S_{t}=s\\right]$。\n第三步：计算 “现在能拿到的平均奖励”\nE[Rt+1∣St=s]=∑a∈Aπ(a∣s)E[Rt+1∣St=s,At=a]=∑a∈Aπ(a∣s)∑r∈Rp(r∣s,a)r(2.5)\r\\begin{aligned}\r\\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s\\right] \u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s, A_{t}=a\\right] \\\\\r\u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} p(r \\mid s, a) r \\quad (2.5)\r\\end{aligned}\rE[Rt+1​∣St​=s]​=a∈A∑​π(a∣s)E[Rt+1​∣St​=s,At​=a]=a∈A∑​π(a∣s)r∈R∑​p(r∣s,a)r(2.5)​这一步在做什么？\n第一行：我们想算 “在状态 $s$ 下，能拿到多少平均奖励”。但奖励不仅依赖状态，还依赖动作。所以我们要考虑所有可能的动作 $a$，每个动作按策略 $\\pi(a\\mid s)$ 的概率被选中，然后计算 “在状态 $s$ 且选动作 $a$ 的条件下，能拿到多少平均奖励”。\n第二行：进一步，“在状态 $s$ 且选动作 $a$ 的条件下，能拿到多少平均奖励” 就是所有可能的奖励值 $r$，按概率 $p(r\\mid s,a)$ 加权平均，即 $\\sum_{r \\in \\mathcal{R}} p(r \\mid s, a) r$。\n举个例子：假设在状态 $s$，策略是：80% 选动作 $a_1$（能拿 5 分），20% 选动作 $a_2$（能拿 3 分）。那么平均奖励 = $0.8 \\times 5 + 0.2 \\times 3 = 4.6$ 分。\n第四步：计算 “未来能拿到的平均价值”\n利用马尔可夫性（未来回报仅依赖当前状态，与历史无关），可得： E[Gt+1∣St=s]=∑s′∈SE[Gt+1∣St+1=s′]p(s′∣s)=∑s′∈Svπ(s′)∑a∈Ap(s′∣s,a)π(a∣s)(2.6)\r\\begin{aligned}\r\\mathbb{E}\\left[G_{t+1} \\mid S_{t}=s\\right] \u0026= \\sum_{s' \\in \\mathcal{S}} \\mathbb{E}\\left[G_{t+1} \\mid S_{t+1}=s'\\right] p\\left(s' \\mid s\\right) \\\\\r\u0026= \\sum_{s' \\in \\mathcal{S}} v_{\\pi}\\left(s'\\right) \\sum_{a \\in \\mathcal{A}} p\\left(s' \\mid s, a\\right) \\pi(a \\mid s) \\quad (2.6)\r\\end{aligned}\rE[Gt+1​∣St​=s]​=s′∈S∑​E[Gt+1​∣St+1​=s′]p(s′∣s)=s′∈S∑​vπ​(s′)a∈A∑​p(s′∣s,a)π(a∣s)(2.6)​这一步在做什么？\n第一行：我们想算 “在状态 $s$ 下，未来能拿到多少平均价值”。但未来价值取决于下一步会到哪个状态 $s’$。所以我们要考虑所有可能的下一个状态 $s’$，按转移概率 $p(s’\\mid s)$ 加权，然后计算 “在状态 $s’$ 的条件下，能拿到多少未来价值”。\n关键点：$\\mathbb{E}\\left[G_{t+1} \\mid S_{t+1}=s’\\right]$ 就是状态价值 $v_{\\pi}(s’)$ 的定义！因为 $G_{t+1}$ 就是从 $t+1$ 时刻开始的回报，在状态 $s’$ 的条件下，它的期望就是 $v_{\\pi}(s’)$。\n第二行：转移概率 $p(s’\\mid s)$ 需要进一步分解。因为转移不仅依赖状态，还依赖动作。所以 $p(s’\\mid s) = \\sum_{a \\in \\mathcal{A}} p(s’\\mid s,a) \\pi(a\\mid s)$，即所有可能的动作 $a$，按策略 $\\pi(a\\mid s)$ 的概率被选中，然后计算在动作 $a$ 下从 $s$ 转移到 $s’$ 的概率。\n举个例子：假设在状态 $s$，下一步可能到 $s_1’$（概率 0.6，价值 10 分）或 $s_2’$（概率 0.4，价值 8 分）。那么未来平均价值 = $0.6 \\times 10 + 0.4 \\times 8 = 9.2$ 分。\n第五步：得到贝尔曼方程的最终形式\n将式 (2.5) 和 (2.6) 代入式 (2.4)，得到： vπ(s)=∑a∈Aπ(a∣s)[∑r∈Rp(r∣s,a)r+γ∑s′∈Sp(s′∣s,a)vπ(s′)],∀s∈S(2.7)\r\\begin{aligned}\rv_{\\pi}(s) \u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left[\\sum_{r \\in \\mathcal{R}} p(r \\mid s, a) r+\\gamma \\sum_{s' \\in \\mathcal{S}} p\\left(s' \\mid s, a\\right) v_{\\pi}\\left(s'\\right)\\right], \\\\\r\u0026\\forall s \\in \\mathcal{S} \\quad (2.7)\r\\end{aligned}\rvπ​(s)​=a∈A∑​π(a∣s)[r∈R∑​p(r∣s,a)r+γs′∈S∑​p(s′∣s,a)vπ​(s′)],∀s∈S(2.7)​这个公式的直观理解：\n外层求和：对所有可能的动作 $a$ 求和，按策略 $\\pi(a\\mid s)$ 加权； 内层第一项：选动作 $a$ 后能拿到的平均即时奖励； 内层第二项：选动作 $a$ 后，会转移到某个状态 $s’$，$s’$ 的价值乘以折扣 $\\gamma$，再对所有可能的 $s’$ 按概率加权平均。 关键说明：\n贝尔曼方程是 “方程组”：针对所有状态 $s \\in \\mathcal{S}$ 建立方程，联合求解所有状态价值； 包含已知量和未知量： 已知量：策略 $\\pi(a\\mid s)$、系统模型 $p(r\\mid s,a)$ 和 $p(s’\\mid s,a)$； 未知量：所有状态价值 $v_{\\pi}(s)$； 核心意义：状态价值由 “即时奖励的期望” 和 “未来状态价值的折扣期望” 组成。 等价形式：\n结合全概率公式 $p(s’\\mid s,a)=\\sum_{r \\in \\mathcal{R}} p(s’,r\\mid s,a)$，可改写为： vπ(s)=∑a∈Aπ(a∣s)∑s′∈S∑r∈Rp(s′,r∣s,a)[r+γvπ(s′)]v_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p\\left(s', r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s'\\right)\\right]vπ​(s)=a∈A∑​π(a∣s)s′∈S∑​r∈R∑​p(s′,r∣s,a)[r+γvπ​(s′)]若奖励仅依赖下一状态（$r=r(s’)$），则 $p(r(s’)\\mid s,a)=p(s’\\mid s,a)$，代入得： vπ(s)=∑a∈Aπ(a∣s)∑s′∈Sp(s′∣s,a)[r(s′)+γvπ(s′)]v_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} p\\left(s' \\mid s, a\\right)\\left[r(s')+\\gamma v_{\\pi}\\left(s'\\right)\\right]vπ​(s)=a∈A∑​π(a∣s)s′∈S∑​p(s′∣s,a)[r(s′)+γvπ​(s′)]","求解贝尔曼方程的state-value#求解贝尔曼方程的State Value":"由矩阵 - 向量形式$v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}$，整理得闭式解：\nvπ=(I−γPπ)−1rπv_{\\pi} = \\left(I - \\gamma P_{\\pi}\\right)^{-1} r_{\\pi}vπ​=(I−γPπ​)−1rπ​当然这个逆太难顶了，所以一般不这么做，而是迭代解：\nvk+1=rπ+γPπvk,k=0,1,2,⋯(2.11)v_{k+1} = r_{\\pi} + \\gamma P_{\\pi} v_k, \\quad k=0,1,2,\\cdots \\quad (2.11)vk+1​=rπ​+γPπ​vk​,k=0,1,2,⋯(2.11)其中$v_0$是初始猜测值。\n收敛性：当$k \\to \\infty$时，$v_k$收敛到闭式解：$v_k \\to v_{\\pi} = \\left(I - \\gamma P_{\\pi}\\right)^{-1} r_{\\pi}$。\n收敛性证明（简要）：定义误差$\\delta_k = v_k - v_{\\pi}$，代入迭代式得$\\delta_{k+1} = \\gamma P_{\\pi} \\delta_k$。由于$\\gamma \u003c 1$且$P_{\\pi}$元素非负且不大于 1，故$\\delta_k \\to 0$，迭代收敛。","矩阵与向量形式#矩阵与向量形式":"可将所有状态的方程整合为矩阵 - 向量形式，便于分析和求解。\n定义：\n状态价值向量：$v_{\\pi} = [v_{\\pi}(s_1), v_{\\pi}(s_2), \\cdots, v_{\\pi}(s_n)]^T \\in \\mathbb{R}^n$（$n=|\\mathcal{S}|$为状态数） 即时奖励均值向量：$r_{\\pi} = [r_{\\pi}(s_1), r_{\\pi}(s_2), \\cdots, r_{\\pi}(s_n)]^T$，其中$r_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\sum_{r \\in \\mathcal{R}} p(r|s,a) r$ 转移概率矩阵：$P_{\\pi} \\in \\mathbb{R}^{n \\times n}$，其中$[P_{\\pi}]{ij} = p{\\pi}(s_j|s_i) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s_i) p(s_j|s_i,a)$（策略$\\pi$下从$s_i$转移到$s_j$的概率） 矩阵 - 向量形式：\nvπ=rπ+γPπvπv_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}vπ​=rπ​+γPπ​vπ​"},"title":"第2章 贝尔曼公式"},"/docs/self-study/ai/rl/chapter-3/":{"data":{"":"","31-最优策略与最优状态价值#3.1 最优策略与最优状态价值":"让我们从直觉出发理解什么是\"最优\"。想象你在玩一个迷宫游戏，每个路口都有多个选择，每个选择会带来不同的奖励。最优策略就是：在所有可能的路口，都选择那个能带来最大长期收益的路。\n用数学语言表达：如果存在一个策略$\\pi^$，使得对于任意状态$s$，都有$v_{\\pi^}(s) \\geq v_{\\pi}(s)$对所有其他策略$\\pi$成立，那么$\\pi^$就是最优策略。最优策略对应的状态价值$v^ = v_{\\pi^*}$称为最优状态价值。\n这里有个关键问题：最优策略一定存在吗？如果存在，如何找到它？这正是贝尔曼最优方程（Bellman Optimality Equation, BOE）要回答的。","32-贝尔曼最优方程#3.2 贝尔曼最优方程":"回想第2章的贝尔曼方程，它告诉我们：给定策略$\\pi$，状态价值等于即时奖励的期望加上未来状态价值的折扣期望。但贝尔曼方程只能评估固定策略，不能帮我们找最优策略。\n贝尔曼最优方程的核心思想是：如果我们知道了最优状态价值$v^*$，那么最优策略就是在每个状态选择能最大化\"即时奖励 + 未来最优状态价值折扣\"的动作。\n具体来说，对于状态$s$，最优策略应该选择动作$a$使得：\nq∗(s,a)=∑rp(r∣s,a)r+γ∑s′p(s′∣s,a)v∗(s′)q^*(s,a) = \\sum_r p(r|s,a)r + \\gamma \\sum_{s'} p(s'|s,a)v^*(s')q∗(s,a)=r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)v∗(s′)达到最大。这里$q^*(s,a)$称为最优动作价值，表示在状态$s$选择动作$a$后，按照最优策略行动能获得的期望回报。\n最优状态价值$v^*(s)$应该等于所有可能动作中最优动作价值的最大值：\nv∗(s)=max⁡a∈Aq∗(s,a)=max⁡a∈A[∑rp(r∣s,a)r+γ∑s′p(s′∣s,a)v∗(s′)]v^*(s) = \\max_{a \\in \\mathcal{A}} q^*(s,a) = \\max_{a \\in \\mathcal{A}} \\left[ \\sum_r p(r|s,a)r + \\gamma \\sum_{s'} p(s'|s,a)v^*(s') \\right]v∗(s)=a∈Amax​q∗(s,a)=a∈Amax​[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)v∗(s′)]这就是贝尔曼最优方程的标量形式。将所有状态写成向量形式，可以得到：\nv∗=max⁡π∈Π(rπ+γPπv∗)v^* = \\max_{\\pi \\in \\Pi}(r_{\\pi} + \\gamma P_{\\pi}v^*)v∗=π∈Πmax​(rπ​+γPπ​v∗)其中$\\Pi$表示所有可能策略的集合。这个方程告诉我们：最优状态价值$v^*$必须满足\"对所有策略取最大值\"的条件。\n这个方程看起来很合理，但有两个关键问题需要回答：这个方程有解吗？如果有解，如何求解？ 这需要用到压缩映射定理。","33-压缩映射定理#3.3 压缩映射定理":"要理解贝尔曼最优方程为什么有唯一解，我们需要先理解压缩映射。压缩映射的核心思想是：映射后两点的距离比映射前更近，且收缩比例固定（小于1）。\n数学定义：若存在常数$\\gamma \\in [0,1)$，对任意两个向量$x_1, x_2$，都有：\n∥f(x1)−f(x2)∥≤γ∥x1−x2∥\\|f(x_1) - f(x_2)\\| \\leq \\gamma \\|x_1 - x_2\\|∥f(x1​)−f(x2​)∥≤γ∥x1​−x2​∥则$f$是压缩映射。关键在于距离收缩比例$\\gamma$固定且小于1 —— 无论$x_1, x_2$在哪里，映射后它们的距离都会缩小到原来的$\\gamma$倍以内。\n举个例子，$f(x) = 0.5x$就是压缩映射，因为对任意$x_1, x_2$，有$|f(x_1) - f(x_2)| = 0.5|x_1 - x_2|$，固定收缩比例$\\gamma = 0.5 \u003c 1$。但$f(x) = \\ln x$不是压缩映射，因为收缩比例不固定：取$x_1 = 1, x_2 = e$时，收缩比例约为$0.58$；取$x_1 = e, x_2 = e^2$时，收缩比例约为$0.21$，不满足\"固定$\\gamma \u003c 1$“的要求。\n压缩映射定理（定理3.1）告诉我们：对于方程$x = f(x)$，如果$f$是压缩映射，那么存在唯一的不动点$x^$满足$f(x^) = x^*$，且可以通过迭代法$x_{k+1} = f(x_k)$求解，收敛速度是指数级的。\n为什么压缩映射能保证收敛？核心逻辑是：压缩映射让迭代序列的相邻项距离指数级变小，最终汇聚到不动点$x^*$。\n假设我们从初始值$x_0$开始迭代，由于压缩映射的性质，相邻项的距离满足：\n∥xk+1−xk∥=∥f(xk)−f(xk−1)∥≤γ∥xk−xk−1∥\\|x_{k+1} - x_k\\| = \\|f(x_k) - f(x_{k-1})\\| \\leq \\gamma \\|x_k - x_{k-1}\\|∥xk+1​−xk​∥=∥f(xk​)−f(xk−1​)∥≤γ∥xk​−xk−1​∥递推下去，第$k$步时：$|x_{k+1} - x_k| \\leq \\gamma^k |x_1 - x_0|$。由于$\\gamma \u003c 1$（比如$\\gamma = 0.9$），$\\gamma^k$会随着$k$增大而指数级变小（例如$\\gamma^{10} \\approx 0.35$，$\\gamma^{20} \\approx 0.12$，$\\gamma^{100} \\approx 2.66 \\times 10^{-5}$），这意味着迭代序列的相邻项会越来越近。\n要证明序列一定会汇聚到某个点$x^*$，我们需要说明\"当$k$足够大时，序列中所有后面的项都挤在一起”。取任意两个足够大的项$x_m$和$x_n$（$m \u003e n$），它们的距离可以拆成相邻项距离的和。根据三角不等式：\n∥xm−xn∥≤∥xm−xm−1∥+∥xm−1−xm−2∥+⋯+∥xn+1−xn∥\\|x_m - x_n\\| \\leq \\|x_m - x_{m-1}\\| + \\|x_{m-1} - x_{m-2}\\| + \\dots + \\|x_{n+1} - x_n\\|∥xm​−xn​∥≤∥xm​−xm−1​∥+∥xm−1​−xm−2​∥+⋯+∥xn+1​−xn​∥代入前面的递推结果，这是一个等比数列求和：\n∥xm−xn∥≤γn⋅1−γm−n1−γ∥x1−x0∥≤γn1−γ∥x1−x0∥\\|x_m - x_n\\| \\leq \\gamma^n \\cdot \\frac{1 - \\gamma^{m-n}}{1 - \\gamma} \\|x_1 - x_0\\| \\leq \\frac{\\gamma^n}{1 - \\gamma} \\|x_1 - x_0\\|∥xm​−xn​∥≤γn⋅1−γ1−γm−n​∥x1​−x0​∥≤1−γγn​∥x1​−x0​∥当$n$足够大时，$\\gamma^n$指数级趋近于0，所以$|x_m - x_n|$会变得任意小。这意味着序列后面的项都挤在一起，必然会收敛到某个固定点$x^*$。\n结论：只要$f$是压缩映射，迭代序列$x_{k+1}=f(x_k)$就一定会收敛到唯一的不动点$x^$，且收敛速度是指数级。这正是贝尔曼最优方程能稳定求解最优状态价值$v^$的核心原因（因为BOE右侧的$f(v)$是压缩映射）。","34-从boe求解最优策略的完整推导#3.4 从BOE求解最优策略的完整推导":"现在我们已经理解了压缩映射定理。接下来要证明：贝尔曼最优方程右侧的函数$f(v) = \\max_{\\pi \\in \\Pi}(r_{\\pi} + \\gamma P_{\\pi}v)$是压缩映射（定理3.2），从而保证BOE有唯一解。\n这个证明虽然重要，但技术细节较多。核心思路是：通过分析$f(v)$在最大范数下的行为，可以证明对任意两个状态价值向量$v_1, v_2$，有：\n∥f(v1)−f(v2)∥∞≤γ∥v1−v2∥∞\\|f(v_1) - f(v_2)\\|_{\\infty} \\leq \\gamma \\|v_1 - v_2\\|_{\\infty}∥f(v1​)−f(v2​)∥∞​≤γ∥v1​−v2​∥∞​其中$|\\cdot|_{\\infty}$是最大范数（向量中所有元素绝对值的最大值）。由于$\\gamma \u003c 1$，$f$确实是压缩映射。\n定理3.3（BOE解的存在性、唯一性与求解算法）：由于$f(v)$是压缩映射，根据压缩映射定理，方程$v = f(v)$（即BOE）必有唯一不动点$v^*$（最优状态价值），且可通过迭代算法求解：\nvk+1=max⁡π∈Π(rπ+γPπvk),k=0,1,2,...v_{k+1} = \\max_{\\pi \\in \\Pi}(r_{\\pi} + \\gamma P_{\\pi}v_k), \\quad k = 0,1,2,...vk+1​=π∈Πmax​(rπ​+γPπ​vk​),k=0,1,2,...对任意初始值$v_0$，$v_k$会指数级收敛到$v^*$。这就是后续第4章\"价值迭代算法\"的数学根源。\n有了唯一的$v^$后，下一步是**如何从$v^$得到最优策略$\\pi^*$（定理3.5）**。核心是将\"对策略$\\pi$的最大化\"转化为\"对动作$a$的最大化\"。\n对于状态$s$，策略$\\pi$的期望奖励与转移可以拆分为\"每个动作的加权和\"：\nrπ(s)+γ(Pπv∗)(s)=∑a∈Aπ(a∣s)⋅q∗(s,a)r_{\\pi}(s) + \\gamma (P_{\\pi}v^*)(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\cdot q^*(s,a)rπ​(s)+γ(Pπ​v∗)(s)=a∈A∑​π(a∣s)⋅q∗(s,a)其中$q^(s,a) = \\sum_r p(r|s,a)r + \\gamma \\sum_{s’} p(s’|s,a)v^(s’)$是最优动作价值。\n要最大化$\\sum_{a} \\pi(a|s)q^(s,a)$（约束：$\\sum_a \\pi(a|s)=1$，$\\pi(a|s) \\geq 0$），根据\"权重集中在最大项上\"的数学性质，最优策略是**仅选择使$q^(s,a)$最大的动作**。\n对任意状态$s$，若$a^(s) = \\arg\\max_{a \\in \\mathcal{A}} q^(s,a)$，则最优策略为：\nπ∗(a∣s)={1,a=a∗(s)0,a≠a∗(s)\\pi^*(a|s) = \\begin{cases} 1, \u0026 a = a^*(s) \\\\ 0, \u0026 a \\neq a^*(s) \\end{cases}π∗(a∣s)={1,0,​a=a∗(s)a=a∗(s)​这就是确定性贪心策略：在每个状态，总是选择动作价值最大的动作。\n接下来需要验证**$v^$与$\\pi^$确实是最优的（定理3.4）**。即证明$v^* = v_{\\pi^*} \\geq v_{\\pi}$对任意策略$\\pi$成立。\n由BOE的定义，对任意策略$\\pi$，有：\nv∗=max⁡π′(rπ′+γPπ′v∗)≥rπ+γPπv∗v^* = \\max_{\\pi'} (r_{\\pi'} + \\gamma P_{\\pi'}v^*) \\geq r_{\\pi} + \\gamma P_{\\pi}v^*v∗=π′max​(rπ′​+γPπ′​v∗)≥rπ​+γPπ​v∗将$v^* \\geq r_{\\pi} + \\gamma P_{\\pi}v^*$与普通贝尔曼方程$v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi}v_{\\pi}$相减，得：\nv∗−vπ≥γPπ(v∗−vπ)v^* - v_{\\pi} \\geq \\gamma P_{\\pi}(v^* - v_{\\pi})v∗−vπ​≥γPπ​(v∗−vπ​)反复递推，可得：\nv∗−vπ≥γnPπn(v∗−vπ)(n=1,2,...)v^* - v_{\\pi} \\geq \\gamma^n P_{\\pi}^n (v^* - v_{\\pi}) \\quad (n=1,2,...)v∗−vπ​≥γnPπn​(v∗−vπ​)(n=1,2,...)当$n \\to \\infty$时，因$\\gamma \u003c 1$且$P_{\\pi}^n$是概率矩阵（元素≤1），$\\gamma^n P_{\\pi}^n (v^* - v_{\\pi}) \\to 0$，故$v^* - v_{\\pi} \\geq 0$（即$v^* \\geq v_{\\pi}$）。\n同时，由于$v^* = r_{\\pi^} + \\gamma P_{\\pi^}v^$，而$v_{\\pi^}$也满足$v_{\\pi^} = r_{\\pi^} + \\gamma P_{\\pi^}v_{\\pi^}$，又因普通贝尔曼方程的解唯一，故$v^* = v_{\\pi^*}$。\n综上：$v_{\\pi^} \\geq v_{\\pi}$对任意$\\pi$成立，证明$\\pi^$是最优策略。\n关于最优策略的附加性质：最优策略可能不唯一。如果存在多个动作$a_1, a_2, …, a_k$在同一状态$s$下具有相同的最大$q^(s,a)$，那么\"选择$a_1$\"、“选择$a_2$“甚至\"随机选择这些动作\"的策略，都能使$\\sum_a \\pi(a|s)q^(s,a)$最大化，因此均为最优策略。但必然存在确定性最优策略 —— 即使存在随机最优策略，确定性最优策略也一定存在。\n还有一个有趣的性质（定理3.6）：最优策略对奖励的仿射变换具有不变性。如果将奖励$r$通过仿射变换变为$\\alpha r + \\beta$（$\\alpha \u003e 0$），新的最优状态价值$v’ = \\alpha v^* + \\frac{\\beta}{1 - \\gamma} \\mathbf{1}$，但最优策略保持不变。这意味着策略的\"结构”（在哪些状态选哪些动作）不依赖于奖励的绝对大小，只依赖于奖励的相对大小。","35-summary#3.5 Summary":"本章的核心概念包括最优策略与最优状态价值：若某一策略的状态价值大于或等于所有其他策略的状态价值，则该策略为最优策略；最优策略对应的状态价值即为最优状态价值。\n贝尔曼最优方程是分析最优策略与最优状态价值的核心工具。该方程是一种非线性方程，具有良好的压缩性 —— 其右侧函数是压缩映射。利用压缩映射定理可证明：贝尔曼最优方程的解唯一存在，且解对应于最优状态价值与最优策略。\n完整的推导逻辑链为：“解的存在性”（通过压缩映射定理确定有唯一的最优状态价值$v^$）→“解的形式”（推导能最大化$v^$的策略形式，即贪心策略）→“解的最优性”（验证该策略确实最优）→“解的性质”（补充最优策略的性质），形成了从BOE得到可落地最优策略的完整数学推导闭环。","3bellman-optimality-equation#3、Bellman Optimality Equation":"在第2章中，我们学习了如何评估一个固定策略$\\pi$的好坏 —— 通过计算该策略下的状态价值$v_{\\pi}(s)$。但强化学习的最终目标不是评估，而是找到最优策略。这一章要回答的核心问题是：如何找到能够获得最大长期收益的策略？"},"title":"第2章 贝尔曼最优公式"},"/docs/self-study/ai/rl/chapter-4/":{"data":{"":"Value Iteration and Policy Iteration 写在前面：之前学习贝尔曼方程时确实有些囫囵吞枣，所以这一章我决定换个思路——用大量例子来理解迭代算法。通过具体的数值计算，你会看到算法是如何一步步从\"瞎猜\"走向\"最优\"的。至于 Chapter 2 和 3 的符号体系，后续再慢慢完善。\nValue iteration 价值迭代（Value Iteration）是强化学习中最直观的算法之一。它的核心思想非常简单：不断更新每个状态的价值估计，直到找到最优策略。让我们先看看算法是怎么工作的，然后再通过一个完整的例子来理解它。\n算法描述 Algorithm 4.1: Value iteration algorithm\"\nInitialization: The probability models p(r|s, a) and p(s′|s, a) for all (s, a) are known. Initial guess: v₀ Goal: Search for the optimal state value and an optimal policy for solving the Bellman optimality equation. While vₖ has not converged in the sense that ‖vₖ − vₖ₋₁‖ is greater than a predefined small threshold, for the kth iteration, do:\nFor every state s ∈ S, do: For every action a ∈ A(s), do: q-value: qₖ(s, a) = ∑ᵣ p(r|s, a)r + γ ∑ₛ' p(s'|s, a)vₖ(s') Maximum action value: a*ₖ(s) = arg maxₐ qₖ(s, a) Policy update: πₖ₊₁(a|s) = 1 if a = a*ₖ(s), and πₖ₊₁(a|s) = 0 otherwise Value update: vₖ₊₁(s) = maxₐ qₖ(s, a) 通过例子理解价值迭代 理论说再多，不如看个例子。让我们用一个 2×2 的网格世界来完整走一遍价值迭代的过程。\n第一步：理解环境设置 在开始计算之前，我们需要先搞清楚这个\"游戏\"的规则。想象你在一个 2×2 的网格中移动：\n状态空间：网格是 2 行 2 列，对应 4 个状态：\ns₁（左上） s₂（右上） s₃（左下） s₄（右下） 特殊区域：\n目标区：s₄（到达这里能拿到正奖励，就像游戏中的终点） 禁区/边界：网格外的区域（走到网格外算\"碰边界\"，拿到负奖励，就像撞墙扣分） 状态转移是确定性的：比如在 s₁ 做\"向右\"动作（ a₃），一定能到 s₃（不会随机到其他状态） 第二步：理解动作空间 每个状态有 5 个可选动作（a₁ 到 a₅），每个动作对应\"向某个方向移动\"或\"停留\"，比如：\na₁：是\"向左\"（容易碰边界，q 值常为 -1） a₅：“停留”（不移动，q 值依赖当前状态的 v 值） 关键点：每个动作的效果是固定的（比如 s₂ 的 a₃ 动作一定到 s₄）。这简化了我们的计算，因为不需要考虑随机性。\n第三步：理解奖励机制（这是理解 q 值的关键！） 奖励规则决定了算法会学到什么策略。在这个例子中：\n碰边界/进禁区：奖励 r_boundary = r_forbidden = -1（比如 s₁ 向左走，出网格，拿 -1） 到目标区 s₄：奖励 r_target = 1（比如 s₂ 做 a₃ 到 s₄，拿 1） 其他情况（在网格内移动但没到目标）：奖励看动作是否碰边界（比如 s₁ 向右到 s₃，没碰边界，奖励 0） 第四步：理解折扣率 折扣率 γ = 0.9 用来计算\"未来奖励的现值\"。简单理解：未来的奖励不如现在的奖励值钱。\n举个例子：\n“现在拿 1 分” = 1 分 “下一步拿 1 分” = 0.9 × 1 = 0.9 分（打了 9 折） “下下步拿 1 分” = 0.9² × 1 = 0.81 分（打了 8.1 折） 第五步：q 值和 v 值 q 值（动作价值）：全称\"动作价值\"，含义是\"在状态 s 下做动作 a，未来能拿到的总奖励期望\"\n公式简化（示例是确定性的，不用概率求和）：q(s,a) = 当前动作的即时奖励 + γ × 动作后到达状态的v值 v 值（状态价值）：全称\"状态价值\"，含义是\"在状态 s 下，未来能拿到的最大总奖励期望\"（即选所有动作里 q 值最大的那个）\n直观理解：v 值回答\"这个状态本身有多值钱？\" 迭代逻辑：用当前 v 值算 q 值 → 选 q 最大的动作（更新策略）→ 用最大 q 值更新 v 值 → 重复，直到 v 值不再变或策略最优\n开始迭代：k=0（第一次计算，从\"瞎猜\"开始） 好，现在让我们开始第一次迭代。价值迭代的起点是\"初始 v 值猜测\"——你可以随便猜，但为了简单，我们选择最简单的：所有状态的初始 v₀ = 0。\n为什么从 0 开始？ 这相当于\"一开始假设每个状态未来都拿不到奖励\"，这是一个中性的起点，算法会通过迭代逐步修正这个估计。\n现在，我们需要根据这个初始猜测来计算所有状态-动作对的 q 值。计算依据是表 4.1 的表达式，把 v₀(s₁) = v₀(s₂) = v₀(s₃) = v₀(s₄) = 0 和 γ = 0.9 代入，逐个状态、逐个动作算，这样就得到了下表：\n更新策略 π₁：选\"当前看起来最赚的动作\" 有了 q 值表，我们就可以更新策略了。策略的核心是\"每个状态选 q 值最大的动作\"（贪心策略，因为选当前看起来最赚的）：\ns₁：最大 q 值是 0（a₃ 或 a₅，示例选 a₅）→ π₁(a₅|s₁) = 1（其他动作选 0） s₂：最大 q 值是 1（a₃）→ π₁(a₃|s₂) = 1 s₃：最大 q 值是 1（a₂）→ π₁(a₂|s₃) = 1 s₄：最大 q 值是 1（a₅）→ π₁(a₅|s₄) = 1 s₁ 选 a₅（停留）其实不好 —— 因为停留不能到目标区，所以 π₁ 不是最优策略，需要继续迭代。这就是为什么我们需要多轮迭代的原因。\n更新 v 值到 v₁：每个状态的\"最大奖励期望\" 每个状态的 v₁，就是该状态下所有动作的最大 q 值（因为 v 值是\"未来最大总奖励\"）：\ns₁：最大 q 值是 0 → v₁(s₁) = 0 s₂：最大 q 值是 1 → v₁(s₂) = 1 s₃：最大 q 值是 1 → v₁(s₃) = 1 s₄：最大 q 值是 1 → v₁(s₄) = 1 到这里，k=0 的迭代结束，我们得到了\"中间 v 值 v₁“和\"临时策略 π₁\"。注意 v₁ 比 v₀ 更准确了（至少 s₂、s₃、s₄ 的价值不再是 0），但还不够好。\n继续迭代：k=1（用更准确的 v 值，找到最优策略） 现在进入第二轮迭代。这一轮的计算基础是上一轮的 v₁（不是初始 v₀ 了！），步骤和 k=0 完全一样：先算 q 值，再更新策略，最后更新 v₂。\n每一轮迭代都在用\"更准确的 v 值\"来计算 q 值，所以策略会越来越接近最优。\n计算 k=1 时的所有 q 值 还是用表 4.1 的表达式，但这次代入的是更新后的 v 值：v₁(s₁) = 0、v₁(s₂) = 1、v₁(s₃) = 1、v₁(s₄) = 1、γ = 0.9，逐个算：\n对比 k=0 和 k=1 的 q 值表，你会发现 q 值变得更准确了。这是因为我们用了更准确的 v 值来计算。\n更新策略 π₂：找到最优策略！ 还是选每个状态 q 值最大的动作：\ns₁：最大 q 值是 0.9（a₃）→ π₂(a₃|s₁) = 1 s₂：最大 q 值是 1.9（a₃）→ π₂(a₃|s₂) = 1 s₃：最大 q 值是 1.9（a₂）→ π₂(a₂|s₃) = 1 s₄：最大 q 值是 1.9（a₅）→ π₂(a₅|s₄) = 1 现在让我们验证这个策略为什么\"最优”：\ns₁ 选 a₃ → 到 s₃ s₃ 选 a₂ → 到 s₄（目标区，拿 1 奖励） s₂ 选 a₃ → 直接到 s₄ s₄ 选 a₅ → 停留（持续拿 1 奖励） 所有状态的动作都能\"最快拿到正奖励，不碰负奖励\"，所以 π₂ 是最优策略。注意 s₁ 的策略从\"停留\"改成了\"向右移动\"，这就是迭代的威力！\n更新 v 值到 v₂：验证收敛趋势 每个状态的 v₂ 是该状态的最大 q 值：\ns₁：最大 q 值 0.9 → v₂(s₁) = 0.9 s₂：最大 q 值 1.9 → v₂(s₂) = 1.9 s₃：最大 q 值 1.9 → v₂(s₃) = 1.9 s₄：最大 q 值 1.9 → v₂(s₄) = 1.9 到这里 k=1 迭代结束，因为已经得到最优策略，后续迭代（k=2、k=3…）v 值会逐渐收敛到\"真实最优 v 值\"，但策略不会再变了。这就是价值迭代的收敛性保证。\n价值迭代的本质：三步骤循环 通过这个例子，我们可以看到 Value iteration 的本质是\"从’瞎猜’到’最优’的迭代过程\"，全程只有 3 个重复动作：\n猜 v 值：初始猜 v₀ = 0，后续用\"前一轮的最大 q 值\"更新 v 值 算 q 值：用当前 v 值代入动作奖励公式，算每个\"状态-动作\"的未来奖励 选动作：每个状态选 q 值最大的动作，直到动作能导向目标（最优策略） 猜 v → 算 q → 选动作，循环直到收敛。就这么简单！本质上就是\"算术计算 + 贪心选择\"。\nPolicy iteration：评估与改进的交替进行 与价值迭代不同，策略迭代采用了另一种思路：先评估当前策略的好坏，再根据评估结果改进策略。它不是直接求解贝尔曼最优方程，而是通过\"评估-改进\"的循环逐步逼近最优策略。\n价值迭代是\"边算边改\"（每次迭代都更新策略），策略迭代是\"先算清楚再改\"（先完整评估策略，再改进）。\n在深入算法细节之前，让我们先回答三个关键问题，这些问题的答案将帮助我们理解策略迭代为什么有效。","1-先明确每个状态-动作对的-q-值公式还是从规则来#1. 先明确每个\u0026quot;状态-动作\u0026quot;对的 q 值公式（还是从规则来）":"根据\"动作效果 + 奖励规则\"，我们先列出所有 3 个动作在 s₁ 和 s₂ 的 q 值表达式：\n带入上面的 -10 和 -9 得到：\nq_π₀(s, a) a_ℓ a₀ a_r s₁ -10 -9 -7.1 s₂ -9 -7.1 -9.1 对每个状态，选 q 值最大的动作（因为 q 值越大，未来总奖励越多）：\ns₁ 的 q 值：-10（a_ℓ） \u003c -9（a₀） \u003c -7.1（a_r）→ 选 a_r s₂ 的 q 值：-9.1（a_r） \u003c -9（a_ℓ） \u003c -7.1（a₀）→ 选 a₀","1-状态类型3-种状态功能不同#1. 状态类型：3 种状态，功能不同":"普通状态：大部分格子，可正常移动 禁区：部分格子（原文图 4.4 中标记为 -100），进入会扣重分，且无法停留（比如 (2,2)、(2,3)、(3,3) 等） 目标区：1 个特定格子（原文中是 (4,3)），进入或停留会拿正奖励（目标是让智能体尽量靠近/停在这）","2-动作4-个方向移动#2. 动作：4 个方向移动":"每个状态的动作是\"向四个方向移动”（或含停留），动作效果确定性：\n若移动方向是\"边界\"（比如 (1,1) 向上）：碰边界，留在原状态，扣 1 分 若移动方向是\"禁区\"（比如 (4,2) 向右到 (4,3) 是目标，(4,2) 向左到 (4,1) 是普通状态，(4,2) 向上到 (3,2) 是禁区）：进入禁区，扣 10 分，且强制留在原状态 若移动方向是\"普通状态/目标区\"：正常移动，普通状态无即时奖励，目标区拿 1 分","2-得到新策略-πspan-stylecolor-red这就是最优策略#2. 得到新策略 \u003ccode\u003eπ₁\u003c/code\u003e（\u0026lt;span style={{color: ‘red’}}\u0026gt;\u003cstrong\u003e这就是最优策略！\u003c/strong\u003e\u003c/span\u003e）":"在 s₁：一定选 a_r（π₁(a_r|s₁) = 1）→ 从 s₁ 移到 s₂（靠近目标） 在 s₂：一定选 a₀（π₁(a₀|s₂) = 1）→ 留在 s₂（保持靠近目标，不碰边界）","3-奖励规则比两状态示例更细致#3. 奖励规则（比两状态示例更细致）":"碰边界：r_boundary = -1（轻微扣分） 进入禁区：r_forbidden = -10（严重扣分） 到达/停留目标区：r_target = 1（正奖励） 普通状态间移动：r = 0（无奖励）","4-初始策略随机策略瞎选动作#4. 初始策略：随机策略（\u0026ldquo;瞎选动作\u0026rdquo;）":"初始策略 π₀ 是\"随机的\"—— 每个状态下，智能体随机选择 4 个方向的动作，完全不考虑\"是否碰边界、是否进禁区、是否靠近目标\"。比如：在 (4,2)（目标区 (4,3) 左边），可能随机选\"向左\"（到普通状态）、“向右”（到目标区）、“向上”（到禁区），完全无规律。","5-折扣率依然是-span-stylecolor-redγ--09#5. 折扣率：依然是 \u0026lt;span style={{color: ‘red’}}\u0026gt;\u003cstrong\u003e\u003ccode\u003eγ = 0.9\u003c/code\u003e\u003c/strong\u003e\u003c/span\u003e":"未来奖励的\"现值\"计算规则不变：比如\"现在拿 1 分\" = 1 分，“下一步拿 1 分” = 0.9 分，“下下步拿 1 分” = 0.81 分，距离越远，奖励折扣越多。","policy-iteration评估与改进的交替进行#Policy iteration：评估与改进的交替进行":"","truncated-policy-iteration#Truncated Policy iteration":"value iteration and policy iteration algorithms are two special cases of the truncated policy iteration algorithm\n首先，我们通过梳理两种算法的步骤来对其进行比较。","value-iteration#Value iteration":"","value-iteration-and-policy-iteration#Value Iteration and Policy Iteration":"","三种算法对比总结#三种算法对比总结":"算法名称 核心思想 迭代步骤 关键特点 价值迭代（Value Iteration） 直接求解贝尔曼最优方程（基于压缩映射定理） 策略更新 → 价值更新 中间值 vₖ 非真实状态值，无需初始策略，仅需初始状态值 v₀ 策略迭代（Policy Iteration） 交替执行\"评估当前策略价值\"与\"改进策略\"，逐步提升策略性能 策略评估 → 策略改进 中间值 v_πₖ 是真实状态值，需初始策略 π₀，收敛速度快于价值迭代 截断策略迭代（Truncated PI） 对策略迭代的\"策略评估\"步骤进行有限次迭代截断，平衡效率与收敛速度 有限次策略评估 → 策略改进 中间值为近似状态值，是前两种算法的统一框架（j_truncate = 1 对应价值迭代，j_truncate = ∞ 对应策略迭代）","价值计算步骤的差异分析#价值计算步骤的差异分析":"为进一步明确两种算法的差异，我们重点分析它们的\"价值计算步骤\"。具体而言，假设两种算法从相同的初始条件出发：v₀ = v_π₀（初始状态值 v₀ 等于初始策略 π₀ 的状态值 v_π₀），其流程对比如下表所示。\n由于 v₀ = v_π₀，前三个步骤中两种算法的结果完全一致；从第四步开始，两者出现差异。\n在第四步中：\n价值迭代算法执行的是一步计算：v₁ = r_π₁ + γP_π₁ v₀ 策略迭代算法则需要求解贝尔曼方程：v_π₁ = r_π₁ + γP_π₁ v_π₁，该过程需执行无限次迭代 若将第四步中\"求解 v_π₁ = r_π₁ + γP_π₁ v_π₁“的迭代过程展开，差异便会一目了然。令 v_π₁^(0) = v₀（策略迭代中状态值的初始猜测等于价值迭代的初始状态值），则迭代过程如下：\n从上述过程可得出以下结论：\n若仅执行 1 次迭代，则 v_π₁^(1) 本质上就是价值迭代算法中计算的 v₁ 若执行 无限次迭代，则 v_π₁^(∞) 本质上就是策略迭代算法中计算的 v_π₁ 若执行有限次迭代（迭代次数记为 j_truncate，即\"截断迭代次数”），则这类算法被称为截断策略迭代（Truncated Policy Iteration） 称之为\"截断\"，是因为从 j_truncate 到 ∞ 的剩余迭代过程被终止。\n由此可见，价值迭代与策略迭代可视为截断策略迭代的两种极端情况：\n价值迭代对应\"截断迭代次数 j_truncate = 1\"（仅执行 1 次迭代便终止） 策略迭代对应\"截断迭代次数 j_truncate = ∞\"（执行无限次迭代直至收敛） 步骤 Policy iteration algorithm Value iteration algorithm Comments 1) Policy: π₀ N/A 2) Value: v_π₀ = r_π₀ + γP_π₀ v_π₀ v₀ = v_π₀ 3) Policy: π₁ = arg max_π (r_π + γP_π v_π₀) π₁ = arg max_π (r_π + γP_π v₀) The two policies are the same 4) Value: v_π₁ = r_π₁ + γP_π₁ v_π₁ v₁ = r_π₁ + γP_π₁ v₀ v_π₁ ≥ v₁ since v_π₁ ≥ v_π₀ 5) Policy: π₂ = arg max_π (r_π + γP_π v_π₁) π₂' = arg max_π (r_π + γP_π v₁) … … … …","价值迭代value-iteration#价值迭代（Value Iteration）":"选择任意初始状态值 v₀；在第 k 轮迭代中，执行以下两步：\n步骤 1：策略更新（PU，Policy Update）\n已知当前状态值 vₖ，通过求解以下优化问题得到新策略 πₖ₊₁：\nπₖ₊₁ = arg max_π (r_π + γP_π vₖ)\n步骤 2：价值更新（VU，Value Update）\n已知新策略 πₖ₊₁，通过以下公式计算新的状态值 vₖ₊₁：\nvₖ₊₁ = r_πₖ₊₁ + γP_πₖ₊₁ vₖ","价值迭代的本质三步骤循环#价值迭代的本质：三步骤循环":"","复杂示例55-网格#复杂示例：5×5 网格":"“复杂示例”（5×5 网格），核心是在\"多状态、多禁区、目标明确\"的场景下，展示策略如何从\"随机混乱\"逐步收敛到\"最优\"。相比两状态示例，它多了\"禁区规避\"和\"远距离状态导航\"，但逻辑依然是 “策略评估→策略改进\"的循环。","奖励规则span-stylecolor-red核心公式里的-#奖励规则（\u0026lt;span style={{color: ‘red’}}\u0026gt;\u003cstrong\u003e核心！公式里的 “-1”、“1” 从这来\u003c/strong\u003e\u003c/span\u003e）":"奖励规则决定了策略的好坏。在这个例子中：\n碰边界（s₁ 选 a_ℓ、s₂ 选 a_r）：奖励 r_boundary = -1（扣分） 到目标/靠近目标（只有 s₁ 选 a_r 时，会移到 s₂，而 s₂ 是\"靠近目标的状态\"）：奖励 r_target = 1（加分） 其他动作（s₁/s₂ 选 a₀、s₂ 选 a_ℓ）：没有额外奖励，记为 r = 0","开始迭代k0第一次计算从瞎猜开始#开始迭代：k=0（第一次计算，从\u0026quot;瞎猜\u0026quot;开始）":"","推导策略评估的方程#推导策略评估的方程":"让我们一步步推导出策略评估的方程。根据贝尔曼方程，状态价值 = 即时奖励 + 折扣 × 未来状态价值。\n（1）计算 v_π₀(s₁)（s₁ 用 π₀ 的价值）\nπ₀ 在 s₁ 只选 a_ℓ（向左碰边界）：\n即时奖励：碰边界，得 r = -1 动作后状态：留在 s₁，所以未来价值是\"折扣率 × s₁ 的价值\"（即 γ × v_π₀(s₁)） 根据\"状态价值 = 即时奖励 + 折扣 × 未来状态价值\"，可得：\nv_π₀(s₁) = -1 + γ × v_π₀(s₁)\n这个方程是自包含的（右边也包含 v_π₀(s₁)），我们需要解这个方程。\n（2）计算 v_π₀(s₂)（s₂ 用 π₀ 的价值）\nπ₀ 在 s₂ 选 a_ℓ（向左移到 s₁）：\n即时奖励：没碰边界，得 r = 0 动作后状态：到 s₁，所以未来价值是 γ × v_π₀(s₁) 因此方程为：\nv_π₀(s₂) = 0 + γ × v_π₀(s₁)\n这个方程依赖于 v_π₀(s₁)，所以我们需要先解出 s₁ 的价值。","更新-v-值到-v每个状态的最大奖励期望#更新 v 值到 \u003ccode\u003ev₁\u003c/code\u003e：每个状态的\u0026#34;最大奖励期望\u0026#34;":"","更新-v-值到-v验证收敛趋势#更新 v 值到 \u003ccode\u003ev₂\u003c/code\u003e：验证收敛趋势":"","更新策略-πspan-stylecolor-red找到最优策略#更新策略 \u003ccode\u003eπ₂\u003c/code\u003e：\u0026lt;span style={{color: ‘red’}}\u0026gt;\u003cstrong\u003e找到最优策略！\u003c/strong\u003e\u003c/span\u003e":"","更新策略-π选当前看起来最赚的动作#更新策略 \u003ccode\u003eπ₁\u003c/code\u003e：选\u0026#34;当前看起来最赚的动作\u0026#34;":"","核心思想#核心思想":"三者均遵循**广义策略迭代（Generalized Policy Iteration）**思想 —— 即通过\"价值更新\"与\"策略更新\"的交互循环，逐步优化状态值与策略。这一思想是后续多数强化学习算法的核心框架。\n三种算法均属于基于模型（Model-Based）的动态规划算法，需依赖系统模型（即已知状态转移概率 p(s'|s,a) 与奖励概率 p(r|s,a)）。","环境设置理解状态和动作#环境设置：理解状态和动作":"状态空间：只有 2 个状态，记为 s₁ 和 s₂。可以想象成\"两个相邻的格子\"：s₁ 在左，s₂ 在右（如图 4.3 所示），没有其他状态。\n动作空间：A = {a_ℓ, a₀, a_r}，每个动作的效果是确定性的（做动作 A 就一定到状态 B，不会随机）：\na_ℓ：向左移动 若在 s₁（最左边）选 a_ℓ：会\"碰边界\"（左边没有格子了），所以只能留在 s₁ 若在 s₂（右边）选 a_ℓ：会从 s₂ 移到 s₁ a₀：停留不动 不管在 s₁ 还是 s₂，选 a₀ 都留在当前状态 a_r：向右移动 若在 s₂（最右边）选 a_r：会\"碰边界\"，只能留在 s₂ 若在 s₁（左边）选 a_r：会从 s₁ 移到 s₂（这是靠近目标的动作！）","第一步理解环境设置#第一步：理解环境设置":"","第三步理解奖励机制span-stylecolor-red这是理解-q-值的关键#第三步：理解奖励机制（\u0026lt;span style={{color: ‘red’}}\u0026gt;\u003cstrong\u003e这是理解 q 值的关键！\u003c/strong\u003e\u003c/span\u003e）":"","第二步理解动作空间#第二步：理解动作空间":"","第五步q-值和-v-值#第五步：q 值和 v 值":"","第四步理解折扣率#第四步：理解折扣率":"","策略迭代policy-iteration#策略迭代（Policy Iteration）":"选择任意初始策略 π₀；在第 k 轮迭代中，执行以下两步：\n步骤 1：策略评估（PE，Policy Evaluation）\n已知当前策略 πₖ，通过求解以下贝尔曼方程得到该策略对应的状态值 v_πₖ：\nv_πₖ = r_πₖ + γP_πₖ v_πₖ\n步骤 2：策略改进（PI，Policy Improvement）\n已知策略 πₖ 的状态值 v_πₖ，通过求解以下优化问题得到新策略 πₖ₊₁：\nπₖ₊₁ = arg max_π (r_π + γP_π v_πₖ)","策略迭代第一步策略评估算笨策略的真实价值#策略迭代第一步：策略评估（算\u0026quot;笨策略\u0026quot;的真实价值）":"现在让我们开始策略迭代。第一步是策略评估：搞清楚\"用当前策略（π₀）玩，每个状态到底值多少分\"。\n为什么叫\"笨策略\"？ 因为初始策略 π₀ 可能不是最优的（比如总是选 a_ℓ 向左走，容易碰边界），但我们需要先知道它到底有多\"笨\"，才能改进它。\n具体来说，我们需要计算 v_π₀(s₁) 和 v_π₀(s₂)，这两个值代表\"从该状态出发，用 π₀ 玩，未来能拿到的总奖励期望\"。","策略迭代第二步--策略改进把笨策略改成好策略#策略迭代第二步 —— 策略改进（把\u0026quot;笨策略\u0026quot;改成\u0026quot;好策略\u0026quot;）":"策略改进的目的：基于策略评估的结果（v_π₀(s₁) = -10，v_π₀(s₂) = -9），给每个状态选\"更赚的动作\"，得到新策略 π₁。\n核心工具是 q 值：q 值 = “在状态 s 选动作 a 的即时奖励 + 折扣 × 动作后状态的价值”，代表\"选这个动作到底值多少分\"。","算法描述#算法描述":"","算法描述-1#算法描述":"Algorithm 4.2: Policy iteration algorithm\"\nInitialization: The system model, p(r|s, a) and p(s'|s, a) for all (s, a), is known. Initial guess π₀. Goal: Search for the optimal state value and an optimal policy. While v_πₖ has not converged, for the kth iteration, do:\nPolicy evaluation:\nInitialization: an arbitrary initial guess v_πₖ^(0) While v_πₖ^(j) has not converged, for the jth iteration, do: For every state s ∈ S, do: v_πₖ^(j+1)(s) = ∑_a πₖ(a|s) [∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)v_πₖ^(j)(s')] Policy improvement:\nFor every state s ∈ S, do: For every action a ∈ A, do: q_πₖ(s, a) = ∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)v_πₖ(s') a*ₖ(s) = arg max_a q_πₖ(s, a) πₖ₊₁(a|s) = 1 if a = a*ₖ(s), and πₖ₊₁(a|s) = 0 otherwise","算法描述-2#算法描述":"Algorithm 4.3: Truncated policy iteration algorithm\nInitialization: The probability models p(r|s, a) and p(s'|s, a) for all (s, a) are known. Initial guess π₀. Goal: Search for the optimal state value and an optimal policy. While vₖ has not converged, for the kth iteration, do:\nPolicy evaluation:\nInitialization: select the initial guess as vₖ^(0) = vₖ₋₁. The maximum number of iterations is set as j_truncate. While j \u003c j_truncate, do: For every state s ∈ S, do: vₖ^(j+1)(s) = ∑_a πₖ(a|s) [∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)vₖ^(j)(s')] Set vₖ = vₖ^(j_truncate) Policy improvement:\nFor every state s ∈ S, do: For every action a ∈ A(s), do: qₖ(s, a) = ∑_r p(r|s, a)r + γ ∑_s' p(s'|s, a)vₖ(s') a*ₖ(s) = arg max_a qₖ(s, a) πₖ₊₁(a|s) = 1 if a = a*ₖ(s), and πₖ₊₁(a|s) = 0 otherwise","算法步骤对比#算法步骤对比":"","算法流程表示#算法流程表示":"两种算法的上述步骤可直观表示为：\n策略迭代：π₀ →[PE] v_π₀ →[PI] π₁ →[PE] v_π₁ →[PI] π₂ →[PE] v_π₂ →[PI] ... 价值迭代：v₀ →[PU] π₁' →[VU] v₁ →[PU] π₂' →[VU] v₂ →[PU] ... 可见，两种算法的流程极为相似。","算法特点#算法特点":"简而言之，截断策略迭代算法与策略迭代算法完全相同，唯一区别在于它仅在策略评估步骤中执行有限次迭代。其实现细节总结于算法 4.3 中。\n值得注意的是，算法中的 vₖ 与 vₖ^(j) 并非状态值，而是真实状态值的近似值 —— 这是因为策略评估步骤仅执行了有限次迭代，未完全收敛到真实状态值。\n与策略迭代相比：它仅需在策略评估步骤中执行有限次迭代，无需等待状态值完全收敛，因此计算效率更高。\n与价值迭代相比：它可通过在策略评估步骤中多执行几次迭代（而非仅 1 次），让近似状态值更接近真实值，从而加快整体收敛速度。","继续迭代k1用更准确的-v-值找到最优策略#继续迭代：k=1（用更准确的 v 值，找到最优策略）":"","观察现象#观察现象":"现象 1：靠近目标的状态先找到最优策略\n原因：靠近目标的状态\"一步就能拿到正奖励\"，策略评估时它们的状态值很高，策略改进时很容易发现\"选靠近目标的动作 q 值最大\"。\n现象 2：状态值随距离目标的远近递减\n原因：状态值是\"未来总奖励的期望\"，远离目标的状态需要多走几步才能到目标，每一步的奖励都会被折扣（γ = 0.9）。\n多轮迭代后，最优策略从目标区向外扩散，最终覆盖全网格。","解方程得到真实价值#解方程得到真实价值":"解这两个方程（代入 γ = 0.9）：\nv_π₀(s₁) = -10 v_π₀(s₂) = 0.9 × (-10) = -9 两个状态的价值都是负数！这说明策略 π₀ 确实很\"笨\"——从任何状态出发，按这个策略行动，长期来看都会得到负的累积奖励。\n从\"初始猜测 v_π₀^(0)(s₁) = v_π₀^(0)(s₂) = 0“开始，逐步逼近真实值：\n（1）第 1 次迭代（j=1）\nv_π₀^(1)(s₁) = -1 + 0.9 × v_π₀^(0)(s₁) = -1 + 0.9 × 0 = -1\rv_π₀^(1)(s₂) = 0 + 0.9 × v_π₀^(0)(s₁) = 0 + 0.9 × 0 = 0 （2）第 2 次迭代（j=2）\nv_π₀^(2)(s₁) = -1 + 0.9 × v_π₀^(1)(s₁) = -1 + 0.9 × (-1) = -1.9\rv_π₀^(2)(s₂) = 0 + 0.9 × v_π₀^(1)(s₁) = 0 + 0.9 × (-1) = -0.9 （3）第 3 次迭代（j=3）\nv_π₀^(3)(s₁) = -1 + 0.9 × v_π₀^(2)(s₁) = -1 + 0.9 × (-1.9) = -2.71\rv_π₀^(3)(s₂) = 0 + 0.9 × v_π₀^(2)(s₁) = 0 + 0.9 × (-1.9) = -1.71 （4）迭代趋势\n继续算下去会发现：v_π₀^(j)(s₁) 逐渐靠近 -10，v_π₀^(j)(s₂) 逐渐靠近 -9 —— 和我们解方程的结果一致！\n这说明\"策略评估的结果是对的”：用 π₀ 玩，s₁ 未来总奖励是 -10 分，s₂ 是 -9 分（都扣分，因为策略笨）。","计算-k1-时的所有-q-值#计算 k=1 时的所有 q 值":"","通俗理解三种算法的类比#通俗理解：三种算法的类比":"或者你也可以这样通俗的理解（用\"学生考试\"的类比）：\n策略迭代：给学生 πₖ “无限次模拟考”（j → ∞），直到分数 v_πₖ 完全准确，再根据分数改进学习方法 → πₖ₊₁ 截断策略迭代：给学生 πₖ “j 次模拟考”（j 是有限数），用 j 次考试的平均/最后一次分数 vₖ 近似真实水平，再改进方法 → πₖ₊₁ 价值迭代：先根据上学期分数 vₖ₋₁，给学生制定新学习方法 → πₖ，然后只让新方法的学生 “考 1 次试”（j = 1），用这 1 次的分数 vₖ 当近似水平，再进入下一轮 显然，“只考 1 次试” 是 “考 j 次试” 的极端情况（j = 1）—— 这就是价值迭代与截断策略迭代的关系。","通过例子理解价值迭代#通过例子理解价值迭代":"","通过例子理解策略迭代#通过例子理解策略迭代":"为了更清楚地理解策略迭代，我们用一个更简单的例子：两状态三动作。","问题-1策略评估步骤中如何计算-v_πₖ#问题 1：策略评估步骤中，如何计算 \u003ccode\u003ev_πₖ\u003c/code\u003e？":"在第 2 章中，我们已介绍过两种求解式（4.3）所示贝尔曼方程的方法，此处简要回顾：\n方法 1：闭式解（Closed-Form Solution）\nv_πₖ = (I - γP_πₖ)⁻¹ r_πₖ\n这种闭式解对理论分析有帮助，但实现效率较低 —— 因为计算矩阵的逆需要借助其他数值算法。\n方法 2：迭代算法（Iterative Algorithm）\n该方法易于实现，公式如下：\nv_πₖ^(j+1) = r_πₖ + γP_πₖ v_πₖ^(j), j = 0, 1, 2, ... (4.4)\n其中，v_πₖ^(j) 表示 v_πₖ 的第 j 次估计值。从任意初始猜测值 v_πₖ^(0) 出发，可保证当 j → ∞ 时，v_πₖ^(j) → v_πₖ（详细内容见第 2.7 节）。","问题-2策略改进步骤中为何-πₖ-优于-πₖ#问题 2：策略改进步骤中，为何 \u003ccode\u003eπₖ₊₁\u003c/code\u003e 优于 \u003ccode\u003eπₖ\u003c/code\u003e？":"如下所示，策略改进步骤确实能对给定策略进行优化：\n引理 4.1（策略改进引理，Policy Improvement Lemma）\n若 πₖ₊₁ = arg max_π (r_π + γP_π v_πₖ)，则 v_πₖ₊₁ ≥ v_πₖ。\n此处的 v_πₖ₊₁ ≥ v_πₖ 表示：对所有状态 s，均有 v_πₖ₊₁(s) ≥ v_πₖ(s)。\n证明：\n因此，\nv_πₖ - v_πₖ₊₁ ≤ γ² P_πₖ₊₁² (v_πₖ - v_πₖ₊₁) ≤ ... ≤ γⁿ P_πₖ₊₁ⁿ (v_πₖ - v_πₖ₊₁)\r≤ lim_{n→∞} γⁿ P_πₖ₊₁ⁿ (v_πₖ - v_πₖ₊₁) = 0 该极限成立的原因在于：当 n → ∞ 时，γⁿ → 0（γ 为折扣率，通常满足 0 ≤ γ \u003c 1）；且对任意 n，P_πₖ₊₁ⁿ 均为非负随机矩阵（随机矩阵指\"所有元素非负且每行元素之和为 1\"的矩阵）。","问题-3策略迭代算法为何最终能找到最优策略#问题 3：策略迭代算法为何最终能找到最优策略？":"策略迭代算法会生成两个序列：\n策略序列：{π₀, π₁, ..., πₖ, ...} 状态值序列：{v_π₀, v_π₁, ..., v_πₖ, ...} 设 v* 为最优状态值，则对所有 k，均有 v_πₖ ≤ v*。根据引理 4.1，策略会不断改进，因此可得：\nv_π₀ ≤ v_π₁ ≤ v_π₂ ≤ ... ≤ v_πₖ ≤ ... ≤ v*\n由于 v_πₖ 是单调递增序列，且始终以 v* 为上界，根据单调收敛定理 [12]（见附录 C），当 k → ∞ 时，v_πₖ 会收敛到某一常数，记为 v_∞。\n往往这里还需要一个 Convergence of Policy Iteration 来证明策略迭代的收敛性，即 v_∞ = v*。\n不过数学的答辩已经吃够了，现在直接来看 elementwise form of this algorithm 吧："},"title":"第4章 价值迭代与策略迭代"},"/docs/self-study/ai/rl/chapter-5/":{"data":{"1-episode-长度对策略和值估计的影响#1. Episode 长度对策略和值估计的影响":"在深入分析之前，让我们先明确两个核心概念�?\n概念 定义（通俗版） 示例中的具体表现�?.4.3�? *step（步�? 智能体与环境�?一次完整交�?�?. 智能体在当前状�?s 选动�?a�?. 环境给奖�?r，并切换到新状�?s'；这 3 个元素（s �?a �?r �?s'）构�?1 �?step�? 1 �?step = 智能体在网格某格（如 s₁）选动作（�?a₂）→ 环境给奖励（�?0）→ 走到新格子（�?s₂）�? episode（情节） �?起始状�?动作�?开始，�?结束条件\"为止的所�?step 的序列。（注：若环境无明确\"结束\"，会�?固定步数\"作为 episode 的终止条件） 示例中无明确结束（目标状态可循环停留），故用\"100 万步\"作为 1 �?episode 的长�?�?1 �?episode = 100 万个连续�?step�? 首先我们将证明：episode 长度对最终得到的最优策略有显著影响。具体而言，上图展示了蒙特卡洛基础算法在不�?episode 长度下的最终结果： **�?episode 长度过短�?，无论是策略还是值估计都无法达到最优（见上�?(a)~(d)）。在\"episode 长度 = 1\"的极端情况下，只有与目标状态相邻的状态才有非零值，其他所有状态的值均�?0——这是因�?episode 太短，智能体无法到达目标状态或获得正奖励�?- **随着 episode 长度的增�?，策略和值估计会逐渐趋近于最优�?","1-动作-a循环在-s拿负奖#1. 动作 \u003ccode\u003ea₁\u003c/code\u003e：循环在 \u003ccode\u003es₁\u003c/code\u003e，拿负奖�?":"步骤 1：确�?a₁ �?episode 轨迹\n�?(s�? a�? 出发，根据初始策�?π₀，动�?a₁ 会让智能体一直停留在 s₁（即循环：s�?→[a₁] s�?→[a₁] s�?→[a₁] ...）�? 所以每一步的奖励 R_{t+1} = R_{t+2} = ... = -1（对�?r_boundary = -1）�? 步骤 2：写出回�?G_t 的表达式\n根据回报定义，从 s₁ �?a₁ 的回报是�? G_t = R_{t+1} + γ R_{t+2} + γ² R_{t+3} + ...\n代入每一步的奖励 R = -1，得到：\nG_t = (-1) + γ(-1) + γ²(-1) + γ³(-1) + ...\n*步骤 3：用等比级数求和�?Q �?\n这是一�?首项 a = -1，公�?r = γ = 0.9“的无穷等比级数。根据求和公式：\nq_{π₀}(s�? a�? = G_t = a/(1 - r) = (-1)/(1 - 0.9) = (-1)/0.1 = ****","2-episode-长度与状态值的空间分布规律#2. Episode 长度与状态值的空间分布规律":"随着 episode 长度增加，会出现一个有趣的空间分布规律�?距离目标状态越近的状态，越早出现非零�?�? 原因分析：从任意状态出发，智能体需要至少一定步数才能到达目标状态并获得正奖励；�?episode 长度小于这个\"最小必要步�?，则该状态的回报必然�?0，其估计值也会为 0�? 在本示例中，episode 长度至少需要达�?****—�?5 是从左下角状态出发到达目标状态所需的最小步数�?","2-动作-a到达目标状#2. 动作 \u003ccode\u003ea₂\u003c/code\u003e：到达目标状�?":"�?(s�? a�? 出发，episode 轨迹为：s�?→[a₂] s�?→[a₃] s�?→[a₃] ...（即智能体从 s₁ �?a₂ �?s₂，再�?a₃ 到目标状�?s₅，之后在 s₅ 中循环）�? 该动作的动作值等于该 episode 的折扣回报，计算如下�? q_{π₀}(s�? a�? = 0 + γ·0 + γ²·0 + γ³(1) + γ�?1) + ... = γ³/(1 - γ)","3-episode-长度非无限要求#3. Episode 长度�?非无限�?要求":"尽管上述分析表明 episode 需�?足够�?，但并非要求\"无限�?。如图所示，�?episode 长度 = 30 时，算法已能找到最优策略（尽管此时的值估计尚未完全最优）�?","3-动作-a另一条到达目标的路径#3. 动作 \u003ccode\u003ea₃\u003c/code\u003e：另一条到达目标的路径":"�?(s�? a�? 出发，episode 轨迹为：s�?→[a₃] s�?→[a₂] s�?→[a₃] ...（即智能体从 s₁ �?a₃ �?s₄，再�?a₂ 到目标状�?s₅，之后在 s₅ 中循环）�? 该动作的动作值等于该 episode 的折扣回报，计算如下�? q_{π₀}(s�? a�? = 0 + γ·0 + γ²·0 + γ³(1) + γ�?1) + ... = γ³/(1 - γ)","4-与稀疏奖励问题的关联#4. 与稀疏奖励问题的关联":"上述分析涉及一个重要的奖励设计问题—�?稀疏奖励（sparse reward�?，即\"只有到达目标状态才能获得正奖励\"的场景�? 稀疏奖励的问题�?- 稀疏奖励设置要�?episode 必须足够长（以确保智能体能到达目标）\n但当状态空间较大时，这一要求很难满足 最终会导致算法的学习效率下�? *解决方案：设计非稀疏奖�? 解决稀疏奖励问题的一个简单方法是**设计非稀疏奖�?。例如，在上述网格世界示例中，我们可以重新设计奖励规则：让智能体到达\"目标附近的状�?时也能获得少量正奖励�? 这样一来，目标状态周围会形成一�?**吸引域（attractive field�?\"，帮助智能体更轻松地找到目标�?\n实践建议：在实际应用中，如果遇到稀疏奖励问题，可以考虑�?\u003e 1. 设计中间奖励（如距离目标的负距离�?\u003e 2. 使用奖励塑形（reward shaping�?\u003e 3. 使用课程学习（curriculum learning）逐步增加难度","4-动作-a回s-并循#4. 动作 \u003ccode\u003ea₄\u003c/code\u003e：回�?\u003ccode\u003es₁\u003c/code\u003e 并循�?":"�?(s�? a�? 出发，episode 轨迹为：s�?→[a₄] s�?→[a₁] s�?→[a₁] ...（即智能体从 s₁ �?a₄ 回到 s₁，之后经 a₁ �?s₁ 中循环）�? 该动作的动作值等于该 episode 的折扣回报，计算如下�? q_{π₀}(s�? a�? = -1 + γ(-1) + γ²(-1) + ... = (-1)/(1 - γ)","5-动作-a延迟回s-并循#5. 动作 \u003ccode\u003ea₅\u003c/code\u003e：延迟回�?\u003ccode\u003es₁\u003c/code\u003e 并循�?":"�?(s�? a�? 出发，episode 轨迹为：s�?→[a₅] s�?→[a₁] s�?→[a₁] ...（即智能体从 s₁ �?a₅ 回到 s₁，之后经 a₁ �?s₁ 中循环）�? 该动作的动作值等于该 episode 的折扣回报，计算如下�? q_{π₀}(s�? a�? = 0 + γ(-1) + γ²(-1) + ... = (-γ)/(1 - γ)","episode-长度的影响为什么需要足够长的轨迹#Episode 长度的影响：为什么需要足够长的轨迹？":"通过上面的例子，我们已经理解�?MC Basic 的基本工作原理。但这里有一个关键问题：episode 需要多长？\n在实际应用中，episode 的长度对算法的性能有重要影响。太短了，智能体来不及到达目标；太长了，计算成本又太高。让我们通过一个综合例子来理解这一点�?\n核心问题：episode 太短，智能体来不及到达目标；episode 太长，计算成本高。如何找到平衡点�?","exploration-and-exploitation-of-ϵ--greedy-policies#Exploration and exploitation of ϵ -greedy policies":"通过前面的学习，我们已经掌握�?MC ϵ-Greedy 算法的基本流程。但你可能已经注意到一个关键参数：**ε �?*。这个参数控制着探索与利用的平衡，是算法性能的关键�? 在深入分析之前，让我们先理解一个强化学习中的核心问题：探索与利用的矛盾�? 在强化学习中�?探索（exploration�? �?*利用（exploitation�? 是一对永恒的矛盾�?\n探索：策略应尽可能尝试更多不同的动作，通过这种方式，所有动作都能被访问并得到充分评�?- 利用：改进后的策略应选择\"动作值最大的贪婪动作\" 然而，由于当前时刻得到的动作值可能因探索不充分而存在偏差，因此我们在进行利用的同时必须保持探索，以避免错过潜在的最优动作�?\n核心矛盾：如果只探索不利用，我们永远学不到最优策略；如果只利用不探索，我们可能陷入局部最优�? 那么，如何平衡探索和利用呢？\n这就�?ϵ-贪婪策略要解决的问题�? *ϵ-贪婪策略（�?greedy policies�? 为平衡探索与利用提供了一种解决方案：\n一方面，�?贪婪策略选择贪婪动作的概率更高，能够基于已估计的动作值进�?利用\" 另一方面，该策略也会以一定概率选择其他动作，从而维�?探索\"能力 利用与最优性的关系�? 利用与最优性密切相�?—�?因为最优策略本质上是贪婪策略（始终选择当前动作值最大的动作）。�?贪婪策略的核心思想�?**通过牺牲部分最优�?利用效率来增强探索能�?*\"�?\n若想提升利用效率与策略最优性，需**减小 ε �?* 若想增强探索能力，则需**增大 ε �?* 直观理解：�?值就像探索的\"温度�?—�?ε 越大，探索越\"�?（尝试更多动作）；�?越小，探索越\"�?（更偏向利用已知的好动作）�? 接下来，我们通过几个具体示例来分析这种权衡关系，看看不同�?ε 值会带来什么影响�?","mc-basic#MC Basic":"现在让我们看看如何将策略迭代\"改�?成无模型版本。MC Basic 算法可以看作是策略迭代的\"无模型版�?�?\n关键区别：策略迭代需�?解方�?（贝尔曼方程），MC Basic 只需�?算平均�?（经验平均）。这就是为什�?MC 方法更实�?—�?因为它不需要知道环境的内部机制�? 核心思路：既然我们不知道环境模型，那就用\"经验\"来替�?理论\"。具体来说，我们通过收集大量�?episode，用这些 episode 的回报平均值来估计动作价值，而不是通过求解贝尔曼方程�?","mc-basic-a-model-free-variant-of-policy-iteration#MC Basic (a model-free variant of policy iteration)":"Initialization: Initial guess π₀. Goal: Search for an optimal policy. For the kth iteration (k = 0, 1, 2, ...), do: For every state s �?S, do: For every action a �?A(s), do: Collect sufficiently many episodes starting from (s, a) by following πₖ Policy evaluation: q_π�?s, a) �?q�?s, a) = the average return of all the episodes starting from (s, a) Policy improvement: a*�?s) = arg max_a q�?s, a) πₖ₊�?a|s) = 1 if a = a*�?s), and πₖ₊�?a|s) = 0 otherwise 关键理解：与策略迭代不同，MC Basic 不需要求解贝尔曼方程，而是通过收集 episode 数据，用经验平均来估计动作价值。这就是\"无模�?的含义�?","mc-exploring-starts#MC Exploring Starts":"通过前面的学习，我们已经掌握�?MC Basic 的基本原理。但你可能已经注意到一个问题：MC Basic 的效率似乎不太高�? 确实，在实际应用中，我们不仅希望算法能找到最优策略，还希望它�?高效地利用样�?�?及时地更新策�?。让我们看看如何改进 MC Basic 算法�?\n改进方向：我们可以从两个角度来提升效率：\n更高效地利用样本：一�?episode 可能访问多个状�?动作对，如何充分利用这些信息�?\u003e 2. 更及时地更新策略：是否必须等到所�?episode 收集完毕才能更新�?","mc-exploring-starts-an-efficient-variant-of-mc-basic#MC Exploring Starts (an efficient variant of MC Basic)":"Initialization: Initial policy π₀(a|s) and initial value q(s, a) for all (s, a). Returns(s, a) = 0 and Num(s, a) = 0 for all (s, a). Goal: Search for an optimal policy. For each episode, do: Episode generation: Select a starting state-action pair (s₀, a₀) and ensure that all pairs can be possibly selected (this is the exploring-starts condition). Following the current policy, generate an episode of length T: s₀, a₀, r�? ..., s_{T-1}, a_{T-1}, r_T. Initialization for each episode: g �?0 For each step of the episode, t = T-1, T-2, ..., 0, do: g �?γg + r_{t+1} Returns(s_t, a_t) �?Returns(s_t, a_t) + g Num(s_t, a_t) �?Num(s_t, a_t) + 1 Policy evaluation: q(s_t, a_t) �?Returns(s_t, a_t) / Num(s_t, a_t) Policy improvement: π(a|s_t) = 1 if a = arg max_a q(s_t, a) and π(a|s_t) = 0 otherwise","mc-exploring-starts-算法综合两种改#MC Exploring Starts 算法：综合两种改�?":"综合上述两种技术（高效样本利用和高效策略更新），我们可以得�?蒙特卡洛探索起点算法（MC Exploring Starts�?�?\n改进总结：MC Exploring Starts = MC Basic + 每次访问策略 + 逐情节更新。这样既提高了样本利用效率，又加快了策略更新速度�?","mc-ϵ-greedy-a-variant-of-mc-exploring-starts#MC ϵ-Greedy (a variant of MC Exploring Starts)":"Initialization: Initial policy π₀(a|s) and initial value q(s, a) for all (s, a). Returns(s, a) = 0 and Num(s, a) = 0 for all (s, a). ε �?(0, 1] Goal: Search for an optimal policy. For each episode, do: Episode generation: Select a starting state-action pair (s₀, a₀) (the exploring starts condition is not required). Following the current policy, generate an episode of length T: s₀, a₀, r�? ..., s_{T-1}, a_{T-1}, r_T. Initialization for each episode: g �?0 For each step of the episode, t = T-1, T-2, ..., 0, do: g �?γg + r_{t+1} Returns(s_t, a_t) �?Returns(s_t, a_t) + g Num(s_t, a_t) �?Num(s_t, a_t) + 1 Policy evaluation: q(s_t, a_t) �?Returns(s_t, a_t) / Num(s_t, a_t) Policy improvement: Let a* = arg max_a q(s_t, a) and π(a|s_t) = 1 - (|A(s_t)| - 1)ε / |A(s_t)| if a = a* π(a|s_t) = ε / |A(s_t)| if a �?a*","monte-carlo-methods#Monte Carlo Methods":"Monte Carlo Methods 写在前面：在前面的章节中，我们学习了价值迭代和策略迭代，它们都假设我们已知环境模型（即状态转移概�?p(s'|s,a) 和奖励概�?p(r|s,a)）。但在很多实际场景中，我�?没有环境模型*，只有智能体与环境交互产生的数据（episode）。这一章我们要学习如何\"**用数据替代模�?*\"�?\u003e 蒙特卡洛方法（Monte Carlo Methods）就是解决这�?无模�?（model-free）问题的经典方法。它的核心思想很简单：通过收集大量�?episode 数据，用经验平均来估计动作价值，然后基于这些估计改进策略�?\u003e 通过这一章的学习，你会看到：即使不知道环境的\"内部机制\"，我们也能通过\"试错\"�?经验总结\"来学习最优策略。这就是蒙特卡洛方法的魅力所在�? 核心思想：If we do not have a model, we must have some data. If we do not have data, we must have a model. If we have neither, then we are not able to find optimal policies.\nThe “data” in reinforcement learning usually refers to the agent’s interaction experiences with the environment","step-级更新理解-mc-ϵ-greedy-的细粒度更新机制#Step 级更新：理解 MC ϵ-Greedy 的细粒度更新机制":"在深入理解算法之前，让我们先澄清一个常见的误解�?\n常见误解：你以为\"1 �?episode 对应 1 次策略更�?，但 MC ϵ-Greedy 的策略更新是\"�?step 反向更新\" —�?1 �?episode 包含 N �?step（示例中 N=100 万），这 N �?step 会逐个参与策略更新，相当于 1 �?episode 里有 N �?细粒度更�?，而不�?1 �?粗粒度更�?�? 为什么这样设计？ 因为每个 step 都包含了从该 step �?episode 结束的所有信息，所以我们可以立即利用这些信息来更新策略，而不需要等到整�?episode 结束�?","summary#Summary":"通过这一章的学习，我们已经掌握了蒙特卡洛方法的核心思想和技术细节。让我们来总结一下关键要点�?","ε--05中等探#ε = 0.5：中等探�?":"再考虑 ε = 0.5 �?ε-贪婪策略（如上图（d）所示）：尽�?episode 足够长时仍能访问所有动作，但各动作的访问次数分布极不均衡�?\n观察：�?值减�?�?探索能力下降 �?更偏向利�?�?访问次数集中�?看起来更�?的动作上�?","ε--1最强探索均匀随机策略#ε = 1：最强探索（均匀随机策略�?":"考虑 ε = 1 �?ε-贪婪策略（如上图（a）所示）：此时策略在任意状态下选择任意动作的概率均�?0.2（均匀随机策略），探索能力最强�? 特点�?- 由于探索能力极强，只�?episode 长度足够，单一�?episode 就能多次访问所有状�?动作对（见上图（c））\n所有状�?动作对的访问次数几乎均匀分布 适用场景：适合学习初期，需要快速探索整个状态空间�?","ϵ-贪婪策略实现软策略的经典方#ϵ-贪婪策略：实现软策略的经典方�?":"软策略的定义：若某一策略�?任意状态下选择任意动作的概率均为正”，则该策略称为软策略�?\n*为什么叫\"软策�?�? 因为它不�?硬�?地只选一个动作（概率�?1），而是\"软�?地给所有动作都分配一定的概率�? 考虑一种极端情况：我们仅有一�?episode。在软策略下，只要这�?episode 足够长，就能多次访问所有状�?动作对。因此，我们无需生成大量从不同状�?动作对起始的 episode，进而可移除\"探索起点\"的要求�? 那么，如何设计一个软策略呢？\nϵ-贪婪策略就是实现软策略的经典方法�? **ϵ-贪婪策略的特�?�? ϵ-贪婪策略是一类常见的软策略。它是一种随机策略，其特点是：选择\"贪婪动作\"的概率更高，同时选择其他任意动作的概率均为非零且相等。其中，”**贪婪动作（greedy action�?“指的�?动作值最大的动作”�? 数学形式�? 具体来说，设参数 ε �?[0, 1]，对应的 ϵ-贪婪策略概率形式如下�?\nπ(a|s) = { 1 - (|A(s)| - 1)ε / |A(s)|, �?a 为贪婪动�? ε / |A(s)|, �?a 为其�?|A(s)| - 1 个动�?} 其中，|A(s)| 表示\"状�?s 对应的动作数�?�? 特殊情况�?- �?ε = 0 时，ϵ-贪婪策略退化为贪婪策略（仅选择贪婪动作，概率为 1�?- �?ε = 1 时，选择任意动作的概率均等于 1/|A(s)|（即均匀随机策略�? 概率性质�? 对任�?ε �?[0, 1]，选择贪婪动作的概率始终大于选择其他任意动作的概率，推导如下�? 1 - (|A(s)| - 1)ε / |A(s)| = 1 - ε + ε/|A(s)| �?ε/|A(s)|\n实现方式�? 尽管 ϵ-贪婪策略是随机策略，但我们可通过以下方式遵循该策略选择动作�?\n首先生成一个服�?[0, 1] 区间均匀分布的随机数 x �?x �?ε，则选择贪婪动作 �?x \u003c ε，则从状�?s 的动作集�?A(s) 中随机选择一个动作（选择概率均为 1/|A(s)|，可能再次选中贪婪动作�? 通过这种方式，选择贪婪动作的总概率为 1 - ε + ε/|A(s)|，选择其他任意单个动作的概率均�?ε/|A(s)|�?","ϵ-贪婪策略的探索能力分析如何选择-ε-值#ϵ-贪婪策略的探索能力分析：如何选择 ε 值？":"通过上面的分析，我们已经看到�?ε 值对策略最优性的影响。现在让我们从另一个角度来分析�?不同 ε 值下的探索能力如何？*\n**思�?*：探索能力越强，是否意味着算法性能越好？答案取决于你的目标——是快速探索整个状态空间，还是快速收敛到最优策略？","ϵ-贪婪策略的最优性分析ε-值如何影响性能#ϵ-贪婪策略的最优性分析：ε 值如何影响性能�?":"让我们通过具体例子来看�?ε 值如何影响策略的最优性�?\n关键问题：�?值越大，探索能力越强，但这是否意味着性能越好？答案可能出乎你的意料�? **图（a�?*展示�?贪婪最优策�?及其对应的最优状态值；**图（b�?（d�?*则展示了若干\"一�?ε-贪婪策略\"的状态值�?\n一致策略的定义：若两个 ε-贪婪策略�?概率最高的动作\"相同，则称这两个策略�?一致的\"�? 观察结果�? 从图中可观察到：随着 ε 值增大，ε-贪婪策略的状态值逐渐降低，这表明其最优性在不断变差。尤其当 ε 增大�?**** 时，目标状态的状态值达到最�?—�?这是因为 ε 值越大，智能体从目标区域出发�?进入周围禁止区域\"的概率越高，从而更易获得负奖励�?","ϵ-贪婪策略融入蒙特卡洛学习#�?ϵ-贪婪策略融入蒙特卡洛学习":"现在我们已经理解�?ϵ-贪婪策略的原理，接下来要做的就是将它融入蒙特卡洛学习�? 核心思路：要�?ϵ-贪婪策略融入蒙特卡洛学习，只需�?策略改进步骤\"中的\"贪婪策略\"替换�?ϵ-贪婪策略\"即可�?\n关键改变：之前我们总是选择\"最好的动作\"（贪婪策略），现在我们要�?最好的动作\"�?其他动作\"之间做平衡（ϵ-贪婪策略）�? 原始策略改进�? 蒙特卡洛基础算法（MC Basic）或蒙特卡洛探索起点算法（MC Exploring Starts）中的策略改进步骤，目标是求解以下问题：\nπ_{k+1}(s) = arg max_{π �?Π} ∑_a π(a|s) q_{π_k}(s, a) (5.4)\n其中，Π 表示\"所有可能策略的集合\"。我们已知式�?.4）的解是一个贪婪策略：\nπ_{k+1}(a|s) = { 1, �?a = a_k^* 0, �?a �?a_k^* } 这里，a_k^* = arg max_a q_{π_k}(s, a)（即状�?s 下的贪婪动作）�? 修改后的策略改进�? 如今，我们将策略改进步骤修改为求解以下问题：\nπ_{k+1}(s) = arg max_{π �?Π_ε} ∑_a π(a|s) q_{π_k}(s, a) (5.5)\n其中，Π_ε 表示\"给定参数 ε 下，所�?ϵ-贪婪策略的集�?。通过这种修改，我们强制策略为 ϵ-贪婪策略�? 式（5.5）的解为�?\nπ_{k+1}(a|s) = { 1 - (|A(s)| - 1)ε / |A(s)|, �?a = a_k^* ε / |A(s)|, �?a �?a_k^* } 这里，a_k^* = arg max_a q_{π_k}(s, a)�? 经过上述修改，我们得到了一种新算法—�?蒙特卡洛 ϵ-贪婪算法（MC ϵ-Greedy�?�?\n算法优势：MC ϵ-Greedy 不需要探索起点条件，这意味着它可以在更实际的环境中应用。你只需要从环境的初始状态开始，让智能体按照 ϵ-贪婪策略行动，就能逐步学习到最优策略�?","三种-mc-算法对比#三种 MC 算法对比":"算法名称 核心特点 主要改进 *蒙特卡洛基础算法（MC Basic�? 最简单的基于蒙特卡洛的强化学习算�? 将策略迭代算法中\"基于模型的策略评估步�?，替换为\"无模型的蒙特卡洛估计模块\"。在样本足够多的前提下，可保证该算法收敛到最优策略和最优状态值�? *蒙特卡洛探索起点算法（MC Exploring Starts�? MC Basic 的变�? 通过�?MC Basic 中采�?首次访问策略（first-visit strategy�?�?每次访问策略（every-visit strategy�?，能更高效地利用样本�? *蒙特卡洛 ε-贪婪算法（MC ε-Greedy�? MC Exploring Starts 的变�? �?策略改进步骤\"中，不再搜索\"贪婪策略\"，而是搜索\"最�?ε-贪婪策略\"。通过这种方式，策略的探索能力得到增强，从而可移除\"探索起点\"这一约束条件�?","为什么是反向更新#为什么是反向更新�?":"这要结合 MC 算法的核心机�?—�?�?**回报（G�?*“估计 Q 值，而回报需要从\"当前 step 之后的所有奖�?计算。为了高效计算，MC ϵ-Greedy 会在 1 �?episode 结束后，从最�?1 �?step（T-1 步）反向遍历到第 1 �?step�? 步），每遍历 1 �?step 就做一次更新�?\n**记忆技�?*：回报是�?当前时刻”�?未来\"的所有奖励，所以从后往前算最方便 —�?先算最后一步的回报，再往前累加�?","从有模型到无模型为什么需要蒙特卡洛方法#从有模型到无模型：为什么需要蒙特卡洛方法？":"在深入算法细节之前，让我们先思考一个问题：为什么我们需要无模型的方法？\n想象一下，你要训练一个机器人学习走路。在价值迭代和策略迭代中，你需要知�?如果机器人向前迈一步，它会以多大的概率摔�?（这就是环境模型）。但在现实中，你可能根本不知道这个概率是多少，你只能让机器人实际去走，然后观察它是否摔倒�? 这就是无模型强化学习的核心场景：我们不知道环境的内部机制，但我们有大量的交互数据�? 蒙特卡洛方法就是利用这些数据来学习的方法。它的名字来源于著名�?蒙特卡洛赌场\"——就像通过大量随机试验来估计概率一样，蒙特卡洛方法通过大量 episode 来估计动作价值�?","关键要点回顾#关键要点回顾":"**无模�?vs 有模�?*：MC 方法不需要环境模型，只需�?episode 数据 样本效率：每次访问策略比初始访问策略更高�?3. 更新频率：逐情节更新比批量更新更及�?4. **探索与利�?：�?贪婪策略通过调整 ε 值来平衡探索和利�?5. **Step 级更�?：MC ϵ-Greedy 采用反向遍历，每�?step 都会触发更新 记忆口诀：MC = 无模�?+ 经验平均 + 探索利用平衡。从 MC Basic �?MC Exploring Starts �?MC ϵ-Greedy，每一步都在提升算法的实用性和效率�?","具体操作流程#具体操作流程":"我们用示例中\"1 �?100 万步�?episode\"为例，拆�?step 级更�?的具体操作（对应算法 5.3 的核心步骤）�? **假设 1 �?episode �?step 序列�?*�? s₀, a₀, r�?�?s�? a�? r�?�?s�? a�? r�?�?... �?s₉₉₉₉₉₈, a₉₉₉₉₉₈, r₉₉₉₉₉₉ �?s₉₉₉₉₉₉, a₉₉₉₉₉₉, r₁₀₀₀₀₀₀\n（共 100 万步，记�?t = 0 �?t = 999999，最后一步是 t = 999999�? **策略更新的流程（反向遍历每个 step�?�? **初始�?：每�?episode 开始前，重置折扣回�?g = 0（用于累计当�?step 之后的所有折扣奖励）�? **从最�?1 步（t = 999999）反向算到第 1 步（t = 0�?*�?\n�?t = 999999（最�?1 步）�? 1. **算回�?：g = γ·g + r₁₀₀₀₀₀₀（此�?g = 0，故 g = r₁₀₀₀₀₀₀，即最后一步的奖励�? 2. 累积累计回报：Returns(s₉₉₉₉₉₉, a₉₉₉₉₉₉) += g 3. 累加访问次数：Num(s₉₉₉₉₉₉, a₉₉₉₉₉₉) += 1 4. **更新 Q �?：q(s₉₉₉₉₉₉, a₉₉₉₉₉₉) = Returns / Num（用平均回报估计 Q 值） 5. 更新策略：根据当�?Q 值选贪婪动�?a*，按 ε-贪婪公式调整 π(a|s₉₉₉₉₉₉) 的概�?\n�?t = 999998（倒数第二步）�? 1. **算回�?：g = γ·g + r₉₉₉₉₉₉（此�?g 已包�?t = 999999 步的折扣奖励，现在加上当前步�?r₉₉₉₉₉₉，就�?t = 999998 步之后的总折扣回报） 2. **重复上述 2-5 的操�?，更�?(s₉₉₉₉₉₈, a₉₉₉₉₉₈) �?Returns、Num、Q 值和策略\n… 以此类推，直到遍历完 t = 0（第一步），完成整�?episode 的更新�?","实践建议动态调ε-#实践建议：动态调�?ε �?":"在实际应用中，一种常用的策略�?**初始设置较大�?ε 值以增强探索，随后逐步减小 ε 值以保证最终策略的最优�?*\"—�?这种动态调整方式能在学习初期充分探索状态空间，后期则聚焦于利用已学习到的最优动作�?\n**记忆技�?*：�?�?= 探索�?温度\"—�?开始时\"高温\"（高探索），逐渐\"降温\"（高利用），最�?冷却\"（最优策略）�?","探索起点条件的要求与限制#探索起点条件的要求与限制":"“探索起点条件“要求：从每一个状�?动作对起始，都要生成足够多的 episode。根据大数定律，只有当所有状�?动作对都被充分探索时，我们才能准确估计它们的动作值，进而成功找到最优策略�?\n反之，若某一动作未被充分探索，其动作值的估计可能存在偏差 —�?即便该动作实际上是最优动作，也可能不被策略选中�? 蒙特卡洛基础算法（MC Basic）和蒙特卡洛探索起点算法（MC Exploring Starts）均需满足探索起点条件。然而，在许多实际应用中（尤其是涉及与环境物理交互的场景），这一条件很难满足�? 实际困难：想象你在训练一个机器人，你无法随意\"重置\"到任意状�?动作对。你只能从环境的初始状态开始，这大大限制了探索的灵活性�? 那么，有没有办法移除这个限制呢？\n答案是肯定的。接下来，我们对蒙特卡洛探索起点算法（MC Exploring Starts）进行扩展，移除�?探索起点条件”。该条件本质上要求所有状�?动作对都能被充分访问，而这一要求也可通过”**软策略（soft policies�?*“实现�?\n核心思路：如果我们能让策略在任意状态下都有概率选择任意动作，那么即使从固定起点开始，只要 episode 足够长，我们也能访问到所有状�?动作对�?","更高效地利用样本一episode-的多种用#更高效地利用样本：一�?episode 的多种用�?":"让我们先思考一个问题：*一�?episode 只能用来估计一个状�?动作对吗�?\n答案是否定的。蒙特卡洛（MC）强化学习的一个重要方面，就是如何更高效地利用样本。具体来说，假设我们遵循某一策略 π，得到了如下一�?episode 的样本：\ns�?→[a₂] s�?→[a₄] s�?→[a₂] s�?→[a₃] s�?→[a₁] ... (5.3)\n其中，下标代表状态或动作的索引，而非时间步�?\n关键概念：在一�?episode 中，某一状�?动作对每出现一次，就称为对该状�?动作对的一�?**访问（visit�?*\"�? 观察：这�?episode 不仅访问了起始状�?动作�?(s�? a�?，还访问�?(s�? a�?、(s�? a�?、(s�? a�? 等多个其他状�?动作对。我们能否利用这些信息来估计更多状�?动作对的动作值呢�?","更高效地更新策略何时更新#更高效地更新策略：何时更新？":"现在让我们思考另一个问题：我们必须在收集完所�?episode 之后才能更新策略吗？\n蒙特卡洛强化学习的另一个重要方面，�?何时更新策略\"。目前有两种可用策略�? 第一种策略：批量更新\n在策略评估步骤中，先收集所有从\"同一状�?动作�?起始�?episode，再利用这些 episode 的回报平均值来近似该状�?动作对的动作值�? 蒙特卡洛基础算法（MC Basic）采用的就是这种策略。其缺点在于，智能体必须等到所�?episode 都收集完毕后，才能更新动作值的估计结果�?\n⚠️ 问题：这意味着我们需要等待很长时间才能看到算法的改进，这在实时学习中是不可接受的�? *第二种策略：逐情节更�?\n可克服上述缺点，即利�?单段 episode 的回�?来近似对应状�?动作对的动作值。这样一来，每获取一�?episode，就能立即得到一个粗略的动作值估计，进而以\"**逐情节（episode-by-episode�?*“的方式更新策略�?\n疑问解答：有人可能会疑问：单�?episode 的回报无法准确近似动作值，这种策略是否可行？事实上，该策略属于上一章介绍的”**广义策略迭代（generalized policy iteration�?“范畴 —�?即便动作值的估计不够精确，我们仍然可以更新策略。关键在于：**不完美的估计 + 及时的更�?\u003e 完美的估�?+ 延迟的更�?�?","最ε-贪婪策略-vs-贪婪最优策#最�?ε-贪婪策略 vs 贪婪最优策�?":"上图展示�?最�?ε-贪婪策略\"（即�?ε-贪婪策略集合 Π_ε 中最优的策略）及其对应的状态值：\n**�?ε = 0 �?：该策略即为贪婪策略，且在所有策略中均最�?- **�?ε 较小�?（如 ε = 0.1）：最�?ε-贪婪策略�?贪婪最优策�?保持一�?- **但当 ε 增大到一定程�?*（如 ε = 0.2）：得到的最�?ε-贪婪策略�?贪婪最优策�?不再一�? 若想让最�?ε-贪婪策略�?贪婪最优策�?保持一致，ε 值需设置得足够小�? 为什�?ε 值较大时策略会不一致？\n我们可从目标状态的策略选择来解释：\n在贪婪策略下：目标状态的最优动作是\"停留不动\"，以持续获得正奖�?- 但当 ε 值较大时�?停留不动\"仍有较高概率因探索而误入禁止区域、获得负奖励 结果：此时，目标状态的最优动作会�?停留\"变为\"逃离\"，导致策略一致性被打破 直观理解：�?值太�?�?探索太强 �?即使在目标区域也会随机走 �?容易走到禁区 �?不如主动\"逃离\"目标区域更安�?�?策略改变","核心思想#核心思想":"总结一下，Monte Carlo estimation 是一类广义的技术方法，指通过\"随机样本\"来解决近似问题的技术总称�? 无模型蒙特卡洛强化学习的核心思想：将\"有模型的策略迭代算法\"转化�?无模型算�?。具体而言，策略迭代算法依赖系统模型计算值函数，而蒙特卡洛强化学习则将策略迭代中\"基于模型的策略评估步�?，替换为\"无模型的蒙特卡洛策略评估步骤\"�?\n核心转变：从\"知道环境如何工作\"�?通过试错来学�?。这就是蒙特卡洛方法的革命性意义�?","策略改进选择最优动#策略改进：选择最优动�?":"通过比较上述 5 个动作的动作值，我们发现�? q_{π₀}(s�? a�? = q_{π₀}(s�? a�? = γ³/(1 - γ) \u003e 0\n这两个动作值是最大的。因此，可得到新策略�? π�?a₂|s�? = 1 �?π�?a₃|s�? = 1\n重要说明：对于每个动作，理论上需要收集足够多且足够长�?episode，才能有效近似其动作值。但由于本示例中�?策略”�?环境模型\"均为确定性的（即每次执行相同动作都会产生相同轨迹），多次运行也会生成完全一致的轨迹，因此每个动作值的估计仅需 1 �?episode 即可�? 实际应用：在真实场景中，环境通常是随机的，所以需要收集多�?episode 来平均，以减少随机性的影响�?","算法描述#算法描述":"让我们先看看算法的整体框架，然后再通过例子深入理解�?","算法描述-1#算法描述":"让我们看看完整的算法流程�?","结论#结论":"核心要点�? �?episode�?00 万步）里，每�?step 都会触发一�?Q 值更�?+ 策略更新\"，相当于 1 �?episode 包含 100 万次细粒度的策略调整，而不�?1 次更新。这就是 step �?episode 细粒度更小的核心体现�? 为什么这样设计？ 因为每个 step 的回报都包含�?从该 step �?episode 结束\"的所有信息，所以每处理一�?step，我们就能立即更新对应的 Q 值和策略，不需要等到整�?episode 结束。这样可以让算法更快地学习和适应�?","访问策略三种不同的样本利用方式#访问策略：三种不同的样本利用方式":"我们可以采用不同的策略来利用这些访问信息。在深入细节之前，让我们先明确三种访问策略的定义�?\nQ：什么是初始访问策略（initial-visit）、首次访问策略（first-visit）和每次访问策略（every-visit）？\n*A�? 它们是利�?episode 中样本的不同策略。一�?episode 可能会访问多个状�?动作对，三种策略的定义如下：\n初始访问策略：仅利用整个 episode 来估�?episode 起始状�?动作�?的动作�?\u003e - 每次访问策略：每当某个状�?动作对在 episode 中被访问时，就利用该访问之后�?episode 轨迹来估计该状�?动作对的动作值，能充分利用样�?\u003e - 首次访问策略：仅在某个状�?动作�?第一次被访问\"时，利用该访问之后的 episode 轨迹来估计其动作值，同样能高效利用样�? *第一种策略：初始访问（initial visit�? 即一�?episode 仅用于估计该 episode 起始的状�?动作对的动作值。以式（5.3）的 episode 为例，初始访问策略仅会估计状�?动作�?(s�? a�? 的动作值�? 蒙特卡洛基础算法（MC Basic）采用的就是初始访问策略。然而，这种策略的样本效率较�?—�?因为�?episode 还访问了 (s�? a�?、(s�? a�?、(s�? a�? 等多个其他状�?动作对，这些访问信息同样可用于估计对应状�?动作对的动作值�? *第二种策略：每次访问（every-visit�?\n我们可以将式�?.3）的 episode 分解为多�?**子情节（subepisode�?*\"�? 某一状�?动作对被访问后所生成的轨迹，可视为一段新�?episode。这些新 episode 能用于估计更多状�?动作对的动作值，从而更高效地利用原�?episode 中的样本�? *第三种策略：首次访问（first-visit�?\n此外，某一状�?动作对在一�?episode 中可能被访问多次。例如，在式�?.3）的 episode 中，状�?动作�?(s�? a�? 就被访问了两次�?\n**首次访问策略（first-visit strategy�?*：若仅统计该状�?动作对在 episode 中的\"第一次访�? **每次访问策略（every-visit strategy�?*：若统计该状�?动作对在 episode 中的\"每一次访�? 效率对比：从样本利用效率来看，每次访问策略是最优的。若一�?episode 足够长，能多次访问所有状�?动作对，那么仅通过这一�?episode，采用每次访问策略就可能完成对所有状�?动作对动作值的估计�? 注意事项：每次访问策略所获取的样本存在相关�?—�?因为从某状�?动作�?第二次访�?开始的轨迹，本质是�?第一次访�?起始轨迹的子集。但如果两次访问在轨迹中相距较远，这种相关性会较弱，对估计结果的影响也有限�?","通过例子理解-mc-basic从经验中学#通过例子理解 MC Basic：从经验中学�?":"理论说再多，不如看个例子。让我们通过一个具体例子来理解 MC Basic 算法是如何工作的�?\n学习策略：通过这个例子，你会看�?MC 方法如何�?经验\"来估�?理论�?。虽然估计可能不完美，但足够指导策略改进�? 我们重点分析\"状�?s₁ �?5 个动作（a₁ �?a₅�?，每个动作对应不同的 episode 轨迹，进而推�?Q 值�?"},"title":"第5章 蒙特卡洛方法"},"/docs/self-study/ai/rl/chapter-6/":{"data":{"a-deterministic-formulation-of-sgd#A deterministic formulation of SGD":"式（6.13）中 SGD 的经典表述涉及随机变量，但实际应用中，我们常会遇到不涉及任何随机变量的 SGD “确定性表述”。\n考虑一组实数集合 ${x_i}{i=1}^n$（其中 $x_i$ 不一定是某个随机变量的样本），需解决的优化问题为 “最小化平均值”：$\\min{w} J(w) = \\frac{1}{n}\\sum_{i=1}^n f(w, x_i)$。其中，$f(w, x_i)$ 是参数化函数（$w$ 是待优化参数）。求解该问题的常规梯度下降算法为：\nwk+1=wk−αk∇wJ(wk)=wk−αk⋅1n∑i=1n∇wf(wk,xi)w_{k+1} = w_k - \\alpha_k \\nabla_w J(w_k) = w_k - \\alpha_k \\cdot \\frac{1}{n}\\sum_{i=1}^n \\nabla_w f(w_k, x_i)wk+1​=wk​−αk​∇w​J(wk​)=wk​−αk​⋅n1​i=1∑n​∇w​f(wk​,xi​)若集合 ${x_i}_{i=1}^n$ 的规模较大（如海量样本），且每次仅能从集合中获取一个数值，则采用增量方式更新 $w_k$ 更为高效，算法形式为：\nwk+1=wk−αk∇wf(wk,xk)(6.16) \\begin{aligned} w_{k+1} = w_k - \\alpha_k \\nabla_w f(w_k, x_k) \\tag{6.16} \\end{aligned} wk+1​=wk​−αk​∇w​f(wk​,xk​)​(6.16)需特别注意：此处的 $x_k$ 是第 $k$ 步 “获取的数值”，而非集合 ${x_i}_{i=1}^n$ 中固定的第 $k$ 个元素。\n式（6.16）的算法形式与 SGD 高度相似，但因不涉及随机变量或期望，其与 SGD 的关系曾存在争议（例如：该算法是否属于 SGD？应按顺序还是随机使用集合中的数值？）。对此的核心解释是：可通过引入随机变量，将确定性表述转化为 SGD 的经典随机表述。具体步骤如下：\n定义随机变量 $X$，其取值空间为集合 ${x_i}_{i=1}^n$； 假设 $X$ 服从均匀概率分布，即 $p(X = x_i) = \\frac{1}{n}$（每个数值被选中的概率相等）； 此时，确定性优化问题可严格等价于随机性优化问题：$\\min_{w} J(w) = \\frac{1}{n}\\sum_{i=1}^n f(w, x_i) = \\mathbb{E}[f(w, X)]$ 由此可见，式（6.16）的算法本质就是 SGD。只要 $x_k$ 是从集合 ${x_i}_{i=1}^n$ 中独立均匀采样得到的（而非按固定顺序选取），其估计值就能收敛。需注意：由于采样的随机性，$x_k$ 可能重复取集合中的同一个数值。","application-to-mean-estimation#Application to mean estimation":"wk+1=wk+αk(xk−wk)w_{k+1} = w_k + \\alpha_k(x_k - w_k)wk+1​=wk​+αk​(xk​−wk​)当 $\\alpha_k = \\frac{1}{k}$ 时，我们可得到 $w_{k+1}$ 的解析表达式：$w_{k+1} = \\frac{1}{k}\\sum_{i=1}^k x_i$。但当 $\\alpha_k$ 取一般值时，无法得到这样的解析表达式，此时的收敛性分析也更为复杂。不过我们可以证明，该情况下的均值估计算法本质是一种特殊的 RM 算法，其收敛性也因此可由 RM 定理直接推导得出。\n特别地，定义函数：\ng(w)≜w−E[X]g(w) \\triangleq w - \\mathbb{E}[X]g(w)≜w−E[X]我们的原始问题是求解 $\\mathbb{E}[X]$（随机变量 $X$ 的期望），这一问题可转化为求根问题：求解 $g(w) = 0$。给定任意 $w$ 值，我们能获得的带噪观测值定义为 $\\tilde{g} \\triangleq w - x$（其中 $x$ 是 $X$ 的一个样本）。\n需注意，$\\tilde{g}$ 可改写为：\ng~(w,η)=w−x=w−x+E[X]−E[X]=(w−E[X])+(E[X]−x)≜g(w)+η \\begin{aligned} \\tilde{g}(w, \\eta) \u0026= w - x \\\\ \u0026= w - x + \\mathbb{E}[X] - \\mathbb{E}[X] \\\\ \u0026= (w - \\mathbb{E}[X]) + (\\mathbb{E}[X] - x) \\\\ \u0026\\triangleq g(w) + \\eta \\end{aligned} g~​(w,η)​=w−x=w−x+E[X]−E[X]=(w−E[X])+(E[X]−x)≜g(w)+η​其中 $\\eta \\triangleq \\mathbb{E}[X] - x$（$\\eta$ 为观测误差）。\n针对该求根问题的 RM 算法为：\nwk+1=wk−αkg~(wk,ηk)=wk−αk(wk−xk)w_{k+1} = w_k - \\alpha_k \\tilde{g}(w_k, \\eta_k) = w_k - \\alpha_k(w_k - x_k)wk+1​=wk​−αk​g~​(wk​,ηk​)=wk​−αk​(wk​−xk​)显然，该式与式（6.4）中的均值估计算法完全一致。因此，根据定理 6.1，若满足 条件： $\\sum_{k=1}^{\\infty} \\alpha_k = \\infty$、条件： $\\sum_{k=1}^{\\infty} \\alpha_k^2 \u003c \\infty$，且样本序列 ${x_k}$ 为独立同分布（i.i.d.）序列，则 $w_k$ 几乎必然收敛到 $\\mathbb{E}[X]$。值得一提的是，该收敛性质不依赖于对 $X$ 分布的任何假设。","application-to-mean-estimation-1#Application to mean estimation":"我们将均值估计问题转化为如下优化问题：\nmin⁡wJ(w)=E[12∥w−X∥2]≜E[f(w,X)](6.14) \\begin{aligned} \\min_{w} J(w) = \\mathbb{E}\\left[ \\frac{1}{2}\\|w - X\\|^2 \\right] \\triangleq \\mathbb{E}[f(w, X)] \\tag{6.14} \\end{aligned} wmin​J(w)=E[21​∥w−X∥2]≜E[f(w,X)]​(6.14)其中，$f(w, X) = \\frac{1}{2}|w - X|^2$（$|\\cdot|$ 为范数），其对 $w$ 的梯度为 $\\nabla_w f(w, X) = w - X$。通过求解 $\\nabla_w J(w) = 0$，可验证该优化问题的最优解为 $w^* = \\mathbb{E}[X]$。因此，该优化问题与均值估计问题是等价的。\n求解式（6.14）的梯度下降算法为：\nwk+1=wk−αk∇wJ(wk)=wk−αkE[∇wf(wk,X)]=wk−αkE[wk−X] \\begin{aligned} w_{k+1} \u0026= w_k - \\alpha_k \\nabla_w J(w_k) \\\\ \u0026= w_k - \\alpha_k \\mathbb{E}[\\nabla_w f(w_k, X)] \\\\ \u0026= w_k - \\alpha_k \\mathbb{E}[w_k - X] \\end{aligned} wk+1​​=wk​−αk​∇w​J(wk​)=wk​−αk​E[∇w​f(wk​,X)]=wk​−αk​E[wk​−X]​该梯度下降算法无法直接应用，因为等式右侧的 $\\mathbb{E}[w_k - X]$（即 $\\mathbb{E}[X]$）是未知的——而 $\\mathbb{E}[X]$ 正是我们需要求解的目标。\n求解式（6.14）的 SGD 算法为：\nwk+1=wk−αk∇wf(wk,xk)=wk−αk(wk−xk)w_{k+1} = w_k - \\alpha_k \\nabla_w f(w_k, x_k) = w_k - \\alpha_k (w_k - x_k)wk+1​=wk​−αk​∇w​f(wk​,xk​)=wk​−αk​(wk​−xk​)其中，$x_k$ 是第 $k$ 步迭代时获取的样本。值得注意的是，该 SGD 算法与式（6.4）中的迭代式均值估计算法完全一致。因此，式（6.4）是专为求解均值估计问题设计的 SGD 算法。","bgd-sgd-and-mini-batch-gd#BGD, SGD, and mini-batch GD":"随机梯度下降（SGD）在每次迭代中仅使用一个样本，接下来我们将介绍小批量梯度下降（MBGD）—— 它在每次迭代中会使用更多几个样本。若每次迭代都使用全部样本，则该算法被称为批量梯度下降（BGD）。\n具体来说，假设给定随机变量 $X$ 的一组随机样本 ${x_i}_{i=1}^n$，我们希望找到能最小化目标函数 $J(w) = \\mathbb{E}[f(w, X)]$ 的最优解。用于求解该问题的批量梯度下降（BGD）、随机梯度下降（SGD）和小批量梯度下降（MBGD）算法分别如下：\nwk+1=wk−αk⋅1n∑i=1n∇wf(wk,xi)（BGD）w_{k+1} = w_k - \\alpha_k \\cdot \\frac{1}{n}\\sum_{i=1}^n \\nabla_w f(w_k, x_i) \\quad \\text{（BGD）}wk+1​=wk​−αk​⋅n1​i=1∑n​∇w​f(wk​,xi​)（BGD）wk+1=wk−αk⋅1m∑j∈Ik∇wf(wk,xj)（MBGD）w_{k+1} = w_k - \\alpha_k \\cdot \\frac{1}{m}\\sum_{j \\in I_k} \\nabla_w f(w_k, x_j) \\quad \\text{（MBGD）}wk+1​=wk​−αk​⋅m1​j∈Ik​∑​∇w​f(wk​,xj​)（MBGD）wk+1=wk−αk∇wf(wk,xk)（SGD）w_{k+1} = w_k - \\alpha_k \\nabla_w f(w_k, x_k) \\quad \\text{（SGD）}wk+1​=wk​−αk​∇w​f(wk​,xk​)（SGD）在批量梯度下降（BGD）算法中，每次迭代都会使用全部样本。当样本量 $n$ 较大时，$\\frac{1}{n}\\sum_{i=1}^n \\nabla_w f(w_k, x_i)$ 会接近真实梯度 $\\mathbb{E}[\\nabla_w f(w_k, X)]$。\n在小批量梯度下降（MBGD）算法中，$I_k$ 是第 $k$ 步得到的集合 ${1, \\dots, n}$ 的子集，该子集的大小为 $|I_k| = m$（即小批量样本数为 $m$），且假设 $I_k$ 中的样本服从独立同分布（i.i.d.）。\n在随机梯度下降（SGD）算法中，$x_k$ 是第 $k$ 步从样本集 ${x_i}_{i=1}^n$ 中随机采样得到的样本。\n小批量梯度下降（MBGD）可看作是随机梯度下降（SGD）与批量梯度下降（BGD）之间的中间形式，其与后两者的对比优势如下：\n与 SGD 相比：MBGD 使用多个样本（而非单个样本）计算梯度，随机性被平均削弱，迭代过程更稳定； 与 BGD 相比：MBGD 无需每次迭代都使用全部样本，在样本量较大时灵活性更高，计算成本更低。 需要注意的是：若 $m = 1$（小批量大小为 1），则 MBGD 会退化为 SGD；但即使 $m = n$（小批量大小等于总样本数），MBGD 也不一定等同于 BGD—— 因为 MBGD 是从样本集中随机选取 $n$ 个样本（可能存在重复），而 BGD 是固定使用全部 $n$ 个样本，随机选取的 $n$ 个样本可能无法覆盖原始样本集中的所有元素。\n一般来说，小批量梯度下降（MBGD）的收敛速度比随机梯度下降（SGD）更快。原因在于：SGD 仅用单个样本的梯度 $\\nabla_w f(w_k, x_k)$ 近似真实梯度，而 MBGD 使用 $\\frac{1}{m}\\sum_{j \\in I_k} \\nabla_w f(w_k, x_j)$ 近似真实梯度 —— 通过对多个样本的梯度取平均，随机性被抵消，使得该近似值更接近真实梯度。MBGD 的收敛性可参照 SGD 的收敛性证明思路推导得出。 具体来说，给定一组数值 ${x_i}{i=1}^n$，我们的目标是计算其均值 $\\bar{x} = \\frac{1}{n}\\sum{i=1}^n x_i$。该问题可等价转化为如下优化问题：\nmin⁡wJ(w)=12n∑i=1n∥w−xi∥2\\min_{w} J(w) = \\frac{1}{2n}\\sum_{i=1}^n \\|w - x_i\\|^2wmin​J(w)=2n1​i=1∑n​∥w−xi​∥2该优化问题的最优解为 $w^* = \\bar{x}$（即均值）。用于求解该问题的三种梯度下降算法分别如下：\nwk+1=wk−αk⋅1n∑i=1n(wk−xi)=wk−αk(wk−xˉ)（BGD）w_{k+1} = w_k - \\alpha_k \\cdot \\frac{1}{n}\\sum_{i=1}^n (w_k - x_i) = w_k - \\alpha_k (w_k - \\bar{x}) \\quad \\text{（BGD）}wk+1​=wk​−αk​⋅n1​i=1∑n​(wk​−xi​)=wk​−αk​(wk​−xˉ)（BGD）wk+1=wk−αk⋅1m∑j∈Ik(wk−xj)=wk−αk(wk−xˉk(m))（MBGD）w_{k+1} = w_k - \\alpha_k \\cdot \\frac{1}{m}\\sum_{j \\in I_k} (w_k - x_j) = w_k - \\alpha_k \\left( w_k - \\bar{x}_k^{(m)} \\right) \\quad \\text{（MBGD）}wk+1​=wk​−αk​⋅m1​j∈Ik​∑​(wk​−xj​)=wk​−αk​(wk​−xˉk(m)​)（MBGD）wk+1=wk−αk(wk−xk)（SGD）w_{k+1} = w_k - \\alpha_k (w_k - x_k) \\quad \\text{（SGD）}wk+1​=wk​−αk​(wk​−xk​)（SGD）其中，$\\bar{x}k^{(m)} = \\frac{1}{m}\\sum{j \\in I_k} x_j$（即第 $k$ 步小批量样本的均值）。\n进一步地，若取步长 $\\alpha_k = \\frac{1}{k}$，可对上述方程求解得到：\nwk+1=1k∑j=1kxˉ=xˉ（BGD）w_{k+1} = \\frac{1}{k}\\sum_{j=1}^k \\bar{x} = \\bar{x} \\quad \\text{（BGD）}wk+1​=k1​j=1∑k​xˉ=xˉ（BGD）wk+1=1k∑j=1kxˉj(m)（MBGD）w_{k+1} = \\frac{1}{k}\\sum_{j=1}^k \\bar{x}_j^{(m)} \\quad \\text{（MBGD）}wk+1​=k1​j=1∑k​xˉj(m)​（MBGD）wk+1=1k∑j=1kxj（SGD）w_{k+1} = \\frac{1}{k}\\sum_{j=1}^k x_j \\quad \\text{（SGD）}wk+1​=k1​j=1∑k​xj​（SGD）上述方程的推导过程与式（6.3）类似，此处省略。从结果可看出：批量梯度下降（BGD）在每一步的估计值都恰好是最优解 $w^* = \\bar{x}$；小批量梯度下降（MBGD）的收敛速度比随机梯度下降（SGD）更快，因为 $\\bar{x}_k^{(m)}$ 本身已是小批量样本的均值，随机性更低。","convergence-of-sgd#Convergence of SGD":"下面给出随机梯度下降（SGD）收敛性的严格证明。\n定理 6.4（SGD 的收敛性）：对于式（6.13）所示的随机梯度下降算法，若满足以下条件，则迭代序列 $w_k$ 几乎必然收敛到方程 $\\nabla_w \\mathbb{E}[f(w, X)] = 0$ 的根。\n（a） $0 \u003c c_1 \\leq \\nabla_w^2 f(w, X) \\leq c_2$； （b） $\\sum_{k=1}^{\\infty} a_k = \\infty$ 且 $\\sum_{k=1}^{\\infty} a_k^2 \u003c \\infty$； （c） 样本序列 ${x_k}_{k=1}^{\\infty}$ 服从独立同分布（i.i.d.）。 以下对定理 6.4 中的三个条件进行说明：\n条件（a）：与函数 $f$ 的凸性相关，要求 $f$ 的曲率存在上下界。此处假设 $w$ 为标量，因此 $\\nabla_w^2 f(w, X)$（$f$ 对 $w$ 的二阶偏导数）也为标量；该条件可推广到 $w$ 为向量的情况 —— 此时 $\\nabla_w^2 f(w, X)$ 即为著名的海森矩阵（Hessian matrix）。\n条件（b）：与罗宾斯-门罗（RM）算法的步长条件类似。事实上，随机梯度下降（SGD）本身就是一种特殊的 RM 算法（如 6.1 节方框中的证明所示）。在实际应用中，步长 $a_k$ 常被选为一个足够小的常数；此时虽然条件（b）不再满足（因为 $\\sum_{k=1}^{\\infty} a_k^2 = \\infty$，而非 $\\sum_{k=1}^{\\infty} a_k^2 \u003c \\infty$），但算法在某种意义下仍能收敛 [24，1.5 节]。\n条件（c）：是样本序列的常见要求，即样本需服从独立同分布。","convergence-pattern-of-sgd#Convergence pattern of SGD":"随机梯度下降（SGD）算法的核心思想是用随机梯度替换真实梯度。但由于随机梯度具有随机性，我们可能会疑问：SGD 的收敛速度是否较慢，或是收敛过程本身具有随机性？幸运的是，通常情况下 SGD 能高效收敛。其存在一种有趣的收敛模式：当估计值 $w_k$ 与最优解 $w^$ 距离较远时，SGD 的表现与常规梯度下降算法相似；仅当 $w_k$ 接近 $w^$ 时，SGD 的收敛过程才会表现出更强的随机性。\n随机梯度与真实梯度之间的相对误差定义为：\nδk≜∣∇wf(wk,xk)−E[∇wf(wk,X)]∣∣E[∇wf(wk,X)]∣\\delta_k \\triangleq \\frac{|\\nabla_w f(w_k, x_k) - \\mathbb{E}[\\nabla_w f(w_k, X)]|}{|\\mathbb{E}[\\nabla_w f(w_k, X)]|}δk​≜∣E[∇w​f(wk​,X)]∣∣∇w​f(wk​,xk​)−E[∇w​f(wk​,X)]∣​为简化分析，我们考虑 $w$（待优化参数）与 $\\nabla_w f(w, x)$（梯度）均为标量的情况。由于 $w^$ 是最优解，满足 $\\mathbb{E}[\\nabla_w f(w^, X)] = 0$，此时相对误差可重写为：\nδk=∣∇wf(wk,xk)−E[∇wf(wk,X)]∣∣E[∇wf(wk,X)]−E[∇wf(w∗,X)]∣=∣∇wf(wk,xk)−E[∇wf(wk,X)]∣∣E[∇w2f(w~k,X)(wk−w∗)]∣(6.15) \\begin{aligned} \\delta_k = \\frac{|\\nabla_w f(w_k, x_k) - \\mathbb{E}[\\nabla_w f(w_k, X)]|}{|\\mathbb{E}[\\nabla_w f(w_k, X)] - \\mathbb{E}[\\nabla_w f(w^*, X)]|} = \\frac{|\\nabla_w f(w_k, x_k) - \\mathbb{E}[\\nabla_w f(w_k, X)]|}{|\\mathbb{E}[\\nabla_w^2 f(\\tilde{w}_k, X)(w_k - w^*)]|} \\tag{6.15} \\end{aligned} δk​=∣E[∇w​f(wk​,X)]−E[∇w​f(w∗,X)]∣∣∇w​f(wk​,xk​)−E[∇w​f(wk​,X)]∣​=∣E[∇w2​f(w~k​,X)(wk​−w∗)]∣∣∇w​f(wk​,xk​)−E[∇w​f(wk​,X)]∣​​(6.15)其中，最后一个等号由中值定理[7,8] 推导得出，且 $\\tilde{w}_k \\in [w_k, w^]$（$\\tilde{w}_k$ 是 $w_k$ 与 $w^$ 之间的某个值）。\n假设 $f$ 为严格凸函数，即对所有 $w$ 和 $X$，其二阶偏导数满足 $\\nabla_w^2 f \\geq c \u003e 0$（$c$ 为正的常数）。此时，式（6.15）中的分母可进一步推导为：\n∣E[∇w2f(w~k,X)(wk−w∗)]∣=∣E[∇w2f(w~k,X)]∣⋅∣wk−w∗∣≥c∣wk−w∗∣\\left| \\mathbb{E}[\\nabla_w^2 f(\\tilde{w}_k, X)(w_k - w^*)] \\right| = \\left| \\mathbb{E}[\\nabla_w^2 f(\\tilde{w}_k, X)] \\right| \\cdot |w_k - w^*| \\geq c|w_k - w^*|​E[∇w2​f(w~k​,X)(wk​−w∗)]​=​E[∇w2​f(w~k​,X)]​⋅∣wk​−w∗∣≥c∣wk​−w∗∣将上述不等式代入式（6.15），可得：\n来个例子，目标函数为 $f(w, X) = \\frac{1}{2}|w - X|^2$，由此可推导：\n随机梯度：$\\nabla_w f(w, x_k) = w - x_k$（$x_k$ 是 $X$ 的第 $k$ 个样本）\n真实梯度：$\\mathbb{E}[\\nabla_w f(w, X)] = w - \\mathbb{E}[X] = w - w^$（$w^ = \\mathbb{E}[X]$ 是最优解）\n将上述梯度代入相对误差公式，可得：\nδk=∣∇wf(wk,xk)−E[∇wf(wk,X)]∣∣E[∇wf(wk,X)]∣=∣(wk−xk)−(wk−E[X])∣∣wk−w∗∣=∣E[X]−xk∣∣wk−w∗∣\\delta_k = \\frac{|\\nabla_w f(w_k, x_k) - \\mathbb{E}[\\nabla_w f(w_k, X)]|}{|\\mathbb{E}[\\nabla_w f(w_k, X)]|} = \\frac{|(w_k - x_k) - (w_k - \\mathbb{E}[X])|}{|w_k - w^*|} = \\frac{|\\mathbb{E}[X] - x_k|}{|w_k - w^*|}δk​=∣E[∇w​f(wk​,X)]∣∣∇w​f(wk​,xk​)−E[∇w​f(wk​,X)]∣​=∣wk​−w∗∣∣(wk​−xk​)−(wk​−E[X])∣​=∣wk​−w∗∣∣E[X]−xk​∣​该表达式清晰地印证了此前的结论：\n$\\delta_k$ 与 $|w_k - w^|$ 成反比 —— 当 $w_k$ 远离 $w^$ 时，相对误差小，SGD 接近梯度下降的收敛效率\n$\\delta_k$ 与 $|\\mathbb{E}[X] - x_k|$（样本与真实期望的偏差）成正比，因此 $\\delta_k$ 的均值与 $X$ 的方差成正比（方差越大，样本偏差的平均水平越高）","convergence-properties#Convergence properties":"","dvoretzkys-convergence-theorem#Dvoretzky\u0026rsquo;s convergence theorem":"说明：This section is slightly mathematically intensive. Readers who are interested in the convergence analyses of stochastic algorithms are recommended to study this section. Otherwise, this section can be skipped.\n上面是赵老师的话，实际上这一部分就是德沃雷茨基收敛定理来证明 RM 算法及多种强化学习算法的收敛性的。但是出于工程思维，数学推导还是先暂时略过，后面有缘再回来补吧。","robbins-monro-algorithm#Robbins-Monro algorithm":"Robbins-Monro（简称 RM）算法是一种随机逼近（Stochastic Approximation）算法，与基于梯度等其他多种求根算法相比，随机逼近的优势在于无需知道目标函数的表达式及其导数。\n著名的随机梯度下降算法正是 RM 算法的一种特殊形式。\n假设我们需要求解方程的根：$g(w) = 0$\n我们只能获得 $g(w)$ 的带噪观测值：$\\tilde{g}(w, \\eta) = g(w) + \\eta$\n其中，$\\eta \\in \\mathbb{R}$ 是观测噪声，其分布可为高斯分布，也可为非高斯分布。综上，这是一个黑箱系统 —— 我们仅知晓输入 $w$ 和带噪输出 $\\tilde{g}(w, \\eta)$（见图 6.2）。我们的目标是利用 $w$ 和 $\\tilde{g}$ 求解方程 $g(w) = 0$。\n可用于求解 $g(w) = 0$ 的 RM 算法如下：\nwk+1=wk−akg~(wk,ηk),k=1,2,3,…(6.5) \\begin{aligned} w_{k+1} = w_k - a_k \\tilde{g}(w_k, \\eta_k), \\quad k = 1, 2, 3, \\dots \\tag{6.5} \\end{aligned} wk+1​=wk​−ak​g~​(wk​,ηk​),k=1,2,3,…​(6.5)其中，$w_k$ 是第 $k$ 次迭代得到的根的估计值，$\\tilde{g}(w_k, \\eta_k)$ 是第 $k$ 次带噪观测值，关键参数 $a_k$ 是正系数。可见，RM 算法无需任何关于函数 $g$ 的信息，仅需利用输入和输出即可迭代。\n设 $g(w) = w^3 - 5$，该方程的真实根为 $5^{1/3} \\approx 1.71$。假设我们仅能观测到输入 $w$ 和输出 $\\tilde{g}(w) = g(w) + \\eta$（其中 $\\eta$ 独立同分布且服从均值为 0、标准差为 1 的标准正态分布），初始猜测值设为 $w_1 = 0$，系数取 $a_k = 1/k$。$w_k$ 的迭代演化过程如上图，尽管观测值受到噪声 $\\eta_k$ 的干扰，估计值 $w_k$ 仍能收敛到真实根。\n选择初始猜测值才能保证收敛！","robbins-monro-theorem#Robbins-Monro theorem":"在上式所示的 RM 算法中，若满足以下条件：\n（a）梯度条件：对所有 $w$，有 $0 \u003c c_1 \\leq \\nabla_w g(w) \\leq c_2$（其中 $\\nabla_w g(w)$ 表示 $g(w)$ 对 $w$ 的梯度）；\n（b）系数序列条件：系数序列需满足两个关键条件：第一个条件是 $\\sum_{k=1}^{\\infty} a_k = \\infty$，第二个条件是 $\\sum_{k=1}^{\\infty} a_k^2 \u003c \\infty$；\n（c）噪声条件：观测噪声 $\\eta_k$ 满足 $\\mathbb{E}[\\eta_k \\mid H_k] = 0$ 且 $\\mathbb{E}[\\eta_k^2 \\mid H_k] \u003c \\infty$（其中 $H_k = {w_k, w_{k-1}, \\dots}$ 表示截至第 $k$ 步的历史观测信息集）；\n则迭代序列 $w_k$ 会几乎必然收敛到满足 $g(w^) = 0$ 的真实根 $w^$。\n下面对定理中的三个条件进行解释：\n条件（a）：$0 \u003c c_1 \\leq \\nabla_w g(w)$ 表明 $g(w)$ 是单调递增函数，这一条件能保证 $g(w) = 0$ 的根存在且唯一。若 $g(w)$ 是单调递减函数，可将 $-g(w)$ 定义为新函数（此时新函数单调递增），仍满足该条件。\n实际应用中，若目标是优化函数 $J(w)$，可将优化问题转化为求根问题：令 $g(w) \\triangleq \\nabla_w J(w) = 0$（即目标函数梯度为零的点）。此时 $g(w)$ 单调递增等价于 $J(w)$ 是凸函数—— 这是优化问题中常用的假设。\n而 $\\nabla_w g(w) \\leq c_2$ 表明 $g(w)$ 的梯度有上界。例如，$g(w) = \\tanh(w-1)$ 满足该条件，但 $g(w) = w^3 - 5$ 不满足（其梯度 $3w^2$ 无界）。\n条件（b）（关于系数 ${a_k}$）：这一条件在强化学习算法中很常见，需重点理解：\n$\\sum_{k=1}^{\\infty} a_k^2 \u003c \\infty$ 意味着当 $n \\to \\infty$ 时，$\\sum_{k=1}^{n} a_k^2$ 的极限有上界，即要求 $a_k$ 随 $k \\to \\infty$ 逐步收敛到 0；\n$\\sum_{k=1}^{\\infty} a_k = \\infty$ 意味着当 $n \\to \\infty$ 时，$\\sum_{k=1}^{n} a_k$ 的极限为无穷大，即要求 $a_k$ 收敛到 0 的速度不能过快。\n这两个子条件的具体作用将在后续进一步分析。\n条件（c）：这是一个温和条件，不要求观测噪声 $\\eta_k$ 服从高斯分布。一个重要的特殊情况是：${\\eta_k}$ 为独立同分布（i.i.d.）随机序列，且满足 $\\mathbb{E}[\\eta_k] = 0$、$\\mathbb{E}[\\eta_k^2] \u003c \\infty$。此时因 $\\eta_k$ 与历史信息集 $H_k$ 独立，可得 $\\mathbb{E}[\\eta_k \\mid H_k] = \\mathbb{E}[\\eta_k] = 0$、$\\mathbb{E}[\\eta_k^2 \\mid H_k] = \\mathbb{E}[\\eta_k^2]$，满足条件（c）。","stochastic-approximation#Stochastic Approximation":"Stochastic Approximation 写在前面：本章将填补知识空白，介绍随机逼近的基础知识。虽然本章不介绍任何特定的强化学习算法，但它为学习后续章节奠定了必要的基础。\n计算样本均值 $\\bar{x}$ 有两种方法。\n第一种是非增量方法：先收集所有样本，再计算平均值。这种方法的缺点是，若样本数量较大，我们可能需要等待很长时间，直到所有样本都收集完毕才能进行计算；\n第二种是增量方法，它能避免这一缺点 —— 因为该方法会以增量的方式逐步计算平均值。\n假设：$w_{k+1} \\triangleq \\frac{1}{k}\\sum_{i=1}^k x_i, \\quad k = 1, 2, \\dots$\n由此可推出：$w_k = \\frac{1}{k-1}\\sum_{i=1}^{k-1} x_i, \\quad k = 2, 3, \\dots$\n则 $w_{k+1}$ 可表示为关于 $w_k$ 的形式：\nwk+1=1k∑i=1kxi=1k(∑i=1k−1xi+xk)=1k[(k−1)wk+xk]=wk−1k(wk−xk) \\begin{aligned} w_{k+1} \u0026= \\frac{1}{k}\\sum_{i=1}^k x_i \\\\ \u0026= \\frac{1}{k}\\left( \\sum_{i=1}^{k-1} x_i + x_k \\right) \\\\ \u0026= \\frac{1}{k}\\left[ (k-1)w_k + x_k \\right] \\\\ \u0026= w_k - \\frac{1}{k}(w_k - x_k) \\end{aligned} wk+1​​=k1​i=1∑k​xi​=k1​(i=1∑k−1​xi​+xk​)=k1​[(k−1)wk​+xk​]=wk​−k1​(wk​−xk​)​因此，我们得到如下增量算法：\nwk+1=wk−1k(wk−xk)(6.4) \\begin{aligned} w_{k+1} = w_k - \\frac{1}{k}(w_k - x_k) \\tag{6.4} \\end{aligned} wk+1​=wk​−k1​(wk​−xk​)​(6.4)该算法可用于以增量方式计算样本均值 $\\bar{x}$。我们可通过以下计算验证其正确性：\nw1=x1,w2=w1−11(w1−x1)=x1,w3=w2−12(w2−x2)=x1−12(x1−x2)=12(x1+x2),w4=w3−13(w3−x3)=13(x1+x2+x3),⋮wk+1=1k∑i=1kxi(6.3) \\begin{aligned} w_1 \u0026= x_1, \\\\ w_2 \u0026= w_1 - \\frac{1}{1}(w_1 - x_1) = x_1, \\\\ w_3 \u0026= w_2 - \\frac{1}{2}(w_2 - x_2) = x_1 - \\frac{1}{2}(x_1 - x_2) = \\frac{1}{2}(x_1 + x_2), \\\\ w_4 \u0026= w_3 - \\frac{1}{3}(w_3 - x_3) = \\frac{1}{3}(x_1 + x_2 + x_3), \\\\ \u0026\\vdots \\\\ w_{k+1} \u0026= \\frac{1}{k}\\sum_{i=1}^k x_i \\tag{6.3} \\end{aligned} w1​w2​w3​w4​wk+1​​=x1​,=w1​−11​(w1​−x1​)=x1​,=w2​−21​(w2​−x2​)=x1​−21​(x1​−x2​)=21​(x1​+x2​),=w3​−31​(w3​−x3​)=31​(x1​+x2​+x3​),⋮=k1​i=1∑k​xi​​(6.3)","stochastic-gradient-descent#Stochastic gradient descent":"随机梯度下降（SGD）！SGD 就是一种特殊的罗宾斯-门罗（RM）算法，而均值估计算法又是一种特殊的 SGD 算法。\n考虑如下优化问题：\nmin⁡wJ(w)=E[f(w,X)](6.10) \\begin{aligned} \\min_{w} J(w) = \\mathbb{E}[f(w, X)] \\tag{6.10} \\end{aligned} wmin​J(w)=E[f(w,X)]​(6.10)其中，$w$ 是待优化的参数，$X$ 是随机变量，期望 $\\mathbb{E}[\\cdot]$ 针对 $X$ 计算。此处 $w$ 和 $X$ 既可以是标量，也可以是向量；函数 $f(\\cdot)$ 为标量函数。\n求解式（6.10）的一种直接方法是梯度下降。具体来说，$\\mathbb{E}[f(w, X)]$ 对 $w$ 的梯度为 $\\nabla_w \\mathbb{E}[f(w, X)] = \\mathbb{E}[\\nabla_w f(w, X)]$，因此梯度下降算法的迭代公式为：\nwk+1=wk−αk∇wJ(wk)=wk−αkE[∇wf(wk,X)](6.11) \\begin{aligned} w_{k+1} = w_k - \\alpha_k \\nabla_w J(w_k) = w_k - \\alpha_k \\mathbb{E}[\\nabla_w f(w_k, X)] \\tag{6.11} \\end{aligned} wk+1​=wk​−αk​∇w​J(wk​)=wk​−αk​E[∇w​f(wk​,X)]​(6.11)在满足一些温和条件（如 $f$ 为凸函数）的情况下，该梯度下降算法能够找到最优解 $w^*$。\n梯度下降算法需要计算期望 $\\mathbb{E}[\\nabla_w f(w_k, X)]$。获取该期望的一种方式是基于 $X$ 的概率分布，但在实际场景中，$X$ 的分布往往是未知的；另一种方式是收集 $X$ 的大量独立同分布（i.i.d.）样本 ${x_i}_{i=1}^n$，通过样本近似期望：\nE[∇wf(wk,X)]≈1n∑i=1n∇wf(wk,xi)\\mathbb{E}[\\nabla_w f(w_k, X)] \\approx \\frac{1}{n}\\sum_{i=1}^n \\nabla_w f(w_k, x_i)E[∇w​f(wk​,X)]≈n1​i=1∑n​∇w​f(wk​,xi​)将其代入式（6.11），可得：\nwk+1=wk−αkn∑i=1n∇wf(wk,xi)(6.12) \\begin{aligned} w_{k+1} = w_k - \\frac{\\alpha_k}{n}\\sum_{i=1}^n \\nabla_w f(w_k, x_i) \\tag{6.12} \\end{aligned} wk+1​=wk​−nαk​​i=1∑n​∇w​f(wk​,xi​)​(6.12)式（6.12）所示算法存在一个问题：每次迭代都需要使用全部样本。但在实际应用中，样本往往是逐个收集的，此时更希望每收集一个样本就更新一次参数 $w$。为此，可采用如下算法：\nwk+1=wk−αk∇wf(wk,xk)(6.13) \\begin{aligned} w_{k+1} = w_k - \\alpha_k \\nabla_w f(w_k, x_k) \\tag{6.13} \\end{aligned} wk+1​=wk​−αk​∇w​f(wk​,xk​)​(6.13)其中，$x_k$ 是第 $k$ 步迭代时收集的样本。这就是著名的随机梯度下降算法（SGD）。该算法之所以被称为\"随机\"，是因为其依赖于随机样本序列 ${x_k}$。\n与式（6.11）的梯度下降算法相比，SGD 用随机梯度 $\\nabla_w f(w_k, x_k)$ 替代了真实梯度 $\\mathbb{E}[\\nabla_w f(w, X)]$。由于 $\\nabla_w f(w_k, x_k) \\neq \\mathbb{E}[\\nabla_w f(w, X)]$，这种替代能否保证当 $k \\to \\infty$ 时 $w_k \\to w^*$ 呢？答案是肯定的，有证明。\n具体来说，可将随机梯度拆分为真实梯度与误差项之和：\n∇wf(wk,xk)=E[∇wf(wk,X)]+(∇wf(wk,xk)−E[∇wf(wk,X)])≜E[∇wf(wk,X)]+ηk\\nabla_w f(w_k, x_k) = \\mathbb{E}[\\nabla_w f(w_k, X)] + \\left( \\nabla_w f(w_k, x_k) - \\mathbb{E}[\\nabla_w f(w_k, X)] \\right) \\triangleq \\mathbb{E}[\\nabla_w f(w_k, X)] + \\eta_k∇w​f(wk​,xk​)=E[∇w​f(wk​,X)]+(∇w​f(wk​,xk​)−E[∇w​f(wk​,X)])≜E[∇w​f(wk​,X)]+ηk​其中，$\\eta_k = \\nabla_w f(w_k, x_k) - \\mathbb{E}[\\nabla_w f(w_k, X)]$ 为随机梯度与真实梯度的误差（扰动项）。\n将上式代入式（6.13），SGD 算法可改写为：\nwk+1=wk−αkE[∇wf(wk,X)]−αkηkw_{k+1} = w_k - \\alpha_k \\mathbb{E}[\\nabla_w f(w_k, X)] - \\alpha_k \\eta_kwk+1​=wk​−αk​E[∇w​f(wk​,X)]−αk​ηk​由此可见，SGD 算法与常规梯度下降算法的唯一区别在于多了一项扰动项 $\\alpha_k \\eta_k$。由于样本 ${x_k}$ 是独立同分布的，有 $\\mathbb{E}_{x_k}[\\nabla_w f(w_k, x_k)] = \\mathbb{E}_X[\\nabla_w f(w_k, X)]$，因此：\nE[ηk]=E[∇wf(wk,xk)−E[∇wf(wk,X)]]=Exk[∇wf(wk,xk)]−EX[∇wf(wk,X)]=0\\mathbb{E}[\\eta_k] = \\mathbb{E}\\left[ \\nabla_w f(w_k, x_k) - \\mathbb{E}[\\nabla_w f(w_k, X)] \\right] = \\mathbb{E}_{x_k}[\\nabla_w f(w_k, x_k)] - \\mathbb{E}_X[\\nabla_w f(w_k, X)] = 0E[ηk​]=E[∇w​f(wk​,xk​)−E[∇w​f(wk​,X)]]=Exk​​[∇w​f(wk​,xk​)]−EX​[∇w​f(wk​,X)]=0扰动项 $\\eta_k$ 的均值为 0，这从直观上表明它不会破坏算法的收敛性。","为何条件b对-rm-算法的收敛至关重要#为何条件（b）对 RM 算法的收敛至关重要？":"后续将通过定理的严谨证明回答这一问题，此处先给出直观解释：\n首先，条件： $\\sum_{k=1}^{\\infty} a_k^2 \u003c \\infty$ 意味着 $a_k \\to 0$（$k \\to \\infty$）。若带噪观测值 $\\tilde{g}(w_k, \\eta_k)$ 有界，则 $a_k \\tilde{g}(w_k, \\eta_k) \\to 0$，进而 $w_{k+1} - w_k \\to 0$—— 即当迭代次数足够大时，相邻两次的估计值 $w_{k+1}$ 与 $w_k$ 会逐渐接近，避免序列持续波动。若 $a_k$ 不收敛到 0，$w_k$ 可能在迭代后期仍大幅波动，无法稳定到真实根。\n其次，条件： $\\sum_{k=1}^{\\infty} a_k = \\infty$ 意味着 $a_k$ 收敛到 0 的速度不能过快。对迭代公式进行累加：\nw2−w1=−a1g~(w1,η1),w3−w2=−a2g~(w2,η2),w4−w3=−a3g~(w3,η3),…w_2 - w_1 = -a_1 \\tilde{g}(w_1, \\eta_1), \\quad w_3 - w_2 = -a_2 \\tilde{g}(w_2, \\eta_2), \\quad w_4 - w_3 = -a_3 \\tilde{g}(w_3, \\eta_3), \\quad \\dotsw2​−w1​=−a1​g~​(w1​,η1​),w3​−w2​=−a2​g~​(w2​,η2​),w4​−w3​=−a3​g~​(w3​,η3​),…累加后可得：\nw1−w∞=∑k=1∞akg~(wk,ηk)w_1 - w_{\\infty} = \\sum_{k=1}^{\\infty} a_k \\tilde{g}(w_k, \\eta_k)w1​−w∞​=k=1∑∞​ak​g~​(wk​,ηk​)若 $\\sum_{k=1}^{\\infty} a_k \u003c \\infty$，则 $\\left| \\sum_{k=1}^{\\infty} a_k \\tilde{g}(w_k, \\eta_k) \\right|$ 有界（设上界为 $b$），即：\n∣w1−w∞∣=∣∑k=1∞akg~(wk,ηk)∣≤b|w_1 - w_{\\infty}| = \\left| \\sum_{k=1}^{\\infty} a_k \\tilde{g}(w_k, \\eta_k) \\right| \\leq b∣w1​−w∞​∣=​k=1∑∞​ak​g~​(wk​,ηk​)​≤b若初始估计值 $w_1$ 与真实根 $w^$ 的距离满足 $|w_1 - w^| \u003e b$，则由上式可知 $w_{\\infty}$ 不可能等于 $w^*$，即算法无法找到真实根。因此，条件 $\\sum_{k=1}^{\\infty} a_k = \\infty$ 是保证\"任意初始估计值下算法均能收敛\"的必要条件。","哪些序列满足条件b#哪些序列满足条件（b）？":"哪些序列满足 $\\sum_{k=1}^{\\infty} a_k = \\infty$ 且 $\\sum_{k=1}^{\\infty} a_k^2 \u003c \\infty$？\n一个典型序列是 关键序列 $a_k = 1/k$：\n一方面，当 $n \\to \\infty$ 时，有：\nlim⁡n→∞(∑k=1n1k−ln⁡n)=κ\\lim_{n \\to \\infty} \\left( \\sum_{k=1}^{n} \\frac{1}{k} - \\ln n \\right) = \\kappan→∞lim​(k=1∑n​k1​−lnn)=κ其中 $\\kappa \\approx 0.577$ 称为欧拉-马歇罗尼常数（Euler-Mascheroni constant）。由于 $\\ln n \\to \\infty$（$n \\to \\infty$），可得 $\\sum_{k=1}^{\\infty} \\frac{1}{k} = \\infty$。事实上，$H_n = \\sum_{k=1}^{n} \\frac{1}{k}$ 在数论中被称为调和数（harmonic number）。\n另一方面，已知 $\\sum_{k=1}^{\\infty} \\frac{1}{k^2} = \\frac{\\pi^2}{6} \u003c \\infty$—— 求解该无穷级数的问题在数学中被称为巴塞尔问题（Basel problem）。\n综上，序列 ${a_k = \\frac{1}{k}}$ 满足定理的条件（b）。值得注意的是，对该序列进行微小调整（如 $a_k = \\frac{1}{k+1}$，或 $a_k = \\frac{c_k}{k}$ 且 $c_k$ 有界）后，仍能满足条件（b）。","条件b的重要性分析#条件（b）的重要性分析":""},"title":"第6章 随机逼近"},"/docs/self-study/ai/rl/chapter-7/":{"data":{"algorithm-72-optimal-policy-learning-via-q-learning-on-policy-version#Algorithm 7.2: Optimal policy learning via Q-learning (on-policy version)":"Initialization: $\\alpha_t(s, a) = \\alpha \u003e 0$ for all $(s, a)$ and all $t$. $\\epsilon \\in (0, 1)$. Initial $q_0(s, a)$ for all $(s, a)$. Initial $\\epsilon$-greedy policy $\\pi_0$ derived from $q_0$. Goal: Learn an optimal path that can lead the agent to the target state from an initial state $s_0$. For each episode, do: If $s_t$ ($t = 0, 1, 2, \\dots$) is not the target state, do: Collect the experience sample $(a_t, r_{t+1}, s_{t+1})$ given $s_t$: generate $a_t$ following $\\pi_t(s_t)$; generate $r_{t+1}, s_{t+1}$ by interacting with the environment. Update q-value for $(s_t, a_t)$: qt+1(st,at)=qt(st,at)−αt(st,at)[qt(st,at)−(rt+1+γmax⁡aqt(st+1,a))]q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \\alpha_t(s_t, a_t)\\left[ q_t(s_t, a_t) - (r_{t+1} + \\gamma \\max_a q_t(s_{t+1}, a)) \\right]qt+1​(st​,at​)=qt​(st​,at​)−αt​(st​,at​)[qt​(st​,at​)−(rt+1​+γmaxa​qt​(st+1​,a))] Update policy for $s_t$: πt+1(a∣st)={1−ϵ+ϵ∣A(st)∣if a=arg⁡max⁡aqt+1(st,a)ϵ∣A(st)∣otherwise\\pi_{t+1}(a|s_t) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|A(s_t)|} \u0026 \\text{if } a = \\arg\\max_a q_{t+1}(s_t, a) \\\\ \\frac{\\epsilon}{|A(s_t)|} \u0026 \\text{otherwise} \\end{cases}πt+1​(a∣st​)={1−ϵ+∣A(st​)∣ϵ​∣A(st​)∣ϵ​​if a=argmaxa​qt+1​(st​,a)otherwise​","algorithm-73-optimal-policy-learning-via-q-learning-off-policy-version#Algorithm 7.3: Optimal policy learning via Q-learning (off-policy version)":"Initialization: Initial guess $q_0(s, a)$ for all $(s, a)$. Behavior policy $\\pi_b(a|s)$ for all $(s, a)$. $\\alpha_t(s, a) = \\alpha \u003e 0$ for all $(s, a)$ and all $t$. Goal: Learn an optimal target policy $\\pi_T$ for all states from the experience samples generated by $\\pi_b$. For each episode ${s_0, a_0, r_1, s_1, a_1, r_2, \\dots}$ generated by $\\pi_b$, do: For each step $t = 0, 1, 2, \\dots$ of the episode, do: Update q-value for $(s_t, a_t)$: qt+1(st,at)=qt(st,at)−αt(st,at)[qt(st,at)−(rt+1+γmax⁡aqt(st+1,a))]q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \\alpha_t(s_t, a_t)\\left[ q_t(s_t, a_t) - (r_{t+1} + \\gamma \\max_a q_t(s_{t+1}, a)) \\right]qt+1​(st​,at​)=qt​(st​,at​)−αt​(st​,at​)[qt​(st​,at​)−(rt+1​+γmaxa​qt​(st+1​,a))] Update target policy for $s_t$: πT,t+1(a∣st)={1if a=arg⁡max⁡aqt+1(st,a)0otherwise\\pi_{T,t+1}(a|s_t) = \\begin{cases} 1 \u0026 \\text{if } a = \\arg\\max_a q_{t+1}(s_t, a) \\\\ 0 \u0026 \\text{otherwise} \\end{cases}πT,t+1​(a∣st​)={10​if a=argmaxa​qt+1​(st​,a)otherwise​","algorithm-description#Algorithm description":"初始时智能体在 “起点 s₀”，按策略 π 选了动作 a₀（比如 “向右”）； 执行 a₀后，环境反馈一个回报 r₁（比如 “没碰到墙，得 - 1 分”，因为每走一步消耗成本），同时智能体进入新状态 s₁（“下一个路口”）； 在 s₁，智能体再按策略 π 选动作 a₁（比如还是 “向右”）； 重复这个过程，就记录下序列：(s₀,a₀,r₁,s₁,a₁,r₂,s₂,a₂,…,sₜ,aₜ,rₜ₊₁,sₜ₊₁,aₜ₊₁)—— 这就是 “依据策略 π 生成的经验样本”，每一段 (sₜ,aₜ,rₜ₊₁,sₜ₊₁,aₜ₊₁) 都对应 “t 时刻的决策→环境反馈→t+1 时刻的决策”。\n给定一个策略 $\\pi$，我们的目标是估计其动作值。假设我们拥有依据策略 $\\pi$ 生成的经验样本：$(s_0, a_0, r_1, s_1, a_1, \\dots, s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \\dots)$。可通过以下 Sarsa 算法估计动作值：\nqt+1(st,at)=qt(st,at)−αt(st,at)[qt(st,at)−(rt+1+γqt(st+1,at+1))](7.12)\r\\begin{aligned}\rq_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \\alpha_t(s_t, a_t)\\left[ q_t(s_t, a_t) - (r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})) \\right] \\tag{7.12}\r\\end{aligned}\rqt+1​(st​,at​)=qt​(st​,at​)−αt​(st​,at​)[qt​(st​,at​)−(rt+1​+γqt​(st+1​,at+1​))]​(7.12)对于所有 $(s, a) \\neq (s_t, a_t)$，有：\nqt+1(s,a)=qt(s,a)\r\\begin{aligned}\rq_{t+1}(s, a) = q_t(s, a)\r\\end{aligned}\rqt+1​(s,a)=qt​(s,a)​其中，$t = 0, 1, 2, \\dots$，$\\alpha_t(s_t, a_t)$ 为学习率；$q_t(s_t, a_t)$ 是动作值 $q_{\\pi}(s_t, a_t)$ 的估计值。在时刻 $t$，仅更新 $(s_t, a_t)$ 对应的动作值（q 值），其余 $(s, a)$ 的 q 值保持不变。\n智能体在 t 时刻只做了一件事：在 sₜ选了 aₜ，得到了 rₜ₊₁和 sₜ₊₁—— 这仅能告诉我们 “(sₜ,aₜ) 的效果如何”，没有任何关于 “其他状态（比如 s₁、s₂）或其他动作（比如在 sₜ选‘向左’）” 的新信息；没有新信息，就无法判断 “之前对其他 (s,a) 的估计是否需要改”，所以只能保持原来的估计（qₜ₊₁(s,a)=qₜ(s,a)）。\n为何该算法被命名为 Sarsa？ 因为算法的每一次迭代都需要用到样本 $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$，而 “Sarsa” 正是 “状态-动作-回报-状态-动作\"（state-action-reward-state-action）的缩写。\nSarsa 算法为何如此设计？ 不难发现，Sarsa 算法与 7.1 节式 (7.1) 的 TD 算法存在相似性。事实上，只需将 TD 算法中的 “状态值估计” 替换为 “动作值估计”，即可轻松得到 Sarsa 算法。\n从数学角度看，Sarsa 算法的作用是什么？ 与 7.1 节式 (7.1) 的 TD 算法类似，Sarsa 算法是用于求解给定策略贝尔曼方程的随机近似算法，其对应的贝尔曼方程为：\nqπ(s,a)=E[R+γqπ(S′,A′)∣s,a],∀(s,a)(7.13)\r\\begin{aligned}\rq_\\pi(s, a) = \\mathbb{E}\\left[ R + \\gamma q_\\pi(S', A') \\mid s, a \\right], \\quad \\forall (s, a) \\tag{7.13}\r\\end{aligned}\rqπ​(s,a)=E[R+γqπ​(S′,A′)∣s,a],∀(s,a)​(7.13)式 (7.13) 是基于动作值的贝尔曼方程表达形式，其证明过程直接跳了。\nSarsa 算法是否收敛？ 由于 Sarsa 算法是 7.1 节式 (7.1) TD 算法的 “动作值版本”，其收敛性结论与定理 7.1 类似，具体如下：\n定理 7.2（Sarsa 算法的收敛性）：给定一个策略 $\\pi$，若对所有 $(s, a)$，均满足 $\\sum_{t} \\alpha_t(s, a) = \\infty$ 且 $\\sum_{t} \\alpha_t^2(s, a) \u003c \\infty$，则通过式 (7.12) 的 Sarsa 算法，当 $t \\to \\infty$ 时，对所有 $(s, a)$，动作值估计 $q_t(s, a)$ 几乎必然收敛到真实动作值 $q_{\\pi}(s, a)$。\n该定理的证明过程与定理 7.1 类似，此处省略。需注意，$\\sum_{t} \\alpha_t(s, a) = \\infty$ 与 $\\sum_{t} \\alpha_t^2(s, a) \u003c \\infty$ 这两个条件需对所有 $(s, a)$ 成立。其中，$\\sum_{t} \\alpha_t(s, a) = \\infty$ 要求每个状态-动作对都必须被访问无限多次（或足够多次）。在时刻 $t$，若 $(s, a) = (s_t, a_t)$，则 $\\alpha_t(s, a) \u003e 0$；否则 $\\alpha_t(s, a) = 0$。","convergence-analysis#Convergence analysis":"定理 7.1（TD 学习的收敛性）：给定一个策略 $\\pi$，若对所有状态 $s \\in S$，均满足 $\\sum_{t} \\alpha_{t}(s)=\\infty$ 且 $\\sum_{t} \\alpha_{t}^{2}(s)\u003c\\infty$，则通过式 (7.1) 的 TD 算法，当 $t \\to \\infty$ 时，对所有 $s \\in S$，状态值估计 $v_{t}(s)$ 几乎必然收敛到真实状态值 $v_{\\pi}(s)$。\n关于学习率 $\\alpha_{t}$ 的几点说明如下：\n条件 $\\sum_{t} \\alpha_{t}(s)=\\infty$ 与 $\\sum_{t} \\alpha_{t}^{2}(s)\u003c\\infty$ 必须对所有 $s \\in S$ 均成立。需要注意的是，在时刻 $t$，若状态 $s$ 被访问，则 $\\alpha_{t}(s)\u003e0$；否则 $\\alpha_{t}(s)=0$。\n条件 $\\sum_{t} \\alpha_{t}(s)=\\infty$ 要求状态 $s$ 被访问无限多次（或足够多次），这需要满足探索起始条件（exploring starts）或采用探索性策略（exploratory policy），以确保每个状态-动作对（state-action pair）都有可能被多次访问。\n在实际应用中，学习率 $\\alpha_{t}$ 通常被选为一个较小的正常数。此时，$\\sum_{t} \\alpha_{t}^{2}(s)\u003c\\infty$ 这一条件不再成立。但即便 $\\alpha$ 为常数，仍可证明该算法在期望意义下收敛。","off-policy-vs-on-policy#Off-policy vs On-policy":"Q 学习与其他时序差分（TD）算法相比，稍显特殊的一点在于：Q 学习属于离线策略学习，而其他 TD 算法均属于在线策略学习。\n在任何强化学习任务中，都存在两种策略：行为策略（Behavior Policy） 和 目标策略（Target Policy）。\n行为策略：用于生成经验样本的策略； 目标策略：不断更新以逐步收敛到最优策略的策略。 当行为策略与目标策略相同时，这种学习过程被称为 在线策略学习；反之，当两者不同时，学习过程被称为 离线策略学习。\n离线策略学习的优势在于，它可以基于其他策略生成的经验样本学习最优策略 —— 例如，这些样本可能来自人类操作者执行的策略。一个重要的应用场景是：行为策略可被选为具有强探索性的策略。例如，若要估计所有状态-动作对的动作值，必须生成足够多的回合，确保每个状态-动作对都被充分访问。尽管 Sarsa 算法会采用 $\\epsilon$-贪婪策略来维持一定的探索能力，但 $\\epsilon$ 值通常较小，导致探索能力有限。相比之下，若我们能使用探索能力更强的策略生成回合，再通过离线策略学习来优化策略，学习效率将显著提升。\n如何判断算法属于在线策略还是离线策略？ 可从两个维度判断： 算法旨在求解的数学问题； 算法所需的经验样本形式。 1. Sarsa 算法属于在线策略学习\n原因如下：Sarsa 算法的每次迭代包含两个步骤：\n第一步：通过求解某一策略 $\\pi$ 的贝尔曼方程，对该策略进行评估。要实现这一步，需要策略 $\\pi$ 生成的样本，因此 $\\pi$ 是行为策略； 第二步：基于策略 $\\pi$ 的估计值，得到一个改进后的策略。因此，$\\pi$ 同时也是目标策略 —— 它会不断更新，最终收敛到最优策略。 综上，Sarsa 算法的行为策略与目标策略完全相同，故属于在线策略学习。\n从 “算法所需样本” 的角度也可验证：Sarsa 算法每次迭代需要的样本为 $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$，其生成过程可表示为：\nst→πbat→模型（model）rt+1,st+1→πbat+1s_t \\xrightarrow{\\pi_b} a_t \\xrightarrow{\\text{模型（model）}} r_{t+1}, s_{t+1} \\xrightarrow{\\pi_b} a_{t+1}st​πb​​at​模型（model）​rt+1​,st+1​πb​​at+1​可见，行为策略 $\\pi_b$ 负责在状态 $s_t$ 生成动作 $a_t$，在状态 $s_{t+1}$ 生成动作 $a_{t+1}$。Sarsa 算法的目标是估计某一策略 $\\pi_T$ 下 $(s_t, a_t)$ 的动作值，而 $\\pi_T$ 会基于每次迭代的估计值进行改进，因此是目标策略。实际上，$\\pi_T$ 与 $\\pi_b$ 完全相同 —— 因为对 $\\pi_T$ 的评估依赖于样本 $(r_{t+1}, s_{t+1}, a_{t+1})$，而 $a_{t+1}$ 正是由 $\\pi_b$ 生成的。换句话说，Sarsa 算法评估的策略，与生成样本的策略是同一个。\n2. Q 学习算法属于离线策略学习\n根本原因在于：Q 学习是用于求解贝尔曼最优方程的算法，而 Sarsa 算法是用于求解给定策略的贝尔曼方程的算法。求解贝尔曼方程仅能评估对应的策略，而求解贝尔曼最优方程可直接得到最优值和最优策略。\n具体来看 Q 学习所需的样本：Q 学习每次迭代需要的样本为 $(s_t, a_t, r_{t+1}, s_{t+1})$，其生成过程可表示为：\nst→πbat→模型（model）rt+1,st+1s_t \\xrightarrow{\\pi_b} a_t \\xrightarrow{\\text{模型（model）}} r_{t+1}, s_{t+1}st​πb​​at​模型（model）​rt+1​,st+1​可见，行为策略 $\\pi_b$ 仅负责在状态 $s_t$ 生成动作 $a_t$。Q 学习的目标是估计 $(s_t, a_t)$ 的最优动作值，这一估计过程仅依赖样本 $(r_{t+1}, s_{t+1})$；而 $(r_{t+1}, s_{t+1})$ 的生成不涉及 $\\pi_b$—— 它由系统模型（或智能体与环境的交互）决定。因此，对 $(s_t, a_t)$ 最优动作值的估计与 $\\pi_b$ 无关，我们可使用任意行为策略在 $s_t$ 生成动作 $a_t$。此外，此处的目标策略 $\\pi_T$ 是基于估计的最优值得到的贪婪策略（见算法 7.3），行为策略无需与目标策略相同。\n3. 蒙特卡洛（MC）学习属于在线策略学习\n原因与 Sarsa 算法类似：待评估和改进的目标策略，与生成样本的行为策略是同一个。\n易混淆概念：在线学习（Online Learning）与离线学习（Offline Learning）\n“在线策略 / 离线策略” 常与 “在线学习 / 离线学习” 混淆，二者的区别如下：\n在线学习：智能体在与环境交互的过程中，同步更新价值和策略； 离线学习：智能体不与环境交互，仅使用预先收集的经验数据更新价值和策略。 两者的关联的是：若算法属于在线策略学习，则仅能以在线方式实现，无法使用其他策略预先收集的数据；若算法属于离线策略学习，则既可在线实现（边交互边学习），也可离线实现（用预收集数据学习）。\n由于 Q 学习属于离线策略算法，因此它既可以采用在线策略方式实现，也可以采用离线策略方式实现。\nQ 学习的在线策略版本如算法 7.2 所示。该实现方式与算法 7.1 中 Sarsa 算法的实现方式类似：此处的行为策略与目标策略完全相同，均为 $\\epsilon$-贪婪策略。\nQ 学习的离线策略版本如算法 7.3 所示。行为策略 $\\pi_b$ 可以是任意策略，只要它能生成足够的经验样本即可；通常当 $\\pi_b$ 具有探索性时，算法表现更为有利。此处的目标策略 $\\pi_T$ 为贪婪策略（而非 $\\epsilon$-贪婪策略），原因是该目标策略无需用于生成样本，因此不需要具备探索性。此外，本节介绍的 Q 学习离线策略版本采用离线方式实现：先收集所有经验样本，再对其进行处理。","optimal-policy-learning-by-sarsa#Optimal policy learning by Sarsa":"Initialization: $\\alpha_t(s, a) = \\alpha \u003e 0$ for all $(s, a)$ and all $t$. $\\epsilon \\in (0, 1)$. Initial $q_0(s, a)$ for all $(s, a)$. Initial $\\epsilon$-greedy policy $\\pi_0$ derived from $q_0$. Goal: Learn an optimal policy that can lead the agent to the target state from an initial state $s_0$. For each episode, do: Generate $a_0$ at $s_0$ following $\\pi_0(s_0)$ If $s_t$ ($t = 0, 1, 2, \\dots$) is not the target state, do: Collect an experience sample $(r_{t+1}, s_{t+1}, a_{t+1})$ given $(s_t, a_t)$: generate $r_{t+1}, s_{t+1}$ by interacting with the environment; generate $a_{t+1}$ following $\\pi_t(s_{t+1})$. Update q-value for $(s_t, a_t)$: qt+1(st,at)=qt(st,at)−αt(st,at)[qt(st,at)−(rt+1+γqt(st+1,at+1))]q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \\alpha_t(s_t, a_t)\\left[ q_t(s_t, a_t) - (r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})) \\right]qt+1​(st​,at​)=qt​(st​,at​)−αt​(st​,at​)[qt​(st​,at​)−(rt+1​+γqt​(st+1​,at+1​))] Update policy for $s_t$: $s_t \\leftarrow s_{t+1}$, $a_t \\leftarrow a_{t+1}$ πt+1(a∣st)={1−ϵ+ϵ∣A(st)∣if a=arg⁡max⁡aqt+1(st,a)ϵ∣A(st)∣otherwise\\pi_{t+1}(a|s_t) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|A(s_t)|} \u0026 \\text{if } a = \\arg\\max_a q_{t+1}(s_t, a) \\\\ \\frac{\\epsilon}{|A(s_t)|} \u0026 \\text{otherwise} \\end{cases}πt+1​(a∣st​)={1−ϵ+∣A(st​)∣ϵ​∣A(st​)∣ϵ​​if a=argmaxa​qt+1​(st​,a)otherwise​ 如算法 7.1 所示，每一次迭代包含两个核心步骤： 第一步是更新已访问状态-动作对的 q 值；第二步是将策略更新为 $\\epsilon$-贪婪策略。其中，q 值更新步骤仅更新时刻 $t$ 访问的那个状态-动作对的 q 值，随后立即更新 $s_t$ 对应的策略——这意味着在更新策略前，我们并未对当前策略进行充分评估，其理论依据是 “广义策略迭代\"（generalized policy iteration）思想。此外，策略更新后会立即用于生成下一个经验样本，而此处采用 $\\epsilon$-贪婪策略的目的是保证策略具有探索性。 为验证 Sarsa 算法的有效性，图 7.2 给出了一个仿真示例。与本书此前介绍的任务不同，该任务的目标是寻找从 “特定初始状态” 到 “目标状态” 的最优路径，而非为所有状态寻找最优策略。","optimal-policy-learning-via-sarsa#Optimal policy learning via Sarsa":"式 (7.12) 的 Sarsa 算法仅能估计给定策略的动作值，若要寻找最优策略，需将其与策略改进步骤相结合。这种 “动作值估计+策略改进” 的组合方法通常也被称为 Sarsa 算法：","property-analysis#Property analysis":"首先，我们对 TD 算法的表达式进行更细致的分析:","summary#Summary":"本章介绍了强化学习中的一类重要算法 —— 时序差分（TD）学习算法。我们具体介绍的算法包括 Sarsa 算法、n 步 Sarsa 算法和 Q 学习算法。所有这些算法均可视为用于求解贝尔曼方程或贝尔曼最优方程的随机近似算法。\nTD 之所以叫 TD，这来自这些算法的 TD 误差，该误差表示新样本与当前估计值之间的差异。由于这种差异是在不同时间步之间计算得出的，因此被称为 “时序差分（temporal-difference）”，这也是 “TD” 的由来。而 “Learning” 本质上就是 “Estimating”：即通过样本估计状态值或动作值，再基于估计得到的价值确定策略。\n本章介绍的 TD 算法中，除 Q 学习外，其余算法均用于评估给定策略：即通过经验样本估计给定策略的状态值或动作值。将这些算法与策略改进步骤相结合，即可用于学习最优策略。此外，这些算法均属于在线策略算法：目标策略会作为行为策略，用于生成经验样本。\nQ 学习与其他 TD 算法相比稍显特殊，因为它属于离线策略算法 —— 在 Q 学习中，目标策略可与行为策略不同。Q 学习之所以是离线策略算法，根本原因在于：它以求解贝尔曼最优方程为目标，而非求解给定策略的贝尔曼方程。\n值得一提的是，存在一些方法可将在线策略算法转换为离线策略算法，重要性采样（Importance Sampling） 便是其中一种广泛应用的方法，我们将在第 10 章对其进行介绍。最后，本章介绍的 TD 算法还存在一些变体与扩展形式。例如，TD ($\\lambda$) 方法为时序差分学习提供了更具一般性的统一框架。","td-learning-of-action-values-n-step-sarsa#TD learning of action values: n-step Sarsa":"本节介绍 n 步 Sarsa 算法，它是 Sarsa 算法的扩展形式。后文将表明，Sarsa 算法与蒙特卡洛（MC）学习均为 n 步 Sarsa 算法的两种极端情况。\n回顾动作值的定义：\nqπ(s,a)=E[Gt∣St=s,At=a](7.16)\r\\begin{aligned}\rq_\\pi(s, a) = \\mathbb{E}[G_t \\mid S_t = s, A_t = a] \\tag{7.16}\r\\end{aligned}\rqπ​(s,a)=E[Gt​∣St​=s,At​=a]​(7.16)其中，$G_t$ 为折扣回报，满足： Gt=Rt+1+γRt+2+γ2Rt+3+…\r\\begin{aligned}\rG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\r\\end{aligned}\rGt​=Rt+1​+γRt+2​+γ2Rt+3​+…​事实上，$G_t$ 还可分解为多种形式：","td-learning-of-action-values-sarsa#TD learning of action values: Sarsa":"上面的第一种 TD Learning 仅能估计 state value，而本节的 Sarsa 可直接估计 action value。估计动作值具有重要意义，因为它可与策略改进步骤相结合，从而学习最优策略。","td-learning-of-optimal-action-values-q-learning#TD learning of optimal action values: Q-learning":"本节将介绍 Q 学习算法，它是强化学习领域最经典的算法之一。回顾可知，Sarsa 算法仅能估计给定策略的动作值，若要寻找最优策略，必须将其与策略改进步骤相结合。与之不同的是，Q 学习可直接估计最优动作值并找到最优策略。\nQ 学习算法的表达式如下：\nqt+1(st,at)=qt(st,at)−αt(st,at)[qt(st,at)−(rt+1+γmax⁡a∈A(st+1)qt(st+1,a))](7.18)\r\\begin{aligned}\rq_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \\alpha_t(s_t, a_t)\\left[ q_t(s_t, a_t) - \\left( r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}(s_{t+1})} q_t(s_{t+1}, a) \\right) \\right] \\tag{7.18}\r\\end{aligned}\rqt+1​(st​,at​)=qt​(st​,at​)−αt​(st​,at​)[qt​(st​,at​)−(rt+1​+γa∈A(st+1​)max​qt​(st+1​,a))]​(7.18)对于所有 $(s, a) \\neq (s_t, a_t)$，有： qt+1(s,a)=qt(s,a)\r\\begin{aligned}\rq_{t+1}(s, a) = q_t(s, a)\r\\end{aligned}\rqt+1​(s,a)=qt​(s,a)​ 其中 $t = 0, 1, 2, \\dots$。式中，$q_t(s_t, a_t)$ 是状态-动作对 $(s_t, a_t)$ 最优动作值的估计值，$\\alpha_t(s_t, a_t)$ 是状态-动作对 $(s_t, a_t)$ 对应的学习率。\nQ 学习与 Sarsa 的表达式较为相似，TD 目标的前半部分 $r_{t+1}$ 是即时回报—— 表示 “在 $t$ 时刻执行动作 $a_t$ 后，环境立即反馈的奖励”。二者的差异仅体现在 TD 目标的后半部分上：Q 学习的 TD 目标为 $r_{t+1} + \\gamma \\max_a q_t(s_{t+1}, a)$，而 Sarsa 的 TD 目标为 $r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})$。\nSarsa 的 TD 目标 —— $r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})$\n这里的 $a_{t+1}$ 是有下标的 “特定动作\"，需要先明确它的来源：$a_{t+1}$ 是 “智能体在 $t+1$ 时刻，处于状态 $s_{t+1}$ 时，按照当前策略 $\\pi$ 实际选择的动作”（比如 Sarsa 用 $\\epsilon$-贪婪策略，在 $s_{t+1}$ 有 90% 概率选当前 q 值最大的动作，10% 随机选，$a_{t+1}$ 就是这次实际选的那个）；$q_t(s_{t+1}, a_{t+1})$ 是 “到 $t$ 时刻为止，对’在 $s_{t+1}$ 选 $a_{t+1}$’ 这个动作值的估计”。\n所以 Sarsa 的 TD 目标数学含义是：“当前动作 $(s_t, a_t)$ 的’理想价值’ = 即时回报 $r_{t+1}$ + 折扣后的’下一步实际动作的估计价值’\"—— 它依赖 “下一步实际做了什么动作\"，是一种 “跟随当前策略的保守估计\"。\n举个例子：假设智能体在 $t$ 时刻处于 “路口 $s_t$\"，选了 “向右 $a_t$\"，得到即时回报 $r_{t+1}=-1$（走一步成本），进入新状态 “下一个路口 $s_{t+1}$\"；按当前 $\\epsilon$-贪婪策略，在 $s_{t+1}$ 实际选了 “向上 $a_{t+1}$\"，且当前对 $(s_{t+1}, a_{t+1})$ 的估计是 $q_t(s_{t+1}, a_{t+1})=-8$（预计还要走 8 步到终点）。那么 Sarsa 的 TD 目标就是：$-1 + 0.9 \\times (-8) = -8.2$—— 它基于 “下一步实际选的向上动作” 来计算当前动作的理想价值。\nQ 学习的 TD 目标 —— $r_{t+1} + \\gamma \\max_a q_t(s_{t+1}, a)$\n这里的 $a$ 是无下标的 “所有可能动作\"，$\\max_a$ 是 “对所有动作取最大值”，需要拆解两个关键点：\n无下标的 $a$：代表 “状态 $s_{t+1}$ 下所有可选的动作”（比如 $s_{t+1}$ 是路口，可选动作是 “上、下、左、右”，这里的 $a$ 就遍历这 4 个动作）； $\\max_a q_t(s_{t+1}, a)$：表示 “在 $s_{t+1}$ 的所有可选动作中，找到当前估计值 $q_t$ 最大的那个动作，取它的 q 值”—— 这是 “不考虑当前策略实际选了什么，只追求’理论上最好的动作价值’\"。 所以 Q 学习的 TD 目标数学含义是：“当前动作 $(s_t, a_t)$ 的’理想价值’ = 即时回报 $r_{t+1}$ + 折扣后的’下一步所有动作中最好的估计价值’\"—— 它不依赖 “下一步实际做了什么”，只关心 “下一步理论上最好的结果\"，是一种 “追求最优的激进估计\"。\n延续上面的例子：智能体在 $t$ 时刻选 “向右 $a_t$” 后，进入 $s_{t+1}$，得到 $r_{t+1}=-1$；$s_{t+1}$ 的 4 个动作估计值分别是：上（-8）、下（-15）、左（-10）、右（-5）。那么 Q 学习的 TD 目标就是：$-1 + 0.9 \\times \\max(-8, -15, -10, -5) = -1 + 0.9 \\times (-5) = -5.5$—— 它不管下一步实际选了 “向上” 还是 “向右”，只取所有动作中 q 值最大的 “向右” 来计算当前动作的理想价值。\n此外，给定状态-动作对 $(s_t, a_t)$ 时，Sarsa 算法每次迭代需用到样本 $(r_{t+1}, s_{t+1}, a_{t+1})$，而 Q 学习仅需用到样本 $(r_{t+1}, s_{t+1})$。\n简单来说：Sarsa 是 “走一步看一步，跟着当前策略的实际动作算价值\"，所以需要下一步的实际动作；Q 学习是 “走一步看最好的一步，不管当前策略实际走了什么，只按理论最优算价值\"，所以不需要下一步的实际动作。这其实是在线离线策略的一种对比体现，在后面会详细说明。\nQ 学习为何设计成式 (7.18) 的形式？其数学意义是什么？ Q 学习是用于求解以下方程的随机近似算法：\nq(s,a)=E[Rt+1+γmax⁡aq(St+1,a)∣St=s,At=a](7.19)\r\\begin{aligned}\rq(s, a) = \\mathbb{E}\\left[ R_{t+1} + \\gamma \\max_a q(S_{t+1}, a) \\mid S_t = s, A_t = a \\right] \\tag{7.19}\r\\end{aligned}\rq(s,a)=E[Rt+1​+γamax​q(St+1​,a)∣St​=s,At​=a]​(7.19)该方程是基于动作值的贝尔曼最优方程，其证明过程直接跳了。","td-learning-of-state-values#TD learning of state values":"给定一个策略 $\\pi$，我们的目标是估计所有状态 $s \\in S$ 对应的状态值 $v_{\\pi}(s)$。假设我们拥有依据策略 $\\pi$ 生成的若干经验样本 $(s_0, r_1, s_1, \\dots, s_t, r_{t+1}, s_{t+1}, \\dots)$，其中 $t$ 表示时间步。可通过以下 TD 算法利用这些样本估计状态值：\nvt+1(st)=vt(st)−αt(st)[vt(st)−(rt+1+γvt(st+1))](7.1)\r\\begin{aligned}\rv_{t+1}(s_t) = v_t(s_t) - \\alpha_t(s_t)[v_t(s_t) - (r_{t+1} + \\gamma v_t(s_{t+1}))] \\tag{7.1}\r\\end{aligned}\rvt+1​(st​)=vt​(st​)−αt​(st​)[vt​(st​)−(rt+1​+γvt​(st+1​))]​(7.1)对所有 $s \\neq s_t$，有：\nvt+1(s)=vt(s)(7.2)\r\\begin{aligned}\rv_{t+1}(s) = v_t(s) \\tag{7.2}\r\\end{aligned}\rvt+1​(s)=vt​(s)​(7.2)其中 $t = 0, 1, 2, \\dots$。式中，$v_t(s_t)$ 表示 $t$ 时刻对 $v_{\\pi}(s_t)$ 的估计值；$\\alpha_t(s_t)$ 表示 $t$ 时刻状态 $s_t$ 对应的学习率。\n需要注意的是，在 $t$ 时刻，仅对已访问状态 $s_t$ 的状态值进行更新。如式 (7.2) 所示，未访问状态（$s \\neq s_t$）的状态值保持不变。为简洁起见，式 (7.2) 常被省略，但需牢记其存在 —— 若缺少该式，算法在数学层面将不完整。\n首次接触 TD 学习算法的读者可能会疑惑，该算法为何如此设计。事实上，它可被视为用于求解贝尔曼方程的一种特殊随机近似算法。要理解这一点，首先回顾状态值的定义：\nvπ(s)=E[Rt+1+γGt+1∣St=s],s∈S(7.3)\r\\begin{aligned}\rv_{\\pi}(s) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s], \\quad s \\in S \\tag{7.3}\r\\end{aligned}\rvπ​(s)=E[Rt+1​+γGt+1​∣St​=s],s∈S​(7.3)我们可将式 (7.3) 重写为：\nvπ(s)=E[Rt+1+γvπ(St+1)∣St=s],s∈S(7.5)\r\\begin{aligned}\rv_{\\pi}(s) = \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s], \\quad s \\in S \\tag{7.5}\r\\end{aligned}\rvπ​(s)=E[Rt+1​+γvπ​(St+1​)∣St​=s],s∈S​(7.5)这是因为 $\\mathbb{E}[G_{t+1} \\mid S_t = s] = \\sum_a \\pi(a|s) \\sum_{s’} p(s’|s, a) v_{\\pi}(s’) = \\mathbb{E}[v_{\\pi}(S_{t+1}) \\mid S_t = s]$。式 (7.5) 是贝尔曼方程的另一种表达形式，有时也被称为贝尔曼期望方程。\n将 RM 算法（第 6 章）应用于求解式 (7.5) 中的贝尔曼方程，即可推导出 TD 算法。","temporal-difference-methods#Temporal-Difference Methods":"Temporal-Difference Methods与时序差分（TD）学习类似，蒙特卡洛（MC）学习同样属于无模型方法，但 TD 学习因其增量式的形式而具备一些优势。"},"title":"第7章 时序差分方法"},"/docs/self-study/ai/rl/chapter-8/":{"data":{"algorithm-83-deep-q-learning-off-policy-version#Algorithm 8.3: Deep Q-learning (off-policy version)":"Initialization: A main network and a target network with the same initial parameter.\nGoal: Learn an optimal target network to approximate the optimal action values from the experience samples generated by a given behavior policy $\\pi_b$.\nStore the experience samples generated by $\\pi_b$ in a replay buffer $B = {(s, a, r, s’)}$ For each iteration, do: Uniformly draw a mini-batch of samples from $B$ For each sample $(s, a, r, s’)$, calculate the target value as $y_T = r + \\gamma \\max_{a \\in A(s’)} \\hat{q}(s’, a, w_T)$, where $w_T$ is the parameter of the target network Update the main network to minimize $(y_T - \\hat{q}(s, a, w))^2$ using the mini-batch of samples Set $w_T = w$ every $C$ iterations","algorithm-83-q-learning-with-function-approximation-on-policy-version#Algorithm 8.3: Q-learning with function approximation (on-policy version)":"Initialization: Initial parameter $w_0$. Initial policy $\\pi_0$. $\\alpha_t = \\alpha \u003e 0$ for all $t$. $\\epsilon \\in (0, 1)$.\nGoal: Learn an optimal path that can lead the agent to the target state from an initial state $s_0$.\nFor each episode, do:\nIf $s_t$ ($t = 0, 1, 2, \\dots$) is not the target state, do:\nCollect the experience sample $(a_t, r_{t+1}, s_{t+1})$ given $s_t$: generate $a_t$ following $\\pi_t(s_t)$; generate $r_{t+1}, s_{t+1}$ by interacting with the environment. Update q-value: wt+1=wt+αt[rt+1+γmax⁡a∈A(st+1)q^(st+1,a,wt)−q^(st,at,wt)]∇wq^(st,at,wt)\rw_{t+1} = w_t + \\alpha_t \\left[ r_{t+1} + \\gamma \\max_{a \\in A(s_{t+1})} \\hat{q}(s_{t+1}, a, w_t) - \\hat{q}(s_t, a_t, w_t) \\right] \\nabla_w \\hat{q}(s_t, a_t, w_t)\rwt+1​=wt​+αt​[rt+1​+γmaxa∈A(st+1​)​q^​(st+1​,a,wt​)−q^​(st​,at​,wt​)]∇w​q^​(st​,at​,wt​) Update policy: πt+1(a∣st)={1−ϵ∣A(st)∣(∣A(st)∣−1)if a=arg⁡max⁡a∈A(st)q^(st,a,wt+1)ϵ∣A(st)∣otherwise\r\\pi_{t+1}(a|s_t) = \\begin{cases}\r1 - \\frac{\\epsilon}{|A(s_t)|}(|A(s_t)| - 1) \u0026 \\text{if } a = \\arg\\max_{a \\in A(s_t)} \\hat{q}(s_t, a, w_{t+1}) \\\\\r\\frac{\\epsilon}{|A(s_t)|} \u0026 \\text{otherwise}\r\\end{cases}\rπt+1​(a∣st​)={1−∣A(st​)∣ϵ​(∣A(st​)∣−1)∣A(st​)∣ϵ​​if a=argmaxa∈A(st​)​q^​(st​,a,wt+1​)otherwise​","algorithm-description#Algorithm description":"从数学角度来看，深度 Q 学习的目标是最小化以下目标函数：\nJ=E[(R+γmax⁡a∈A(S′)q^(S′,a,w)−q^(S,A,w))2](8.37)\r\\begin{aligned}\rJ = \\mathbb{E}\\left[ \\left( R + \\gamma \\max_{a \\in A(S')} \\hat{q}(S', a, w) - \\hat{q}(S, A, w) \\right)^2 \\right] \\tag{8.37}\r\\end{aligned}\rJ=E[(R+γa∈A(S′)max​q^​(S′,a,w)−q^​(S,A,w))2]​(8.37)其中，随机变量 $(S, A, R, S’)$ 分别表示状态、动作、即时奖励和下一状态。\n该目标函数可视为贝尔曼最优误差的平方，原因如下：对所有状态 $s$ 和动作 $a$，贝尔曼最优方程为：\nq(s,a)=E[Rt+1+γmax⁡a∈A(St+1)q(St+1,a)∣St=s,At=a]\r\\begin{aligned}\rq(s, a) = \\mathbb{E}\\left[ R_{t+1} + \\gamma \\max_{a \\in A(S_{t+1})} q(S_{t+1}, a) \\mid S_t = s, A_t = a \\right]\r\\end{aligned}\rq(s,a)=E[Rt+1​+γa∈A(St+1​)max​q(St+1​,a)∣St​=s,At​=a]​因此，当 $\\hat{q}(S, A, w)$ 能准确近似最优动作值时，在期望意义下，$R + \\gamma \\max_{a \\in A(S’)} \\hat{q}(S’, a, w) - \\hat{q}(S, A, w)$ 应等于 0。\n为最小化式（8.37）中的目标函数，可采用梯度下降算法。为此，需要计算目标函数 $J$ 对参数 $w$ 的梯度。需注意的是，参数 $w$ 不仅出现在 $\\hat{q}(S, A, w)$ 中，还出现在 $y \\triangleq R + \\gamma \\max_{a \\in A(S’)} \\hat{q}(S’, a, w)$ 中，因此梯度计算并不简单。\n为简化计算，我们假设 $y$ 中的 $w$ 在短时间内固定不变，这样梯度计算会大幅简化。具体而言，我们引入两个网络：\n主网络（main network）：表示动作值近似函数 $\\hat{q}(s, a, w)$，参数为 $w$； 目标网络（target network）：表示动作值近似函数 $\\hat{q}(s, a, w_T)$，参数为 $w_T$。 此时，目标函数可改写为：\nJ=E[(R+γmax⁡a∈A(S′)q^(S′,a,wT)−q^(S,A,w))2]\r\\begin{aligned}\rJ = \\mathbb{E}\\left[ \\left( R + \\gamma \\max_{a \\in A(S')} \\hat{q}(S', a, w_T) - \\hat{q}(S, A, w) \\right)^2 \\right]\r\\end{aligned}\rJ=E[(R+γa∈A(S′)max​q^​(S′,a,wT​)−q^​(S,A,w))2]​当 $w_T$ 固定时，$J$ 对 $w$ 的梯度为：\n∇wJ=−E[(R+γmax⁡a∈A(S′)q^(S′,a,wT)−q^(S,A,w))∇wq^(S,A,w)](8.38)\r\\begin{aligned}\r\\nabla_w J = -\\mathbb{E}\\left[ \\left( R + \\gamma \\max_{a \\in A(S')} \\hat{q}(S', a, w_T) - \\hat{q}(S, A, w) \\right) \\nabla_w \\hat{q}(S, A, w) \\right] \\tag{8.38}\r\\end{aligned}\r∇w​J=−E[(R+γa∈A(S′)max​q^​(S′,a,wT​)−q^​(S,A,w))∇w​q^​(S,A,w)]​(8.38)其中，为不失一般性，省略了部分常数系数。","deep-q-learning-dqn#Deep Q-learning (DQN)":"深度 Q 学习是最早且最成功的深度强化学习算法之一。值得注意的是，这里的神经网络未必需要很深：对于像本书中网格世界这样的简单任务，带有 1~2 个隐藏层的浅层网络可能就足够了。","objective-function#Objective function":"设 $v_{\\pi}(s)$ 和 $\\hat{v}(s, w)$ 分别表示状态 $s \\in S$ 的真实状态值和近似状态值。我们需要解决的问题是：找到最优参数 $w$，使得 $\\hat{v}(s, w)$ 对每个状态 $s$ 都能最好地近似 $v_{\\pi}(s)$。\n具体而言，目标函数定义为：\nJ(w)=E[(vπ(S)−v^(S,w))2](8.3)\r\\begin{aligned}\rJ(w) = \\mathbb{E}\\left[(v_{\\pi}(S) - \\hat{v}(S, w))^2\\right] \\tag{8.3}\r\\end{aligned}\rJ(w)=E[(vπ​(S)−v^(S,w))2]​(8.3)其中，$w$ 是 “价值函数的参数向量”，比如对于线性近似场景，近似价值函数是 $\\hat{v}(s,w) = w_1 \\cdot x + w_2 \\cdot y + w_3$（$x$、$y$ 是状态的行列索引），这里的 $w$ 就是向量 $w = [w_1, w_2, w_3]^T$（$w_1$、$w_2$ 是系数，$w_3$ 是偏置）。有了 $w$，输入一个状态的 $x=2$、$y=3$，就能算出 $\\hat{v}(s,w) = 2w_1 + 3w_2 + w_3$；而对于神经网络场景，$w$ 就是神经网络里的 “所有权重和偏置”—— 比如输入层到隐藏层的权重矩阵、隐藏层到输出层的权重矩阵、各层的偏置向量，这些参数打包在一起就构成了向量 $w$。\n期望 $\\mathbb{E}[\\cdot]$ 是基于随机变量 $S \\in \\mathcal{S}$ 计算的。尽管 $S$ 是随机变量，但其概率分布是什么呢？\n定义 $S$ 的概率分布主要有以下两种方式：\n方式一：采用均匀分布\n即把所有状态视为同等重要，将每个状态的概率设为 $1/n$（$n$ 为状态总数）。此时，式（8.3）所示的目标函数可改写为：\nJ(w)=1n∑s∈S(vπ(s)−v^(s,w))2(8.4)\r\\begin{aligned}\rJ(w) = \\frac{1}{n} \\sum_{s \\in \\mathcal{S}} \\left(v_{\\pi}(s) - \\hat{v}(s, w)\\right)^2 \\tag{8.4}\r\\end{aligned}\rJ(w)=n1​s∈S∑​(vπ​(s)−v^(s,w))2​(8.4)该式表示所有状态近似误差的平均值。然而，这种方式未考虑马尔可夫过程在给定策略下的真实动态 —— 由于某些状态可能被策略访问的频率很低，将所有状态视为同等重要可能并不合理。\n方式二：采用平稳分布\n平稳分布（stationary distribution）描述了马尔可夫决策过程的长期行为：具体而言，当智能体执行给定策略足够长的时间后，其处于任一状态的概率均可由该平稳分布描述。\n设 ${d_{\\pi}(s)}{s \\in S}$ 表示马尔可夫过程在策略 $\\pi$ 下的平稳分布，即智能体长期执行策略后访问状态 $s$ 的概率为 $d{\\pi}(s)$。根据定义，所有状态的平稳分布概率之和满足 $\\sum_{s \\in S} d_{\\pi}(s) = 1$。\n此时，式（8.3）的目标函数可改写为：\nJ(w)=∑s∈Sdπ(s)(vπ(s)−v^(s,w))2(8.5)\r\\begin{aligned}\rJ(w) = \\sum_{s \\in \\mathcal{S}} d_{\\pi}(s) \\left(v_{\\pi}(s) - \\hat{v}(s, w)\\right)^2 \\tag{8.5}\r\\end{aligned}\rJ(w)=s∈S∑​dπ​(s)(vπ​(s)−v^(s,w))2​(8.5)该式表示近似误差的加权平均值，其中访问概率更高的状态会被赋予更大的权重。","optimization-algorithm#Optimization algorithm":"为最小化式（8.3）中的目标函数 $J(w)$，可采用梯度下降算法：$w_{k+1} = w_k - \\alpha_k \\nabla_w J(w_k)$\n$\\alpha_k$ 是 “步长”（学习率），控制每次更新的幅度\n其中，梯度的计算过程如下：\n∇wJ(wk)=∇wE[(vπ(S)−v^(S,wk))2]=E[∇w(vπ(S)−v^(S,wk))2]=2E[(vπ(S)−v^(S,wk))(−∇wv^(S,wk))]=−2E[(vπ(S)−v^(S,wk))∇wv^(S,wk)]\r\\begin{aligned}\r\\nabla_w J(w_k) \u0026= \\nabla_w \\mathbb{E}\\left[\\left(v_{\\pi}(S) - \\hat{v}(S, w_k)\\right)^2\\right] \\\\\r\u0026= \\mathbb{E}\\left[\\nabla_w \\left(v_{\\pi}(S) - \\hat{v}(S, w_k)\\right)^2\\right] \\\\\r\u0026= 2\\mathbb{E}\\left[\\left(v_{\\pi}(S) - \\hat{v}(S, w_k)\\right)\\left(-\\nabla_w \\hat{v}(S, w_k)\\right)\\right] \\\\\r\u0026= -2\\mathbb{E}\\left[\\left(v_{\\pi}(S) - \\hat{v}(S, w_k)\\right)\\nabla_w \\hat{v}(S, w_k)\\right]\r\\end{aligned}\r∇w​J(wk​)​=∇w​E[(vπ​(S)−v^(S,wk​))2]=E[∇w​(vπ​(S)−v^(S,wk​))2]=2E[(vπ​(S)−v^(S,wk​))(−∇w​v^(S,wk​))]=−2E[(vπ​(S)−v^(S,wk​))∇w​v^(S,wk​)]​因此，梯度下降算法可改写为：\nwk+1=wk+2αkE[(vπ(S)−v^(S,wk))∇wv^(S,wk)](8.11)\r\\begin{aligned}\rw_{k+1} = w_k + 2\\alpha_k \\mathbb{E}\\left[\\left(v_{\\pi}(S) - \\hat{v}(S, w_k)\\right)\\nabla_w \\hat{v}(S, w_k)\\right] \\tag{8.11}\r\\end{aligned}\rwk+1​=wk​+2αk​E[(vπ​(S)−v^(S,wk​))∇w​v^(S,wk​)]​(8.11) 其中，$\\alpha_k$ 前面的系数 2 可不失一般性地合并到 $\\alpha_k$ 中。\n式（8.11）所示的算法需要计算期望，也就是要遍历所有可能的状态 $S$，计算每个状态的 “$(v_{\\pi}(S)-\\hat{v}(S,w_k))\\nabla_w \\hat{v}(S,w_k)$\"，再求平均。但实际场景中，状态空间可能非常大（比如有成千上万个状态），遍历所有状态计算期望不现实—— 这时候就需要按照随机梯度下降的思路，用 “单个随机样本” 的梯度，近似 “所有样本的平均梯度（期望）\"。\n比如在时刻 $t$，我们随机采样一个状态 $s_t$（比如智能体实际交互中遇到的状态），用 $s_t$ 的 “$(v_{\\pi}(s_t)-\\hat{v}(s_t,w_t))\\nabla_w \\hat{v}(s_t,w_t)$\"，代替式（8.11）中 “所有状态的期望”。\n此时式（8.11）变为：\nwt+1=wt+αt(vπ(st)−v^(st,wt))∇wv^(st,wt)(8.12)\r\\begin{aligned}\rw_{t+1} = w_t + \\alpha_t \\left(v_{\\pi}(s_t) - \\hat{v}(s_t, w_t)\\right)\\nabla_w \\hat{v}(s_t, w_t) \\tag{8.12}\r\\end{aligned}\rwt+1​=wt​+αt​(vπ​(st​)−v^(st​,wt​))∇w​v^(st​,wt​)​(8.12)其中，$s_t$ 是时刻 $t$ 时状态 $S$ 的一个样本。\n根据 “大数定律”，当采样的样本足够多时，“单个样本的梯度” 会逐渐逼近 “所有样本的平均梯度（期望）”—— 既降低了计算量，又能保证算法收敛（只要步长设置合理）。 需要注意的是，式（8.12）不具备可实现性 —— 它需要用到真实状态值 $v_{\\pi}$，而 $v_{\\pi}$ 是未知的，必须通过估计得到。所以必须找一个 “近似值” 来代替 $v_{\\pi}(s_t)$，本质都是 “用可观测的信息估计 $v_{\\pi}(s_t)$\"，使算法能够落地执行，具体可采用以下两种方法：\n蒙特卡洛方法：假设存在一个回合 $(s_0, r_1, s_1, r_2, \\dots)$，令 $g_t$ 表示从状态 $s_t$ 出发的折扣回报，则可将 $g_t$ 作为 $v_{\\pi}(s_t)$ 的近似值。此时，式（8.12）所示的算法变为： wt+1=wt+αt(gt−v^(st,wt))∇wv^(st,wt)\r\\begin{aligned}\rw_{t+1} = w_t + \\alpha_t \\left(g_t - \\hat{v}(s_t, w_t)\\right)\\nabla_w \\hat{v}(s_t, w_t)\r\\end{aligned}\rwt+1​=wt​+αt​(gt​−v^(st​,wt​))∇w​v^(st​,wt​)​ 蒙特卡洛的思路很直接：等一个回合结束，拿到从 $s_t$ 出发的 “完整累积奖励”，用它作为 $v_{\\pi}(s_t)$ 的近似。 具体来说：假设一个回合的轨迹是 $(s_0, r_1, s_1, r_2, \\dots, s_T)$（$s_T$ 是终止状态），从 $s_t$ 出发的 “折扣回报” $g_t$ 定义为：$g_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots + \\gamma^{T-t} r_T$（$\\gamma$ 是折扣因子，未来的奖励打折扣）。为什么能用 $g_t$ 近似 $v_{\\pi}(s_t)$？因为 $v_{\\pi}(s_t)$ 的定义就是 “从 $s_t$ 出发，遵循策略 $\\pi$ 的长期折扣回报的期望”——$g_t$ 是 “一次实际回合中 $s_t$ 的真实累积回报”，是 $v_{\\pi}(s_t)$ 的一个 “样本”，能近似它。 这就是基于函数近似的蒙特卡洛学习算法。\n时序差分方法：按照时序差分（TD）学习的思路，可将 $r_{t+1} + \\gamma \\hat{v}(s_{t+1}, w_t)$ 作为 $v_{\\pi}(s_t)$ 的近似值。此时，式（8.12）所示的算法变为： wt+1=wt+αt[rt+1+γv^(st+1,wt)−v^(st,wt)]∇wv^(st,wt)(8.13)\r\\begin{aligned}\rw_{t+1} = w_t + \\alpha_t \\left[r_{t+1} + \\gamma \\hat{v}(s_{t+1}, w_t) - \\hat{v}(s_t, w_t)\\right] \\nabla_w \\hat{v}(s_t, w_t) \\tag{8.13}\r\\end{aligned}\rwt+1​=wt​+αt​[rt+1​+γv^(st+1​,wt​)−v^(st​,wt​)]∇w​v^(st​,wt​)​(8.13) 蒙特卡洛有个缺点：必须等 “整个回合结束” 才能计算 $g_t$，实时性差。时序差分（TD）的改进在于：不用等回合结束，每走一步就能用 “当前看到的即时奖励 + 下一状态的近似值” 估计 $v_{\\pi}(s_t)$。 具体来说：用 “$r_{t+1} + \\gamma \\hat{v}(s_{t+1},w_t)$” 作为 $v_{\\pi}(s_t)$ 的近似，这个值也叫 “TD 目标”。为什么能这样近似？根据贝尔曼方程，$v_{\\pi}(s_t) = \\mathbb{E}\\left[r_{t+1} + \\gamma v_{\\pi}(s_{t+1})\\right]$—— 真实状态值 $v_{\\pi}(s_t)$ 等于 “即时奖励 + 下一状态真实值的折扣期望”。由于 $v_{\\pi}(s_{t+1})$ 也未知，我们就用它的近似值 $\\hat{v}(s_{t+1},w_t)$ 代替，得到 “$r_{t+1} + \\gamma \\hat{v}(s_{t+1},w_t)$\"，作为 $v_{\\pi}(s_t)$ 的近似。 这就是基于函数近似的时序差分学习算法。","q-learning-with-function-approximation#Q-Learning with function approximation":"表格型 Q 学习算法同样可扩展到函数近似场景，其参数更新规则为：\nwt+1=wt+αt[rt+1+γmax⁡a∈A(st+1)q^(st+1,a,wt)−q^(st,at,wt)]∇wq^(st,at,wt)(8.36)\r\\begin{aligned}\rw_{t+1} = w_t + \\alpha_t \\left[ r_{t+1} + \\gamma \\max_{a \\in A(s_{t+1})} \\hat{q}(s_{t+1}, a, w_t) - \\hat{q}(s_t, a_t, w_t) \\right] \\nabla_w \\hat{q}(s_t, a_t, w_t) \\tag{8.36}\r\\end{aligned}\rwt+1​=wt​+αt​[rt+1​+γa∈A(st+1​)max​q^​(st+1​,a,wt​)−q^​(st​,at​,wt​)]∇w​q^​(st​,at​,wt​)​(8.36)上述更新规则与式（8.35）类似，唯一区别在于：将式（8.35）中的 $\\hat{q}(s_{t+1}, a_{t+1}, w_t)$ 替换为 “对下一状态 $s_{t+1}$ 所有可能动作 $a$ 取 $\\hat{q}(s_{t+1}, a, w_t)$ 的最大值”（即 $\\max_{a \\in A(s_{t+1})} \\hat{q}(s_{t+1}, a, w_t)$）。\n与表格型 Q 学习类似，式（8.36）可通过 on-policy 或 off-policy 两种方式实现，其在线策略版本总结于算法 8.3 中。","sarsa-with-function-approximation#Sarsa with function approximation":"假设策略 $\\pi$ 下的动作值 $q_{\\pi}(s, a)$ 由近似函数 $\\hat{q}(s, a, w)$ 表示，将式（8.13）中的 $\\hat{v}(s, w)$ 替换为 $\\hat{q}(s, a, w)$，可得：\nwt+1=wt+αt[rt+1+γq^(st+1,at+1,wt)−q^(st,at,wt)]∇wq^(st,at,wt)(8.35)\r\\begin{aligned}\rw_{t+1} = w_t + \\alpha_t \\left[ r_{t+1} + \\gamma \\hat{q}(s_{t+1}, a_{t+1}, w_t) - \\hat{q}(s_t, a_t, w_t) \\right] \\nabla_w \\hat{q}(s_t, a_t, w_t) \\tag{8.35}\r\\end{aligned}\rwt+1​=wt​+αt​[rt+1​+γq^​(st+1​,at+1​,wt​)−q^​(st​,at​,wt​)]∇w​q^​(st​,at​,wt​)​(8.35)对式（8.35）的分析与式（8.13）类似，此处不再赘述。\n当采用线性函数近似时，动作值近似函数可表示为：\nq^(s,a,w)=ϕT(s,a)w\r\\begin{aligned}\r\\hat{q}(s, a, w) = \\phi^T(s, a)w\r\\end{aligned}\rq^​(s,a,w)=ϕT(s,a)w​其中，$\\phi(s, a)$ 是状态-动作对 $(s, a)$ 的特征向量。此时，近似函数关于参数 $w$ 的梯度为 $\\nabla_w \\hat{q}(s, a, w) = \\phi(s, a)$。\n式（8.35）中的值估计步骤可与策略改进步骤相结合，以学习最优策略，具体流程总结于算法 8.2 中。","sarsa-with-function-approximation-1#Sarsa with function approximation":"Initialization: Initial parameter $w_0$. Initial policy $\\pi_0$. $\\alpha_t = \\alpha \u003e 0$ for all $t$. $\\epsilon \\in (0, 1)$.\nGoal: Learn an optimal policy that can lead the agent to the target state from an initial state $s_0$.\nFor each episode, do:\nGenerate $a_0$ at $s_0$ following $\\pi_0(s_0)$ If $s_t$ ($t = 0, 1, 2, \\dots$) is not the target state, do: Collect the experience sample $(r_{t+1}, s_{t+1}, a_{t+1})$ given $(s_t, a_t)$: generate $r_{t+1}, s_{t+1}$ by interacting with the environment; generate $a_{t+1}$ following $\\pi_t(s_{t+1})$. Update q-value: wt+1=wt+αt[rt+1+γq^(st+1,at+1,wt)−q^(st,at,wt)]∇wq^(st,at,wt)\rw_{t+1} = w_t + \\alpha_t \\left[ r_{t+1} + \\gamma \\hat{q}(s_{t+1}, a_{t+1}, w_t) - \\hat{q}(s_t, a_t, w_t) \\right] \\nabla_w \\hat{q}(s_t, a_t, w_t)\rwt+1​=wt​+αt​[rt+1​+γq^​(st+1​,at+1​,wt​)−q^​(st​,at​,wt​)]∇w​q^​(st​,at​,wt​) Update policy: πt+1(a∣st)={1−ϵ∣A(st)∣(∣A(st)∣−1)if a=arg⁡max⁡a∈A(st)q^(st,a,wt+1)ϵ∣A(st)∣otherwise\r\\pi_{t+1}(a|s_t) = \\begin{cases}\r1 - \\frac{\\epsilon}{|A(s_t)|}(|A(s_t)| - 1) \u0026 \\text{if } a = \\arg\\max_{a \\in A(s_t)} \\hat{q}(s_t, a, w_{t+1}) \\\\\r\\frac{\\epsilon}{|A(s_t)|} \u0026 \\text{otherwise}\r\\end{cases}\rπt+1​(a∣st​)={1−∣A(st​)∣ϵ​(∣A(st​)∣−1)∣A(st​)∣ϵ​​if a=argmaxa∈A(st​)​q^​(st​,a,wt+1​)otherwise​ $s_t \\leftarrow s_{t+1}$, $a_t \\leftarrow a_{t+1}$ 需注意的是，要准确估计给定策略下的动作值，需多次执行式（8.35）；但在算法 8.2 中，每次切换到策略改进步骤前，仅执行一次式（8.35）的更新 —— 这一点与表格型 Sarsa 算法一致。此外，算法 8.2 的实现目标是 “从指定初始状态找到一条通往目标状态的较优路径”，因此无法为所有状态找到最优策略；但如果有充足的经验数据，可轻松调整该实现流程，使其能为所有状态找到最优策略。","selection-of-function-approximators#Selection of function approximators":"要应用式（8.13）所示的时序差分算法，需选择合适的近似状态值函数 $\\hat{v}(s, w)$，具体有以下两种方式：\n第一种方式是采用人工神经网络作为非线性函数近似器：神经网络的输入为状态 $s$，输出为近似状态值 $\\hat{v}(s, w)$，网络参数为 $w$。\n第二种方式是直接采用线性函数：\nv^(s,w)=ϕT(s)w\r\\begin{aligned}\r\\hat{v}(s, w) = \\phi^T(s)w\r\\end{aligned}\rv^(s,w)=ϕT(s)w​其中，$\\phi(s) \\in \\mathbb{R}^m$ 是状态 $s$ 的特征向量，$\\phi(s)$ 与 $w$ 的维度均为 $m$（通常远小于状态总数）。\n在线性函数近似的情况下，近似函数关于 $w$ 的梯度为：$\\nabla_w \\hat{v}(s, w) = \\phi(s)$\n将其代入式（8.13），可得：\nwt+1=wt+αt[rt+1+γϕT(st+1)wt−ϕT(st)wt]ϕ(st)(8.14)\r\\begin{aligned}\rw_{t+1} = w_t + \\alpha_t \\left[r_{t+1} + \\gamma \\phi^T(s_{t+1})w_t - \\phi^T(s_t)w_t\\right] \\phi(s_t) \\tag{8.14}\r\\end{aligned}\rwt+1​=wt​+αt​[rt+1​+γϕT(st+1​)wt​−ϕT(st​)wt​]ϕ(st​)​(8.14)这就是基于线性函数近似的时序差分学习算法，简称 TD-Linear 算法。\n在线性函数近似的情况下，算法的理论性质远比非线性函数近似更清晰易懂，但它的近似能力有限，且针对复杂任务选择合适的特征向量并非易事。相比之下，人工神经网络可作为 “黑箱式通用非线性近似器” 实现值的近似，使用起来更便捷。\n对于赵老师这本书所涉及的简单网格世界任务，线性函数近似已足够解决问题；更重要的是，线性情况具有很强的包容性 —— 表格法可视为线性函数近似的一种特殊情况，在接下来的专栏里面有讲解，这里直接跳过有缘再见。","td-learning-of-action-values-based-on-function-approximation#TD learning of action values based on function approximation":"前一节介绍了状态值估计问题，本节则将介绍如何估计动作值。我们将表格型 Sarsa 算法与表格型 Q 学习算法扩展到价值函数近似的场景，这种扩展过程十分直观。","td-learning-of-state-values-based-on-function-approximation#TD learning of state values based on function approximation":"","td-learning-of-state-values-with-function-approximation#TD learning of state values with function approximation":"Initialization: A function $\\hat{v}(s, w)$ that is differentiable in $w$. Initial parameter $w_{0}$.\nGoal: Learn the true state values of a given policy $\\pi$.\nFor each episode ${(s_{t}, r_{t+1}, s_{t+1})}_{t}$ generated by $\\pi$, do:\nFor each sample $(s_{t}, r_{t+1}, s_{t+1})$, do:\nIn the general case: $w_{t+1}=w_{t}+\\alpha_{t}[r_{t+1}+\\gamma \\hat{v}(s_{t+1}, w_{t})-\\hat{v}(s_{t}, w_{t})] \\nabla_{w} \\hat{v}(s_{t}, w_{t})$ In the linear case: $w_{t+1}=w_{t}+\\alpha_{t}[r_{t+1}+\\gamma \\phi^{T}(s_{t+1}) w_{t}-\\phi^{T}(s_{t}) w_{t}] \\phi(s_{t})$","theoretical-analysis#Theoretical Analysis":"从 Objective Function 出发，为优化该目标函数，我们引入了式（8.12）所示的随机算法；随后，由于算法中的真实值函数未知，我们用近似值对其进行替换，最终得到了式（8.13）所示的 TD 算法。\n接下来要做的就是数学部分，收敛性分析（Convergence analysis）、TD learning minimizes the projected Bellman error、Least-squares TD 这一块内容，咱第一遍过直接跳过，有缘再见。","value-function-methods#Value Function Methods":"Value Function Methods","value-representation-from-table-to-function#Value representation: From table to function":"假设存在 $n$ 个状态 ${s_i}{i=1}^n$，其状态值为 ${v{\\pi}(s_i)}{i=1}^n$，其中 $\\pi$ 是给定的策略。用 ${\\hat{v}(s_i)}{i=1}^n$ 表示真实状态值的估计值。若采用表格法，可将估计值存储在下表中。该表格可作为数组或向量存储在内存中，要检索或更新某个值，直接读取或改写表格中对应的条目即可。\n状态（State） $s_1$ $s_2$ $\\cdots$ $s_n$ 估计值（Estimated value） $\\hat{v}(s_1)$ $\\hat{v}(s_2)$ $\\cdots$ $\\hat{v}(s_n)$ 接下来我们将说明，上述表格中的值可通过一个函数来近似。具体而言，在下图","深度-q-learning-的两个关键问题与改进#深度 Q-Learning 的两个关键问题与改进":"假设我们用 “深度网络直接替换线性近似” 做 Q-Learning，会发生什么？\n打砖块游戏的状态是 “游戏画面”（高维，比如 $210 \\times 160$ 像素），动作是 “左移、右移、发射”。我们用一个 3 层神经网络（输入：画面像素，隐藏层：200 个神经元，输出：3 个动作的 Q 值）代替线性特征，直接按 Q-Learning 的规则更新：\n更新公式（无经验回放 + 无双网络）：\nwt+1=wt+α⋅[rt+1+γmax⁡aq^(st+1,a,wt)−q^(st,at,wt)]⋅∇wq^(st,at,wt)w_{t+1} = w_t + \\alpha \\cdot \\left[ r_{t+1} + \\gamma \\max_{a} \\hat{q}(s_{t+1}, a, w_t) - \\hat{q}(s_t, a_t, w_t) \\right] \\cdot \\nabla_w \\hat{q}(s_t, a_t, w_t)wt+1​=wt​+α⋅[rt+1​+γamax​q^​(st+1​,a,wt​)−q^​(st​,at​,wt​)]⋅∇w​q^​(st​,at​,wt​)此时会遇到两个致命问题，而这两个问题在 “线性近似 Q-Learning” 中几乎不存在。\n问题 1：样本序列相关性太强，深度网络学 “偏” 了（为什么需要经验回放）\n线性 Q-Learning 为什么没问题？\n比如之前的网格世界，状态是 “$(x,y)$ 坐标”（低维），每次探索的状态序列是 “$(1,1) \\to (1,2) \\to (2,2) \\to \\dots$\"，状态之间的关联性弱（比如 $(1,1)$ 和 $(2,2)$ 的特征差异大），即使在线更新（拿到样本就更参数），模型也不会学偏。\n深度网络 + 打砖块游戏：没有经验回放会怎样？\n游戏中，连续 10 帧画面的状态几乎一样（比如球在左边慢慢移动，连续 10 帧都是 “球在左半屏”），此时我们拿到的样本序列是：\n(s1,a1,r2,s2)→(s2,a2,r3,s3)→⋯→(s10,a10,r11,s11)(s_1,a_1,r_2,s_2) \\to (s_2,a_2,r_3,s_3) \\to \\dots \\to (s_{10},a_{10},r_{11},s_{11})(s1​,a1​,r2​,s2​)→(s2​,a2​,r3​,s3​)→⋯→(s10​,a10​,r11​,s11​)其中 $s_1$ 到 $s_{10}$ 的画面高度相似（都是球在左半屏），对应的 “最优动作” 都是 “左移”（让挡板接住球）。如果直接用这些 “连续相似样本” 在线更新：每次更新都会让网络认为 “左移动作的 Q 值应该更大”，参数 $w$ 会一直往 “强化左移” 的方向调整；即使后来球到了右半屏，网络也会因为之前的 “过度训练左移”，依然优先选左移，导致挡板接不到球 —— 这就是 “样本序列相关性” 导致的模型学 “偏”，训练永远无法收敛到最优策略。\n经验回放怎么解决这个问题？\n经验回放就像一个 “样本仓库”：我们把每次游戏的经验 $(s,a,r,s’)$ 都存进仓库（比如存 10000 个样本）；每次更新网络时，不直接用 “刚拿到的样本”，而是从仓库里随机抽 100 个样本（mini-batch）来更新；这 100 个样本可能包含 “球在左半屏\"“球在右半屏\"“球快落地” 等各种场景，样本之间的关联性被打破，网络能学到 “不同场景下的最优动作”，不会再学偏。比如抽中的样本里，既有 “左移接左球”，也有 “右移接右球”，网络会均衡调整参数，最终学会 “根据球的位置选动作”—— 这就是经验回放的核心作用：打破样本序列相关性，让样本分布更接近目标函数要求的 “均匀分布”。\n问题 2：目标值跟着参数 “跑”，深度网络训练 “震荡”（为什么需要双网络）\nQ-Learning 的更新核心是 “用目标值（$r+\\gamma\\max_a \\hat{q}(s’,a,w)$）减去当前 Q 值，得到误差，再调整参数”。关键在于：目标值的计算依赖参数 $w$。\n线性 Q-Learning 为什么没问题？\n线性模型的参数少（比如傅里叶基 6 维），每次更新 $w$ 的幅度很小，目标值（$r+\\gamma\\max_a \\phi^T(s’,a)w$）的变化也很小，误差不会剧烈波动，训练能稳定收敛。\n深度网络 + 打砖块游戏：没有双网络会怎样？\n深度网络的参数多（比如 200 个神经元的隐藏层，仅输入到隐藏层的权重就有 “$210 \\times 160 \\times 200$” 个），每次更新 $w$ 的幅度可能很大，导致 “目标值跟着 $w$ 一起剧烈变化”—— 我们称之为 “目标值震荡”。\n举个具体数值例子（假设 $\\gamma=0.9$）：\nt 时刻：参数 $w_t$，计算 $s’$ 的 max Q 值是 $\\hat{q}(s’,a_{max},w_t)=4$，目标值 = $r + \\gamma \\times 4 = 1 + 0.9 \\times 4 = 4.6$；当前 Q 值 $\\hat{q}(s_t,a_t,w_t)=3$，误差 = $4.6-3=1.6$，用这个误差更新 $w_t \\to w_{t+1}$； t+1 时刻：参数 $w_{t+1}$ 已经变了，再算 $s’$ 的 max Q 值，可能变成 $\\hat{q}(s’,a_{max},w_{t+1})=5$，目标值 = $1 + 0.9 \\times 5=5.5$；当前 Q 值 $\\hat{q}(s_{t+1},a_{t+1},w_{t+1})=3.5$，误差 = $5.5-3.5=2$，又用这个误差更新 $w_{t+1} \\to w_{t+2}$； 循环往复：每次更新后，目标值都跟着 $w$ 变高，误差永远无法缩小，网络参数在 “追着波动的目标值” 震荡，永远收敛不了 —— 就像你要打一个移动的靶子，但靶子的位置会因为你开枪的动作而改变，永远打不准。 双网络怎么解决这个问题？\n双网络相当于 “把靶子固定一段时间”：\n主网络（参数 $w$）：负责 “实时学习”—— 用经验回放的样本更新，每次迭代都调整 $w$，输出当前的 Q 值 $\\hat{q}(s,a,w)$； 目标网络（参数 $w_T$）：负责 “提供稳定的目标值”—— 初始时 $w_T = w$，之后每 1000 次迭代才同步一次 $w_T = w$（固定一段时间），目标值用 $w_T$ 计算：$y_T = r + \\gamma \\max_a \\hat{q}(s’,a,w_T)$； 再看刚才的例子：\nt 时刻：$w_T =$ 1000 次前的 $w$，计算目标值 = $1 + 0.9 \\times 4=4.6$（$w_T$ 固定，目标值不变）；用误差 $1.6$ 更新主网络 $w \\to w_{t+1}$，但 $w_T$ 还是老参数，下次目标值还是 $4.6$； t+1 到 t+999 时刻：每次更新主网络 $w$，目标值都用固定的 $w_T$ 计算（一直是 $4.6$），误差会逐渐缩小（比如 t+100 次时，当前 Q 值变成 $4.5$，误差 = $0.1$）； t+1000 时刻：同步 $w_T =$ 当前的 $w$，目标值更新为新的稳定值（比如 $5.0$），之后继续用这个固定目标值训练； 这样一来，目标值不再 “实时波动”，网络能朝着一个稳定的方向学习，误差会逐渐收敛到零 —— 这就是双网络的核心作用：固定目标值的计算参数，避免目标值震荡，稳定训练。\n要利用式（8.38）的梯度最小化目标函数，需重点关注以下两项技术：\n技术 1：双网络架构（主网络 + 目标网络）\n如计算式（8.38）梯度时所提及，深度 Q 学习需同时维护主网络和目标网络，具体实现细节如下：\n设主网络参数为 $w$，目标网络参数为 $w_T$，初始时将两者设置为相同值； 每次迭代中，从回放缓冲区（replay buffer，后续将详细说明）中抽取一小批量（mini-batch）样本 ${(s, a, r, s’)}$； 主网络的输入为状态 $s$ 和动作 $a$，输出 $y = \\hat{q}(s, a, w)$ 为估计的 Q 值； 输出的目标值定义为 $y_T \\triangleq r + \\gamma \\max_{a \\in A(s’)} \\hat{q}(s’, a, w_T)$（即基于目标网络计算的目标 Q 值）； 更新主网络的参数 $w$，以最小化所有样本 ${(s, a, y_T)}$ 的 TD 误差（也称为损失函数）$\\sum (y - y_T)^2$。 需要注意的是，主网络参数 $w$ 的更新并非显式使用式（8.38）的梯度，而是依赖现有的神经网络训练工具。因此，与式（8.38）中基于单个样本更新主网络不同，训练网络需要一小批量样本 —— 这是深度强化学习算法与非深度强化学习算法的一个显著区别。\n主网络在每次迭代中都会更新，而目标网络则会每间隔一定迭代次数就与主网络同步一次参数（即令 $w_T = w$），以此满足计算式（8.38）梯度时 “$w_T$ 固定” 的假设。\n技术 2：经验回放（Experience Replay）\n经验回放技术的核心是：收集到经验样本后，不按样本的采集顺序使用，而是将其存储在一个名为 “回放缓冲区”（replay buffer）的数据集里。\n具体而言，设 $(s, a, r, s’)$ 为一个经验样本，$B \\triangleq {(s, a, r, s’)}$ 为回放缓冲区；每次更新主网络时，从回放缓冲区中抽取一小批量样本，且抽取过程需遵循均匀分布（这种抽取过程也称为 “经验回放”）。\n为什么深度 Q 学习需要经验回放？为什么回放必须遵循均匀分布？\n答案藏在式（8.37）的目标函数中：要合理定义该目标函数，必须明确随机变量 $S, A, R, S’$ 的概率分布。其中，一旦给定 $(S, A)$，$R$ 和 $S’$ 的分布由系统模型决定；而描述状态-动作对 $(S, A)$ 分布的最简单方式，就是假设其服从均匀分布。\n但在实际场景中，状态-动作样本是根据行为策略（behavior policy）按序列生成的，样本序列存在相关性，无法保证服从均匀分布。为满足 “均匀分布” 假设，需打破样本间的序列相关性 —— 通过从回放缓冲区中均匀抽取样本，恰好能实现这一点。这就是经验回放必不可少、且必须遵循均匀分布的数学原因。\n随机抽样的另一个好处是，每个经验样本可被多次使用，能提高数据利用效率，这在数据量有限时尤为重要。","马尔可夫决策过程mdp的平稳分布#马尔可夫决策过程（MDP）的平稳分布":"要理解平稳分布，首先需明确其基础 —— 策略 $\\pi$ 下的状态转移概率矩阵 $P_{\\pi}$：\n定义：$P_{\\pi} \\in \\mathbb{R}^{n \\times n}$（$n$ 为状态总数），矩阵元素 $[P_{\\pi}]_{ij}$ 表示 “智能体在策略 $\\pi$ 下，从状态 $s_i$ 一步转移到状态 $s_j$ 的概率”。 例：若 $[P_{\\pi}]_{12}=0.3$，则智能体从 $s_1$ 出发，一步到 $s_2$ 的概率是 30%。 关联背景：文档提到其完整定义见 2.6 节，核心是 “策略 $\\pi$ 决定了每个状态下的动作选择概率，进而结合环境转移规则（$p(r,s’|s,a)$）得到状态间的转移概率”。 $P_{\\pi}$ 的 $k$ 次幂（记为 $P_{\\pi}^k$）描述 “多步转移概率”，是理解长期行为的关键：\n核心结论：矩阵元素 $[P_{\\pi}^k]{ij}$ 表示 “智能体从 $s_i$ 出发，恰好经过 $k$ 步转移到 $s_j$ 的概率”，记为 $p{ij}^{(k)} = \\Pr(S_{t+k}=s_j | S_t=s_i)$。 直观解释： 当 $k=1$ 时，$P_{\\pi}^1 = P_{\\pi}$，$[P_{\\pi}]_{ij}$ 就是一步转移概率（符合定义）； 当 $k=2$ 时，$P_{\\pi}^2 = P_{\\pi} \\times P_{\\pi}$，其元素 $[P_{\\pi}^2]{ij} = \\sum{q=1}^n [P_{\\pi}]{iq} \\cdot [P{\\pi}]{qj}$。这是 “从 $s_i$ 先到中间状态 $s_q$（概率 $[P{\\pi}]{iq}$），再从 $s_q$ 到 $s_j$（概率 $[P{\\pi}]_{qj}$）” 的联合概率，对所有中间状态 $s_q$ 求和后，就是 “两步从 $s_i$ 到 $s_j$ 的总概率”； 同理，$k=3,4,\\dots$ 时，$P_{\\pi}^k$ 的元素对应 “恰好 $k$ 步转移概率”。 平稳分布的本质是 “马尔可夫过程长期运行后，状态分布不再变化”，文档通过 “初始分布→$k$ 步分布→极限分布→平稳条件” 逐步推导： 1. 初始分布与 $k$ 步分布\n初始分布 $d_0$：$d_0 \\in \\mathbb{R}^n$ 是初始时刻（$t=0$）的状态概率分布，例如 “从 $s_1$ 出发” 则 $d_0(s_1)=1$，其余元素为 0； $k$ 步分布 $d_k$：$d_k \\in \\mathbb{R}^n$ 是执行策略 $\\pi$ $k$ 步后，智能体处于各状态的概率分布，满足： 元素形式：$d_k(s_i) = \\sum_{j=1}^n d_0(s_j) \\cdot [P_{\\pi}^k]_{ji}$（从所有初始状态 $s_j$，经 $k$ 步转移到 $s_i$ 的概率之和）； 矩阵形式：$d_k^T = d_0^T \\cdot P_{\\pi}^k$（$d_k^T$ 表示行向量，便于矩阵乘法）。 2. 极限分布：长期行为与初始状态无关\n当 $k$ 趋近于无穷大（即智能体执行策略足够久）时，若满足某些条件（后续会讲），则：\nlim⁡k→∞Pπk=1n⋅dπT\r\\begin{aligned}\r\\lim_{k \\to \\infty} P_{\\pi}^k = 1_n \\cdot d_{\\pi}^T\r\\end{aligned}\rk→∞lim​Pπk​=1n​⋅dπT​​其中：\n$1_n = [1,1,\\dots,1]^T \\in \\mathbb{R}^n$（全 1 列向量）； $1_n \\cdot d_{\\pi}^T$ 是一个 $n \\times n$ 矩阵，每行都等于 $d_{\\pi}^T$（即所有行相同，均为平稳分布的行向量）。 将此代入 $k$ 步分布的矩阵形式，可得：\nlim⁡k→∞dkT=d0T⋅lim⁡k→∞Pπk=d0T⋅(1n⋅dπT)=dπT\r\\begin{aligned}\r\\lim_{k \\to \\infty} d_k^T = d_0^T \\cdot \\lim_{k \\to \\infty} P_{\\pi}^k = d_0^T \\cdot (1_n \\cdot d_{\\pi}^T) = d_{\\pi}^T\r\\end{aligned}\rk→∞lim​dkT​=d0T​⋅k→∞lim​Pπk​=d0T​⋅(1n​⋅dπT​)=dπT​​这里因 $d_0^T \\cdot 1_n = 1$（初始分布概率和为 1），最终 $k$ 步分布收敛到 $d_{\\pi}^T$。\n这个收敛后的分布 $d_{\\pi}$ 称为极限分布，其核心性质是：与初始分布 $d_0$ 无关—— 无论智能体从哪个状态出发，长期运行后，处于各状态的概率最终都会稳定到 $d_{\\pi}$。\n3. 平稳条件：分布不再变化的数学表达\n若 $d_k$ 收敛到 $d_{\\pi}$，则当 $k$ 足够大时，$d_k \\approx d_{k-1} \\approx d_{\\pi}$。对 $k$ 步分布公式 $d_k^T = d_{k-1}^T \\cdot P_{\\pi}$ 两边取极限，可得：\ndπT=dπT⋅Pπ(8.10)\r\\begin{aligned}\rd_{\\pi}^T = d_{\\pi}^T \\cdot P_{\\pi} \\tag{8.10}\r\\end{aligned}\rdπT​=dπT​⋅Pπ​​(8.10)这就是平稳分布的核心条件：\n含义：若当前状态分布是 $d_{\\pi}$，执行一步策略 $\\pi$ 后，分布仍保持为 $d_{\\pi}$（“平稳” 即 “不变化”）； 线性代数视角：$d_{\\pi}$ 是转移矩阵 $P_{\\pi}$ 对应特征值 1 的左特征向量（左特征向量满足 $\\vec{v}^T \\cdot M = \\lambda \\vec{v}^T$，此处 $\\lambda=1$）； 约束：平稳分布需满足 $\\sum_{s \\in S} d_{\\pi}(s) = 1$（概率和为 1），且所有 $d_{\\pi}(s) \u003e 0$（后续解释原因）。 并非所有马尔可夫过程都有唯一平稳分布，“不可约（irreducible）” 和 “正则（regular）” 是关键条件，先明确基础定义： 1. 基础概念：可达性与互通性\n可达性：若存在有限步数 $k$，使得 $[P_{\\pi}^k]_{ij} \u003e 0$，则称 “状态 $s_j$ 可从 $s_i$ 到达”（智能体从 $s_i$ 出发，能在有限步内到 $s_j$）； 互通性：若 $s_j$ 可从 $s_i$ 到达，且 $s_i$ 可从 $s_j$ 到达，则称 “$s_i$ 与 $s_j$ 互通”。 2. 不可约马尔可夫过程\n定义：所有状态两两互通（即从任意状态出发，都能在有限步内到达其他所有状态）； 例：4 个格子状态（s1-s4），若从 s1 能到 s2、s3、s4，从 s2 也能到 s1、s3、s4，以此类推，则过程不可约。 3. 正则马尔可夫过程\n定义：存在某个正整数 $k$，使得 $P_{\\pi}^k$ 的所有元素都大于 0（即任意两个状态之间，都能在 $k$ 步内到达，且概率为正）； 与不可约的关系： 正则过程一定是不可约的（所有元素正→所有状态互通）； 不可约过程不一定是正则的（例如 “s1→s2→s3→s1” 的循环，$k=3$ 时 $P_{\\pi}^3$ 对角元素为 1，其他为 0，不满足 “所有元素正”）； 例外：若不可约过程中存在某个状态 $s_i$，满足 $[P_{\\pi}]_{ii} \u003e 0$（能自转移），则该过程是正则的（自转移可打破循环，确保有限步内到达所有状态）。 4. 唯一性结论\n若马尔可夫过程是正则的（或满足更强条件），则其平稳分布唯一，且极限分布等于平稳分布（即 $\\lim_{k \\to \\infty} d_k = d_{\\pi}$）；正则过程的平稳分布满足 $d_{\\pi}(s) \u003e 0$（所有状态长期都有正概率被访问，无 “不可达” 或 “零概率” 状态）。 当策略 $\\pi$ 确定后，MDP 就退化为一个马尔可夫过程，其长期行为由策略和环境模型共同决定。文档指出：\n核心策略类型：探索性策略（如 $\\epsilon$-贪婪策略）能产生正则马尔可夫过程，进而有唯一平稳分布； 原因：探索性策略在任意状态下，对所有动作都有正概率选择（例如 $\\epsilon$-贪婪策略中，以 $\\epsilon$ 概率随机选动作）。即使环境模型允许状态互通，这种 “全动作正概率” 也能确保：从任意状态出发，都能通过探索性动作到达其他所有状态，满足正则过程的 “所有元素正” 条件。"},"title":"第8章 价值函数方法"},"/docs/self-study/ai/rl/chapter-9/":{"data":{"":"Policy Gradient MethodsPolicy representation: From table to function 当策略的表示方式从表格型切换为函数型时，有必要明确这两种表示方法之间的差异：\n第一，如何定义最优策略？\n当采用表格型表示时，若某策略能最大化每个状态的价值（state value），则该策略被定义为最优策略；当采用函数型表示时，若某策略能最大化特定标量指标（scalar metric），则该策略被定义为最优策略。\n第二，如何更新策略？\n当采用表格型表示时，可通过直接修改表格中的条目来更新策略；当采用参数化函数表示时，策略无法再通过这种方式更新，而是只能通过调整参数 $\\theta$ 来实现更新。\n第三，如何获取动作的概率？\n在表格型表示中，动作的概率可通过直接查询表格中对应的条目获得；在函数型表示中，需将（状态 s，动作 a）输入函数以计算该动作的概率。根据函数结构的不同，也可仅输入状态 s，进而输出所有动作的概率。\n策略梯度方法的基本思想可总结如下：假设 $J(\\theta)$ 为一个标量指标，通过基于梯度的算法对该指标进行优化，即可获得最优策略，算法公式如下：\nθt+1=θt+α∇θJ(θt)\r\\begin{aligned}\r\\theta_{t+1} = \\theta_t + \\alpha\\nabla_{\\theta}J(\\theta_t)\r\\end{aligned}\rθt+1​=θt​+α∇θ​J(θt​)​其中，$\\nabla_{\\theta}J$ 表示指标 $J$ 对参数 $\\theta$ 的梯度，$t$ 为时间步（time step），$\\alpha$ 为学习率（optimization rate）。\n基于这一基本思想，本章后续内容将解答以下三个问题：\n应采用哪些指标？（9.2 节） 如何计算指标的梯度？（9.3 节） 如何利用经验样本计算梯度？（9.4 节） Metrics for defining optimal policies 若策略以函数形式表示，则存在两类用于定义最优策略的标量指标：一类基于状态值（state values），另一类基于即时奖励（immediate rewards）。\n指标 1：平均状态值（Average State Value）\n第一类指标是平均状态值，简称平均值，其定义为：\nv‾π=∑s∈Sd(s)vπ(s)\r\\begin{aligned}\r\\overline{v}_{\\pi} = \\sum_{s \\in \\mathcal{S}} d(s)v_{\\pi}(s)\r\\end{aligned}\rvπ​=s∈S∑​d(s)vπ​(s)​其中，$d(s)$ 表示状态 $s$ 的权重，满足对任意 $s \\in \\mathcal{S}$，$d(s) \\geq 0$，且 $\\sum_{s \\in \\mathcal{S}} d(s) = 1$。因此，可将 $d(s)$ 解释为状态 $s$ 的概率分布。\n此时，该指标可改写为期望形式：\nv‾π=ES∼d[vπ(S)]\r\\begin{aligned}\r\\overline{v}_{\\pi} = \\mathbb{E}_{S \\sim d}\\left[v_{\\pi}(S)\\right]\r\\end{aligned}\rvπ​=ES∼d​[vπ​(S)]​**如何选择分布 $d$？**这是一个关键问题，主要分为两种情况：\n情况 1：$d$ 与策略 $\\pi$ 无关\n此时，我们特意将 $d$ 记为 $d_0$，将 $\\overline{v}{\\pi}$ 记为 $\\overline{v}{\\pi}^0$，以表明该分布与策略无关。\n一种场景是认为所有状态的重要性相同，此时选择 $d_0(s) = 1/|\\mathcal{S}|$（其中 $|\\mathcal{S}|$ 表示状态空间 $\\mathcal{S}$ 中状态的总数）； 另一种场景是仅关注某个特定状态 $s_0$（例如，智能体始终从 $s_0$ 出发），此时可设定： d0(s0)=1,d0(s≠s0)=0\r\\begin{aligned}\rd_0(s_0) = 1,\\quad d_0(s \\neq s_0) = 0\r\\end{aligned}\rd0​(s0​)=1,d0​(s=s0​)=0​情况 2：$d$ 与策略 $\\pi$ 相关\n此时，通常选择 $d$ 为策略 $\\pi$ 下的平稳分布（stationary distribution），记为 $d_{\\pi}$。\n平稳分布 $d_{\\pi}$ 的一个基本性质是满足：\ndπTPπ=dπT\r\\begin{aligned}\rd_{\\pi}^T P_{\\pi} = d_{\\pi}^T\r\\end{aligned}\rdπT​Pπ​=dπT​​其中 $P_{\\pi}$ 是策略 $\\pi$ 对应的状态转移概率矩阵。关于平稳分布的更多信息，可参考 8.1 节的补充内容（Box 8.1）。\n选择 $d_{\\pi}$ 的含义如下：平稳分布反映了马尔可夫决策过程（MDP）在给定策略下的长期行为 —— 若某个状态在长期中被频繁访问，则其重要性更高，应赋予更大权重；若某个状态极少被访问，则其重要性较低，应赋予更小权重。\n平均状态值 $\\overline{v}_{\\pi}$ 的等价表达式\n顾名思义，$\\overline{v}{\\pi}$ 是状态值的加权平均。参数 $\\theta$ 的取值不同，$\\overline{v}{\\pi}$ 的值也会不同。我们的最终目标是找到最优策略（或等价地，找到最优参数 $\\theta$），以最大化 $\\overline{v}_{\\pi}$。\n以下介绍 $\\overline{v}_{\\pi}$ 的另外两个重要等价表达式：\n等价表达式 1：基于奖励序列的期望形式\n假设智能体遵循给定策略 $\\pi(\\theta)$，收集到奖励序列 ${R_{t+1}}_{t=0}^{\\infty}$。读者在文献中常能看到以下指标：\nJ(θ)=lim⁡n→∞E[∑t=0nγtRt+1]=E[∑t=0∞γtRt+1](9.1)\r\\begin{aligned}\rJ(\\theta) = \\lim_{n \\to \\infty} \\mathbb{E}\\left[\\sum_{t=0}^{n} \\gamma^t R_{t+1}\\right] = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1}\\right] \\tag{9.1}\r\\end{aligned}\rJ(θ)=n→∞lim​E[t=0∑n​γtRt+1​]=E[t=0∑∞​γtRt+1​]​(9.1)该指标初看之下不易理解，但实际上它与 $\\overline{v}_{\\pi}$ 相等。证明如下：\nE[∑t=0∞γtRt+1]=∑s∈Sd(s)E[∑t=0∞γtRt+1∣S0=s]=∑s∈Sd(s)vπ(s)=v‾π\r\\begin{aligned}\r\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1}\\right] = \\sum_{s \\in \\mathcal{S}} d(s)\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s\\right] = \\sum_{s \\in \\mathcal{S}} d(s)v_{\\pi}(s) = \\overline{v}_{\\pi}\r\\end{aligned}\rE[t=0∑∞​γtRt+1​]=s∈S∑​d(s)E[t=0∑∞​γtRt+1​∣S0​=s]=s∈S∑​d(s)vπ​(s)=vπ​​上述等式中，第一个等号由全期望公式（law of total expectation）推导得出，第二个等号由状态值的定义（$v_{\\pi}(s)$ 是从状态 $s$ 出发的长期折扣奖励期望）推导得出。\n等价表达式 2：向量内积形式\n$\\overline{v}_{\\pi}$ 也可改写为两个向量的内积。具体地，令：\nvπ=[…,vπ(s),… ]T∈R∣S∣\r\\begin{aligned}\rv_{\\pi} = \\left[\\dots, v_{\\pi}(s), \\dots\\right]^T \\in \\mathbb{R}^{|\\mathcal{S}|}\r\\end{aligned}\rvπ​=[…,vπ​(s),…]T∈R∣S∣​d=[…,d(s),… ]T∈R∣S∣\r\\begin{aligned}\rd = \\left[\\dots, d(s), \\dots\\right]^T \\in \\mathbb{R}^{|\\mathcal{S}|}\r\\end{aligned}\rd=[…,d(s),…]T∈R∣S∣​则有：\nv‾π=dTvπ\r\\begin{aligned}\r\\overline{v}_{\\pi} = d^T v_{\\pi}\r\\end{aligned}\rvπ​=dTvπ​​该表达式在后续分析 $\\overline{v}_{\\pi}$ 的梯度时会非常有用。指标 2：平均奖励（Average Reward）\n第二类指标是单步平均奖励，简称平均奖励 [2, 64, 65]。具体地，其定义为：\nr‾π≐∑s∈Sdπ(s)rπ(s)=ES∼dπ[rπ(S)](9.2)\r\\begin{aligned}\r\\overline{r}_{\\pi} \\doteq \\sum_{s \\in \\mathcal{S}} d_{\\pi}(s)r_{\\pi}(s) = \\mathbb{E}_{S \\sim d_{\\pi}}\\left[r_{\\pi}(S)\\right] \\tag{9.2}\r\\end{aligned}\rrπ​≐s∈S∑​dπ​(s)rπ​(s)=ES∼dπ​​[rπ​(S)]​(9.2)其中：\n$d_{\\pi}$ 是策略 $\\pi$ 下的平稳分布； $r_{\\pi}(s)$ 是即时奖励的期望，其定义为： rπ(s)≐∑a∈Aπ(a∣s,θ)r(s,a)=EA∼π(s,θ)[r(s,A)∣s](9.3)\r\\begin{aligned}\rr_{\\pi}(s) \\doteq \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s, \\theta)r(s, a) = \\mathbb{E}_{A \\sim \\pi(s, \\theta)}\\left[r(s, A) \\mid s\\right] \\tag{9.3}\r\\end{aligned}\rrπ​(s)≐a∈A∑​π(a∣s,θ)r(s,a)=EA∼π(s,θ)​[r(s,A)∣s]​(9.3)这里的 $r(s, a)$ 表示在状态 $s$ 执行动作 $a$ 时的即时奖励期望，即 $r(s, a) \\doteq \\mathbb{E}\\left[R \\mid s, a\\right] = \\sum_{r} r p(r \\mid s, a)$（其中 $p(r \\mid s, a)$ 是在状态 $s$ 执行动作 $a$ 时获得奖励 $r$ 的概率）。\n平均奖励 $\\overline{r}_{\\pi}$ 的等价表达式\n以下介绍 $\\overline{r}_{\\pi}$ 的另外两个重要等价表达式：\n等价表达式 1：基于长期平均奖励的极限形式\n假设智能体遵循给定策略 $\\pi(\\theta)$，收集到奖励序列 ${R_{t+1}}_{t=0}^{\\infty}$。读者在文献中常能看到以下指标：\nJ(θ)=lim⁡n→∞1nE[∑t=0n−1Rt+1](9.4)\r\\begin{aligned}\rJ(\\theta) = \\lim_{n \\to \\infty} \\frac{1}{n}\\mathbb{E}\\left[\\sum_{t=0}^{n-1} R_{t+1}\\right] \\tag{9.4}\r\\end{aligned}\rJ(θ)=n→∞lim​n1​E[t=0∑n−1​Rt+1​]​(9.4)该指标初看之下不易理解，但实际上它与 $\\overline{r}_{\\pi}$ 相等：\nlim⁡n→∞1nE[∑t=0n−1Rt+1]=∑s∈Sdπ(s)rπ(s)=r‾π(9.5)\r\\begin{aligned}\r\\lim_{n \\to \\infty} \\frac{1}{n}\\mathbb{E}\\left[\\sum_{t=0}^{n-1} R_{t+1}\\right] = \\sum_{s \\in \\mathcal{S}} d_{\\pi}(s)r_{\\pi}(s) = \\overline{r}_{\\pi} \\tag{9.5}\r\\end{aligned}\rn→∞lim​n1​E[t=0∑n−1​Rt+1​]=s∈S∑​dπ​(s)rπ​(s)=rπ​​(9.5)式（9.5）的证明可参考 9.1 节的补充内容（Box 9.1）。\n等价表达式 2：向量内积形式\n式（9.2）中的平均奖励 $\\overline{r}_{\\pi}$ 也可改写为两个向量的内积。具体地，令：\nrπ=[…,rπ(s),… ]T∈R∣S∣\r\\begin{aligned}\rr_{\\pi} = \\left[\\dots, r_{\\pi}(s), \\dots\\right]^T \\in \\mathbb{R}^{|\\mathcal{S}|}\r\\end{aligned}\rrπ​=[…,rπ​(s),…]T∈R∣S∣​dπ=[…,dπ(s),… ]T∈R∣S∣\r\\begin{aligned}\rd_{\\pi} = \\left[\\dots, d_{\\pi}(s), \\dots\\right]^T \\in \\mathbb{R}^{|\\mathcal{S}|}\r\\end{aligned}\rdπ​=[…,dπ​(s),…]T∈R∣S∣​其中 $r_{\\pi}(s)$ 的定义如式（9.3）所示。此时显然有：\nr‾π=∑s∈Sdπ(s)rπ(s)=dπTrπ\r\\begin{aligned}\r\\overline{r}_{\\pi} = \\sum_{s \\in \\mathcal{S}} d_{\\pi}(s)r_{\\pi}(s) = d_{\\pi}^T r_{\\pi}\r\\end{aligned}\rrπ​=s∈S∑​dπ​(s)rπ​(s)=dπT​rπ​​该表达式在后续推导 $\\overline{r}_{\\pi}$ 的梯度时会非常有用。\n几点说明\n指标 表达式 1 表达式 2 表达式 3 平均状态值 $\\overline{v}_{\\pi}$ $\\sum_{s \\in \\mathcal{S}} d(s)v_{\\pi}(s)$ $\\mathbb{E}{S \\sim d}\\left[v{\\pi}(S)\\right]$ $\\lim_{n \\to \\infty} \\mathbb{E}\\left[\\sum_{t=0}^{n} \\gamma^t R_{t+1}\\right]$ 平均奖励 $\\overline{r}_{\\pi}$ $\\sum_{s \\in \\mathcal{S}} d_{\\pi}(s)r_{\\pi}(s)$ $\\mathbb{E}{S \\sim d{\\pi}}\\left[r_{\\pi}(S)\\right]$ $\\lim_{n \\to \\infty} \\frac{1}{n}\\mathbb{E}\\left[\\sum_{t=0}^{n-1} R_{t+1}\\right]$ 表 9.2：平均状态值 $\\overline{v}{\\pi}$ 与平均奖励 $\\overline{r}{\\pi}$ 的不同但等价表达式汇总\n到目前为止，我们已介绍两类指标：平均状态值 $\\overline{v}{\\pi}$ 与平均奖励 $\\overline{r}{\\pi}$。每类指标都有多个不同但等价的表达式，具体汇总于表 9.2。我们有时会用 $\\overline{v}{\\pi}$ 特指 “状态分布为平稳分布 $d{\\pi}$” 的情况，用 $\\overline{v}_{\\pi}^0$ 特指 “分布 $d_0$ 与策略 $\\pi$ 无关” 的情况。\n以下是关于这些指标的几点说明：\n所有这些指标都是策略 $\\pi$ 的函数。由于策略 $\\pi$ 由参数 $\\theta$ 参数化，因此这些指标也是 $\\theta$ 的函数。换句话说，$\\theta$ 的取值不同，指标的数值也会不同。因此，我们可通过寻找 $\\theta$ 的最优值来最大化这些指标 —— 这正是策略梯度方法的核心思想。\n在折扣因子 $\\gamma \u003c 1$ 的折扣场景下，平均状态值 $\\overline{v}{\\pi}$ 与平均奖励 $\\overline{r}{\\pi}$ 是等价的。具体可证明：\nr‾π=(1−γ)v‾π\\overline{r}_{\\pi} = (1 - \\gamma)\\overline{v}_{\\pi}rπ​=(1−γ)vπ​上述等式表明，两类指标可同时达到最大化。该等式的证明有缘再见。\nGradients of the metrics 我们可采用基于梯度的方法来最大化这些指标。要实现这一点，首先需计算这些指标的梯度。本章最重要的理论结果是下述定理：\n定理 9.1（策略梯度定理）\n指标 $J(\\theta)$ 的梯度为：\n∇θJ(θ)=∑s∈Sη(s)∑a∈A∇θπ(a∣s,θ)qπ(s,a)(9.8)\\nabla_{\\theta}J(\\theta) = \\sum_{s \\in \\mathcal{S}} \\eta(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta}\\pi(a|s, \\theta)q_{\\pi}(s, a) \\tag{9.8}∇θ​J(θ)=s∈S∑​η(s)a∈A∑​∇θ​π(a∣s,θ)qπ​(s,a)(9.8)其中，$\\eta$ 为状态分布，$\\nabla_{\\theta}\\pi$ 表示策略 $\\pi$ 对参数 $\\theta$ 的梯度。\n此外，式（9.8）可表示为更简洁的期望形式：\n∇θJ(θ)=ES∼η,A∼π(S,θ)[∇θln⁡π(A∣S,θ)qπ(S,A)](9.9)\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{S \\sim \\eta,A \\sim \\pi(S,\\theta)}\\left[\\nabla_{\\theta} \\ln \\pi(A|S, \\theta)q_{\\pi}(S, A)\\right] \\tag{9.9}∇θ​J(θ)=ES∼η,A∼π(S,θ)​[∇θ​lnπ(A∣S,θ)qπ​(S,A)](9.9)其中，$\\ln$ 表示自然对数。\n关于定理 9.1 的几点重要说明\n需注意，定理 9.1 是对定理 9.2、定理 9.3 和定理 9.5 结果的汇总。这三个定理分别对应不同场景：涵盖不同指标（如 $\\overline{v}{\\pi}^0$、$\\overline{v}{\\pi}$、$\\overline{r}_{\\pi}$）及 “折扣 / 无折扣” 情况。这些场景下的梯度表达式形式相似，因此汇总为定理 9.1。$J(\\theta)$ 与 $\\eta$ 的具体表达式未在定理 9.1 中给出，需参考上述三个细分定理；式（9.8）中的等式可能是严格等式，也可能是近似等式，且分布 $\\eta$ 在不同场景下也会有所不同。 需特别注意：为保证 $\\ln \\pi(a|s, \\theta)$ 有意义（即对数的真数为正），所有 $(s, a)$（状态 - 动作对）对应的策略概率 $\\pi(a|s, \\theta)$ 都必须为正值。这一要求可通过 softmax 函数实现，具体形式为：\nπ(a∣s,θ)=eh(s,a,θ)∑a′∈Aeh(s,a′,θ),a∈A(9.12)\\pi(a|s, \\theta) = \\frac{e^{h(s,a,\\theta)}}{\\sum_{a' \\in \\mathcal{A}} e^{h(s,a',\\theta)}}, \\quad a \\in \\mathcal{A} \\tag{9.12}π(a∣s,θ)=∑a′∈A​eh(s,a′,θ)eh(s,a,θ)​,a∈A(9.12)其中，$h(s, a, \\theta)$ 是 “状态 $s$ 下选择动作 $a$ 的偏好函数”（可理解为对动作 $a$ 的 “打分”）。\n式（9.12）定义的策略满足两个关键性质：\n对任意状态 $s \\in \\mathcal{S}$，任意动作 $a \\in \\mathcal{A}$，$\\pi(a|s, \\theta) \\in (0, 1)$（概率值在 0 到 1 之间）； 对任意状态 $s \\in \\mathcal{S}$，$\\sum_{a \\in \\mathcal{A}} \\pi(a|s, \\theta) = 1$（所有动作的概率和为 1，符合概率分布的定义）。 该策略可通过神经网络实现：网络的输入为状态 $s$，输出层采用 softmax 层，最终输出所有动作 $a$ 对应的 $\\pi(a|s, \\theta)$（且输出值的和为 1），具体可参考图 9.2 (b)。由于所有动作的概率 $\\pi(a|s, \\theta) \u003e 0$，该策略是随机策略—— 它不会直接指定 “应选择哪个动作”，而是要求根据策略的概率分布 “随机采样生成动作”，因此天然具备 “探索性”（即不会永远只选当前最优动作，也会尝试概率较低的动作）。\nDerivation of the gradients in the discounted case 接下来，我们推导折扣场景（其中 $\\gamma \\in (0, 1)$）下指标的梯度。折扣场景中的状态值（state value）与动作值（action value）定义如下：\nvπ(s)=E[Rt+1+γRt+2+γ2Rt+3+⋯∣St=s]v_{\\pi}(s) = \\mathbb{E}\\left[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s\\right]vπ​(s)=E[Rt+1​+γRt+2​+γ2Rt+3​+⋯∣St​=s]qπ(s,a)=E[Rt+1+γRt+2+γ2Rt+3+⋯∣St=s,At=a]q_{\\pi}(s, a) = \\mathbb{E}\\left[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s, A_t = a\\right]qπ​(s,a)=E[Rt+1​+γRt+2​+γ2Rt+3​+⋯∣St​=s,At​=a]易知以下关系成立：$v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s, \\theta) q_{\\pi}(s, a)$，且状态值满足贝尔曼方程（Bellman equation）。\n首先，我们证明 $\\overline{v}{\\pi}(\\theta)$ 与 $\\overline{r}{\\pi}(\\theta)$ 是等价指标\n引理 9.1（$\\overline{v}{\\pi}(\\theta)$ 与 $\\overline{r}{\\pi}(\\theta)$ 的等价性）：在 $\\gamma \\in (0, 1)$ 的折扣场景下，有\nr‾π=(1−γ)v‾π(9.13)\\overline{r}_{\\pi} = (1 - \\gamma)\\overline{v}_{\\pi} \\tag{9.13}rπ​=(1−γ)vπ​(9.13)证明：注意到 $\\overline{v}{\\pi}(\\theta) = d{\\pi}^T v_{\\pi}$ 且 $\\overline{r}{\\pi}(\\theta) = d{\\pi}^T r_{\\pi}$，其中 $v_{\\pi}$ 与 $r_{\\pi}$ 满足贝尔曼方程 $v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}$。在贝尔曼方程两侧同时左乘 $d_{\\pi}^T$，可得：\nv‾π=r‾π+γdπTPπvπ=r‾π+γdπTvπ=r‾π+γv‾π\\overline{v}_{\\pi} = \\overline{r}_{\\pi} + \\gamma d_{\\pi}^T P_{\\pi} v_{\\pi} = \\overline{r}_{\\pi} + \\gamma d_{\\pi}^T v_{\\pi} = \\overline{r}_{\\pi} + \\gamma \\overline{v}_{\\pi}vπ​=rπ​+γdπT​Pπ​vπ​=rπ​+γdπT​vπ​=rπ​+γvπ​由此可推出式（9.13）。\n其次，下述引理给出了任意状态 $s$ 对应的 $v_{\\pi}(s)$ 的梯度\n引理 9.2（$v_{\\pi}(s)$ 的梯度）：在折扣场景下，对任意 $s \\in \\mathcal{S}$，有\n∇θvπ(s)=∑s′∈SPrπ(s′∣s)∑a∈A∇θπ(a∣s′,θ)qπ(s′,a)(9.14)\\nabla_{\\theta} v_{\\pi}(s) = \\sum_{s' \\in \\mathcal{S}} \\text{Pr}_{\\pi}(s' \\mid s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi(a \\mid s', \\theta) q_{\\pi}(s', a) \\tag{9.14}∇θ​vπ​(s)=s′∈S∑​Prπ​(s′∣s)a∈A∑​∇θ​π(a∣s′,θ)qπ​(s′,a)(9.14)其中，$\\text{Pr}{\\pi}(s’ \\mid s) \\doteq \\sum{k=0}^{\\infty} \\gamma^k \\left[P_{\\pi}^k\\right]{ss’} = \\left[(I_n - \\gamma P{\\pi})^{-1}\\right]{ss’}$ 表示在策略 $\\pi$ 下，从状态 $s$ 转移到状态 $s’$ 的折扣总概率。此处，$[\\cdot]{ss’}$ 表示矩阵中 “第 $s$ 行、第 $s’$ 列” 的元素，而 $\\left[P_{\\pi}^k\\right]_{ss’}$ 表示在策略 $\\pi$ 下，从状态 $s$ 经过恰好 $k$ 步转移到状态 $s’$ 的概率。\n借助引理 9.2 的结果，我们现在可以推导平均状态值 $\\overline{v}_{\\pi}^0$ 的梯度。\n定理 9.2（折扣场景下 $\\overline{v}_{\\pi}^0$ 的梯度）\n在 $\\gamma \\in (0, 1)$ 的折扣场景下，平均状态值 $\\overline{v}{\\pi}^0 = d_0^T v{\\pi}$ 的梯度为：\n∇θv‾π0=E[∇θln⁡π(A∣S,θ)qπ(S,A)]\\nabla_{\\theta}\\overline{v}_{\\pi}^0 = \\mathbb{E}\\left[\\nabla_{\\theta}\\ln\\pi(A \\mid S, \\theta)q_{\\pi}(S, A)\\right]∇θ​vπ0​=E[∇θ​lnπ(A∣S,θ)qπ​(S,A)]其中，状态 $S$ 服从分布 $\\rho_{\\pi}$（即 $S \\sim \\rho_{\\pi}$），动作 $A$ 服从策略分布 $\\pi(S, \\theta)$（即 $A \\sim \\pi(S, \\theta)$）。\n此处的状态分布 $\\rho_{\\pi}$ 定义为：\nρπ(s)=∑s′∈Sd0(s′)Prπ(s∣s′),s∈S(9.19)\\rho_{\\pi}(s) = \\sum_{s' \\in \\mathcal{S}} d_0(s')Pr_{\\pi}(s \\mid s'),\\quad s \\in \\mathcal{S} \\tag{9.19}ρπ​(s)=s′∈S∑​d0​(s′)Prπ​(s∣s′),s∈S(9.19)其中，$Pr_{\\pi}(s \\mid s’) = \\sum_{k=0}^{\\infty} \\gamma^k [P_{\\pi}^k]{s’s} = [(I - \\gamma P{\\pi})^{-1}]_{s’s}$，表示在策略 $\\pi$ 下，从状态 $s’$ 转移到状态 $s$ 的折扣总概率。\n借助引理 9.1 和引理 9.2，我们可推导平均奖励 $\\overline{r}{\\pi}$ 与平均状态值 $\\overline{v}{\\pi}$ 的梯度。\n定理 9.3（折扣场景下 $\\overline{r}{\\pi}$ 与 $\\overline{v}{\\pi}$ 的梯度）\n在 $\\gamma \\in (0, 1)$ 的折扣场景下，平均奖励 $\\overline{r}{\\pi}$ 与平均状态值 $\\overline{v}{\\pi}$ 的梯度满足：\n∇θr‾π=(1−γ)∇θv‾π≈∑s∈Sdπ(s)∑a∈A∇θπ(a∣s,θ)qπ(s,a)=E[∇θln⁡π(A∣S,θ)qπ(S,A)]\\nabla_{\\theta}\\overline{r}_{\\pi} = (1 - \\gamma)\\nabla_{\\theta}\\overline{v}_{\\pi} \\approx \\sum_{s \\in \\mathcal{S}} d_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta}\\pi(a \\mid s, \\theta)q_{\\pi}(s, a) = \\mathbb{E}\\left[\\nabla_{\\theta}\\ln\\pi(A \\mid S, \\theta)q_{\\pi}(S, A)\\right]∇θ​rπ​=(1−γ)∇θ​vπ​≈s∈S∑​dπ​(s)a∈A∑​∇θ​π(a∣s,θ)qπ​(s,a)=E[∇θ​lnπ(A∣S,θ)qπ​(S,A)]其中，状态 $S$ 服从平稳分布 $d_{\\pi}$（即 $S \\sim d_{\\pi}$），动作 $A$ 服从策略分布 $\\pi(S, \\theta)$（即 $A \\sim \\pi(S, \\theta)$）。此处，$\\gamma$ 越接近 1，该近似的精度越高。\nDerivation of the gradients in the undiscounted case 接下来，我们将说明如何在 $\\gamma = 1$ 的无折扣场景下计算指标的梯度。读者可能会疑惑，为何在《深度强化学习》的前半内容中仅讨论了折扣场景，现在却突然开始考虑无折扣场景。事实上，平均奖励 $\\overline{r}{\\pi}$ 的定义对折扣场景和无折扣场景均适用。尽管折扣场景下 $\\overline{r}{\\pi}$ 的梯度是近似值，但我们将看到，无折扣场景下其梯度的形式更为简洁优雅。\n状态值与泊松方程 在无折扣场景下，需要重新定义状态值（state value）与动作值（action value）。由于无折扣奖励和（即$\\mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3} + \\dots \\mid S_t = s]$）可能会发散，因此需以特殊方式定义状态值与动作值[64]：\nvπ(s)≐E[(Rt+1−r‾π)+(Rt+2−r‾π)+(Rt+3−r‾π)+⋯∣St=s],v_{\\pi}(s) \\doteq \\mathbb{E}\\left[(R_{t+1} - \\overline{r}_{\\pi}) + (R_{t+2} - \\overline{r}_{\\pi}) + (R_{t+3} - \\overline{r}_{\\pi}) + \\dots \\mid S_t = s\\right],vπ​(s)≐E[(Rt+1​−rπ​)+(Rt+2​−rπ​)+(Rt+3​−rπ​)+⋯∣St​=s],\nqπ(s,a)≐E[(Rt+1−r‾π)+(Rt+2−r‾π)+(Rt+3−r‾π)+⋯∣St=s,At=a],q_{\\pi}(s, a) \\doteq \\mathbb{E}\\left[(R_{t+1} - \\overline{r}_{\\pi}) + (R_{t+2} - \\overline{r}_{\\pi}) + (R_{t+3} - \\overline{r}_{\\pi}) + \\dots \\mid S_t = s, A_t = a\\right],qπ​(s,a)≐E[(Rt+1​−rπ​)+(Rt+2​−rπ​)+(Rt+3​−rπ​)+⋯∣St​=s,At​=a],其中，$\\overline{r}{\\pi}$为平均奖励，其值由给定的策略$\\pi$唯一确定。在文献中，$v{\\pi}(s)$有多种不同名称，例如“差分奖励（differential reward）”[65]或“偏差（bias）”[2，第8.2.1节]。可以验证，上述定义的状态值满足以下类贝尔曼方程（Bellman-like equation）：\nvπ(s)=∑aπ(a∣s,θ)[∑rp(r∣s,a)(r−r‾π)+∑s′p(s′∣s,a)vπ(s′)](9.22)v_{\\pi}(s) = \\sum_{a} \\pi(a \\mid s, \\theta) \\left[ \\sum_{r} p(r \\mid s, a)(r - \\overline{r}_{\\pi}) + \\sum_{s'} p(s' \\mid s, a)v_{\\pi}(s') \\right] \\tag{9.22}vπ​(s)=a∑​π(a∣s,θ)[r∑​p(r∣s,a)(r−rπ​)+s′∑​p(s′∣s,a)vπ​(s′)](9.22)由于$v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s, \\theta) q_{\\pi}(s, a)$，可推出动作值满足：$q_{\\pi}(s, a) = \\sum_{r} p(r \\mid s, a)(r - \\overline{r}{\\pi}) + \\sum{s’} p(s’ \\mid s, a)v_{\\pi}(s’)$。式（9.22）的矩阵-向量形式为：\nvπ=rπ−r‾π1n+Pπvπ(9.23)v_{\\pi} = r_{\\pi} - \\overline{r}_{\\pi} \\mathbf{1}_n + P_{\\pi} v_{\\pi} \\tag{9.23}vπ​=rπ​−rπ​1n​+Pπ​vπ​(9.23)其中，$\\mathbf{1}_n = [1, \\dots, 1]^T \\in \\mathbb{R}^n$（即$n$维全1向量）。式（9.23）与贝尔曼方程类似，它有一个专门的名称——泊松方程（Poisson equation）[65, 67]。\n如何从泊松方程求解$v_{\\pi}$？ 下述定理给出了该问题的答案。\n定理9.4（泊松方程的解） 设\nvπ∗=(In−Pπ+1ndπT)−1rπ(9.24)v_{\\pi}^* = (I_n - P_{\\pi} + \\mathbf{1}_n d_{\\pi}^T)^{-1} r_{\\pi} \\tag{9.24}vπ∗​=(In​−Pπ​+1n​dπT​)−1rπ​(9.24)则$v_{\\pi}^*$是式（9.23）所示泊松方程的一个解。此外，泊松方程的任意解都具有以下形式：\nvπ=vπ∗+c1nv_{\\pi} = v_{\\pi}^* + c \\mathbf{1}_nvπ​=vπ∗​+c1n​其中，$c \\in \\mathbb{R}$（即$c$为任意实数）。\n该定理表明，泊松方程的解可能不唯一。 具有唯一性。具体而言，由泊松方程可推出：\nr‾π1n=rπ+(Pπ−In)vπ=rπ+(Pπ−In)(vπ∗+c1n)=rπ+(Pπ−In)vπ∗.\\begin{aligned}\r\\overline{r}_{\\pi}\\mathbf{1}_n \u0026= r_{\\pi} + (P_{\\pi} - I_n)v_{\\pi} \\\\\r\u0026= r_{\\pi} + (P_{\\pi} - I_n)(v_{\\pi}^* + c\\mathbf{1}_n) \\\\\r\u0026= r_{\\pi} + (P_{\\pi} - I_n)v_{\\pi}^*.\r\\end{aligned}rπ​1n​​=rπ​+(Pπ​−In​)vπ​=rπ​+(Pπ​−In​)(vπ∗​+c1n​)=rπ​+(Pπ​−In​)vπ∗​.​值得注意的是，待定值 $c$ 在此过程中被消去，因此 $\\overline{r}_{\\pi}$ 具有唯一性。基于此，我们可计算无折扣场景下r‾π\\overline{r}_{\\pi}rπ​的梯度。此外，由于vπv_{\\pi}vπ​不唯一，v‾π\\overline{v}_{\\pi}vπ​（平均状态值）也同样不唯一，因此本章不研究无折扣场景下v‾π\\overline{v}_{\\pi}vπ​的梯度。对于感兴趣的读者，值得一提的是：我们可通过添加更多约束条件，从泊松方程中唯一求解vπv_{\\pi}vπ​。例如，通过假设存在一个循环状态（recurrent state），可确定该循环状态的状态值，进而确定c的值。除此之外，还有其他方法可唯一确定vπv_{\\pi}vπ​。以下给出无折扣场景下r‾π\\overline{r}_{\\pi}rπ​的梯度。 定理9.5（无折扣场景下r‾π\\overline{r}_{\\pi}rπ​的梯度）在无折扣场景下，平均奖励r‾π\\overline{r}_{\\pi}rπ​的梯度为：\n∇θr‾π=∑s∈Sdπ(s)∑a∈A∇θπ(a∣s,θ)qπ(s,a)=E[∇θln⁡π(A∣S,θ)qπ(S,A)]\\begin{aligned}\r\\nabla_{\\theta}\\overline{r}_{\\pi} \u0026= \\sum_{s \\in \\mathcal{S}} d_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta}\\pi(a \\mid s, \\theta)q_{\\pi}(s, a) \\\\\r\u0026= \\mathbb{E}\\left[\\nabla_{\\theta}\\ln\\pi(A \\mid S, \\theta)q_{\\pi}(S, A)\\right]\r\\end{aligned}∇θ​rπ​​=s∈S∑​dπ​(s)a∈A∑​∇θ​π(a∣s,θ)qπ​(s,a)=E[∇θ​lnπ(A∣S,θ)qπ​(S,A)]​其中，状态S服从平稳分布dπd_{\\pi}dπ​（即S∼dπS \\sim d_{\\pi}S∼dπ​），动作A服从策略分布π(S,θ)\\pi(S, \\theta)π(S,θ)（即A∼π(S,θ)A \\sim \\pi(S, \\theta)A∼π(S,θ)）。与定理9.3所示的折扣场景相比，无折扣场景下r‾π\\overline{r}_{\\pi}rπ​的梯度更简洁优雅：一方面，式（9.28）严格成立（无需近似）；另一方面，状态S服从平稳分布，理论性质更清晰。\nMonte Carlo policy gradient (REINFORCE) 借助定理 9.1 中给出的梯度，我们接下来将说明如何使用基于梯度的方法优化指标，以获得最优策略。用于最大化 $J(\\theta)$ 的梯度上升算法为：\nθt+1=θt+α∇θJ(θt)=θt+αE[∇θln⁡π(A∣S,θt)qπ(S,A)](9.31)\\begin{aligned}\r\\theta_{t+1} \u0026= \\theta_t + \\alpha\\nabla_{\\theta}J(\\theta_t) \\\\\r\u0026= \\theta_t + \\alpha\\mathbb{E}\\left[\\nabla_{\\theta}\\ln\\pi(A|S, \\theta_t)q_{\\pi}(S, A)\\right] \\tag{9.31}\r\\end{aligned}θt+1​​=θt​+α∇θ​J(θt​)=θt​+αE[∇θ​lnπ(A∣S,θt​)qπ​(S,A)]​(9.31)其中，$\\alpha \u003e 0$ 是常数学习率。由于式（9.31）中的真实梯度未知，我们可以用随机梯度替代真实梯度，得到如下算法：\nθt+1=θt+α∇θln⁡π(at∣st,θt)qt(st,at)(9.32)\\theta_{t+1} = \\theta_t + \\alpha\\nabla_{\\theta}\\ln\\pi(a_t|s_t, \\theta_t)q_t(s_t, a_t) \\tag{9.32}θt+1​=θt​+α∇θ​lnπ(at​∣st​,θt​)qt​(st​,at​)(9.32)其中，$q_t(s_t, a_t)$ 是 $q_{\\pi}(s_t, a_t)$ 的近似值。若qt(st,at)q_t(s_t, a_t)qt​(st​,at​)通过蒙特卡洛估计得到，则该算法被称为REINFORCE算法[68]（或蒙特卡洛策略梯度算法），它是最早、最简单的策略梯度算法之一。式（9.32）所示的算法具有重要意义，因为许多其他策略梯度算法都可通过对其扩展得到。接下来，我们将更深入地解读式（9.32）的含义。由∇θln⁡π(at∣st,θt)=∇θπ(at∣st,θt)π(at∣st,θt)\\nabla_{\\theta}\\ln\\pi(a_t|s_t, \\theta_t) = \\frac{\\nabla_{\\theta}\\pi(a_t|s_t, \\theta_t)}{\\pi(a_t|s_t, \\theta_t)}∇θ​lnπ(at​∣st​,θt​)=π(at​∣st​,θt​)∇θ​π(at​∣st​,θt​)​，可将式（9.32）改写为：θt+1=θt+α(qt(st,at)π(at∣st,θt))⏟βt∇θπ(at∣st,θt)\\theta_{t+1} = \\theta_t + \\alpha\\underbrace{\\left( \\frac{q_t(s_t, a_t)}{\\pi(a_t|s_t, \\theta_t)} \\right)}_{\\beta_t}\\nabla_{\\theta}\\pi(a_t|s_t, \\theta_t)θt+1​=θt​+αβt​(π(at​∣st​,θt​)qt​(st​,at​)​)​​∇θ​π(at​∣st​,θt​)进一步简化为： θt+1=θt+αβt∇θπ(at∣st,θt)(9.33)\\theta_{t+1} = \\theta_t + \\alpha\\beta_t\\nabla_{\\theta}\\pi(a_t|s_t, \\theta_t) \\tag{9.33}θt+1​=θt​+αβt​∇θ​π(at​∣st​,θt​)(9.33) 从该式中可得出两个重要解读：解读一：梯度上升对动作概率的调节作用由于式（9.33）是简单的梯度上升算法，可得出以下结论：若βt≥0\\beta_t \\geq 0βt​≥0，则选择（st,ats_t, a_tst​,at​）的概率会增强，即： π(at∣st,θt+1)≥π(at∣st,θt)\\pi(a_t|s_t, \\theta_{t+1}) \\geq \\pi(a_t|s_t, \\theta_t)π(at​∣st​,θt+1​)≥π(at​∣st​,θt​)βt\\beta_tβt​的值越大，增强效果越强。若βt\u003c0\\beta_t \u003c 0βt​\u003c0，则选择（st,ats_t, a_tst​,at​）的概率会降低，即： π(at∣st,θt+1)\u003cπ(at∣st,θt)\\pi(a_t|s_t, \\theta_{t+1}) \u003c \\pi(a_t|s_t, \\theta_t)π(at​∣st​,θt+1​)\u003cπ(at​∣st​,θt​)上述结论的证明如下：当θt+1−θt\\theta_{t+1} - \\theta_tθt+1​−θt​足够小时，由泰勒展开可得：π(at∣st,θt+1)≈π(at∣st,θt)+(∇θπ(at∣st,θt))T(θt+1−θt)=π(at∣st,θt)+αβt(∇θπ(at∣st,θt))T(∇θπ(at∣st,θt))(代入式（9.33）)=π(at∣st,θt)+αβt∥∇θπ(at∣st,θt)∥22\\begin{aligned}\r\\pi(a_t|s_t, \\theta_{t+1}) \u0026\\approx \\pi(a_t|s_t, \\theta_t) + \\left(\\nabla_{\\theta}\\pi(a_t|s_t, \\theta_t)\\right)^T (\\theta_{t+1} - \\theta_t) \\\\\r\u0026= \\pi(a_t|s_t, \\theta_t) + \\alpha\\beta_t\\left(\\nabla_{\\theta}\\pi(a_t|s_t, \\theta_t)\\right)^T \\left(\\nabla_{\\theta}\\pi(a_t|s_t, \\theta_t)\\right) \\quad (\\text{代入式（9.33）}) \\\\\r\u0026= \\pi(a_t|s_t, \\theta_t) + \\alpha\\beta_t\\left\\|\\nabla_{\\theta}\\pi(a_t|s_t, \\theta_t)\\right\\|_2^2\r\\end{aligned}π(at​∣st​,θt+1​)​≈π(at​∣st​,θt​)+(∇θ​π(at​∣st​,θt​))T(θt+1​−θt​)=π(at​∣st​,θt​)+αβt​(∇θ​π(at​∣st​,θt​))T(∇θ​π(at​∣st​,θt​))(代入式（9.33）)=π(at​∣st​,θt​)+αβt​∥∇θ​π(at​∣st​,θt​)∥22​​显然，当βt≥0\\beta_t \\geq 0βt​≥0时，π(at∣st,θt+1)≥π(at∣st,θt)\\pi(a_t|s_t, \\theta_{t+1}) \\geq \\pi(a_t|s_t, \\theta_t)π(at​∣st​,θt+1​)≥π(at​∣st​,θt​)；当βt\u003c0\\beta_t \u003c 0βt​\u003c0时，π(at∣st,θt+1)\u003cπ(at∣st,θt)\\pi(a_t|s_t, \\theta_{t+1}) \u003c \\pi(a_t|s_t, \\theta_t)π(at​∣st​,θt+1​)\u003cπ(at​∣st​,θt​)。解读二：探索与利用的平衡由βt=qt(st,at)π(at∣st,θt)\\beta_t = \\frac{q_t(s_t, a_t)}{\\pi(a_t|s_t, \\theta_t)}βt​=π(at​∣st​,θt​)qt​(st​,at​)​的表达式可知，该算法能在一定程度上实现探索（exploration）与利用（exploitation）的平衡：利用（ exploitation ）：βt\\beta_tβt​与qt(st,at)q_t(s_t, a_t)qt​(st​,at​)成正比。若（st,ats_t, a_tst​,at​）的动作值较大，则π(at∣st,θt)\\pi(a_t|s_t, \\theta_t)π(at​∣st​,θt​)会被增强，使得选择ata_tat​的概率上升——算法倾向于“利用”价值更高的动作。探索（ exploration ）：当qt(st,at)\u003e0q_t(s_t, a_t) \u003e 0qt​(st​,at​)\u003e0时，βt\\beta_tβt​与π(at∣st,θt)\\pi(a_t|s_t, \\theta_t)π(at​∣st​,θt​)成反比。若选择ata_tat​的概率较小，则π(at∣st,θt)\\pi(a_t|s_t, \\theta_t)π(at​∣st​,θt​)会被增强，使得选择ata_tat​的概率上升——算法倾向于“探索”概率较低的动作。此外，由于式（9.32）通过采样近似式（9.31）中的真实梯度，明确“如何获取采样样本”十分重要：如何采样状态S？ 真实梯度E[∇θln⁡π(A∣S,θt)qπ(S,A)]\\mathbb{E}\\left[\\nabla_{\\theta}\\ln\\pi(A|S, \\theta_t)q_{\\pi}(S, A)\\right]E[∇θ​lnπ(A∣S,θt​)qπ​(S,A)]中的S需服从分布η\\etaη，该分布可为策略π\\piπ下的平稳分布dπd_{\\pi}dπ​，或式（9.19）中的折扣总概率分布ρπ\\rho_{\\pi}ρπ​。无论是dπd_{\\pi}dπ​还是ρπ\\rho_{\\pi}ρπ​，都反映了策略π\\piπ下的长期行为。如何采样动作A？ 真实梯度中的A需服从π(A∣S,θ)\\pi(A|S, \\theta)π(A∣S,θ)的分布。理想的采样方式是遵循π(a∣st,θt)\\pi(a|s_t, \\theta_t)π(a∣st​,θt​)选择ata_tat​——因此，策略梯度算法是同策略（on-policy） 算法（即采样与更新使用同一策略）。遗憾的是，在实际应用中，由于样本利用效率较低，上述理想的S和A采样方式并未被严格遵循。式（9.32）的一种更高样本效率的实现方式如算法9.1所示：首先遵循策略π(θ)\\pi(θ)π(θ)生成一个回合（episode），然后利用回合中的每一个经验样本对θ\\thetaθ进行多次更新。","derivation-of-the-gradients-in-the-discounted-case#Derivation of the gradients in the discounted case":"","derivation-of-the-gradients-in-the-undiscounted-case#Derivation of the gradients in the undiscounted case":"","gradients-of-the-metrics#Gradients of the metrics":"","metrics-for-defining-optimal-policies#Metrics for defining optimal policies":"","monte-carlo-policy-gradient-reinforce#Monte Carlo policy gradient (REINFORCE)":"","policy-gradient-by-monte-carlo-reinforce#Policy Gradient by Monte Carlo (REINFORCE)":"Initialization: Initial parameter $\\theta$; $\\gamma \\in (0, 1)$; $\\alpha \u003e 0$.\nGoal: Learn an optimal policy for maximizing $J(\\theta)$.\nFor each episode, do:\nGenerate an episode ${s_0, a_0, r_1, \\dots, s_{T-1}, a_{T-1}, r_T}$ following $\\pi(\\theta)$. For $t = 0, 1, \\dots, T - 1$: Value update: $q_t(s_t, a_t) = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} r_k$ Policy update: $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} \\ln \\pi(a_t|s_t, \\theta) q_t(s_t, a_t)$","policy-gradient-methods#Policy Gradient Methods":"","policy-representation-from-table-to-function#Policy representation: From table to function":"","summary#Summary":"本章介绍了策略梯度方法，它是许多现代强化学习算法的基础。策略梯度方法属于“基于策略（policy-based）”的方法，是一大突破——因为前序章节介绍的所有方法均为“value-based”的方法。 策略梯度方法的核心思想十分简洁：选择合适的标量指标，通过梯度上升算法对其进行优化。 策略梯度方法中最复杂的部分是指标梯度的推导——这是因为我们需要区分“不同指标”“折扣/无折扣场景”等多种情况。幸运的是，不同场景下的梯度表达式具有相似性，因此我们将其汇总为定理9.1（本章最重要的理论结果）。\n对许多读者而言，无需理解证明过程，只需掌握该定理的结论即可。 式（9.32）所示的策略梯度算法必须被准确理解，因为它是许多高级策略梯度算法的基础。在下一章中，该算法将被扩展为另一种重要的策略梯度方法——Actor-Critic。"},"title":"第9章 策略梯度方法"},"/docs/self-study/embeded/":{"data":{"3d建模#3D建模":"软件：Fusion360 制造：3D打印（FDM/SLA）","参考资料#参考资料":"小智AI开源项目 嘉立创开源硬件平台 Fusion360官方教程 四足机器人相关论文和资料","学习目标#学习目标":"掌握嵌入式系统硬件设计（PCB设计、元器件选型） 学习Fusion360进行3D建模和3D打印 深入理解四足机器人控制算法和步态规划 实践嵌入式软件开发（实时控制、通信协议） 完成一个完整的嵌入式项目从设计到实现","学习路径#学习路径":"graph TD\rA[项目概述] --\u003e B[硬件设计]\rA --\u003e C[3D建模]\rA --\u003e D[嵌入式软件]\rA --\u003e E[控制算法]\rB --\u003e B1[PCB设计]\rB --\u003e B2[元器件选型]\rB --\u003e B3[电源管理]\rC --\u003e C1[Fusion360基础]\rC --\u003e C2[结构设计]\rC --\u003e C3[3D打印]\rD --\u003e D1[开发环境]\rD --\u003e D2[驱动开发]\rD --\u003e D3[通信协议]\rE --\u003e E1[步态规划]\rE --\u003e E2[运动控制]\rE --\u003e E3[动作库]","嵌入式系统学习#嵌入式系统学习":"嵌入式系统学习 通过复刻小智AI四足桌宠项目，深化本科嵌入式知识，从硬件到软件，从理论到实践。","技术栈#技术栈":"","文档结构#文档结构":"","硬件#硬件":"PCB设计：嘉立创EDA 主控：ESP32 / STM32 执行器：舵机（SG90/MG90S等） 传感器：IMU、触摸传感器等 通信：蓝牙、WiFi","软件#软件":"开发环境：Arduino IDE / PlatformIO / Keil 编程语言：C/C++ 控制算法：逆运动学、步态规划","项目概述#项目概述":"小智AI四足桌宠是一个桌面级四足机器人项目，具有以下特点：\n🦵 四足结构：12自由度（每条腿3个舵机） 🎮 交互控制：支持多种交互方式（蓝牙、WiFi等） 🤖 智能行为：实现基础步态和动作库 🎨 个性化：可自定义外观和动作"},"title":"嵌入式"},"/docs/self-study/embeded/01-project-plan/":{"data":{"主控芯片#主控芯片":"选项1：ESP32\n✅ 优点：内置WiFi/蓝牙、丰富外设、Arduino生态好 ❌ 缺点：实时性相对较弱 适用场景：需要WiFi/蓝牙通信，对实时性要求不高 选项2：STM32F4\n✅ 优点：实时性强、性能高、外设丰富 ❌ 缺点：需要额外蓝牙模块 适用场景：对实时性要求高，需要精确控制 初步选择：ESP32（便于快速原型开发，后续可升级）","功能需求#功能需求":"基础运动\n前进、后退、转向 站立、坐下、趴下 基础步态（trot、walk等） 交互功能\n蓝牙连接控制 WiFi连接（可选） 触摸响应 LED状态指示 扩展功能\n动作库（挥手、点头等） 传感器反馈（姿态检测） 语音控制（可选）","开发计划#开发计划":"","开源资源#开源资源":"","性能指标#性能指标":"尺寸：桌面级，约15-20cm长度 重量：\u003c 500g（含电池） 续航：\u003e 30分钟 响应速度：舵机响应 \u003c 100ms 稳定性：能稳定行走和站立","执行器选型#执行器选型":"舵机类型：\nSG90：便宜，但扭矩小（1.8kg·cm） MG90S：金属齿轮，扭矩中等（2.5kg·cm） MG996R：大扭矩（10kg·cm），但体积大 初步选择：MG90S（性价比高，适合桌面级）","技术选型#技术选型":"","电源方案#电源方案":"电池：18650锂电池（3.7V，2600mAh） 稳压：AMS1117-5V（舵机供电）+ AMS1117-3.3V（主控供电） 充电：TP4056充电模块","硬件参考#硬件参考":"嘉立创开源硬件平台 GitHub上的四足机器人项目 Thingiverse上的3D模型","硬件架构#硬件架构":"┌─────────────┐\r│ ESP32 │\r│ (主控) │\r└──────┬──────┘\r│\r├─── I2C ─── IMU传感器\r├─── UART ── 蓝牙模块（如需要）\r├─── GPIO ── LED指示灯\r├─── GPIO ── 触摸传感器\r│\r└─── PWM ─── 12路舵机控制\r│\r├─── 左前腿（3个舵机）\r├─── 右前腿（3个舵机）\r├─── 左后腿（3个舵机）\r└─── 右后腿（3个舵机）","第一阶段硬件设计2-3周#第一阶段：硬件设计（2-3周）":"原理图设计 PCB布局 元器件采购 PCB打样和焊接","第三阶段基础软件2-3周#第三阶段：基础软件（2-3周）":"开发环境搭建 舵机驱动开发 基础控制函数 单腿测试","第二阶段3d建模2-3周#第二阶段：3D建模（2-3周）":"Fusion360学习 结构设计 3D打印测试 装配验证","第五阶段通信与交互2周#第五阶段：通信与交互（2周）":"蓝牙通信协议 上位机开发（可选） 交互逻辑实现","第六阶段优化与完善2-3周#第六阶段：优化与完善（2-3周）":"性能优化 稳定性提升 功能完善 文档整理","第四阶段控制算法3-4周#第四阶段：控制算法（3-4周）":"逆运动学实现 基础步态实现 动作库开发 稳定性调试","软件参考#软件参考":"Arduino舵机库 ESP32蓝牙示例 四足机器人控制算法开源项目","软件架构#软件架构":"┌─────────────────────┐\r│ 应用层 │\r│ - 动作库 │\r│ - 交互逻辑 │\r└──────────┬──────────┘\r│\r┌──────────▼──────────┐\r│ 控制层 │\r│ - 步态规划 │\r│ - 逆运动学 │\r│ - 轨迹生成 │\r└──────────┬──────────┘\r│\r┌──────────▼──────────┐\r│ 驱动层 │\r│ - 舵机驱动 │\r│ - 传感器驱动 │\r│ - 通信协议 │\r└──────────┬──────────┘\r│\r┌──────────▼──────────┐\r│ 硬件抽象层 │\r│ - PWM控制 │\r│ - I2C/SPI/UART │\r└─────────────────────┘","通信方案#通信方案":"主要：蓝牙4.0/5.0（低功耗） 可选：WiFi（ESP32内置）","需求分析#需求分析":"","项目规划#项目规划":"项目规划 在开始动手之前，先做好详细的规划，明确需求、技术选型和开发计划。"},"title":"项目规划"},"/docs/self-study/embeded/02-hardware/":{"data":{"pcb布局要点#PCB布局要点":"电源走线：尽量宽，减少压降 信号线：避免与电源线平行，减少干扰 接地：大面积铺铜，降低噪声 元器件布局：按功能模块分区 过孔：合理使用，保证信号完整性","pcb设计#PCB设计":"","q-pcb打样注意事项#Q: PCB打样注意事项？":"A:\n最小线宽/线距：0.1mm（嘉立创标准工艺） 过孔大小：最小0.2mm 板厚：1.6mm（标准）","q-电池续航短#Q: 电池续航短？":"A:\n使用更大容量电池 优化控制算法，减少不必要的运动 添加休眠功能","q-舵机抖动怎么办#Q: 舵机抖动怎么办？":"A:\n检查电源是否稳定（增加滤波电容） 检查PWM信号是否稳定 检查机械结构是否卡顿","主控电路#主控电路":"ESP32最小系统：\nESP32-WROOM-32模块 复位电路 晶振电路（ESP32内置，可选） 电源指示灯","传感器#传感器":"名称 型号 数量 接口 IMU MPU6050 1 I2C 触摸传感器 TTP223 1 GPIO","传感器接口#传感器接口":"IMU传感器（MPU6050）：I2C接口 触摸传感器：GPIO输入 LED指示灯：GPIO输出","元器件清单#元器件清单":"","其他#其他":"名称 规格 数量 LED 3mm 2-4 电阻 10kΩ, 1kΩ 若干 电容 100uF, 10uF, 0.1uF 若干 排针/排母 2.54mm 若干","功耗估算#功耗估算":"ESP32：工作电流约80-240mA（取决于工作模式） 舵机（单个）：空载约50-100mA，负载约500-1000mA 12个舵机同时工作：峰值可达12A（实际不会同时满载）","原理图设计#原理图设计":"","参考资源#参考资源":"嘉立创EDA教程 ESP32官方原理图 舵机驱动电路参考设计","常见问题#常见问题":"","打样与焊接#打样与焊接":"使用嘉立创打样（5元/5片，非常便宜） 元器件可以在嘉立创商城购买 焊接时注意温度控制，避免损坏元器件","核心元器件#核心元器件":"名称 型号/规格 数量 备注 主控模块 ESP32-WROOM-32 1 或ESP32开发板 舵机 MG90S 12 每条腿3个 电池 18650 3.7V 2600mAh 1 带保护板 充电模块 TP4056 1 USB充电 稳压芯片 AMS1117-5V 1 舵机供电 稳压芯片 AMS1117-3.3V 1 主控供电","电源方案选择#电源方案选择":"方案1：单电池+双路稳压\n优点：简单，成本低 缺点：电池负担重，续航短 方案2：双电池（分别供电）\n优点：电源分离，稳定性好 缺点：重量增加，成本增加 推荐方案1，但需要：\n大容量电池（3000mAh+） 足够的滤波电容 合理的电源管理（避免所有舵机同时启动）","电源管理#电源管理":"18650电池 (3.7V)\r│\r├── TP4056充电模块\r│ └── USB充电接口\r│\r├── AMS1117-5V\r│ └── 舵机供电 (5V)\r│\r└── AMS1117-3.3V\r└── ESP32供电 (3.3V) 注意事项：\n舵机启动电流大，需要足够的电容滤波 电池保护电路（过充/过放保护） 电源开关设计","电源设计详解#电源设计详解":"","硬件设计#硬件设计":"硬件设计 硬件是基础，好的硬件设计能让后续开发事半功倍。","舵机控制#舵机控制":"12路PWM输出：\nESP32有16路PWM通道，足够使用 每路PWM需要独立控制 建议使用PCA9685舵机驱动板（可选，简化设计） 舵机连接：\n红色：VCC (5V) 黑色：GND 黄色/橙色：PWM信号线","设计工具#设计工具":"使用嘉立创EDA进行PCB设计，原因：\n✅ 免费且功能强大 ✅ 国产软件，中文支持好 ✅ 可以直接在嘉立创打样 ✅ 有丰富的元器件库","调试接口#调试接口":"设计时预留：\nUSB转串口：用于程序下载和调试 SWD接口（如使用STM32）：用于调试 测试点：关键信号测试点"},"title":"硬件设计"},"/docs/self-study/embeded/03-fusion360/":{"data":{"1-草图sketch#1. 草图（Sketch）":"创建平面草图 绘制基本图形（线、圆、矩形等） 添加约束（尺寸、几何关系） 完成草图","1-身体body#1. 身体（Body）":"安装主控板 安装电池 连接四条腿 预留传感器安装位置 设计要点：\n重心尽量低 内部空间合理利用 预留走线空间","2-大腿thigh#2. 大腿（Thigh）":"连接身体和膝关节 安装第一个舵机（髋关节）","2-特征feature#2. 特征（Feature）":"拉伸（Extrude）：将草图拉伸成3D 旋转（Revolve）：绕轴旋转草图 扫掠（Sweep）：沿路径扫掠 放样（Loft）：连接多个轮廓","3-小腿shank#3. 小腿（Shank）":"连接膝关节和踝关节 安装第二个舵机（膝关节）","3-装配assembly#3. 装配（Assembly）":"创建组件 添加约束（配合、对齐等） 运动仿真","3d建模与fusion360#3D建模与Fusion360":"3D建模与Fusion360 Fusion360是Autodesk的免费3D建模软件，功能强大，适合机械设计和3D打印。","3d打印#3D打印":"","4-足部foot#4. 足部（Foot）":"接触地面 安装第三个舵机（踝关节） 考虑防滑设计","5-舵机支架#5. 舵机支架":"固定舵机 连接相邻部件 考虑散热","fusion360基础#Fusion360基础":"","q-打印出来的零件尺寸不对#Q: 打印出来的零件尺寸不对？":"A:\n检查打印机校准 考虑材料收缩率（PLA约0.2%） 调整模型尺寸补偿","q-装配时太紧或太松#Q: 装配时太紧或太松？":"A:\n调整孔的公差（通常+0.2mm） 使用不同打印参数测试","q-零件强度不够#Q: 零件强度不够？":"A:\n增加壁厚 提高填充率 改变打印方向 使用更强材料","主要部件#主要部件":"","后处理#后处理":"去除支撑 打磨表面 钻孔（如需要） 装配测试","四足机器人结构设计#四足机器人结构设计":"","基础操作#基础操作":"","学习资源#学习资源":"Fusion360官方教程 YouTube上的Fusion360教程 3D打印社区（如Thingiverse）","常见问题#常见问题":"","打印参数#打印参数":"层高：0.2mm（平衡速度和质量）\r填充：20-30%（根据部件要求）\r支撑：需要时添加\r打印速度：50-60mm/s\r温度：根据材料调整","打印注意事项#打印注意事项":"方向选择：考虑强度和打印质量 支撑添加：悬垂部分需要支撑 尺寸补偿：考虑打印收缩 公差设计：预留装配间隙","打印设置#打印设置":"","材料选择#材料选择":"PLA：易打印，强度一般，适合原型 PETG：强度好，韧性好，推荐 ABS：强度高，但需要加热床 TPU：柔性材料，适合特殊部件 推荐：PETG（平衡了强度和打印难度）","界面介绍#界面介绍":"主要工作区：\n浏览器：管理设计历史和时间轴 视图立方体：快速切换视角 工具栏：常用工具 时间轴：参数化设计历史","装配#装配":"","装配工具#装配工具":"螺丝刀（M2、M3） 内六角扳手 热熔胶枪（固定线材） 扎带（整理线材）","装配顺序#装配顺序":"安装舵机到支架 组装单条腿 将腿安装到身体 安装PCB和电池 走线和固定 最终调试","设计参数#设计参数":"关键尺寸（需要根据实际舵机尺寸调整）：\n身体尺寸：约 80mm × 60mm × 40mm\r大腿长度：约 60mm\r小腿长度：约 80mm\r足部尺寸：约 30mm × 20mm\r总高度：约 120mm（站立时）\r总长度：约 150mm","设计思路#设计思路":"模块化设计：每条腿独立设计，便于修改 参数化：关键尺寸设为参数，便于调整 轻量化：在保证强度的前提下减轻重量 易装配：考虑3D打印和装配的便利性","设计步骤#设计步骤":"创建参数表\n定义关键尺寸为参数 便于后续调整 设计单条腿\n从足部开始 逐步向上设计 考虑舵机安装 设计身体\n根据PCB尺寸设计 预留安装孔 装配验证\n在Fusion360中装配 检查干涉 运动仿真 优化设计\n减轻重量（挖空、减薄） 增加强度（加强筋） 优化外观","软件获取#软件获取":"个人版：免费（功能受限，但足够使用） 教育版：免费（需要教育邮箱） 商业版：付费"},"title":"3D建模"},"/docs/self-study/embeded/04-embedded-software/":{"data":{"esp32-pwm配置#ESP32 PWM配置":"// 使用ESP32的LEDC（PWM）功能 #include \"driver/ledc.h\" #define SERVO_PIN 2 #define SERVO_CHANNEL LEDC_CHANNEL_0 #define SERVO_FREQ 50 // 50Hz #define SERVO_RESOLUTION LEDC_TIMER_13_BIT // 8192级 void setupServo() { ledcSetup(SERVO_CHANNEL, SERVO_FREQ, SERVO_RESOLUTION); ledcAttachPin(SERVO_PIN, SERVO_CHANNEL); } void setServoAngle(int angle) { // 角度转脉宽：0.5ms + angle * 2ms / 180 int pulseWidth = 500 + (angle * 2000 / 180); // 转换为PWM值 int pwmValue = (pulseWidth * 8192) / 20000; ledcWrite(SERVO_CHANNEL, pwmValue); }","mpu6050-imu#MPU6050 (IMU)":"#include \"Wire.h\" #include \"MPU6050.h\" MPU6050 mpu; void setupIMU() { Wire.begin(); mpu.initialize(); // 校准（可选） } void readIMU(float \u0026ax, float \u0026ay, float \u0026az, float \u0026gx, float \u0026gy, float \u0026gz) { int16_t acc[3], gyro[3]; mpu.getMotion6(\u0026acc[0], \u0026acc[1], \u0026acc[2], \u0026gyro[0], \u0026gyro[1], \u0026gyro[2]); // 转换为实际单位 ax = acc[0] / 16384.0; // ±2g范围 // ... 其他轴类似 }","pwm控制原理#PWM控制原理":"舵机通过PWM信号控制角度：\n周期：20ms（50Hz） 脉宽：0.5ms - 2.5ms 角度范围：0° - 180° 0.5ms → 0°\r1.5ms → 90°\r2.5ms → 180°","q-程序运行不稳定#Q: 程序运行不稳定？":"A:\n检查内存使用（避免动态分配） 检查看门狗设置 添加异常处理","q-舵机不响应#Q: 舵机不响应？":"A:\n检查电源是否足够 检查PWM信号是否正确 检查接线是否牢固","q-通信延迟大#Q: 通信延迟大？":"A:\n优化通信协议 减少数据传输频率 使用更高效的编码方式","串口调试#串口调试":"#define DEBUG 1 #if DEBUG #define DEBUG_PRINT(x) Serial.print(x) #define DEBUG_PRINTLN(x) Serial.println(x) #else #define DEBUG_PRINT(x) #define DEBUG_PRINTLN(x) #endif","主程序框架#主程序框架":"#include \"servo.h\" #include \"kinematics.h\" #include \"gait.h\" #include \"communication.h\" #include \"sensor.h\" void setup() { Serial.begin(115200); // 初始化各模块 setupServos(); setupIMU(); setupBluetooth(); // 初始化姿态 stand(); Serial.println(\"Robot Ready!\"); } void loop() { // 读取传感器 updateSensors(); // 处理通信 handleBluetooth(); // 更新步态（如果正在运动） if (isMoving()) { updateGait(); } // 更新舵机位置 updateServos(); delay(20); // 50Hz控制频率 }","传感器驱动#传感器驱动":"","单腿测试#单腿测试":"void testLeg(int legIndex) { // 测试单条腿的运动范围 // 验证逆运动学计算 }","命令协议设计#命令协议设计":"格式：\u003c动作类型\u003e:\u003c参数1\u003e,\u003c参数2\u003e,...\r示例：\r- \"walk:forward,10\" # 向前走10步\r- \"gait:trot\" # 切换到trot步态\r- \"pose:stand\" # 站立姿态\r- \"action:wave\" # 挥手动作","嵌入式软件开发#嵌入式软件开发":"嵌入式软件开发 软件是机器人的\"大脑\"，控制硬件实现各种功能。","常见问题#常见问题":"","开发环境搭建#开发环境搭建":"","性能优化#性能优化":"减少延迟：优化控制循环 PWM频率：根据舵机特性调整 传感器采样：合理设置采样率 通信优化：减少数据传输量","舵机测试#舵机测试":"void testServo(int servoIndex) { for (int angle = 0; angle \u003c= 180; angle += 10) { servos[servoIndex].write(angle); delay(500); } }","舵机类封装#舵机类封装":"class Servo { private: int pin; int channel; int currentAngle; public: Servo(int pin, int channel); void attach(); void write(int angle); void writeMicroseconds(int us); int read(); };","舵机驱动#舵机驱动":"","蓝牙通信#蓝牙通信":"#include \"BluetoothSerial.h\" BluetoothSerial SerialBT; void setupBluetooth() { SerialBT.begin(\"QuadrupedRobot\"); } void handleBluetooth() { if (SerialBT.available()) { String command = SerialBT.readString(); // 解析命令并执行 parseCommand(command); } }","调试技巧#调试技巧":"","选项1arduino-ide#选项1：Arduino IDE":"优点：\n简单易用 库丰富 社区支持好 缺点：\n代码管理不够方便 调试功能较弱 安装步骤：\n下载Arduino IDE 添加ESP32开发板支持 安装必要库","选项2platformio#选项2：PlatformIO":"优点：\n专业的嵌入式开发环境 代码管理方便 调试功能强 支持多平台 缺点：\n学习曲线稍陡 推荐使用PlatformIO（VS Code插件）","选项3esp-idf#选项3：ESP-IDF":"优点：\n官方框架 功能最全 性能最优 缺点：\n学习曲线陡 配置复杂","通信协议#通信协议":"","项目结构#项目结构":"quadruped-robot/\r├── src/\r│ ├── main.cpp # 主程序\r│ ├── servo.h/cpp # 舵机驱动\r│ ├── kinematics.h/cpp # 运动学\r│ ├── gait.h/cpp # 步态规划\r│ ├── communication.h/cpp # 通信协议\r│ └── sensor.h/cpp # 传感器驱动\r├── include/\r│ └── config.h # 配置文件\r├── platformio.ini # PlatformIO配置\r└── README.md"},"title":"嵌入式软件"},"/docs/self-study/embeded/05-control-algorithm/":{"data":{"1-站立stand#1. 站立（Stand）":"所有腿同时支撑，保持稳定。\nvoid stand() { // 设置所有腿到站立位置 Point3D standPos = {0, legOffsetY, -standHeight}; for (int i = 0; i \u003c 4; i++) { setLegPosition(i, standPos); } }","2-行走walk#2. 行走（Walk）":"四足交替移动，始终有3条腿支撑。\n步态序列：\n1. 左前腿抬起 → 前进 → 放下\r2. 右后腿抬起 → 前进 → 放下\r3. 右前腿抬起 → 前进 → 放下\r4. 左后腿抬起 → 前进 → 放下\r循环...","3-小跑trot#3. 小跑（Trot）":"对角腿同步，速度快。\n步态序列：\n支撑相：左前+右后 支撑，右前+左后 摆动\r摆动相：右前+左后 支撑，左前+右后 摆动","4-转向turn#4. 转向（Turn）":"通过调整各腿的步幅实现转向。","q-动作不流畅#Q: 动作不流畅？":"A:\n增加轨迹插值点 调整舵机速度 优化控制频率","q-机器人行走不稳定#Q: 机器人行走不稳定？":"A:\n检查重心位置 调整步态参数 增加平衡控制","q-逆运动学计算错误#Q: 逆运动学计算错误？":"A:\n检查坐标系定义 验证几何参数 添加边界检查","代码实现#代码实现":"struct Point3D { float x, y, z; }; struct LegAngles { float hip; // α float knee; // β float ankle; // γ }; LegAngles inverseKinematics(Point3D target, float L1, float L2) { LegAngles angles; // 髋关节角度 angles.hip = atan2(target.y, target.x); // 投影距离 float r = sqrt(target.x * target.x + target.y * target.y); float s = sqrt(r * r + target.z * target.z); // 检查是否可达 if (s \u003e (L1 + L2) || s \u003c abs(L1 - L2)) { // 不可达，返回错误或限制值 return angles; } // 膝关节角度 float cos_beta = (L1 * L1 + L2 * L2 - s * s) / (2 * L1 * L2); angles.knee = acos(cos_beta); // 踝关节角度 float theta1 = atan2(target.z, r); float theta2 = acos((L1 * L1 + s * s - L2 * L2) / (2 * L1 * s)); angles.ankle = theta1 + theta2; return angles; }","关键参数#关键参数":"步长（Step Length）：影响移动速度 步高（Step Height）：影响越障能力 周期（Cycle Time）：影响步频 支撑相比例：影响稳定性","几何参数#几何参数":"L1: 大腿长度\rL2: 小腿长度\rL3: 足部长度（可选）","动作序列#动作序列":"class ActionSequence { private: struct Action { String name; void (*func)(); int duration; }; Action actions[10]; int count; public: void add(String name, void (*func)(), int duration) { actions[count] = {name, func, duration}; count++; } void play() { for (int i = 0; i \u003c count; i++) { actions[i].func(); delay(actions[i].duration); } } };","动作库#动作库":"","单腿逆运动学#单腿逆运动学":"","参数调优#参数调优":"","坐标系定义#坐标系定义":"身体坐标系（Body Frame）：\rX: 前后方向（前为正）\rY: 左右方向（右为正）\rZ: 上下方向（上为正）\r腿坐标系（Leg Frame）：\r以髋关节为原点","基本原理#基本原理":"逆运动学（Inverse Kinematics, IK）是根据末端执行器的目标位置，计算各个关节的角度。\n对于四足机器人，每条腿有3个自由度：\n髋关节（α）：控制腿的前后摆动 膝关节（β）：控制腿的弯曲 踝关节（γ）：控制足部的角度","基础动作#基础动作":"","姿态控制#姿态控制":"","常见问题#常见问题":"","平衡控制#平衡控制":"使用IMU传感器进行平衡控制：\nvoid balanceControl() { float roll, pitch; readIMUAttitude(roll, pitch); // PID控制 float rollCorrection = pidRoll.compute(0 - roll); float pitchCorrection = pidPitch.compute(0 - pitch); setBodyPose(rollCorrection, pitchCorrection, 0); }","挥手#挥手":"void wave(int legIndex) { Point3D startPos = getLegPosition(legIndex); Point3D wavePos = startPos; for (int i = 0; i \u003c 3; i++) { // 抬起 wavePos.z += 30; setLegPosition(legIndex, wavePos); delay(200); // 左右摆动 wavePos.y += (i % 2 == 0 ? 20 : -20); setLegPosition(legIndex, wavePos); delay(200); } // 恢复 setLegPosition(legIndex, startPos); }","控制算法#控制算法":"控制算法 控制算法是机器人的\"智慧\"，决定了机器人的运动能力和稳定性。","摆动相轨迹#摆动相轨迹":"使用抛物线或正弦曲线生成平滑轨迹：\nPoint3D generateSwingTrajectory(float t, Point3D start, Point3D end, float liftHeight) { // t: 0.0 - 1.0 Point3D pos; // 水平方向线性插值 pos.x = start.x + (end.x - start.x) * t; pos.y = start.y + (end.y - start.y) * t; // 垂直方向抛物线 float z_base = start.z + (end.z - start.z) * t; float z_lift = 4 * liftHeight * t * (1 - t); // 抛物线 pos.z = z_base + z_lift; return pos; }","支撑相轨迹#支撑相轨迹":"身体相对于支撑腿移动：\nvoid updateSupportPhase(int legIndex, float t, float stepLength) { // t: 0.0 - 1.0 Point3D footPos = getFootPosition(legIndex); // 身体向前移动，足部相对向后 footPos.x -= stepLength * t; setLegPosition(legIndex, footPos); }","步态类型#步态类型":"","步态规划#步态规划":"","点头#点头":"void nod() { float originalHeight = standHeight; for (int i = 0; i \u003c 2; i++) { // 降低 setStandHeight(originalHeight - 20); delay(300); // 恢复 setStandHeight(originalHeight); delay(300); } }","计算步骤#计算步骤":"给定目标位置 (x, y, z)：\n计算髋关节角度 α\nα = atan2(y, x) 计算投影距离\nr = sqrt(x² + y²) // 在X-Y平面的投影 s = sqrt(r² + z²) // 到髋关节的距离 计算膝关节角度 β\n// 使用余弦定理 cos_β = (L1² + L2² - s²) / (2 * L1 * L2) β = acos(cos_β) 计算踝关节角度 γ\n// 先计算中间角度 θ1 = atan2(z, r) θ2 = acos((L1² + s² - L2²) / (2 * L1 * s)) γ = θ1 + θ2","调优方法#调优方法":"单参数扫描：固定其他参数，调整一个参数 正交实验：系统性地测试参数组合 实际测试：在真实环境中测试和调整","身体姿态调整#身体姿态调整":"通过调整各腿的长度，实现身体姿态控制：\nvoid setBodyPose(float roll, float pitch, float yaw) { // roll: 左右倾斜 // pitch: 前后倾斜 // yaw: 左右转向 for (int i = 0; i \u003c 4; i++) { Point3D offset = calculatePoseOffset(i, roll, pitch, yaw); Point3D currentPos = getLegPosition(i); currentPos.z += offset.z; setLegPosition(i, currentPos); } }","轨迹生成#轨迹生成":"","逆运动学#逆运动学":""},"title":"控制算法"},"/docs/self-study/embeded/06-communication/":{"data":{"esp32蓝牙配置#ESP32蓝牙配置":"ESP32支持经典蓝牙（BT）和低功耗蓝牙（BLE），推荐使用BLE（功耗更低）。\n#include \"BLEDevice.h\" #include \"BLEServer.h\" #include \"BLEUtils.h\" BLEServer* pServer = NULL; BLECharacteristic* pCharacteristic = NULL; #define SERVICE_UUID \"4fafc201-1fb5-459e-8fcc-c5c9c331914b\" #define CHARACTERISTIC_UUID \"beb5483e-36e1-4688-b7f5-ea07361b26a8\" class MyServerCallbacks: public BLEServerCallbacks { void onConnect(BLEServer* pServer) { Serial.println(\"Client connected\"); } void onDisconnect(BLEServer* pServer) { Serial.println(\"Client disconnected\"); BLEDevice::startAdvertising(); } }; class MyCallbacks: public BLECharacteristicCallbacks { void onWrite(BLECharacteristic *pCharacteristic) { std::string value = pCharacteristic-\u003egetValue(); if (value.length() \u003e 0) { Serial.println(\"Received: \" + String(value.c_str())); handleCommand(String(value.c_str())); } } }; void setupBluetooth() { BLEDevice::init(\"QuadrupedRobot\"); pServer = BLEDevice::createServer(); pServer-\u003esetCallbacks(new MyServerCallbacks()); BLEService *pService = pServer-\u003ecreateService(SERVICE_UUID); pCharacteristic = pService-\u003ecreateCharacteristic( CHARACTERISTIC_UUID, BLECharacteristic::PROPERTY_READ | BLECharacteristic::PROPERTY_WRITE ); pCharacteristic-\u003esetCallbacks(new MyCallbacks()); pService-\u003estart(); BLEAdvertising *pAdvertising = BLEDevice::getAdvertising(); pAdvertising-\u003eaddServiceUUID(SERVICE_UUID); pAdvertising-\u003esetScanResponse(true); pAdvertising-\u003esetMinPreferred(0x06); BLEDevice::startAdvertising(); Serial.println(\"BLE Server started\"); }","led状态指示#LED状态指示":"#define LED_PIN 2 void setupLED() { pinMode(LED_PIN, OUTPUT); } void updateLED() { if (isMoving()) { // 运动时闪烁 digitalWrite(LED_PIN, !digitalRead(LED_PIN)); } else { // 静止时常亮 digitalWrite(LED_PIN, HIGH); } }","python控制脚本#Python控制脚本":"import bluetooth import time def connect_robot(): # 搜索设备 devices = bluetooth.discover_devices() robot_addr = None for addr in devices: if \"QuadrupedRobot\" in bluetooth.lookup_name(addr): robot_addr = addr break if robot_addr: sock = bluetooth.BluetoothSocket(bluetooth.RFCOMM) sock.connect((robot_addr, 1)) return sock return None def send_command(sock, command): sock.send(command + \"\\n\") time.sleep(0.1) # 使用示例 sock = connect_robot() if sock: send_command(sock, \"walk:forward,5\") send_command(sock, \"action:wave\") sock.close()","q-命令延迟大#Q: 命令延迟大？":"A:\n减少数据传输量 优化命令解析 使用更高效的协议","q-多设备连接#Q: 多设备连接？":"A:\nBLE支持多连接（需要配置） 或使用WiFi（ESP32支持）","q-蓝牙连接不稳定#Q: 蓝牙连接不稳定？":"A:\n检查距离和障碍物 检查电源是否稳定 优化天线设计","上位机开发可选#上位机开发（可选）":"","交互功能#交互功能":"","命令格式#命令格式":"采用简单的文本协议，易于解析和调试：\n格式：\u003c命令\u003e:\u003c参数1\u003e,\u003c参数2\u003e,...\r示例：\r- \"walk:forward,10\" # 向前走10步\r- \"walk:backward,5\" # 向后走5步\r- \"walk:left,3\" # 向左转3步\r- \"walk:right,3\" # 向右转3步\r- \"gait:trot\" # 切换到trot步态\r- \"gait:walk\" # 切换到walk步态\r- \"pose:stand\" # 站立\r- \"pose:sit\" # 坐下\r- \"pose:lie\" # 趴下\r- \"action:wave\" # 挥手\r- \"action:nod\" # 点头\r- \"action:dance\" # 跳舞\r- \"stop\" # 停止\r- \"status\" # 查询状态","命令解析#命令解析":"void handleCommand(String command) { int colonIndex = command.indexOf(':'); String cmd = command.substring(0, colonIndex); String params = command.substring(colonIndex + 1); if (cmd == \"walk\") { handleWalk(params); } else if (cmd == \"gait\") { handleGait(params); } else if (cmd == \"pose\") { handlePose(params); } else if (cmd == \"action\") { handleAction(params); } else if (cmd == \"stop\") { stop(); } else if (cmd == \"status\") { sendStatus(); } } void handleWalk(String params) { int commaIndex = params.indexOf(','); String direction = params.substring(0, commaIndex); int steps = params.substring(commaIndex + 1).toInt(); if (direction == \"forward\") { walkForward(steps); } else if (direction == \"backward\") { walkBackward(steps); } else if (direction == \"left\") { turnLeft(steps); } else if (direction == \"right\") { turnRight(steps); } }","安全机制#安全机制":"","常见问题#常见问题":"","性能优化#性能优化":"减少数据传输：只传输必要信息 批量命令：支持命令队列 压缩数据：使用二进制协议（可选） 异步处理：命令处理不阻塞主循环","状态反馈#状态反馈":"","状态数据结构#状态数据结构":"struct RobotStatus { float batteryVoltage; float roll, pitch, yaw; // 姿态 bool isMoving; String currentGait; int stepCount; }; RobotStatus status; void updateStatus() { status.batteryVoltage = readBatteryVoltage(); readIMUAttitude(status.roll, status.pitch, status.yaw); status.isMoving = isMoving(); status.currentGait = getCurrentGait(); status.stepCount = getStepCount(); } void sendStatus() { String statusStr = \"status:\"; statusStr += \"battery=\" + String(status.batteryVoltage) + \",\"; statusStr += \"roll=\" + String(status.roll) + \",\"; statusStr += \"pitch=\" + String(status.pitch) + \",\"; statusStr += \"moving=\" + String(status.isMoving ? \"true\" : \"false\"); pCharacteristic-\u003esetValue(statusStr.c_str()); pCharacteristic-\u003enotify(); }","简单的手机app#简单的手机APP":"可以使用MIT App Inventor或React Native开发简单的控制APP。\n功能：\n连接蓝牙 发送控制命令 显示机器人状态 实时控制（摇杆）","蓝牙通信#蓝牙通信":"","触摸响应#触摸响应":"#define TOUCH_PIN 4 void setupTouch() { pinMode(TOUCH_PIN, INPUT); } void checkTouch() { if (digitalRead(TOUCH_PIN) == HIGH) { // 触摸响应 performRandomAction(); } } void performRandomAction() { int action = random(0, 3); switch(action) { case 0: wave(0); break; case 1: nod(); break; case 2: dance(); break; } }","超时保护#超时保护":"unsigned long lastCommandTime = 0; #define COMMAND_TIMEOUT 5000 // 5秒 void checkTimeout() { if (millis() - lastCommandTime \u003e COMMAND_TIMEOUT) { if (isMoving()) { stop(); // 超时自动停止 } } } void handleCommand(String command) { lastCommandTime = millis(); // ... 处理命令 }","边界检查#边界检查":"bool isValidCommand(String command) { // 检查命令格式 if (command.length() \u003e 100) return false; // 检查命令类型 String validCommands[] = {\"walk\", \"gait\", \"pose\", \"action\", \"stop\", \"status\"}; bool valid = false; for (String cmd : validCommands) { if (command.startsWith(cmd)) { valid = true; break; } } return valid; }","通信与交互#通信与交互":"通信与交互 通信是机器人与外界交互的桥梁，实现远程控制和状态反馈。","通信协议设计#通信协议设计":""},"title":"通信与交互"},"/docs/self-study/embeded/07-debug-optimize/":{"data":{"串口调试#串口调试":"最基础的调试方法：\n#define DEBUG_LEVEL 1 // 0=关闭, 1=基础, 2=详细 #if DEBUG_LEVEL \u003e= 1 #define DEBUG_PRINT(x) Serial.print(x) #define DEBUG_PRINTLN(x) Serial.println(x) #else #define DEBUG_PRINT(x) #define DEBUG_PRINTLN(x) #endif #if DEBUG_LEVEL \u003e= 2 #define DEBUG_VERBOSE(x) Serial.print(x) #define DEBUG_VERBOSE_LN(x) Serial.println(x) #else #define DEBUG_VERBOSE(x) #define DEBUG_VERBOSE_LN(x) #endif // 使用示例 DEBUG_PRINT(\"Servo angle: \"); DEBUG_PRINTLN(angle);","传感器校准#传感器校准":"void calibrateIMU() { Serial.println(\"Calibrating IMU...\"); Serial.println(\"Keep robot still for 5 seconds\"); float sum_ax = 0, sum_ay = 0, sum_az = 0; int samples = 100; for (int i = 0; i \u003c samples; i++) { float ax, ay, az; readIMU(ax, ay, az, 0, 0, 0); sum_ax += ax; sum_ay += ay; sum_az += az; delay(50); } // 计算偏移量 offset_ax = sum_ax / samples; offset_ay = sum_ay / samples; offset_az = (sum_az / samples) - 1.0; // 重力加速度 Serial.print(\"Offsets: \"); Serial.print(offset_ax); Serial.print(\", \"); Serial.print(offset_ay); Serial.print(\", \"); Serial.println(offset_az); }","关键指标#关键指标":"控制频率：目标50Hz（20ms周期） 响应时间：命令响应 \u003c 100ms 稳定性：能稳定行走 \u003e 1分钟 精度：位置误差 \u003c 5mm","内存优化#内存优化":"// 避免动态内存分配 // 不好： String message = \"Hello \" + name; // 好： char message[50]; sprintf(message, \"Hello %s\", name); // 使用固定大小数组 #define MAX_COMMANDS 10 String commandQueue[MAX_COMMANDS]; int queueHead = 0, queueTail = 0;","单元测试#单元测试":"void testInverseKinematics() { // 测试逆运动学计算 Point3D testPoints[] = { {50, 0, -80}, {0, 50, -80}, {-50, 0, -80} }; for (int i = 0; i \u003c 3; i++) { LegAngles angles = inverseKinematics(testPoints[i], 60, 80); // 验证角度范围 assert(angles.hip \u003e= -90 \u0026\u0026 angles.hip \u003c= 90); assert(angles.knee \u003e= 0 \u0026\u0026 angles.knee \u003c= 180); assert(angles.ankle \u003e= -90 \u0026\u0026 angles.ankle \u003c= 90); // 验证正运动学（可选） Point3D result = forwardKinematics(angles, 60, 80); float error = distance(testPoints[i], result); assert(error \u003c 1.0); // 误差小于1mm } }","单腿测试#单腿测试":"void testLeg(int legIndex) { Serial.print(\"Testing leg \"); Serial.println(legIndex); // 测试运动范围 Point3D testPoints[] = { {50, 0, -80}, // 前 {0, 50, -80}, // 右 {-50, 0, -80}, // 后 {0, -50, -80}, // 左 {0, 0, -60}, // 上 {0, 0, -100} // 下 }; for (int i = 0; i \u003c 6; i++) { Serial.print(\"Moving to point \"); Serial.println(i); setLegPosition(legIndex, testPoints[i]); delay(2000); } // 恢复初始位置 setLegPosition(legIndex, {0, 0, -80}); }","常见问题排查#常见问题排查":"","平滑控制#平滑控制":"// 使用低通滤波器平滑控制信号 class LowPassFilter { private: float alpha; float lastValue; public: LowPassFilter(float alpha) : alpha(alpha), lastValue(0) {} float filter(float input) { lastValue = alpha * input + (1 - alpha) * lastValue; return lastValue; } }; LowPassFilter angleFilter(0.1); // 平滑系数 void smoothServoControl(int servoIndex, float targetAngle) { float currentAngle = servos[servoIndex].read(); float filteredAngle = angleFilter.filter(targetAngle); servos[servoIndex].write(filteredAngle); }","性能分析#性能分析":"测量代码执行时间：\nunsigned long startTime, endTime; void measureTime(String label) { startTime = micros(); } void endMeasure(String label) { endTime = micros(); DEBUG_PRINT(label + \": \"); DEBUG_PRINTLN(endTime - startTime); } // 使用示例 measureTime(\"Gait update\"); updateGait(); endMeasure(\"Gait update\");","性能指标#性能指标":"","控制频率优化#控制频率优化":"// 使用定时器中断，保证控制频率 hw_timer_t * timer = NULL; void IRAM_ATTR onTimer() { // 在中断中更新关键控制 updateGait(); updateServos(); } void setupTimer() { timer = timerBegin(0, 80, true); // 1MHz timerAttachInterrupt(timer, \u0026onTimer, true); timerAlarmWrite(timer, 20000, true); // 20ms = 50Hz timerAlarmEnable(timer); }","日志系统#日志系统":"更完善的日志系统：\nenum LogLevel { LOG_ERROR, LOG_WARNING, LOG_INFO, LOG_DEBUG }; void log(LogLevel level, String message) { String prefix; switch(level) { case LOG_ERROR: prefix = \"[ERROR] \"; break; case LOG_WARNING: prefix = \"[WARN] \"; break; case LOG_INFO: prefix = \"[INFO] \"; break; case LOG_DEBUG: prefix = \"[DEBUG] \"; break; } Serial.print(prefix); Serial.println(message); } // 使用示例 log(LOG_INFO, \"Robot initialized\"); log(LOG_ERROR, \"Servo timeout\");","测试流程#测试流程":"","监控方法#监控方法":"struct PerformanceMetrics { unsigned long loopTime; unsigned long maxLoopTime; int servoUpdateCount; int errorCount; }; PerformanceMetrics metrics; void updateMetrics() { static unsigned long lastTime = 0; unsigned long currentTime = micros(); metrics.loopTime = currentTime - lastTime; if (metrics.loopTime \u003e metrics.maxLoopTime) { metrics.maxLoopTime = metrics.loopTime; } lastTime = currentTime; } void printMetrics() { Serial.print(\"Loop time: \"); Serial.print(metrics.loopTime); Serial.print(\"us, Max: \"); Serial.print(metrics.maxLoopTime); Serial.print(\"us, Errors: \"); Serial.println(metrics.errorCount); }","硬件调试#硬件调试":"","稳定性优化#稳定性优化":"","舵机测试#舵机测试":"void testAllServos() { Serial.println(\"Testing all servos...\"); for (int i = 0; i \u003c 12; i++) { Serial.print(\"Servo \"); Serial.print(i); Serial.println(\": 0 -\u003e 90 -\u003e 180\"); servos[i].write(0); delay(1000); servos[i].write(90); delay(1000); servos[i].write(180); delay(1000); servos[i].write(90); delay(500); } Serial.println(\"Test complete\"); }","计算优化#计算优化":"// 预计算常量 const float PI_OVER_180 = PI / 180.0; // 使用查找表（LUT） float sinLUT[360]; void initSinLUT() { for (int i = 0; i \u003c 360; i++) { sinLUT[i] = sin(i * PI_OVER_180); } } float fastSin(int angle) { angle = angle % 360; if (angle \u003c 0) angle += 360; return sinLUT[angle]; }","调试与优化#调试与优化":"调试与优化 调试是开发过程中必不可少的环节，优化让机器人运行得更好。","调试方法#调试方法":"","软件优化#软件优化":"","错误恢复#错误恢复":"void checkServoError() { for (int i = 0; i \u003c 12; i++) { // 检查舵机是否响应 int currentAngle = servos[i].read(); int targetAngle = targetAngles[i]; if (abs(currentAngle - targetAngle) \u003e 10) { // 误差过大，尝试重新设置 log(LOG_WARNING, \"Servo \" + String(i) + \" error detected\"); servos[i].write(targetAngle); } } }","问题1机器人抖动#问题1：机器人抖动":"可能原因：\n电源不稳定 控制频率过高 机械结构松动 解决方法：\n增加滤波电容 降低控制频率 检查机械连接","问题2行走不稳定#问题2：行走不稳定":"可能原因：\n重心偏移 步态参数不当 地面不平 解决方法：\n调整重心位置 优化步态参数 增加平衡控制","问题3通信延迟#问题3：通信延迟":"可能原因：\n数据传输量大 处理速度慢 信号干扰 解决方法：\n减少数据传输 优化处理算法 改善天线设计","集成测试#集成测试":"void testWalkCycle() { // 测试完整行走周期 startWalk(10); // 走10步 int stepCount = 0; while (isMoving()) { update(); if (stepCompleted()) { stepCount++; log(LOG_INFO, \"Step \" + String(stepCount) + \" completed\"); } delay(20); } assert(stepCount == 10); log(LOG_INFO, \"Walk test passed\"); }"},"title":"调试与优化"},"/docs/self-study/embodied/":{"data":{"":"倒是有很多的入门路线指导，包括https://github.com/jiangranlv/embodied-ai-start https://github.com/TianxingChen/Embodied-AI-Guide"},"title":"具身智能"},"/docs/self-study/embodied/vla/":{"data":{"":"视觉语言动作模型相关笔记。"},"title":"VLA"},"/docs/self-study/embodied/vla/act/":{"data":{"":"import CollapsibleBlock from ‘@site/src/components/CollapsibleBlock’; import { NoteBlock } from ‘@site/src/components/HighlightBlock’;\n来入门一下 VLA 这篇吹的神乎其神的《Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware》，是在 2023年 提出的 Action Chunking with Transformers（基于 Transformer 的动作分块算法），所谓 VLA 领域的鼻祖级动作生成算法（但是他们自吹自擂为元宇宙）\n该 ACT 算法是 VLA 领域首个以 “动作分块（Action Chunking）” 为核心的 Transformer 模型，专门解决机器人从 “V-L指令” 到 “精准动作执行” 的端到端映射问题 —— 这正是 VLA 的核心目标\n找这玩意不太容易，Arxiv结果里面还有 cogACT、MM-ACT 这些，其中 CogACT 是将 OpenVLA 的离散动作预测替换为 DiT（让我想起cognav了）MM-ACT 是通过多模态协同优化 VLA 的感知 - 执行效率\n而所谓的 ALOHA 就是 A LOW-COST OPEN-SOURCE HARDWARE SYSTEM FOR BIMANUAL TELEOPERATION，项目地址在 https://tonyzhaozh.github.io/aloha/\n其核心贡献是一个遥操作（teleoperation）系统和一个IL算法，这样就能不用昂贵的硬件去做到下面的一些事情，比如开瓶盖啥的：\n通过遥操作演示收集数据（视觉+动作）之后，就开始用收集的数据训练 ACT 算法，从而训练好的模型可以自主完成任务。然而现有模仿学习算法在需要高频控制和闭环反馈的细粒度任务中表现不佳\nPID控制器（Proportional-Integral-Derivative Controller）是一种经典的闭环反馈控制算法，用于让系统输出精确跟踪目标值。","1-底层关节控制dynamixel电机内置pid#1. \u003cstrong\u003e底层关节控制（Dynamixel电机内置PID）\u003c/strong\u003e":"作用：让机械臂关节精确到达目标位置\n输入：ACT模型预测的目标关节位置（如\"关节1应该转到45度\"） 过程：PID控制器实时计算： 读取当前关节实际位置（如\"现在在40度\"） 计算误差 = 45° - 40° = 5° 根据PID算法计算电机应该施加的力矩 电机转动，关节向目标位置移动 输出：关节实际位置逐渐接近目标位置 为什么需要PID？\nACT模型只给出\"目标位置\"，但实际执行会有误差（摩擦力、负载变化等） PID控制器通过高频反馈（通常几百到几千Hz）实时调整，确保关节精确到达目标","2-主从遥操作中的力控制#2. \u003cstrong\u003e主从遥操作中的力控制\u003c/strong\u003e":"在数据收集阶段：\n主臂：人类操作员手动控制 从臂：跟随主臂动作 PID作用：通过主从臂关节位置的差异来间接控制施加的力 如果从臂位置落后主臂，PID会增加力矩让从臂跟上 这样就能\"感受\"到操作时的力反馈","a-action-chunking-and-temporal-ensemble#A. Action Chunking and Temporal Ensemble":"通过 ALOHA 采集人类演示数据过程会导致误差积累：前序动作的误差会不断叠加，导致机器人状态偏离训练分布。\n为在像素到动作的策略框架下解决模仿学习的误差累积问题，本文选择将一系列动作组合成一个单元进行存储和执行，以降低高频采集的长轨迹任务的有效时间跨度。\n在实现中，文章将分块大小固定为 k：每 k 步，智能体接收一次观测，生成后续 k 步动作并依次执行如下：\n任务的有效时间跨度被缩短为原来的 1/k。具体来说，策略建模的是 πₜ(at:t+k|st)，而非传统的 πₜ(at|st)。动作分块还能处理人类演示中的非马尔可夫行为：单步策略难以应对时间相关的混杂因素（如演示过程中的停顿）因为此类行为不仅依赖当前状态，还与时间步相关；而当混杂因素处于一个分块内时，动作分块可在不引入历史条件策略因果混淆问题的前提下缓解该问题。\n动作分块的朴素实现存在局限性：每 k 步才引入新的环境观测，会导致机器人运动卡顿。为提升运动平滑度、避免执行与观测之间的离散切换，我们在每个时间步都查询策略，使得不同动作块相互重叠，单个时间步会存在多个预测动作。所以上面引入了Temporal Ensemble，对这些预测进行融合：采用指数加权方案 wi = exp(−m×i)（其中 w₀ 为最早动作的权重）对预测动作进行加权平均。参数 m 控制新观测的融合速度，m 越小，融合速度越快。与传统平滑方法（将当前动作与相邻时间步动作聚合，易引入偏差）不同，我们聚合的是针对同一时间步的预测动作，且无需额外训练成本，仅增加推理阶段的计算量。实践证明，动作分块与时间集成是 ACT 实现精准平滑运动的关键。","act-架构详解#ACT 架构详解":"核心目的：ACT 要做的是\"看当前画面 + 机器人关节状态，预测未来 k 步动作\"。上图展示了完整的\"输入→处理→输出\"流程。CVAE 的作用是处理人类演示的噪声（比如不同人做同一动作的差异），Transformer 的作用是处理\"多摄像头图像 + 序列动作\"的依赖关系（比如先看顶部摄像头再看腕部摄像头，动作要连贯）。","b-modeling-human-data#B. Modeling Human Data":"另一项挑战是从含噪声的人类演示中学习：面对相同观测，人类可能采用不同轨迹完成任务，且在精度要求较低的区域行为随机性更强。因此，策略需重点关注高精度需求区域。\n我们通过将动作分块策略训练为生成模型来解决该问题，具体采用**条件变分自编码器（CVAE）**以当前观测为条件生成动作序列。CVAE 包含编码器和解码器两部分：\n编码器：仅用于训练解码器（即策略），测试时被舍弃。具体而言，编码器以当前观测和动作序列为输入，预测风格变量 z（服从对角高斯分布）的均值和方差；为加快训练速度，实际训练中仅使用本体感受观测和动作序列，不含图像观测。 解码器（策略）：以 z 和当前完整观测（图像 + 关节位置）为输入预测动作序列；测试时，将 z 设为先验分布的均值（即 0），实现确定性解码。 模型训练目标是最大化演示动作块的对数似然（即 minθ −∑st,at:t+k∈D log πₜ(at:t+k|st)），采用标准 VAE 损失（含重建损失和编码器高斯先验正则化损失），并通过超参数 β 对正则化损失加权。直观来看，β 越大，z 传递的信息越少。","pid的三个组成部分#PID的三个组成部分":"P（比例项，Proportional）\n根据当前误差（目标值 - 实际值）产生控制信号 误差越大，控制力度越大 问题：可能产生稳态误差（永远达不到目标） I（积分项，Integral）\n累积历史误差，消除稳态误差 如果长时间有误差，积分项会逐渐增大控制力度 问题：可能导致系统响应过慢或振荡 D（微分项，Derivative）\n根据误差变化率预测未来趋势 提前\"刹车\"，防止超调 问题：对噪声敏感 PID输出公式：\nu(t) = Kp × e(t) + Ki × ∫e(t)dt + Kd × de(t)/dt 其中：\ne(t) = 误差（目标值 - 实际值） Kp, Ki, Kd = 三个可调参数","右图cvae-解码器核心测试时实际控制机器人的策略#右图：CVAE 解码器（核心！测试时实际控制机器人的\u0026quot;策略\u0026quot;）":"核心功能：把\"当前多摄像头图像 + 机器人关节状态 + 风格变量 z\"，转换成\"未来 k 步的机器人目标关节动作\"——这就是机器人实际要执行的指令。\n具体流程（分 3 步）：\n第一步：处理图像（CNN + 位置编码）\n每路图像先过 ResNet18（一个图像特征提取网络），把 480×640 的图变成 15×20×512 的特征图（缩小尺寸、提取关键信息，比如夹爪位置、电池轮廓） 然后把特征图展平成 300×512 的序列，再加上2D 正弦位置编码 这一步是为了保留图像的空间信息（比如\"电池在画面左上角\"这个位置关系），不然 Transformer 不知道像素的位置意义 4 路图像都这么处理后，拼接成 1200×512 的总图像特征序列 第二步：融合所有信息（Transformer 编码器）\n把\"总图像特征序列\"和\"经线性层投影到 512 维的关节位置 + z\"拼接起来，形成 1202×512 的\"混合特征序列\"（1200 来自 4 路图像，2 来自关节 + z） 这个混合序列输入到 Transformer 编码器后，会被充分融合 比如\"腕部摄像头看到夹爪快碰到电池（图像特征）+ 关节状态显示夹爪张开（关节特征）+ z 是’轻柔风格’（z 特征）\"，这些信息会被整合在一起，形成\"当前状态的全局特征\" 第三步：生成动作序列（Transformer 解码器）\n解码器的输入是\"固定的位置编码\"（长度 k，512 维）——相当于告诉模型\"要生成 k 步动作，每一步的位置是固定的\" 解码器通过交叉注意力（把编码器的输出当\"键和值\"，自己的位置编码当\"查询\"），一步步生成连贯的动作序列 比如第一步\"夹爪对准电池\"，第二步\"轻微闭合\"，…，第 k 步\"插入电池\" 最后用一个 MLP（简单的全连接网络）把生成的 512 维特征，映射成 14 维的关节位置（双臂各 7 个自由度） 最终输出 k×14 的张量（未来 k 步的目标关节动作） 输出：未来 k 步的机器人目标关节位置——比如\"左臂关节 1 角度 30°、关节 2 角度 15°…，右臂关节 1 角度 20°…\"，底层 PID 控制器会跟踪这些目标位置，让机器人精准执行。","在alohaact中的作用#在ALOHA/ACT中的作用":"在 ALOHA 系统中，PID控制器有两个层面的应用：","左图cvae-编码器训练时用测试时丢弃#左图：CVAE 编码器（训练时用，测试时丢弃）":"核心功能：把\"人类演示的动作序列 + 机器人关节状态\"压缩成一个风格变量 z。z 就像一个\"动作风格密码\"，能捕捉人类操作的随机性（比如有人插电池快、有人慢，但都能成功）。\n具体流程：\n输入：\n人类演示的\"动作块\"（比如插电池的 k 步连续动作：at~at+k） 机器人的关节观测（不含图像，因为图像信息多、训练慢，且关节状态已能反映核心姿态：ōt） 为了适配 Transformer，会在最前面加一个可学习的 \"[CLS]\" 标记（和 BERT 一样，用来汇总全局信息） 最终形成 \"[CLS] + 关节状态 + k 步动作\" 的序列（长度是 k+2） 处理：\n这个序列输入到\"类 BERT 的 Transformer 编码器\"后，会被逐层提取特征 重点是把\"动作块的规律 + 关节状态的关联\"浓缩到 \"[CLS]\" 标记对应的特征向量里 然后用这个特征向量预测一个\"对角高斯分布\"（就是统计里的\"均值 + 方差\"） 从这个分布里采样，就得到了风格变量 z 为什么需要编码器？\n训练时用来\"约束 z 的分布\"（让 z 接近标准正态分布），这样 z 能高效编码人类动作的\"风格差异\"，又不会包含无关噪声 测试时不用它，直接把 z 设为 0（标准正态分布的均值），避免额外计算","左图和右图的关联训练时的闭环#左图和右图的关联：训练时的\u0026quot;闭环\u0026quot;":"训练时，左图的编码器和右图的解码器是一起优化的：\n编码器生成 z，解码器用 z + 实时观测生成\"预测动作块\"（ât~at+k） 用\"预测动作块\"和\"人类演示的真实动作块\"算重建损失（确保预测动作像人类） 同时用编码器预测的\"高斯分布\"和\"标准正态分布\"算 KL 散度（正则化损失，确保 z 的分布合理） 两个损失加起来（L = Lreconst + βLreg），用 Adam 优化器更新所有参数 这样解码器（策略）就能学会\"看观测、按风格，生成精准动作块\"","技术细节补充#技术细节补充":"观测数据包含：\n4 路 480×640 分辨率的 RGB 图像 双机械臂关节位置（共 7+7=14 个自由度） 动作空间为双机械臂的绝对关节位置（14 维向量），因此动作分块策略的输出为 k×14 维张量。\n损失函数：我们采用 L1 损失而非常用的 L2 损失（MSE），因其能更精准地建模动作序列。实验发现，使用关节位置差值作为动作标签会导致性能下降。训练时使用标准 VAE 损失（重建损失 + KL 散度正则化损失）。","推理算法#推理算法":"Input: trained πθ, episode length T, weight m 1: Initialize FIFO buffers B[0 : T], where B[t] stores actions predicted for timestep t 2: for timestep t = 1, 2, ... T do 3: Predict ˆat:t+k with πθ(ˆat:t+k|ot, z) where z = 0 4: Add ˆat:t+k to buffers B[t : t + k] respectively 5: Obtain current step actions At = B[t] 6: Apply at = Σi wiAt[i] / Σi wi, with wi = exp(−m × i) 7: end for","训练算法#训练算法":"Input: Demo dataset D, chunk size k, weight β 1: Let at, ot represent action and observation at timestep t, ¯ot represent ot without image observations 2: Initialize encoder qφ(z|at:t+k, ¯ot) 3: Initialize decoder πθ(ˆat:t+k|ot, z) 4: for iteration n = 1, 2, ... do 5: Sample ot, at:t+k from D 6: Sample z from qφ(z|at:t+k, ¯ot) 7: Predict ˆat:t+k from πθ(ˆat:t+k|ot, z) 8: Lreconst = MSE(ˆat:t+k, at:t+k) 9: Lreg = DKL(qφ(z|at:t+k, ¯ot) ∥ N(0, I)) 10: Update θ, φ with ADAM and L = Lreconst + βLreg 11: end for"},"title":"act"},"/docs/self-study/embodied/vln/":{"data":{"":"UniGoal: Towards Universal Zero-shot Goal-oriented Navigation£¨CVPR2025£© In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.\nRoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics£¨CVPR2025 oral£© Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face significant challenges in spatial reasoning tasks, as their training data are based on general-purpose image datasets that often lack sophisticated spatial understanding. For example, datasets frequently do not capture reference frame comprehension, yet effective spatial reasoning requires understanding whether to reason from ego-, world, or object-centric perspectives. To address this issue, we introduce ROBOSPATIAL, a large-scale dataset for spatial understanding in robotics. It consists of real indoor and tabletop scenes, captured as 3D scans and egocentric images, and annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial relationships, and the pairing of 2D egocentric images with 3D scans makes it both 2D- and 3D- ready. Our experiments show that models trained with ROBOSPATIAL outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robot manipulation.\nVision-and-Language Navigation via Causal Learning£¨CVPR2024£© In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-andlanguage navigation (VLN) agents, hindering their performance in unseen environments. This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference. By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations. Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training. Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches. Code is available at https: //github.com/CrystalSixone/VLN-GOAT.\nNaVILA: Legged Robot Vision-Language-Action Model for Navigation£¨ICLR2025£© This paper proposes to solve the problem of Visionand-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language ? Equal contribution, ordered alphabetically. ? Equal advising. instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-LanguageAction model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., ¡°moving forward 75cm¡±), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially arXiv:2412.04453v2 [cs.RO] 17 Feb 2025 improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, lowlevel controls, and real-world robot experiments.\nNaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation\nSG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation In this paper, we propose a new framework for zero-shot object navigation. Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning. To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLMfriendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges. Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error. We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than 10% SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark. Project page."},"title":"vln"},"/docs/undergraduate/":{"data":{"大三春夏#大三春夏":"大三春夏","大三秋冬#大三秋冬":"生物医学图像处理、硬件描述语言、仪器系统设计待补充。\n大三秋冬","大二春夏#大二春夏":"大二春夏","大二秋冬#大二秋冬":"大二秋冬","学习心得#学习心得":"这些课程涵盖了生物医学工程专业的核心知识体系，从基础理论到实践应用，为我的专业发展奠定了坚实的基础。 每门课程都包含了详细的学习笔记和实践经验，希望能对同样学习这些课程的同学有所帮助。","数理基础#数理基础":"数理基础","本科笔记#本科笔记":"本科笔记","编程基础#编程基础":"编程基础","课程概览#课程概览":"隐去个人信息与全部代码实现，仅提供笔记与实验报告用于参考。 如果今年的选题与明年一致的话，是学院僵化教学的悲哀。 也希望大家用Typst或Latex来进行笔记与报告的撰写。","通识杂项#通识杂项":"通识杂项"},"title":"本科笔记"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/":{"data":{"":"大三春夏学期课程笔记。"},"title":"大三春夏"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/%E4%BB%AA%E5%99%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/":{"data":{"1-判断辨析题20分#1. 判断辨析题（20分）":"共10道题，每题2分 作答时需首先判断对错 如果判断为\"对\"，则直接得分 如果判断为\"错\"，则除了写\"错\"之外，必须写出正确的表述（无需长篇大论，抓住关键词即可） 注意：未判断对错的将至少扣除1分","2-简答题50分#2. 简答题（50分）":"共10道题，每题5分 作答时无需长篇大论，注重理解而非死记硬背 题目可能结合平时作业内容进行改编，例如更改测温仪的接口类型，考查对系统原理的理解及变通能力。即使是小组作业，也请确保对整体设计（如测温仪的蓝牙接口、通讯协议、命令协议等）有清晰的理解","3-设计题30分#3. 设计题（30分）":"共3道设计题，总分30分（每题分值可能为8分、10分、12分不等，但总和为30分） 设计题注重考察学生的整体设计思路，可能允许学生自定义仪器类型、接口类型以及硬件软件设计 作答时需要按照设计步骤进行，并且该画图的地方要画图 考试时无需携带计算器（无复杂计算），但建议携带尺子以方便画图","hw1#hw1":"","hw10#hw10":"","hw2#hw2":"","hw3#hw3":"","hw4#hw4":"","hw5#hw5":"","hw6#hw6":"","hw7#hw7":"","hw8#hw8":"","hw9#hw9":"","pj1#pj1":"","pj2#pj2":"","pj3#pj3":"","project#project":"仪器系统设计大作业报告","仪器系统设计#仪器系统设计":"仪器系统设计","复习笔记#复习笔记":"仪器系统设计复习笔记","大作业#大作业":"待补充……","小作业#小作业":"待补充……","第一讲仪器系统的设计基础#第一讲：仪器系统的设计基础":"理解仪器系统的基本框图和结构框图 理解仪器硬件的几个主要部分（包括与人体的类比） 理解仪器系统的设计要求 理解仪器系统的设计原则（如经济性原则中的性价比概念） 理解软件模块化的划分原则（模块化越大越好还是越小越好） 理解下载和调试的概念与接口（例如调试过程中的焊接顺序） 理解软件测试中的黑盒测试和白盒测试 理解测试用例的过程（能自行设计测试用例）","第七讲软测量与数据融合技术#第七讲：软测量与数据融合技术":"理解软测量技术的原理 掌握应用软测量技术进行系统分析设计的能力（可能涉及创新设计题）","第三讲人机交互接口#第三讲：人机交互接口":"理解键盘去抖的原理 理解键盘单词键入与联机处理的方法 理解键盘扫描的处理方法 理解三基色原理（如RGB数值表示颜色） 理解断码式LED静态显示与动态显示的区别 理解断码式LCD、点阵式LCD和彩色LCD的区别","第九讲嵌入式人工智能技术#第九讲：嵌入式人工智能技术":"理解嵌入式人工智能模型效率的基本指标 理解嵌入式人工智能的应用原则","第二讲仪器的输入输出接口设置#第二讲：仪器的输入输出接口设置":"理解传感器信号的输出类型 理解多路开关和多路分配器的概念 理解信号调理电路的作用 理解采样定律的应用 理解AD转换器的分辨率概念 理解AD转换器的选用原则 理解DA的性能指标与应用（例如锯齿波、正弦波等波形的生成） 理解驱动电路的一般概念（如小功率、中功率、大功率驱动电路选用）","第五讲仪器存储器的接口设计#第五讲：仪器存储器的接口设计":"理解程序存储器与数据存储器的概念 理解各类存储器的区别 理解存储器的掉电保护技术 理解存储器使用中的技术要点（如数据读写错误、寿命问题及处理方法）","第八讲虚拟仪器设计#第八讲：虚拟仪器设计":"理解虚拟仪器的定义与特点 理解虚拟仪器的组成 理解虚拟仪器驱动程序的设计原则 理解GUI软件的设计原则 理解虚拟仪器应用程序的开发环境 理解图形化编程平台的特点 理解虚拟仪器应用程序的软面板设计方法","第六讲仪器的自动校准与质检技术#第六讲：仪器的自动校准与质检技术":"理解仪器自动校准的过程（包括浮点运算如何转化为整数运算，为何这样做） 理解仪器自检的一般概念 理解故障检测与故障诊断的区别（故障检测回答\"是否发生故障\"，故障诊断回答\"哪里发生了故障\"）","第十讲仪器系统的集成与应用#第十讲：仪器系统的集成与应用":"理解仪器系统集成的特点与任务 理解仪器系统集成的步骤（但不会单独考设计步骤，而是融入设计题中） 掌握应用仪器系统设计技术进行系统集成设计的能力","第四讲仪器的通讯接口设计#第四讲：仪器的通讯接口设计":"理解内总线和外总线的区别 理解I²C总线的数据收发方式（一根线还是两根线） 理解RS232与RS485串行通讯的异同点 理解串行通讯的通讯协议和命令协议的概念 理解USB接口的供电方式（总线供电与外供电的时机）","考试内容梳理#考试内容梳理":"考试内容主要围绕以下十讲的理解要点展开：","考试形式#考试形式":"考试总分为100分，分为三大题型：","课程内容#课程内容":""},"title":"仪器系统设计"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/%E7%94%9F%E4%BA%A7%E5%AE%9E%E4%B9%A0/":{"data":{"学习收获#学习收获":"通过生产实习，我深入了解了企业的实际工作流程，提升了实践能力和团队协作能力。","实习内容#实习内容":"企业实践学习 项目开发经验 团队协作能力提升","概述#概述":"大三暑假期间的生产实习经历。","生产实习#生产实习":"生产实习"},"title":"生产实习"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E4%BC%A0%E6%84%9F%E4%B8%8E%E6%A3%80%E6%B5%8B/":{"data":{"lab8#lab8":"对化学和生物传感器仅做体验要求，无实验报告要求。","七大类物理传感器-第3章#七大类物理传感器 (第3章)":"这是物理部分的核心，占分最多。复习每一类传感器时，都应遵循以下四个维度的框架：基本原理/机理、分类、重要特性、测量电路与应用。\n电阻式：原理需要理解电阻与材料尺寸、电阻率的关系公式。重点掌握金属应变片的工作原理、灵敏度系数（约为2），以及电桥电路（单臂、半桥、全桥）在温度补偿和提高灵敏度中的应用。\n电感式：分类包括自感式、互感式、电涡流式。重点掌握**LVDT（差动变压器式）**的结构、工作原理和测量电路，理解其解调过程。掌握电涡流传感器非接触测量的原理。\n电容式：原理基于改变极板间距、面积或介电常数。特性是易受分布电容影响。重点掌握差动式电容传感器如何提高灵敏度并减小非线性误差。其独特的充放电式测量电路是难点。\n压电式：特性是只能进行动态测量，是容性负载。重点理解正/逆压电效应、材料特性（压电陶瓷、石英晶体）、多片叠层的串并联方式，以及电压/电荷放大器的选择与电缆长度的关系。\n磁电式：分类包括电磁感应式（测速度，无需供电）和霍尔式（基于半导体）。重点理解霍尔效应的原理，以及为何要选用半导体材料。\n光电式：重点掌握内/外光电效应，理解光电管、光敏二极管/三极管等器件的伏安特性、光照特性和光谱特性图。光纤传感器的全反射原理也是关键。\n热电式：重点区分热电阻 (RTD)、热敏电阻 (PTC/NTC) 和热电偶的原理、材料、优缺点和适用范围。热电偶无需外部供电是一大特点。","主要化学传感器类型#主要化学传感器类型":"离子传感器：核心是所有类型都基于离子选择性膜和电极电位的测量。重点掌握离子选择性电极 (ISE) 的原理，并了解 ISFET（测沟道电流）和 LAPS（测光电流）是如何将其与半导体技术结合的。\n气体传感器：重点理解不同类型气体传感器的检测机制。电化学式测电流，金属氧化物半导体 (MOS) 测电阻（需要高温加热，理解其响应曲线），固体电解质式测电压，声表面波 (SAW) 是质量敏感型。\n湿度传感器：概念需要区分绝对湿度和相对湿度。类型需要了解电容式和电阻式湿度传感器（测相对湿度）以及干湿球法（测绝对湿度）的原理。","传感器阵列与系统#传感器阵列与系统":"电子鼻 \u0026 电子舌：定义是它们是传感器阵列，而非单个传感器。原理是模仿人类嗅觉/味觉，使用交叉敏感、特异性不强的传感器阵列，结合模式识别算法来识别复杂的混合物（气体/液体）。\n微流控：了解其在微小尺度上操控流体的概念及其在生物芯片中的应用。","作业#作业":"HW 1 HW 2 HW 3 HW 4 HW 5","化学传感器-25分#化学传感器 (25分)":"这部分内容偏向理论和概念，计算题较少。建议用图表和框图来辅助记忆。","化学传感器基础#化学传感器基础":"核心概念：组成需要理解化学传感器的三大模块：识别单元、换能器和信号处理单元。能画出基本框图。分类需要掌握三大类：离子传感器、气体传感器和湿度传感器。\n电化学基础：电极体系需要明确工作电极（发生反应）、参比电极（提供稳定电位基准）和对电极（构成电流回路）各自的作用。能斯特方程需要掌握用于计算离子选择性电极电位的核心公式。双电层需要理解其在电极/电解质界面形成的概念，这是电极电位产生的基础。","实验报告#实验报告":"Lab 1 Lab 2 Lab 3 Lab 4 Lab 5 Lab 6 Lab 7","最后一课#最后一课":"","物理传感器复习要点-占50分#物理传感器复习要点 (占50分)":"物理传感器部分是考试的重点，内容庞杂但有规律可循。复习时可以围绕以下几个核心模块展开。","生物传感器25分#生物传感器(25分)":"刘qj老师讲的比较意识流，而且可能会推销他的便携式柔性传感器的研究，2025学期也没考到。","生物医学传感与检测#生物医学传感与检测":"生物医学传感与检测","电子教材#电子教材":"生物医学传感与检测中文版第4版\n生物医学传感与检测英文版第2版","笔试内容分布#笔试内容分布":"物理传感器 (第1-3章): 占50分。题型包括10个选择题（共20分）和4个简答分析题（共30分），可能需要绘图或运用公式。\n化学传感器 (第4章): 占25分。题型包括5个选择题和2个简答题。\n生物传感器 (第5章): 占25分。","绪论与基本特性-第1-2章#绪论与基本特性 (第1-2章)":"核心概念：传感器定义需要掌握国标和国际行业协会的定义，核心是\"将被测量（非电量）按一定规律转换成可用输出信号（电量）的器件\"。专业词汇方面需要了解侵入式/非侵入式检测、生物芯片、微流控、生物相容性等。\n传感器特性：静态特性需要掌握灵敏度、重复度、迟滞、稳定度等7个关键指标的定义。动态特性需要理解上升时间、超调量等指标，并能区分一阶系统和二阶系统。二阶系统重点理解阻尼系数 (ζ) 在 0.3-0.7 之间系统性能最佳的原因，以及频率响应曲线中\"平坦区\"和\"谐振峰\"的不同应用场景（常规测量用平坦区，超声驱动用谐振峰）。\n测量技术：了解直接/间接测量、有源/无源测量等概念。熟悉典型的测量系统框图，理解传感器作为信息获取前端在整个系统中的位置。","课程考核结构#课程考核结构":"这门课程的总成绩由两部分构成：笔试成绩占总分的60%，平时成绩占总分的40%。\n笔试为开卷考试，总分100分，不允许携带计算器，也去掉了计算题，并且是英文题目，中英作答皆可。这么宽宏的安排，不得不感谢学院组敢于改革的老师们，也因此没有日常笔记，只有对最后一课的总结。这里也非常欢迎大家通过学生会的学海领航去复习。\n建议携带一本英文教材+一本中文教材+一本高级词典。在去除计算题后，最难的题目就是出现陌生生物医学名词的选择题。"},"title":"生物医学传感与检测"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/":{"data":{"":"","参考教材#参考教材":"Rafael C. Gonzalez and Richard E. Woods. Digital Image Processing (4th Edition), Pearson Education, Inc, 2019 Rafael C. Gonzalez, Richard E. Woods, and Steven Eddins. Digital Image Processing with Matlab (4th Edition), Gatesmark Publishing, 2020 Image Processing Place 吴丹，何宏建，张祎，赵立，林子暄，丁秋萍。《磁共振成像原理与应用》，科学出版社，2024（第九、十章） 数字图像处理教材\n数字图像处理教材\n数字图像处理习题解答","复习笔记#复习笔记":"生物医学图像处理复习笔记","大作业#大作业":"明年肯定不会是这个垃圾选题了。\n生物医学图像处理实验报告","实验报告#实验报告":"图像处理 Lab 1 图像处理 Lab 2 图像处理 Lab 3 图像处理 Lab 4 图像处理 Lab 5 图像处理 Lab 6 图像处理 Lab 7","小作业#小作业":"注：有错误之处\n高级程序设计 HW 1 高级程序设计 HW 2 图像处理 HW 1 图像处理 HW 2 图像处理 HW 3 图像处理 HW 4 图像处理 HW 5 图像处理 HW 6"},"title":"生物医学图像处理"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/%E7%94%B5%E5%AD%90%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E8%B7%B5/":{"data":{"关于系统设计#关于系统设计":"系统定义：温度测量系统\n相关技术要求：硬件结构设计、软件设计与调试\n涉及相关知识：微机原理、程序语言、信号处理、电子技术","复习笔记#复习笔记":"电子系统设计与实践","实验报告#实验报告":"","实验步骤#实验步骤":"第一阶段：安装及调试（实验一至实验四），第一阶段验收（实验五及附加要求）。\n第二阶段：系统设计，包括定义系统、方案设计、硬件设计、软件设计与调试，以及总结报告。","实验课wedn-6789#实验课（Wedn 6、7、8、9）":"平日可去周楼407焊\n实验课分为两个阶段：\n第一阶段：包括原理图、印刷电路板、元器件、工具和实验指导，主要任务是安装、调试和设计。\n第二阶段：自主设计，包括定义系统、方案设计、硬件设计、软件设计与调试，以及总结报告。","注意事项#注意事项":"实验课需要2-3人一组，分工合作，自主完成。\n实验课的考核成绩占总成绩的45%，理论课的考核成绩也占45%，平时作业占10%。\n课程内容涉及广泛的电子系统设计知识，需要学生具备一定的基础和实践能力。","理论课mon-67#理论课（Mon 6、7）":"课程模块：\n测量误差分析与实验数据处理 模拟电子线路基础 数字逻辑电路基础 电子线路计算机辅助分析与设计 低频电子线路应用设计 高频电子线路应用设计 数字逻辑电路应用设计 硬件描述语言及其应用 高速数字电路设计基础 综合性电子线路系统设计","电子系统设计与实践#电子系统设计与实践":"电子系统设计与实践","电设红外脉搏仪#电设红外脉搏仪":"电子系统设计与实践pj2","电设记忆示波器#电设记忆示波器":"电子系统设计与实践pj1","考核指标#考核指标":"理论课：书面考试，45%。\n实验课：实验结果和实验报告，45%。\n平时作业：包括一般作业和设计分析作业，10%。","课程内容#课程内容":"课程分为理论课和实验课两部分，具体内容如下："},"title":"电子系统设计与实践"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/%E7%A1%AC%E4%BB%B6%E6%8F%8F%E8%BF%B0%E8%AF%AD%E8%A8%80/":{"data":{"复习笔记#复习笔记":"硬件描述语言复习笔记","学习建议#学习建议":"教学太晦涩，实验太简单，是完美符合【上交生存手册】定义的垃圾课。\nHDLbits刷题\n菜鸟教程辅助\n教材方面以1、3、4、5、7、2的顺序进行教学，推荐英文版教材，但更推荐实验课PPT与学海领航PPT。\n强烈推荐配置VSCODE的iverilog\nDigital-IDE工作流","实验室参观#实验室参观":"硬件描述语言余锋实验室","实验报告#实验报告":"HDL Lab 1 HDL Lab 2 HDL Lab 3 HDL Lab 4 HDL Lab 5 HDL Lab 6 HDL Lab 7","硬件描述语言#硬件描述语言":"硬件描述语言","英文教材#英文教材":"数字图像处理教材 不要看中文教材，那个不是用自然语言写出来的。","课程内容--concepts#课程内容- concepts":"- event driven / time driven - time slots - parallel process - design verification program module master slave handshake clock domain RTL Timing Delay(9种) Best/worst/typical 三类 t输出 + tsetup \u003c= T (worst) t输出 \u003e= thold (best) 采样窗口 程序流程图、Datapath、FSM 采样窗口"},"title":"硬件描述语言"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E6%98%A5%E5%A4%8F/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/":{"data":{"lab3#lab3":"这个算大作业？但其实交一份普通的web开发项目上去就OK，和《高级程序设计》类同。","作业报告#作业报告":"Lab 1 Lab 2","参考资料#参考资料":"计算机网络（第7版）-谢希仁\n[计算机网络(Andrew Tanenbaum David Wetheral)](/pdfs/计算机网络(Andrew Tanenbaum David Wetheral).pdf)","复习笔记#复习笔记":"有的地方不在考纲范围内，注意甄别。\n计网笔记","计算机网络#计算机网络":"计算机网络成绩：40%作业+60%考试 考试不考应用层（意思意思一点点），相较于计院计网增加了很多物理层的东西，所以最后还是要根据ppt再复习一下"},"title":"计算机网络"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E7%A7%8B%E5%86%AC/":{"data":{"":"大三秋冬学期课程笔记。"},"title":"大三秋冬"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E7%A7%8B%E5%86%AC/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F/":{"data":{"信号与系统#信号与系统":"信号与系统","最后一课#最后一课":"数字信号处理总复习"},"title":"信号与系统"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E7%A7%8B%E5%86%AC/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F/":{"data":{"实验报告#实验报告":"Lab 1 Lab 2 Lab 3 Lab 4","嵌入式系统#嵌入式系统":"嵌入式系统22级生仪讲的《嵌入式系统》师资暴打同期《生物医学成像技术》，但效果却并不好。 原因是多样化的，一方面在于，在于这门课本应配置大量的实验课，嵌入式开发这种实验学习最终就应该像《电路综合》那样做一个嵌入式的大作业； 另一方面在于，从上下来的角度分析，《嵌入式系统》实际上可以被表示成70%的计组+30%的OS。但是对应的课时却并不多，要命的是还没有HW，只有四个树莓派小实验，对比其和上学期学习的《微机原理》对UART的讲解就能理解它讲的到底有多快。 而且在顺序上也没有讲好嵌入式Linux，没有学到实际产业的东西，非常符合《上交生存手册》定义，其在大三春夏开课的《电子系统设计与实践》更是开历史的倒车。","最后一课#最后一课":"嵌入式系统最后一课","课程内容#课程内容":"计组部分没什么好说的，不考汇编具体代码的话就是纯背PPT，然而有很多难以理解的概念，需要在平时花时间去研究，不然临考背诵会很痛苦。 研究完历年卷可以发现OS的占比随时间发展越来越低，原先还会考察线程、调度算法、虚拟内存等，现在一年比一年教的少，到后面只剩下银行家算法/producer-comsumer/reader-writer这类非记忆类的考点留下来了。因此前人留下的OS复习大纲与疫情前的回忆卷参考价值都少，需要更新。 考完回来补充，多选的考察真的很细，反而是OS部分不用花太多精力，只要课上理解就行。重点需要把精力放在【存储器管理单元MMU】【JTAG与嵌入式调试结构】【Linux内核驱动】这几节上。"},"title":"嵌入式系统"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E7%A7%8B%E5%86%AC/%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E6%88%90%E5%83%8F%E6%8A%80%E6%9C%AF/":{"data":{"ct#CT":"X-CT","mri#MRI":"MRI","pet#PET":"SPECT_PET","复习笔记#复习笔记":"","生物医学成像技术#生物医学成像技术":"生物医学成像技术这节课大家可能一开始上都会一头雾水，因为直到大三的秋冬学期，我们才真正意义上有了自己专业的第一门专业课。同期仪器的嵌入式系统、高级程序设计（Java web开发）、生物医学信号处理（猴版信号与系统）都大大小小能和其他学院有所交叉。没有领略过《上交生存手册》的同学将在这里了解到其描绘的地狱般的本科教学现况。 不过其实说来这门课程的脉络也挺清晰的。作为本科课程，《生物医学成像技术》并不会介绍研究生领域深入的成像技术，而只是泛泛介绍下列四种：\n超声（超声成像）： 原理：利用高频声波，声波在遇到不同组织时会反射，形成图像。 应用：广泛用于腹部、心脏、孕妇产检、血管等的成像，具有无辐射、实时成像的特点。 CT（计算机断层扫描）： 原理：利用X射线从多个角度扫描身体，计算机将这些X射线数据合成为三维图像，提供详细的解剖结构信息。 应用：广泛用于头部、胸部、腹部和骨骼的成像，适合检测骨折、肿瘤和内部出血等病变。 PET（正电子发射断层成像）： 原理：利用放射性同位素标记的示踪剂，注入人体后会聚集在活跃的组织中。PET 通过探测这些放射性同位素的衰变，生成关于人体功能活动的图像。 应用：广泛用于肿瘤、心脏病和脑部疾病（如阿尔茨海默病）的诊断，因为它可以显示组织的代谢活动。 MRI（磁共振成像）： 原理：利用强磁场和射频波，使人体内的氢原子核产生共振信号，然后计算机将这些信号转化为图像。 应用：对软组织有良好的分辨率，常用于脑部、脊柱、关节和肌肉等部位的检查，特别适用于发现软组织病变。 其中MRI至少在2024年算我们学院的一个研究方向的热门，不过这里主要讨论本课程的备考。接下来的笔记将以物理原理、成像设备、成像过程(信号获取、图像重建)和图像评价(信噪比、对比度、分辨率、伪影等)的结构来组织。","超声#超声":"超声"},"title":"生物医学成像技术"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E7%A7%8B%E5%86%AC/%E8%AF%AF%E5%B7%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/":{"data":{"复习笔记#复习笔记":"来自不知名前辈\n误差理论与数据处理","误差处理与数据分析#误差处理与数据分析":"误差处理与数据分析"},"title":"误差处理与数据分析"},"/docs/undergraduate/%E5%A4%A7%E4%B8%89%E7%A7%8B%E5%86%AC/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/":{"data":{"回忆卷#回忆卷":"感谢前辈们一代一代的接力。\n13-14 回忆卷 15-16 试题卷 15-16 回忆卷 16-17 回忆卷 21-22 回忆卷 22-23 回忆卷 23-24 回忆卷","复习笔记#复习笔记":"硬件描述语言复习笔记","学习建议#学习建议":"教学太晦涩，实验太简单，是完美符合【上交生存手册】定义的垃圾课。\nHDLbits刷题\n菜鸟教程辅助\n教材方面以1、3、4、5、7、2的顺序进行教学，推荐英文版教材，但更推荐实验课PPT与学海领航PPT。\n强烈推荐配置VSCODE的iverilog\nDigital-IDE工作流","实验室参观#实验室参观":"硬件描述语言余锋实验室","实验报告#实验报告":"HDL Lab 1 HDL Lab 2 HDL Lab 3 HDL Lab 4 HDL Lab 5 HDL Lab 6 HDL Lab 7","硬件描述语言#硬件描述语言":"硬件描述语言","英文教材#英文教材":"数字图像处理教材 不要看中文教材，那个不是用自然语言写出来的。","课程内容--concepts#课程内容- concepts":"- event driven / time driven\r- time slots\r- parallel process\r- design verification\rprogram module master slave handshake clock domain RTL Timing Delay(9种) Best/worst/typical 三类 t输出 + tsetup \u003c= T (worst) t输出 \u003e= thold (best) 采样窗口 程序流程图、Datapath、FSM 采样窗口"},"title":"高级程序设计"},"/docs/undergraduate/%E5%A4%A7%E4%BA%8C%E6%98%A5%E5%A4%8F/":{"data":{"":"大二春夏学期课程笔记。"},"title":"大二春夏"},"/docs/undergraduate/%E5%A4%A7%E4%BA%8C%E6%98%A5%E5%A4%8F/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/":{"data":{"微机原理及应用#微机原理及应用":"微机原理及应用","课程概述#课程概述":"微机原理及应用课程主要学习微型计算机的基本原理、体系结构和编程技术，是理解计算机硬件和软件交互的重要课程。主要就是涉及8051微机，我其实挺希望与时俱进。\n本页面内容正在完善中…"},"title":"微机原理及应用"},"/docs/undergraduate/%E5%A4%A7%E4%BA%8C%E6%98%A5%E5%A4%8F/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/":{"data":{"数据结构与算法基础#数据结构与算法基础":"数据结构与算法基础","课程概述#课程概述":"本页面内容正在完善中…"},"title":"数据结构与算法基础"},"/docs/undergraduate/%E5%A4%A7%E4%BA%8C%E6%98%A5%E5%A4%8F/%E7%94%B5%E8%B7%AF%E4%B8%8E%E7%94%B5%E5%AD%90%E6%8A%80%E6%9C%AFii/":{"data":{"电路与电子技术-ii#电路与电子技术 II":"电路与电子技术 II","课程概述#课程概述":"主要内容为模电。"},"title":"电路与电子技术II"},"/docs/undergraduate/%E5%A4%A7%E4%BA%8C%E6%98%A5%E5%A4%8F/%E7%94%B5%E8%B7%AF%E7%BB%BC%E5%90%88%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/":{"data":{"电路综合创新实践#电路综合创新实践":"电路综合创新实践","课程概述#课程概述":"电路综合创新实践是一门8051嵌入式开发板综合性的实践课程，旨在通过实际项目培养学生的电路设计能力和创新思维。\n本页面内容正在完善中…"},"title":"电路综合创新实践"},"/docs/undergraduate/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E5%86%AC/":{"data":{"":"大二秋冬学期课程笔记。"},"title":"大二秋冬"},"/docs/undergraduate/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E5%86%AC/%E7%94%B5%E8%B7%AF%E4%B8%8E%E7%94%B5%E5%AD%90%E6%8A%80%E6%9C%AFi/":{"data":{"复习重点#复习重点":"电路与电子技术I","电路与电子技术i#电路与电子技术I":"电路与电子技术I","课程大纲#课程大纲":"电路原理+数电，当然后面生仪跟着信电他们一起是电原+模电一起上了，参考意义没那么大了。"},"title":"电路与电子技术I"},"/docs/undergraduate/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80/":{"data":{"":"数理基础课程群包含了数学和物理两大类基础学科，涵盖了从基础数学到应用数学，从经典物理到现代物理的广泛内容。","主要课程#主要课程":"","复变函数#复变函数":"","大学物理#大学物理":"","常微分方程#常微分方程":"","微积分#微积分":"","数学类#数学类":"","概率论与数理统计#概率论与数理统计":"","物理类#物理类":"","离散数学#离散数学":"","线性代数#线性代数":""},"title":"数理基础"},"/docs/undergraduate/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/":{"data":{"":"编程基础相关课程笔记。"},"title":"编程基础"},"/docs/undergraduate/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/c/":{"data":{"c-语言程序设计#C 语言程序设计":"C 语言程序设计","课程概述#课程概述":"本页面内容正在完善中…"},"title":"c"},"/docs/undergraduate/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/python/":{"data":{"python-编程学习笔记#Python 编程学习笔记":"Python 编程学习笔记","课程概述#课程概述":"本页面内容正在完善中…"},"title":"python"},"/docs/undergraduate/%E9%80%9A%E8%AF%86%E6%9D%82%E9%A1%B9/":{"data":{"":"通识课程相关笔记。"},"title":"通识杂项"},"/docs/undergraduate/%E9%80%9A%E8%AF%86%E6%9D%82%E9%A1%B9/%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E5%AD%A6/":{"data":{"1-生命的基本特征是什么生命科学的重要性是什么生命科学本课程内容的基本模块有哪些彼此之间是怎样的逻辑联系内容体系或知识图谱#1. 生命的基本特征是什么？生命科学的重要性是什么？生命科学（本课程内容）的基本模块有哪些，彼此之间是怎样的逻辑联系（内容体系或知识图谱）？":"","1-考试形式#1. 考试形式":"期末闭卷统考（卷面100分） 题型构成： 名词解释（40分） 题量：8题（从小分享主题词中选取） 分值：5分/题 问答题（60分） 题量：6选4（多答仅计前4题） 分值：15分/题","10-如何理解遗传与变异在生物多样性和生物进化中的意义如何看待经典遗传学对近代生物学发展的贡献在分子生物学和生物信息学高度发展的今天经典遗传学手段还能应用于哪些方面的研究#10. 如何理解遗传与变异在生物多样性和生物进化中的意义？如何看待经典遗传学对近代生物学发展的贡献？在分子生物学和生物信息学高度发展的今天，经典遗传学手段还能应用于哪些方面的研究？":"经典遗传学包括分离定律和独立分配定律。\n分离定律指一对遗传因子在杂合状态下并不相互影响，而在配子形成中又按原样分配到配子中去。 独立分配定律指两对或两对以上的基因在配子形成过程中的分配彼此独立。由于雌雄配子的随机组合，因而在子代中出现各种性状的各种组合，而且按一定的比例出现。","11-拉马克演化论vs达尔文演化论的比较演化的主要驱动力#11. 拉马克演化论vs.达尔文演化论的比较，演化的主要驱动力。":"","12-地球生态系统碳循环碳达峰碳中和与生态学的关系生物多样性丧失的原因和保护对策#12. 地球生态系统碳循环，碳达峰、碳中和与生态学的关系，生物多样性丧失的原因和保护对策。":"生物碳循环是指生物体通过光合作用将二氧化碳吸收并转化为有机物，生物体死亡或燃烧后，二氧化碳又释放到大气中。而地球物理化学碳循环是指二氧化碳在大气、海洋和土壤中的运动和相互作用。\n碳达峰，就是指在某一个时点，二氧化碳的排放不再增长达到峰值，之后逐步回落。 碳中和则是在一定时间内直接或间接产生的二氧化碳排放总量，通过植树造林、节能减排等形式，以抵消自身产生的二氧化碳排放量，实现二氧化碳\"零排放\"。","16s-rrna#16S rRNA":"是细菌上编码rRNA相对应的DNA序列，存在于所有细菌的基因组中。\n可用于构建细菌系统树。\n是细菌中特有的核糖体30s亚基的组成部分。16S rRNA具有多项功能：\n对于**核糖体蛋白的固定起到脚手架的作用**。 3’末端包含反向的SD序列，用来与mRNA的AUG起始密码子结合。 16S rRNA的3’端与S1、S21的结合被发现与蛋白质合成的开始有关系。 与23S进行交互，帮助两个核糖体子单元的结合。（50S+30S） 在A site 稳定密码子与反密码子的正确配对。","2-植物特有的特征有哪些由此塑造了哪些关联特征如何从基因繁衍的角度理解植物生活史的关键阶段和过程层级分类系统对生物多样性信息的存储和提取有什么优点#2. 植物特有的特征有哪些，由此塑造了哪些关联特征？如何从基因繁衍的角度，理解植物生活史的关键阶段和过程？层级分类系统对生物多样性信息的存储和提取有什么优点？":"植物特有的特征\n自养 (Autotrophic, or self-feeding) 光合作用 (Photosynthesis) 固着生长 (sessile, or immobile) 具细胞壁 (纤维素 Cellulose) 具无限生长 (Indeterminate growth)，具有连续发育的特点（发育模式：无限发育与重复分化） 具有性和无性繁殖 (reproduce asexually and sexually)","3-动物在演化过程中多次发生了生活方式的变化比如从漂浮生活到游泳生活从水生到陆生生活从身体贴地爬行到离地奔跑在每一次变化过程中出现了哪些关键性的特征变化目前利用动物的形态特征构建的动物系统树与利用基因组信息构建的系统树之间具有非常大的区别原因有哪些#3. 动物在演化过程中多次发生了生活方式的变化，比如：从漂浮生活到游泳生活、从水生到陆生生活、从身体贴地爬行到离地奔跑，在每一次变化过程中出现了哪些关键性的特征变化？目前，利用动物的形态特征构建的动物系统树与利用基因组信息构建的系统树之间具有非常大的区别，原因有哪些？":"","4-微生物与人类的关系及其发生原因古菌与细菌的区别#4. 微生物与人类的关系及其发生原因。古菌与细菌的区别。":"","5-熟悉细胞学说了解细胞与病毒的区别细胞中存在多种不同的程序性死亡方式有何生物学意义#5. 熟悉细胞学说，了解细胞与病毒的区别。细胞中存在多种不同的程序性死亡方式有何生物学意义？":"在胚胎发育阶段通过细胞凋亡清除多余的和已完成使命的细胞，保证了胚胎的正常发育；在成年阶段通过细胞凋亡清除衰老和病变的细胞，保证了机体的健康。","6-从母源-胚胎转换的角度如何理解母源基因从个体发育的角度看原肠运动的作用有哪些举例说明胚胎诱导在器官形成中的作用和意义#6. 从母源-胚胎转换的角度，如何理解\u0026quot;母源基因\u0026rdquo;？从个体发育的角度看，原肠运动的作用有哪些？举例说明胚胎诱导在器官形成中的作用和意义。":"卵母细胞向早期胚胎的基因表达模式转换，即\"母源-合子过渡（Maternal-to-zygotic transition，MZT）\"，是早期胚胎发育过程中的第一个关键事件。在模式生物（如果蝇、斑马鱼、非洲爪蟾）和实验动物（小鼠）中的研究都表明\"母源-合子过渡\"过程中母源mRNA的适时降解是胚胎自身基因组转录激活的一个重要前提。\n原肠作用是指囊胚细胞有规则地移动，使未来的内胚层和中胚层细胞迁入胚胎内部，而未来的外胚层细胞铺展在胚胎的表面，从而形成原肠胚，也称为原肠运动。是早期胚胎由囊胚形成原肠胚的发育过程。囊胚表面一定区域迁移到胚胎内部形成内、中、外三胚层，为后继的器官形成奠定基础。通过原肠作用，胚胎细胞首先从未分化状态出现最早的3胚层分化。\n动物在一定的胚胎发育时期, 一部分细胞影响相邻细胞使其向一定方向分化的作用称为近旁组织的相互作用, 或称为胚胎诱导。诱导相邻细胞发育的信号分子是可扩散的蛋白质，称为成型素。能对其他细胞的分化起诱导作用的细胞，即分泌成型素的细胞称为诱导者或组织者(organizer)。如将正常的能够发育成神经组织的细胞从两栖类原肠期的早期胚胎中切下，然后移植到另一个胚胎的可以发育成表皮的区域中，结果，移植来的细胞发育成了表皮而不是神经细胞。 同样，将可以分化发育成表皮组织的细胞移植到能够发育成神经组织的胚胎中，移植的细胞发育成了神经细胞。胚胎诱导一般发生在内胚层和中胚层或外胚层和中胚层之间。","7-下丘脑是如何实现对机体内稳态平衡的调节的以心血管系统为例简介机体维持其稳态的调节机制#7. 下丘脑是如何实现对机体内稳态平衡的调节的？以心血管系统为例，简介机体维持其稳态的调节机制。":"体温调节 调节体温的中枢在下丘脑。破坏哺乳动物的下丘脑后，体温不能保持恒定。下丘脑的体温调节机构除有中枢性温度感受器外，还有控制产热和散热功能的中枢。 摄食行为的调节 动物实验证明，下丘脑的腹内侧区接近正中隆起的两侧受损伤时， 动物的食量大增；如以电流刺激这一部位，则食量大减。因此，这一部位被称为饱中枢(satiety center)。相反，下丘脑外侧区损毁时，动物食量减少，甚至拒食；若刺激这一部位，则食量大增。因而被认为是摄食中枢(feeding center)的所在。在正常机体，这两部位之间可能是互相制约的。至于摄食中枢的自然刺激是什么，有人认为血糖水平的降低是引起摄食中枢兴奋的主要传入信息。实验征明，动物在饥饿状态下。摄食中枢神经元放电频率较高而饱中枢神经元放电频率较低，静脉注入葡萄糖后，摄食中枢神经元放电频率减少而饱中枢神经元放电频率增多。进一步实验证明，饱中枢的活动还与该中枢内神经细胞的糖利用水平有关。糖尿病患者血糖水平增高，但因缺乏胰岛素，饱中枢神经细胞的糖利用率减少，因此其活动降低而使食欲增加。 水平衡的调节 损坏下丘脑外侧区除可引起动物拒食外，饮水也明显减少；刺激下丘脑外侧区某些部位，则可引起动物饮水增多。但控制饮水中枢的确切位置目前还不清楚。 下丘脑控制排水是通过抗利尿激素的分泌来完成的。抗利尿激素是由视上核和室旁核的神经元合成的。神经分泌颗粒沿下丘脑-垂体束的神经纤维向外周运输而贮存于神经垂体内，以高渗盐水注入动物的颈内动脉，可刺激抗利尿激素的分泌。下丘脑内的渗透压感受器可能在视上核和室旁核内。电生理研究观察到，当颈内动脉注入高渗盐水时，视上核内某些神经元放电增多。一般认为，下丘脑控制摄水的区域与抗利尿激素分泌的核团在功能上是有联系的，两者协同调节着水平衡。 腺垂体激素分泌的调节 下丘脑内有些神经元能合成调节腺垂体激素分泌的肽类物质，包括促甲状腺素释放激素、促性腺激素释放激素、生长素释放抑制激素、生长素释放激素、促肾上腺皮质激素释放激素、促黑素细胞激素释放因子、促黑素细胞激素释放抑制因子、催乳素释放因子、催乳素释放抑制因子等。这些肽类物质合成后经轴突运输到正中隆起，由此经垂体门脉系统到达腺垂体，促进或抑制某种腺垂体激素的分泌。此外，下丘脑还有一些神经元对血液中某些激素浓度的变化比较敏感，这种神经元称为觉察细胞(detector cell)，能感受血液中激素浓度变化的信息，反馈调节上述肽类物质的分泌，从而更好地控制腺垂体的激素分泌活动。 对情绪反应的影响 在间脑水平以上切除大脑的猫，常出现一系列交感神经系统过度兴奋的现象，并且张牙舞爪，好似正常猫在搏斗时一样，故称之为\"假怒\"。平时下丘脑的这种活动受到大脑皮层的抑制而不易表现，但切除大脑皮层以后，则这种抑制解除了，以致在微弱的刺激下就能激发强烈的假怒反应。近年来的研究指出，下丘脑内存在所谓\"防御反应区\"，它主要位于下丘脑的腹内侧区。在动物麻醉条件下，电刺激该区可引起血压上升，皮肤及胃肠血管收缩，心率加速等交感神经兴奋性反应。在动物清醒状态下，电刺激该区还可出现防御性行为。下丘脑腹内侧区与杏仁核之间有功能联系，两者与情绪反应活动有关。此外，电刺激下丘脑外侧区可引致动物出现攻击行为，电刺激下丘脑背侧区则出现逃避行为。可见，下丘脑与情绪反应的关系非常密切。 心血管活动的调节：\n一、神经调节： 心脏的神经支配：心脏受植物神经（交感神经和副交感神经）的支配。交感神经节节后纤维支配窦房结、心房肌、房室交界区、房室束及其分支和心室肌。交感神经节后纤维释放的递质为去甲肾上腺素，与心肌上的肾上腺素能β1受体相结合，引起心脏兴奋，使心率加快（正性变时作用），房室传导速度加快（正性变传导作用），心肌收缩能力加强（正性变力作用），结果心输出量增加。副交感神经是迷走神经，节后纤维支配窦房结、心房肌、房室交界区、房室束及其分支。副交感神经节后纤维释放的递质是乙酰胆碱，与心肌上的胆碱能M受体相结合，抑制心脏的活动，作用结果与交感神经兴奋的结果相反。 血管的神经支配：从机能上分为缩血管神经纤维和舒血管神经纤维两大类。缩血管神经纤维均属交感神经纤维，故称交感缩血管神经纤维。体内大部分血管仅受交感缩血管神经纤维的单一支配。 心血管中枢：调节心血管活动的基本中枢在延髓。 心血管反射：颈动脉窦、主动脉弓压力感受性反射，该反射对血压的调节机制为负反馈调节。颈动脉体和主动脉体化学感受性反射。 二、体液调节： 肾上腺素和去甲肾上腺素：肾上腺素的心血管效应是心率加快，心输出量增加，动脉血压上升，内脏等处血流量减少，肝、冠脉血流量上升。去甲肾上腺素的心血管效应是心率减慢，外周阻力上升，动脉血压上升。 血管紧张素：最重要的是血管紧张素II，升压效应约为去甲肾上腺素的40倍。 血管加压素 心房钠尿肽","8-新陈代谢中酶的基本特征和催化原理及生物学意义#8. 新陈代谢中酶的基本特征和催化原理及生物学意义。":"","9-蛋白质组学的特征及其与生物表型相关性癌症发生的原因主要治疗手段及其基本机理#9. 蛋白质组学的特征及其与生物表型相关性。癌症发生的原因，主要治疗手段及其基本机理。":"","一平时课堂小分享题目#（一）平时课堂小分享题目":"构件生物、光合作用、双受精、生活史\n动物的孤雌生殖、动物的再生、黑腹果蝇与模式动物、动物的变态\n疫苗、人类肠道微生物与健康、酿酒微生物与风味、16S rRNA\n细胞凋亡与动物发育、干细胞与癌细胞、线粒体疾病、内吞作用与信号传导\n母源基因、神经胚形成、性别决定基因、全能干细胞和多潜能干细胞\n内稳态、生物节律、激素、细胞免疫\n微量元素、饮食模式、蛋白质折叠、酶\n中心法则、基因敲除、基因沉默、蛋白质组学\n反向遗传学、核外遗传、遗传标记、转基因生物\n趋同演化、协同演化、基因水平转移、分子钟理论\n盖亚假说、生态位与生态幅、营养级、生态足迹","一期末考试题型和分数组成#一、期末考试题型和分数组成":"","三课程总成绩构成#三、课程总成绩构成":"考核项目 权重 说明 线上学习和测试 30% MOOC平台完成情况 课后作业/小测验 10% 各章节随堂检测 讨论发言（小分享） 10% 课堂主题讨论参与度 主题演讲（大分享） 20% 小组汇报与材料提交 期末考试 30% 闭卷统考","中心法则#中心法则":"是指遗传信息从DNA传递给RNA，再从RNA传递给蛋白质，即完成遗传信息的转录和翻译的过程。也可以从DNA传递给DNA，即完成DNA的复制过程。这是所有有细胞结构的生物所遵循的法则。在某些病毒中的RNA自我复制（如烟草花叶病毒等）和在某些病毒中能以RNA为模板逆转录成DNA的过程（某些致癌病毒）是对中心法则的补充。\n意义： 中心法则是现代生物学中最重要最基本的规律之一， 其在探索生命现象的本质及普遍规律方面起了巨大的作用，极大地推动了现代生物学的发展，是现代生物学的理论基石，并为生物学基础理论的统一指明了方向，在生物科学发展过程中占有重要地位。 遗传物质可以是DNA，也可以是RNA。细胞的遗传物质都是DNA，只有一些病毒的遗传物质是RNA。这种以RNA为遗传物质的病毒称为反转录病毒（retrovirus），在这种病毒的感染周期中，单链的RNA分子在反转录酶（reverse transcriptase）的作用下，可以反转录成单链的DNA，然后再以单链的DNA为模板生成双链DNA。双链DNA可以成为宿主细胞基因组的一部分，并同宿主细胞的基因组一起传递给子细胞。在反转录酶催化下，RNA分子产生与其序列互补的DNA分子，这种DNA分子称为互补DNA（complementary DNA），简写为cDNA，这个过程即为逆转录（reverse transcription）。\n由此可见，遗传信息并不一定是从DNA单向地流向RNA，RNA携带的遗传信息同样也可以流向DNA。但是DNA和RNA中包含的遗传信息只是单向地流向蛋白质，迄今为止还没有发现蛋白质的信息逆向地流向核酸。这种遗传信息的流向，就是克里克概括的中心法则(central dogma)的遗传学意义。\n任何一种假设都要经受科学事实的检验。反转录酶的发现，使中心法则对关于遗传信息从DNA单向流入RNA做了修改，遗传信息是可以在DNA与RNA之间相互流动的。那么，对于DNA和RNA与蛋白质分子之间的信息流向是否只有核酸向蛋白质分子的单向流动，还是蛋白质分子的信息也可以流向核酸，中心法则仍然肯定前者。可是，病原体朊粒(Prion)的行为曾对中心法则提出了严重的挑战。\n实验证明，朊粒确实是不含DNA和RNA的蛋白质颗粒，但它不是传递遗传信息的载体，也不能自我复制，而仍是由基因编码产生的一种正常蛋白质的异构体。","二各章课后思考题#（二）各章课后思考题":"","二名词解释#（二）名词解释":"","二期末考试时间#二、期末考试时间":"日期：2023年1月11日（周一） 时段：10:30-12:30 形式：线上考试（具体安排待后续通知）","五复习范围#五、复习范围":"","人类肠道微生物与健康#人类肠道微生物与健康":"是指人肠道中数量庞大的微生物，他们依靠肠道生存，同时帮助人类完成一些生理生化功能，从多方面影响人体的健康。肠道不仅是人体消化吸收的重要场所，同时也是最大的免疫器官，在维持正常免疫防御功能中发挥着极其重要的作用。人体肠道为微生物提供了良好的栖息环境，具有人体自身不具备的代谢功能。影响微生物菌群差异的因素包括宿主的地域、年龄、生理状况、饮食习惯等因素。膳食因素也是改变肠道微生物的重要因素之一，更是最容易改变或控制的因素。不同人群由于膳食习惯的不同，对于膳食因子的摄入有很大不同，由此引起的肠道微生物组成、结构与功能也会存在较大差异。肠道微生物是人体代谢的重要参与者，肠道微生物基因组中富含参与碳水化合物、氨基酸、甲烷、维生素和短链脂肪酸代谢的基因，其中很大一部分是人体自身所不具备的。由于肠道是人体内最大的免疫器官，肠道微生物与宿主在肠道黏膜表面的交流促进了免疫系统的建立和发展，成为人体重要的免疫屏障。 另外，肠道微生物还通过形成\"菌膜屏障“而为人体提供保护功能。 滥用抗生素导致的肠道微生物失调会提高肠道疾病发生的几率。同时肠道微生物也与肥胖、糖尿病、渐冻症、心血管疾病有关。作为人体最庞大、最复杂的微生态系统，肠道微生物本身及其代谢产物不仅能调节人体健康，更在膳食和宿主之间起到了重要的桥梁作用。","光合作用#光合作用":"植物、藻类和蓝菌等生产者利用光能把一氧化二氢、二氧化碳或硫化氢等无机物转变成可以储存化学能的有机物（比如碳水化合物）的生物过程。\n植物的光合作用可分为光反应和暗反应。\n光反应：2H2O→4[H]+O2↑，发生在叶绿体囊状结构薄膜。 暗反应：利用光反应生成[H]和ATP进行碳的同化作用，使气体二氧化碳还原为糖，发生在叶绿体基质。 光合作用的意义:\n能量转换: 植物在同化无机碳化物的同时，把太阳能转变为化学能，储存在所形成的有机化合物中。 调节大气: 大气之所以能经常保持21%的氧含量，主要依赖于光合作用。","全能干细胞#全能干细胞":"全能干细胞是能够分化发育成为各种组织器官的细胞，其全能性很强。全能干细胞是指受精卵到卵裂期32细胞前的所有细胞。全能干细胞是指具有无限分化潜能，能分化成所有组织和器官的干细胞。具有形成完整个体分化潜能。但由于伦理与免疫排斥的原因，目前仅做科学研究，在临床医学上并没有被广泛应用。","内吞作用与信号传导#内吞作用与信号传导":"内吞作用又称入胞作用或胞吞作用，是通过**质膜的变形运动将细胞外物质转运入细胞内的过程**。根据入胞物质的不同大小，以及入胞机制的不同可将内吞作用分为三种类型：吞噬作用、吞饮作用、受体介导的内吞作用。\n信号传导：可以把各种信号通过细胞膜进入细胞, 逐步引起细胞物质主要是蛋白质变化的过程，称为信号传导。它是一个多酶级联反应过程, 各条信号通路之间通过细胞间信号蛋白的相互作用在体内组成一高度有序的调控网络。哺乳动物维持正常的活动需要多种信号转导通路以维持机体细胞对信号刺激反应的完整性和协调性。有许多内吞作用与信号转导有关。","内稳态#内稳态":"是指身体内部所保持的理化状态及化学成分、其他一些生命活动存在一定的动态平衡。细胞由细胞膜与其周围环境隔开，细胞内部情况与细胞周围液体有很大差别，细胞与周围液体不断进行物质交换并保持其内部的恒定性，这就是细胞稳态。保持整个身体的稳态，在高等动物要靠激素和神经系统的整合作用。激素保持身体的稳态，它可有及时使激素释放和停止分泌的能力，这就是激素分泌的稳态的保持。**中枢神经系统**在保持身体稳态中起重要作用，而其本身也要保持稳态。中枢神经系统的稳态依赖于其所接触的内环境的恒定性。\n意义：具有内稳态机制的生物借助于内环境的稳定而相对独立于外界条件，大大提高了生物对生态因子的耐受范围。","分子钟理论#分子钟理论":"任意两个物种，**自从分化成两个物种后，它们之间的遗传差异（DNA或者蛋白序列）的进化速度应该与分化的时间时保持相对稳定的。**某一蛋白在不同物种间的取代数与所研究物种间的分歧时间接近正线性关系，这一理论便是分子钟理论。同理也可以推导，两个物种的遗传差异与它们上一次从共同祖先分离出来的时间，也应该是成正相关比例的。\n机制：“中性演化理论\"使得分子钟得到了解释：基因会发生随机突变，而这些突变中存在着很多中性突变——它对于个体没有好处也没有坏处，因此会被保留下来。这样一来，分子钟的现象就很好理解了：在进化过程中很多突变被保留了下来，而进化距离差得越远，那么分子上的突变就会差的越多，因此我们就可以根据分子上的差距，去推算它们过去进化的过程。 分子钟理论的生物学意义主要有：\n提供了一种研究物种间关系的新方法：利用分子钟理论，可以用分子遗传学方法来研究物种之间的距离和演化关系。 提供了一种研究物种演化时间的新工具：分子钟理论可以用来估算物种演化的时间，进而帮助我们了解物种演化的进程。 为系统发育研究提供了理论依据：分子钟理论通过研究DNA序列的变化，为我们理解物种演化的进程和系统发育关系提供了重要的理论依据。 帮助我们更好的了解物种之间的联系，探究物种的演化脉络。 缺陷：分子钟假说成立的条件DNA或者蛋白质序列的替代速率是恒定的。20世纪80年代以来，随着DNA序列数据快速积累，大量的证据表明：在长期进化过程中，很多类群的绝大多数基因或蛋白质的序列替换速率根本不符合分子钟假说。对于蛋白质序列，在物种适应辐射过程中，其进化速度可能会大大加快。因此，以蛋白质为基础的恒定进化速率并非理想的分子钟；对于核酸分子，不同基因的分子钟速率不同；并且同一基因在不同的生物类群间可能有显著差异，因此同一基因的分子异速进化现象是显而易见的。","动物的变态#动物的变态":"又称间接发育，指动物胚后发育过程中所出现形态和习性上一系列显著变化的过程。大多数无脊椎动物门类中都有进行变态的种类，在脊椎动物中变态仅见于鱼类和两栖类。通过变态，不仅动物成体形态建立，同时其生理特性、行为、活动方式和生态表现都与幼虫期有显著差别。\n昆虫类的变态：其多样化在动物界突出，可分为不完全变态和完全变态。\n完全变态的昆虫一生要经历卵、幼虫、蛹和成虫4个阶段。此类昆虫的幼虫与成虫在外观上有较大的差别，比如毛虫和蝴蝶或蛴螬和甲虫。完全变态昆虫被认为是昆虫纲中进化程度最高的一群，种类也最繁多。 不完全变态的昆虫一生经历卵、若虫和成虫3个阶段。它们的幼虫在外观上与成虫差别一般不大，通常只是体型稍小，没有翅。 意义：同一个物种，在生存的不同阶段，形态完全不同，占据不同生态位，增加生存概率，有效地降低种内斗争，便于种群的繁衍。\n研究昆虫变态类型有以下生物学意义：\n对于控制昆虫的危害有很大的用处，主要在消灭害虫，控制害虫的数量，在生物防治害虫有很大的作用，减少农药的使用量，和对益虫的毒害和杀害的减少。 可以知道害虫在什么时候变态，变态的弱点，可以在最佳时期采取有利的措施，使害虫不能变态，也就是生物制剂来控制大面积大范围的虫害，比如蝗虫、斜纹夜蛾，蚜虫、水稻螟虫等等。 生物制剂的仿制，如保幼素的制剂的人的运用于化妆品，是皮肤娇嫩，皮肤的再生长等。 生物防治害虫，干扰害虫的发育，减少害虫的危害。","协同演化#协同演化":"两个或多个[无亲缘]关系的物种共同生活，在各自演化的过程中相互影响，包括它们的演化方向、速率等。\n两种生物可以是空间上生活在一起，也可以是一方生活在另一方的体内或体表等，但也不会给另一方造成严重的危害。协同演化还表现在生物与环境之间，即生物为了适应某种特定的环境条件而使自身产生了一系列的演化变化，当生活的环境发生改变时，生物随之发生一些主动调整而更好地与生活环境和谐相处。研究显示，在生物与环境的协同演化中，不光是生物为适应其生活的环境做出调整，环境也会因为其中的生物发生的演化而发生一定程度的改变，以使生物与环境高度统一。地质历史中不乏这样的实例。\n这种演化行为是生物在环境的选择压力下出现的。环境因素包括非生物的（无机的）也包括其他生物的（有机的）。因此，它是一种进化机制，可以发生在不同的生物学层次：既可以出现在分子水平上（如DNA和蛋白质序列的突变），也可以出现在宏观水平上（如物种形态性状、行为等的变化）。比如，对于捕食者和被捕食者来说，被捕食者会演化出更好的逃脱机制以逃生，而捕食者则会演化出相应的猎取能力，这些都属于典型的协同演化。\n作为各种生物群落的一个基本特征，协同演化在自然界无处不在，时刻发生着。它对于地球生态系统的多样性也发挥了非常重要的作用，许多动物（如鸟类）和植物之间就存在协同演化关系，比如蜜蜂与兰花。","双受精#双受精":"是被子植物特有的受精现象，指被子植物的雄配子体形成的两个精子，一个与卵融合形成二倍体的合子，另一个与中央细胞的极核（通常两个）融合形成初生胚乳核的现象。双受精后由合子发育成胚，中央细胞发育成胚乳。\n被子植物胚囊中极核同卵一样受精产生了具有父本和母本遗传性的通常是三倍体的胚乳，由这种胚乳\"哺育\"胚可能使后代更加巩固它双亲的特性并更富有生命力，因此有人认为双受精是被子植物繁盛的一个重要原因。","反向遗传学#反向遗传学":"广义的反向遗传学泛指从生物基因组及其所含生物信息出发，采用\"基因→性状\"的研究路线，对生物体进行遗传和变异规律的研究，揭示生物的表现型与基因型之间的关系，探讨生命遗传规律的分子遗传学分支学科。从方法学的角度而言，反向遗传学是在获得生物基因信息的基础上，对基因进行修饰，如基因定点突变、基因插入/缺失和基因置换等，来研究生物基因结构和功能的策略。\n研究方法： 反向遗传学，又称DNA分析遗传学，常采用离体定向诱变技术来制造和分析突变。离体定向诱变与一般的诱变不同，通常所讲的自发突变和各种诱发突变都是在细胞内发生的，要先从表现型上的变化发现突变，继而分离突变体，再采用遗传学方法和其他方法进行分析，才能知道突变的性质。包括基因突变，基因置换，基因重组，基因敲除、基因沉默等方法。\n意义：与反向遗传学操作相关的各种技术统称为反向遗传学技术，包括RNA干扰（RNA interference，RNAi）技术、基因沉默技术、基因体外转录技术等，是DNA重组技术应用范围的扩展与延伸。随着基因组序列测定技术的日渐成熟，反向遗传学技术的应用将越来越广泛。\n目前反向遗传学技术已广泛应用于生命科学研究的各个领域，病毒研究方面的作用尤其明显，特别是RNA病毒研究方面。","四主题演讲材料提交#四、主题演讲材料提交":"","基因敲除#基因敲除":"指利用外源的已突变的基因通过同源重组的方法替换掉内源的正常同源基因，从而使内源基因失活而表现突变体的性状的技术或方法。它是针对某个序列已知但功能未知的序列，改变生物的遗传基因，令特定的基因功能丧失作用，从而使部分功能被屏蔽，并可进一步对生物体造成影响，进而推测出该基因的生物学功能。\n步骤：\n获得干细胞：用于打靶。 载体构建：把目的基因和与细胞内靶基因特异片段同源的DNA分子都重组到带有标记基因（如neo基因，TK基因等）的载体上，此重组载体即为打靶载体。 导入基因：将基因打靶载体通过一定的方式（常用电穿孔法）导入同源的胚胎干细胞（EScell）中，使外源DNA与胚胎干细胞基因组中相应部分发生同源重组，将打靶载体中的DNA序列整合到内源基因组中从而得以表达。 性状改变。 基因敲除就是通过同源重组将外源基因定点整合入靶细胞基因组上某一确定的位点，以达到定点修饰改造染色体上某一基因的目的的一种技术。它克服了随机整合的盲目性和偶然性，是一种理想的修饰、改造生物遗传物质的方法。这项技术的诞生可以说是分子生物学技术上继转基因技术后的又一革命。尤其是条件性、诱导性基因打靶系统的建立，使得对基因靶位时间和空间上的操作更加明确、效果更加精确、可靠，它的发展将为发育生物学、分子遗传学、免疫学及医学等学科提供了一个全新的、强有力的研究、治疗手段，具有广泛的应用前景和商业价值。基因敲除技术主要应用于动物模型的建立，而最成熟的实验动物是小鼠，对于大型哺乳动物的基因敲除模型还处于探索阶段。","基因水平转移#基因水平转移":"指生物将遗传物质传递给[其他细胞而非其子代]的过程，例如：接合、转导及转化。与此相对，“基因垂直传递\"指生物由其祖先继承遗传物质。它打破了亲缘关系的界限，使基因流动的可能变得更为复杂。\n接合：指两个细菌之间发生的一种遗传物质交换现象，属于细菌有性生殖的一个重要阶段。在接合现象发生时，两个细胞直接接合或者通过类似于桥一样的通道接合，并且发生基因的转移。 转导：因病毒（即噬菌体）入侵而将一个细菌的DNA片段转置到另一细菌中的过程，亦可指透过病毒载体把外来DNA带入到细菌中的过程。 转化：细胞通过摄取外源遗传物质(DNA或RNA)而发生遗传学改变的过程。 水平基因转移是细菌抗生素抗药性的主要原因，并且在细菌可降解新型化合物例如人类创建的杀虫剂进化中起著重要作用，并在进化，维护和传输毒性的重要原因。这种基因水平转移经常涉及温和的噬菌体和质粒。\n人工的基因水平转移属于基因工程的一种。","基因沉默#基因沉默":"泛指生物细胞藉各种基因表现调控机制抑制某一基因表现的现象，机制包括DNA甲基化、位置效应、基因组铭印、遗传印记、基因转应、重复序列诱导点突变、副突变、RNA静默（包括RNA干扰、基因压制与非配对DNA介导的减数分裂沉默等，由miRNA、siRNA、piRNA与rasiRNA等小RNA引导）和mRNA降解等，例如许多mRNA的3’非转译区（3’UTR）即可和miRNA结合，以降低mRNA的转译达成基因静默。\n生物研究中，研究人员为研究一个基因的功能，会以RNA干扰或CRISPR等基因敲落技术达成基因静默，大幅降低目标基因的表现以检测其影响，进而推得目标基因的功能。相较于基因剔除完全移除目标基因的表现，基因敲落后目标基因表现虽被抑制，但仍保有一定程度的表现量。\n总之，基因沉默是基因表达调控的一种重要方式，是生物体在基因调控水平上的一种自我保护机制，在外源DNA侵入、病毒侵染和DNA转座、重排中有普遍性。对基因沉默进行深入研究，可帮助人们进一步揭示生物体基因遗传表达调控的本质，在基因克服基因沉默现象，从而使外源基因能更好的按照人们的需要进行有效表达；利用基因沉默在基因治疗中有效抑制有害基因的表达，达到治疗疾病的目的，所以研究基因沉默具有极其重要的理论和实践意义。","多潜能干细胞#多潜能干细胞":"多能干细胞种类繁多，其中间充质干细胞是目前应用最广泛的干细胞类型，临床上主要应用于解决多种血液系统疾病、心血管疾病、神经系统疾病、自身免疫性疾病以及关节修复等。多能干细胞分为两类，一类称为多潜能干细胞，与全能干细胞相比，多潜能干细胞已经失去了发育成完整个体的能力，但仍具有分化为体内全部200多种类型细胞的潜能。如囊胚内细胞团细胞和原始生殖细胞。又如存在于人体骨髓中的造血干细胞，可以分化成红细胞、血小板、粒细胞和淋巴细胞等十种以上的血细胞，但它们都不能分化出体内全部类型的细胞。","大学生物学#大学生物学":"大学生物学下面内容非原创。","孤雌生殖#孤雌生殖":"也称单性生殖，即卵不经过受精也能发育成正常的新个体。\n与无性生殖不同，是由生殖细胞而非体细胞完成的繁殖现象。产生的个体多数为单倍体，或者是进行重组之后的2倍体，而非无性生殖产生的和母体遗传物质完全相同的个体，所以通常把孤雌生殖归类于有性生殖，而非无性生殖。\n均等分裂型孤雌生殖：即卵原细胞正常进行减数分裂，产生3个极体和1个卵细胞，其中卵细胞独立发育为后代个体的现象。（后代为单倍体） 卵核与极体融合型孤雌生殖：即卵原细胞正常进行减数分裂，产生3个极体和1个卵细胞，其中卵细胞与任意极体随机结合，形成\"极体-卵细胞-受精卵\"，并由此细胞发育成后代个体的现象（后代为2倍体） 孤雌生殖可以在环境适宜的情况下迅速扩大种群数量，但不能提高后代的基因多样性。","干细胞与癌细胞#干细胞与癌细胞":"干细胞是一类具有无限的或者永生的自我更新能力的细胞、能够产生至少一种类型的、高度分化的子代细胞。 癌细胞，是一种变异的细胞，是产生癌症的病源。癌细胞与正常细胞不同，有无限增殖、可转化和易转移三大特点，能够无限增殖并破坏正常的细胞组织。癌细胞除了分裂失控外（能进行无限分裂），还会局部侵入周围正常组织甚至经由体内循环系统或淋巴系统转移到身体其他部分。癌细胞分恶性和良性两种。\n肿瘤干细胞与正常干细胞同属于干细胞范畴，具有许多相同的特点：\n均处于未分化状态，具有自我更新和多向分化潜能，增殖的同时可诱导血管形成。 都具有对称分裂和不对称分裂两种分裂方式。 具有相似的调节生长的信号通路。 两者都具有端粒酶活性和扩增端粒酶重复序列，而人类终末分化体细胞不具有端粒酶活性。 肿瘤干细胞并不等同于正常干细胞、有其自身特性：\n干细胞自我更新具有负反馈调节机制，其增殖与分化处于平衡状态，是有序的。而肿瘤干细胞的这种负反馈机制已被破坏，其增殖分化是无序和失控的。 与正常干细胞相比，肿瘤干细胞分化成熟能力缺乏，因此肿瘤细胞往往是低分化的。 肿瘤干细胞具有积累复制错误的倾向，而正常干细胞可通过多种途径防止这种情况发生。 两者某些信号传导通路不同，可作为治疗的靶点，如肿瘤抑制蛋白通路在正常造血干细胞的自我更新和白血病的形成中具有不同的作用途径。","微量元素#微量元素":"约有70种，指的是在人体中含量低于人体质量0.005%~0.01%的元素。包括铁、铜、锰、锌、钴、钼、铬、镍、钒、氟、硒、碘、硅、锡。微量元素在人体内含量虽然极微小，但具有强大的生物学作用，它们参与酶、激素维生素和核酸的代谢过程，其生理功能主要表现为协助输送宏量元素；作为酶的组成成分或激活剂；在激素和维生素中起独特作用;影响核酸代谢等。","性别决定基因#性别决定基因":"决定个体性别发育的基因被称为性别决定基因。最主要的是Y染色体短臂末端的SRY基因，这个基因决定了男性的性发育，如果由于染色体易位等原因如果女性核型的人（女性核型为46,XX，男性核型为46,XY）携带了SRY基因，就会表现出男性的性特征，如果男性的SRY基因出了问题，就会表现出女性的性特征，也就是所谓的假两性畸形。","提交要求#提交要求":"文件类型： 演示文稿（PPT格式） 讲解文稿（Word文档） 内容规范： PPT需包含完整演讲框架 Word文档需详细说明各幻灯片讲解要点 提交方式： 通过\"学在浙大\"平台提交 截止时间：演讲结束后48小时内","构件生物#构件生物":"由一个合子发育而成的由一套相似的构件组成个体的一类生物。\n在其生长发育的各个阶段，可通过其基本的结构单位的反复形成得到进一步发育，其组织、器官等各个部分是可以改变的。由构件生物组成的种群，受精卵首先发育成一整套构体或构件，然后发育成更多的构件，形成分支结构发育的形式和时间是不可预测的。如大多数高等植物、海绵、水螅和珊瑚是构件生物，构件包括叶子、芽和茎，花也是一种类型的构件。\n构件生物是生物有机体结构对环境的适应，如果说对于单体生物以个体数就能反映种群大小，那么对于构件生物就必须进行多个层次的数量统计，即从合子产生的个体数（它与单体生物的个体数相当）和组成每个个体的构件数。只有同时有这数个层次的数量及其变化，才能掌握构件生物的种群动态。","核外遗传#核外遗传":"又称细胞质遗传。指由核外（细胞质）的遗传物质所控制的遗传。真核细胞生物在核外的细胞器，如线粒体、叶绿体中也存在少量的DNA，分别称为线粒体DNA (mtDNA)和叶绿体DNA (ctDNA)，这些遗传单位统称为核外遗传因子。\n核外遗传因子存在于线粒体和叶绿体基因组中，他们能够自主复制，其遗传行为的传递不按核基因的方式进行，也不出现相应的分离比，故又称之为非孟德尔式遗传。核外遗传的特点是：细胞器基因组通过细胞质由一代传给另一代；亲本等位基因的分离比是4:0，而不同于细胞核基因中的2:2分离比；正反交的结果不同，杂交子代的某些性状只具有母本的表现型。\n核外遗传有五项主要特点：\n正反交的结果不同。核外基因通常显示逐代出现单亲遗传（uniparental inheritance）的现象，即所有的后代不论雌雄都只有一个亲体的某一表型（此和性连锁是不同的）。最典型的是高等真核生物中，仅母亲的性状在后代中得到表达，这种现象称母体遗传（maternal inheritance）。这是由雌性配子一卵中含有大量的细胞质，而雄性配子一精子几乎不含细胞质，因此合子的细胞质基本上来自母本，而细胞质中含有大量的细胞器，如线粒体和叶绿体等。有的细胞器带有核外基因，形成了母体遗传。核基因的正反交第一代结果相同都显示显性亲本性状，性-连锁基因正反交第一代结果不同；性状和性染色的组合有关；核外基因正反交第一代结果不同，但F1都显示母本的情况，和显隐性及性染色体的组合无关。 不出现分离比，即非孟德尔式遗传； 母本的表型决定了所有F1代的表型； 遗传物质在细胞器上，不受核移植的影响； 不能进行遗传作图。 核外遗传的分类：\n线粒体遗传：如酵母菌的小菌落突变。 叶绿体遗传：如衣藻的叶绿体遗传。 细胞内敏感性物质的遗传：如草履虫放毒性的遗传，果蝇的感染性遗传。 母体影响：包括短暂的母体影响和持久的母体影响。（由于母体中核基因的某些产物积累在卵母细胞地细胞质中,使子代表型不由自身的基因型所决定而出现与母体表型相同的遗传现象,称母体影响）","母源基因#母源基因":"受精前即卵子发育时母体细胞转录的RNA甚至翻译的蛋白质，产物定位在成熟的卵的细胞质中。母体基因产物量和活性形成空间差异并激活不同的合子基因，使躯体轴线不同区域里呈现一定的活性谱。\n在卵子发生（oogenesis）过程中表达，并将其产物（mRNA或蛋白质）储存在卵母细胞中的基因称为母源基因。在胚胎发育的决定中起重要作用。","激素#激素":"激素是高度分化的内分泌细胞合成并直接分泌入血的化学信息物质，它通过调节各种组织细胞的代谢活动来影响人体的生理活动。\n内分泌细胞产生的可以影响机体内其他细胞活动的化学物质。仅需很小剂量的激素便可以改变细胞的新陈代谢。\n激素的种类较多而数量极微(多数为毫微克甚至微微克水平)，它既非机体的能量来源又非组成机体的结构物质，但通过传递信息，在协调新陈代谢、生长发育等生理过程方面充当了重要的角色。激素的传递方式主要有三种：大多数激素分泌后直接进入血液，随血液循环到达一定的组织细胞才发挥作用，这种细胞叫靶细胞，靶细胞上有具特殊立体构型的物质(激素受体)与相应的激素结合，并识别激素所携带的信息，把它转化为细胞内一系列复杂的化学反应，从而产生特定的生理效应。这种方式的激素要随血流到达靶细胞，所以叫\"远距分泌\"。有些激素分泌出来以后通过细胞间隙液就近扩散，作用于邻近细胞(如某些消化道激素)，这种方式叫\"旁分泌\"。还有一些激素是由神经细胞(如下丘脑)分泌的，叫\"神经激素\"，沿轴突借轴浆流动而到达靶细胞，这种方式叫\"神经分泌\"。激素按其化学本质可分为含氮的蛋白类激素(由氨基酸、肽、蛋白衍生而成)和**类固醇类激素两大类；而就其生理功能来说可分为三大类：一类是调控机体新陈代谢和维持内环境相对稳定的，如胰岛素、胃肠激素、甲状旁腺激素等；一类是促进细胞增殖分化，控制机体生长发育和生殖机能，并影响其衰老过程的，如生长激素、性激素等；还有一类与神经系统密切配合，增强机体对环境的适应，如肾上腺皮质激素和垂体激素等。激素分泌量过多或过少都会引起机体功能的紊乱**，所以临床上常以激素水平的测定做为诊断某些疾病的依据，并将许多激素做为治疗药物应用于临床。","生态位#生态位":"生态位是一个物种所处的环境以及其本身生活习性的总称。每个物种都有自己独特的生态位，借以跟其他物种作出区别。生态位包括该物种觅食的地点，食物的种类和大小，还有其每日的和季节性的生物节律。\n特征：生态位的环境因素（温度，食物，地表湿度，生存环境等）的综合，构成概念生态位空间。这是一种n维超体积，但出于可视化的原因会将它简化为二维或三维龛位图进行显示。每种环境因素成为一个维度。在两个生态龛位中，考虑观察的维度越多，两个生态龛位的差别就越明显，越容易被区分开来。 生态位的生物学意义包括：\n帮助我们理解物种如何在其环境中生存和繁殖。 帮助我们预测物种如何对其环境和其他物种产生影响。 帮助我们设计保护和管理生态系统的策略。 帮助我们理解为什么某些物种会灭绝，而其他物种却能够繁衍生息。","生态幅#生态幅":"指某一生物对环境因子的耐受范围，即其生态上的最高点与最低点之间的范围。主要由该物种的遗传特性决定。\n生态幅度是有机体或有机体的某一生理过程，对生态环境或其中一个或多个生态因素变化的适应范围。生态幅度在种内、种间不同，同一个体的不同生育阶段，生态幅度也不同。\n在生态最适域（最适范围）中，生物生活能力及数量达到最高水平，由于需求能得到满足，生物对该生态因子的变化不敏感；一旦超出最适范围，该生态因子的轻度变化即可引起生物生活能力及分布数量的剧烈变化。\n根据生物对各种生态因子适应的生态幅度，可分为很多类型，对温度因子的有狭温性和广温性；对光因子的有狭光性和广光性；对水因子的有狭水性和广水性；对盐因子的有狭盐性和广盐性；对湿度因子的有狭湿性和广湿性；对食物因子的有狭食性和广食性；对栖息地有狭栖性和广栖性等。而且，同一生物在不同发育阶段，其生态幅度不相同。还可分为广生型生物，狭生性生物。","生态足迹#生态足迹":"指能够持续地提供资源或消纳废物的、具有生物生产力的地域空间，维持一个人、地区、国家的生存所需要的或者指能够容纳人类所排放的废物的、具有生物生产力的地域面积。\n生态足迹越大，证明其对生态的破坏越严重。生态足迹既反映了个人或地区的资源消耗强度，又反映了某个区域的资源供给能力和资源消耗总量。\n意义： 生态足迹分析方法首先通过引入生态生产性土地概念实现了对各种自然资源的统一描述，其次通过引入等价因子和生产力系数进一步实现了各国各地区各类生态生产性土地的可加性和可比性。这使得生态足迹分析具有广泛的应用范围，既可以计算个人、家庭、城市、地区、国家乃至整个世界这些不同对象的生态足迹，对它们的足迹进行纵向的、横向的比较分析。\n总之，生态足迹分析指标为度量可持续性程度提供了一杆\"公平秆”，它能够对时间、空间二维的可持续性程度做出客观量度和比较，使人们能明确知晓现实距离可持续性目标尚有多远，从而有助于监测可持续方案实施的效果。另外，生态足迹计算具有很强的可复制性。这使得将生态足迹计算过程制作成一个软件包成为可能，从而可以推动该指标及方法的普及化。","生活史#生活史":"生活史是在演化生物学以及生态学上是指有关生物体生活史及生命周期的理论。经过长时间的演化，不同的生物体往往会在繁殖期以及繁殖行为、寿命等方面取得一定平衡，拥有从出生、生长、繁殖，再到死亡的独特生活史。\n根据生活史理论，生物体的生活史有7个最重要的特征（trait）：出生时的大小；生长模式；成熟时的年龄和大小；后代的性别比例、大小，以及数量；繁殖投资（按大小和年龄）；死亡时间分布（按大小和年龄）；寿命。\n为了适应生存的环境，生物体会拥有具独特特征的生活史。这个过程称为\"权衡\"（trade-off），比如，生物体在进化上会倾向于获得最大的体型或最大的寿命，或倾向于繁殖更多后代或提升后代的生存率。在生活史中的繁殖上，会分为两种模式：r模式和K模式。\nr模式：种群密度很不稳定，通常出生率高、寿命短、个体小，缺乏保护后代的机制，子代死亡率高、有较强的扩散能力，适应于多变的栖息生境。 K模式：其种群密度比较稳定，经常处于K值周围。这类动物通常出生率低、寿命长、个体大多具有较完善的保护后代的机制，子代死亡率低多不具较强的扩散能力，它们适应于稳定的栖息生境。","生物节律#生物节律":"生物周期现象又叫生物节律（biological clock），亦称\"生物钟”。有机体内部发生的周期性变化过程。最典型的例子是睡眠与觉醒的周期性交替。交替之所以表现出准确的周期性，一方面决定于身体器官和细胞机能状态的周期性变化，另一方面是环境的影响，如太阳和月亮升起、下落的周期变化。太阳、月亮的运动规律直接影响地球上生物的周期性变化，如谷物随季节生长；候鸟随潮汛变化而迁栖（潮汛是月亮对海洋的引力引起的）；人的体温在一昼夜内出现周期性升降等。生物钟现象使人在一天的不同时刻工作效率不一。骤然改变正常的昼夜周期（如从地球的东部迁到西部居住）会使人的机体功能混乱，情绪烦躁，工作效率降低。\n即生物钟，是由生物体生命活动所产生的一种内在节律。人体生物节律是指体力节律、情绪节律和智力节律。 每个人从他出生那天起一直到生命终结,都存在着体力23天、情绪28天、智力33天的**周期性波动规律**,称为人体生物节律。每一个周期中又存在着高潮期、低潮期和临界期。 由于它具有准确的时间性，因此，也称之为人体生物钟。在我们日常生活中，有人会觉得自己的体力、情绪或智力有时很好，有时很坏，人从他诞生之日起，直至生命终结，其自身的体力、情绪和智力都存在着由强至弱、由弱至强的周期性起伏变化。人们把这种现象称作生物节律，或生物节奏、生命节律等。产生这种现象的原因是生物体内存在着生物钟，它自动地调节和控制着人体的行为和活动。\n意义：是分析事故原因、预防事故发生的有力措施；利用生物节律规律来指导运动训练是十分重要的；同时也可以根据节律研发药物治疗疾病。","疫苗#疫苗":"是指由免疫原性物质制成的用于预防接种的生物制品，可以诱导宿主产生特异性、主动性、保护性的宿主免疫。疫苗保留了病原菌刺激动物体免疫系统的特性。当动物体接触到这种不具伤害力的病原菌后，免疫系统便会产生一定的保护物质，如免疫激素、活性生理物质、特殊抗体等；当动物再次接触到这种病原菌时，动物体的免疫系统便会依循其原有的记忆，制造更多的保护物质来阻止病原菌的伤害。疫苗分为减毒疫苗、灭活疫苗、亚单位疫苗、核酸疫苗、基因工程疫苗等。疫苗接种的意义在于接种后机体摆脱相应的传染病或不受感染,人工接种的疫苗和注射的特异性免疫物质,都属于免疫制剂。可以对一些引起传染性疾病的病毒或细菌产生免疫力，提高抗病能力，减少或消灭传染病的传播，为人类创造一个健康的生存环境。","盖亚假说#盖亚假说":"盖亚假说认为，地球上的生物圈就是可以维持地球内稳态的一个活跃的自适应控制系统。这样的一个可以自我调节的整体（包括但不限于生物）就被视为盖亚。盖亚假说指在生命与环境的相互作用之下，能使得地球适合生命持续的生存与发展。也就是说，生命不是为了自己才让地球变得宜居。“这种调节\"让地球更适合生存，是包括生命、空气、海洋和岩石在内的整个进化系统的特点”。\n把地球比作一个自我调节的有生命的有机体。但这并不意味着世界是有生命的，而是说明生命体与自然环境——包括大气、海洋、极地冰盖以及我们脚下的岩石——之间存在着复杂连贯的相互作用。\n意义：全球生态环境恶化是人类当今面临的最严重的问题之一。盖亚假说启示人们，环境问题是涉及整个地球生态系统的问题，要解决这个问题不仅需要用系统的或整体的观点和方法来认识人类生产和生活方式对生态环境影响，而且需要人类共同行动。同时，盖亚假说也从道义上启示人们，包括人类在内的所有生物都是地球母亲的后代，人类既不是地球的主人，也不是地球的管理者，只是地球母亲的后代之一。因此，人类应该热爱和保护地球母亲，并与其他生物和睦相处。","神经胚形成#神经胚形成":"神经胚形成：胚胎在3个胚层建立之后进入神经胚形成阶段，形成[脑和脊髓的原基-神经管](neural tube)，是脊椎动物器官形成的前奏。随后各种器官原基相继形成。\n神经胚（neurula）亦称髓胚。主要是指脊椎动物 的发生中在原肠形成（或者原条期）之后，从出现中枢神经系统原基的神经板时期开始，到神经板闭合变成神经管期间的胚胎。在这一时期外胚层分化出神经板及表皮区，在神经板之下的为头肠、前索板、脊索原基、预定体节的位置，在多数情况下，表皮区的外胚层被侧板的体壁中胚层反包着。主要器官原基在各胚层已开始出现，但组织分化尚未开始。","线粒体疾病#线粒体疾病":"线粒体病是遗传缺损引起线粒体代谢酶缺陷，致使ATP合成障碍、能量来源不足导致的一组异质性病变。线粒体脑肌病的不同类型发病年龄不同。\n线粒体是密切与能量代谢相关的细胞器，无论是细胞的成活（氧化磷酸化）和细胞死亡（凋亡）均与线粒体功能有关，特别是呼吸链的氧化磷酸化异常与许多人类疾病有关。根据线粒体病变部位不同可分为：线粒体肌病，线粒体病变侵犯骨骼肌为主；线粒体脑肌病，病变同时侵犯骨骼肌和中枢神经系统。\n病因：线粒体是细胞内提供能量的细胞器，人类mtDNA是长16569bp的环状双链分子，分轻链和重链，含37个基因，主要编码呼吸链及与能量代谢有关的蛋白。mtDNA缺失或点突变使编码线粒体氧化代谢过程必需的酶或载体发生障碍，糖原和脂肪酸等不能进入线粒体充分利用和产生足够的ATP，导致能量代谢障碍和产生复杂的临床症状。","细胞免疫#细胞免疫":"细胞免疫又称细胞介导免疫。狭义的细胞免疫仅指**T细胞介导的免疫应答**，即T细胞受到抗原刺激后，分化、增殖、转化为致敏T细胞，对抗原的直接杀伤作用及致敏T细胞所释放的**细胞因子的协同杀伤作用。T细胞介导的免疫应答的特征是出现以单核细胞浸润为主的炎症反应和/或特异性的细胞毒性。广义的细胞免疫还应该包括原始的吞噬作用以及NK细胞介导的细胞毒作用。细胞免疫是清除细胞内寄生微生物的最为有效的防御反应**，也是排斥同种移植物或肿瘤抗原的有效手段。\n细胞免疫的产生也分为感应、反应和效应三个阶段。其作用机制包括两个方面：\n致敏T细胞的直接杀伤作用。当致敏T细胞与带有相应抗原的靶细胞再次接触时，两者发生特异性结合，产生刺激作用，使靶细胞膜通透性发生改变，引起靶细胞内渗透压改变，靶细胞肿胀、溶解以致死亡。致敏T细胞在杀伤靶细胞过程中，本身未受伤害，可重新攻击其他靶细胞。参与这种作用的致敏T细胞，称为杀伤T细胞。 通过**淋巴因子相互配合**、协同杀伤靶细胞。如皮肤反应因子可使血管通透性增高，使吞噬细胞易于从血管内游出；巨噬细胞趋化因子可招引相应的免疫细胞向抗原所在部位集中，以利于对抗原进行吞噬、杀伤、清除等。由于各种淋巴因子的协同作用，扩大了免疫效果，达到清除抗原异物的目的。","细胞凋亡与动物发育#细胞凋亡与动物发育":"细胞凋亡（apoptosis）指为维持内环境稳定，由**基因控制的细胞自主的有序的死亡**。细胞凋亡与细胞坏死不同，细胞凋亡不是一件被动的过程，而是主动过程，它涉及一系列基因的激活、表达以及调控等的作用，它并不是病理条件下，自体损伤的一种现象，而是为更好地适应生存环境而主动争取的一种死亡过程。它涉及染色质凝聚和外周化、细胞质减少、核片段化、细胞质致密化、与周围细胞联系中断、内质网与细胞膜融合(最终细胞片段化形成许多细胞凋亡体)被其他细胞吞入。\n细胞凋亡的意义：由基因控制细胞有目的，有选择性的自我消亡过程，这种淘汰机制是保证生命进化的基础。生物适应环境的需要，使各种细胞的生存时限达到平衡。 发育：一个细胞 (受精卵 zygote) 不断分裂和分化, 即一个有机体从其生命开始到性成熟的变化过程称为发育。它是从单个细胞演变成复杂的多细胞生物的过程, 是生命现象的发展、生物有机体以遗传信息为基础的自我构建和自我组织的过程, 是基因按照特定的时间和空间选择性表达并逐步转化为特征表型的过程。\n动物发育的主要特征: 具有严格的时间和空间的次序性，这种次序性由发育的遗传程序控制。 发育是机体各细胞协同作用的结果, 是一系列基因网络性控制的结果。细胞分裂、分化、迁移、凋亡、生长、衰老和死亡等。","营养级#营养级":"生物在食物链之中所占的位置。\n营养级的生物学意义：\n揭示生物之间的相互关系，帮助我们理解生态系统的运作 帮助我们预测生态系统对外界变化的响应 帮助我们设计保护和管理生态系统的策略 为生物多样性的研究提供重要的理论框架","蛋白质折叠#蛋白质折叠":"蛋白质获得其功能性结构和构象的过程。通过这一物理过程，蛋白质从无规则卷曲折叠成特定的功能性三维结构。在从mRNA序列翻译成线性的肽链时，蛋白质都是以去折叠多肽或无规则卷曲的形式存在。\n蛋白质的基本单位为氨基酸，而蛋白质的一级结构指的就是其氨基酸序列。蛋白质会由所含氨基酸残基的亲水性、疏水性、带正电、带负电等特性通过残基间的相互作用而折叠成一立体的三级结构。一般三级才有生物活性。","蛋白质组学#蛋白质组学":"蛋白质组：指的是一个生物体可以表达的所有蛋白质。每个物种都有自己的、独特的蛋白质组。与基因组（每个生物体内的全套基因）不同，蛋白质组的组成随着时间和整个生物体的变化而不断变化。因此，当科学家提到蛋白质组时，他们有时也是指某一特定时间点的蛋白质组（如胚胎与成熟生物体），或指生物体内某一特定细胞类型或组织的蛋白质组。 蛋白质组学：是对蛋白质组的研究，研究不同的蛋白质之间如何相互作用以及它们在生物体内发挥的作用。 通过研究在大规模水平上研究蛋白质的特征，包括蛋白质的表达水平，翻译后的修饰，蛋白与蛋白相互作用等，由此获得蛋白质水平上的关于疾病发生，细胞代谢等过程的整体而全面的认识。\n在蛋白质组学中，有多种方法可以研究蛋白质。 通常，可以通过使用抗体（免疫测定）或质谱法检测蛋白质。 如果分析复杂的生物样品，需要在定量斑点印迹分析(quantitative dot blot analysis) (缩写：qdb) 中使用非常特异的抗体，或者在检测步骤之前需要使用生化分离，因为样品中的分析物太多而无法进行 准确的检测和量化。\n不同的mRNA翻译成不同的蛋白质，而且很多蛋白质被翻译后也在细胞中会有非常多样的化学修饰。这些化学修饰都对蛋白质的功能非常关键。例如磷酸化修饰，甲基化修饰，乙酰基化修饰，糖基化修饰，氧化修饰，硝基化修饰等。\n意义：","趋同演化#趋同演化":"源自不同祖先的生物，由于相似的生活方式，整体或部分形态结构向着同一方向改变。\n趋同是指两种或两种以上亲缘关系甚远的生物，由于栖居于同一类型的环境之中，从而演化成具有相似的形态特征或构造的现象。\n机制：在形态学中，不同物种身处相似环境或生活习性相近时，可能会演化出同功特征。占据相似的生态位（也就是说有独特的生活方式），物种面临的类似问题可能会引出类似的解决方案。 飞行的多次演化是趋同演化的典型案例，带翅昆虫、鸟类、翼龙和蝙蝠都各自独立演化出了飞行能力。由趋同演化产生的相似特征称为同功，而有共同起源的结构或特征则被称为同源，不同生物之间同源特征可能具备不同的功能。鸟类、翼龙、蝙蝠的翅膀是同功结构，负责达成飞行这一能力，而其前肢是同源结构，尽管功能不同，但有共同的起源。","转基因生物#转基因生物":"通过分子生物学技术，人为的分离或修饰具有特定遗传性状的优质目的基因，将其通过特定途径导入到靶标生物体的基因组中，由于导入基因的表达会引起靶标生物体的特定性状发生可遗传的修饰改变，从而达到改造生物特性、获得新的生物体的目的，这种技术称之为人工转基因技术。\n也指能够在不导入外源基因的情况下，通过对生物体本身遗传物质的加工、敲除、屏蔽等方法也能改变生物体的遗传特性，获得人们希望得到的性状。“基因修饰生物”。","遗传标记#遗传标记":"指在遗传分析上用作标记的基因，也称为标记基因。指可追踪染色体、染色体某一节段、某个基因座在家系中传递的任何一种遗传特性。它具有两个基本特征，即可遗传性和可识别性，因此生物的任何有差异表型的基因突变型均可作为遗传标记。\n包括形态学标记(morphological marker，肉眼可见的或仪器测量动物的外部特征 )、细胞学标记(cytological marker，对处理过的动物个体染色体数目和形态进行分析)、生物化学标记(biochemical marker，以动物体内的某些生化性状为遗传标记)、免疫学标记(Immune Genetic Markers，以动物的免疫学特征为遗传标记)和分子标记(molecular marker，以个体间遗传物质内核苷酸序列变异为基础的遗传标记)五种类型。\nWiki定义：遗传标记（英语：genetic marker）是已知在染色体上位置的一种基因或DNA序列，可被用于鉴定生物个体或物种。其可被描述为可观测变异（可因位于基因组基因座中的突变或改变而产生）。遗传标记可能是一段短的DNA序列，例如单个碱基对改变的周围序列（单核苷酸多态性，SNP）或一段长序列，例如小卫星序列。","酶#酶":"由活细胞产生的、对其底物具有高度特异性和高度催化效能的蛋白质或RNA。\n酶具有高效率的催化能力；其效率是一般无机催化剂的10的7次幂～10的13次幂。 酶具有专一性；（每一种酶只能催化一种或一类化学反应。） 酶在生物体内参与每一次反应后，它本身的性质和数量都不会发生改变（与催化剂相似）； 酶的作用条件较温和。 酶所催化的化学反应一般是在比较温和的条件下进行的。 在最适宜的温度和PH条件下，酶的活性最高。温度和PH偏高或偏低，酶活性都会明显降低。一般来说，动物体内的酶最适温度在35～40℃之间；植物体内的酶最适温度在40～50℃之间；动物体内的酶最适PH大多在6.5～8.0之间，但也有例外，如胃蛋白酶的最适PH为1.5；植物体内的酶最适PH大多在4.5～6.5之间。 过酸、过碱或温度过高，会使酶的空间结构遭到破坏，使酶永久失活。0℃左右时，酶的活性很低，但酶的空间结构稳定，在适宜的温度下酶的活性可以升高。 活性可调节性。 有些酶的催化性与辅因子有关。 易变性：大多数酶都是蛋白质，因而会被高温、强酸、强碱等破坏。 酶对化学反应的催化效率称为酶活性。","酿酒微生物与风味#酿酒微生物与风味":"利用微生物发酵生产含一定浓度酒精饮料，其风味随酿酒微生物的不同而各异。酿酒微生物按照其功能,大致可以分为三类：\n糖化菌：把淀粉等营养基质分解为可发酵性糖的微生物菌群,主要为霉菌。（霉菌代谢产生的淀粉水解酶在酿酒过程中作为酿酒发酵中的糖化剂，将酿酒原料的主要成分——淀粉,分解为短链糊精和可发酵性糖(葡萄糖、果糖等),提供给发酵过程中不能直接利用淀粉的微生物,作为前驱物质被利用,使整个酿酒发酵过程得以顺利进行。） 发酵菌：利用葡萄糖等小分子营养物质进一步分解代谢,生成以乙醇为主的小分子发酵产物的微生物菌群,主要为酵母菌类。 生香菌：利用各种营养基质,生成酯、酸、醇、醛等丰富复杂的香味成分的微生物菌群,主要是各种来源的细菌。 研究酿酒微生物群落结构及其生物代谢过程，对于揭示酒的风味形成、品质提升以及工艺优化都具有重要意义。目前，研究人员利用各类微生物分析技术，使其更好地服务于酿酒工业中。","饮食模式#饮食模式":"指饮食中不同食物的数量、比例、种类或者组合，以及习惯性消费的频率。膳食模式不仅反映人们的饮食习惯和生活水平的高低，同时也反映一个民族的传统文化，一个国家的经济发展和一个地区的环境和资源等多方面的情况。\nDASH饮食是由1997年美国的一项大型高血压防治计划发展出来的饮食，在这项计划中发现，饮食中如果能摄食足够的蔬菜、水果、低脂(或脱脂)奶，以维持足够的钾、镁、钙等离子的摄取，并尽量减少饮食中油脂量(特别是富含饱和脂肪酸的动物性油脂)，可以有效地降低血压。因此，常以DASH 饮食来作为预防及控制高血压的饮食模式。 地中海饮食泛指希腊、西班牙、法国和意大利南部等处于地中海沿岸的南欧各国以蔬菜水果、鱼类、五谷杂粮、豆类和橄榄油为主的饮食风格。研究发现地中海饮食可以减少患心脏病的风险，还可以保护大脑免受血管损伤，降低发生中风和记忆力减退的风险。有数据统计，地中海地区居民平均寿命高居于全球之冠，且生活质量较高。现在也用\"地中海式饮食\"代指有利于健康的，简单、清淡以及富含营养的饮食。"},"title":"大学生物学"},"/docs/undergraduate/%E9%80%9A%E8%AF%86%E6%9D%82%E9%A1%B9/%E6%80%A7%E4%B8%8E%E7%94%9F%E6%AE%96/":{"data":{"产生早恋的原因如何看待早恋#产生早恋的原因？如何看待早恋？":"早恋，即青少年时期的早期恋爱，其产生的原因多样且复杂：\n生理成熟：青春期的生理变化导致性激素水平上升，促进了性意识和对异性的兴趣。 心理发展：青少年期是个体探索自我、建立个人身份的阶段，恋爱被视为成长的一部分。 社会和文化因素：媒体和流行文化对恋爱的浪漫化，以及社交媒体的影响。 家庭影响：过度保护或疏远的家庭环境可能促使青少年通过早恋寻求情感支持。 同伴压力：在某些社交圈中，恋爱可能被视为成熟或受欢迎的标志。 对待早恋的态度应当是理解和平衡的：\n理解和尊重：理解青少年的感受和需求，尊重他们的个人发展过程。 引导和教育：提供适当的性教育和情感教育，帮助青少年理解恋爱关系的责任和界限。 沟通和支持：家长和教育工作者应该与青少年保持开放的沟通，提供指导和支持。 关注和干预：如果早恋影响了学习或生活，或者存在不健康的关系模式，则应及时干预。","人类生命的起点是受精卵还是新生命诞生之日为什么#人类生命的起点是受精卵还是新生命诞生之日？为什么？":"这是一个哲学、科学、宗教和伦理学的复杂议题，不同领域有不同观点：\n生物学观点：生命的起点通常被认为是受精时刻，即精子和卵子结合形成受精卵的时刻。此刻起，受精卵拥有独特的遗传物质，开始了连续的生长和发展。 发育生物学观点：强调从受精到出生的整个连续过程，认为生命是一个逐步形成的过程。 伦理和法律观点：不同的文化、宗教和法律体系对生命起点的定义各不相同。一些认为始于受精，另一些则可能认为始于胎儿具有独立生存能力的某个阶段，或直到出生。 个人和文化观点：个人的信仰、文化背景和哲学观念也会影响对这个问题的看法。","你认为性与生殖之间是怎样的关系#你认为性与生殖之间是怎样的关系？":"性与生殖之间的关系是深刻且复杂的，可以从不同的角度进行探讨：\n生物学角度：性行为是许多生物体（包括人类）的生殖过程的一部分，主要目的是传递遗传信息。从进化角度看，性繁殖允许基因的混合和多样性，利于物种适应和进化。 社会和文化角度：在人类社会中，性不仅仅是生殖的工具，也是表达爱、亲密和情感联系的方式。文化、宗教和个人信仰在定义性与生殖关系方面也起着重要作用。 心理和情感角度：性行为在心理层面上承载着多重意义，包括爱、情感的交流、信任和快乐等，这些方面往往与生殖的生物学目的相独立。 健康和生理角度：性行为对于个体的身心健康是有益的，它不仅带来生理快感，还能增进情感上的亲密和满足感。 总的来说，性与生殖虽然在生物学上紧密相关，但在人类社会中，性的意义远远超出了生殖的范畴。","受精与妊娠#受精与妊娠":"单选(2分) 关于受精位置的正确陈述是哪一个？ A. 输卵管伞部【错误】 B. 输卵管峡部和间质部的交界处【错误】 C. 输卵管间质部【错误】 D. 输卵管峡部和壶腹部的交界处【正确】\n单选(2分) 关于威胁性流产的以下哪个陈述是错误的？ A. 规律性子宫收缩【正确】 B. 宫颈管变平【正确】 C. 宫颈扩张【错误】 D. 出血【正确】\n单选(2分) 精子激活发生在哪里？ A. 子宫腔和输卵管【正确】 B. 输卵管峡部和间质部的交界处【错误】 C. 输卵管间质部【错误】 D. 宫颈【错误】\n单选(2分) 足月婴儿的定义是什么？ A. 出生于2836周的婴儿【错误】 B. 出生于3741+6周的婴儿【正确】 C. 出生于2838周的婴儿【错误】 D. 出生于2636+6周的婴儿【错误】\n单选(2分) 越期妊娠的定义是什么？ A. 妊娠持续≥41周【错误】 B. 妊娠持续≥40周【错误】 C. 妊娠持续≥42周【正确】 D. 妊娠持续≥39周【错误】\n单选(2分) 初产妇无硬膜外麻醉的第二产程应不超过 A. 1小时【错误】 B. 2小时【错误】 C. 3小时【正确】 D. 1.5小时【错误】\n单选(2分) 威胁性流产中子宫收缩的特点是什么？ A. 收缩通常持续≥30秒，间隔5-6分钟【正确】 B. 收缩通常持续≥15秒，间隔3-5分钟【错误】 C. 收缩通常持续≥40秒，间隔3-4分钟【错误】 D. 收缩通常持续≥15秒，间隔3-4分钟【错误】\n多选(2分) 决定分娩的因素包括 A. 劳动力【正确】 B. 生产道【正确】 C. 胎儿【正确】 D. 社会/心理因素【正确】\n多选(2分) 第一孕期可能出现的症状包括 A. 绝经【正确】 B. 胃肠反应【正确】 C. 少尿【错误】 D. 蒙哥马利腺肿胀【正确】\n多选(2分) 胎盘在妊娠期间的功能包括 A. 呼吸，允许氧气和二氧化碳交换【正确】 B. 营养和排泄含氮废物【正确】 C. 防御，作为有效屏障【正确】 D. 合成胎儿血液【错误】\n判断(2分)\n受精指的是男女配子融合，形成一个具有父母遗传背景的新个体的过程。【正确】 只有发生顶体反应的精子才能与次级卵母细胞融合。【正确】 卵子在精子进入卵母细胞（受精）后完成第一次减数分裂并形成女性原核。【错误】","哪些不良的生活方式会损害性健康和生殖健康#哪些不良的生活方式会损害性健康和生殖健康？":"不良生活方式：\n不健康的饮食：过多摄入高脂肪、高糖或加工食品。 吸烟和酗酒：吸烟与勃起功能障碍、精子质量下降相关；过量饮酒可能导致性欲降低。 药物滥用：某些非法药物和处方药可能影响性功能和生殖能力。 缺乏运动：与多种健康问题相关，包括性健康问题。 压力和精神压力：长期的压力和焦虑可能影响性欲和生殖健康。 不良睡眠习惯：睡眠不足或质量差可能影响性激素的产生。 日常护理注意事项：\n均衡饮食：保持营养均衡。 规律运动：维持健康的体重和良好的血液循环。 戒烟限酒：避免或减少烟酒摄入。 管理压力：学习压力管理技巧，如冥想、瑜伽等。 保证充足睡眠：保持良好的睡眠习惯。 定期健康检查：定期进行健康检查，特别是生殖健康检查。 安全性行为：采取措施预防性传播疾病和意外怀孕。","性与生殖#性与生殖":"性与生殖","性与生殖健康维护#性与生殖健康维护":"单选(2分) 以下哪项是正确的性观念？ A. 老年人不需要性【错误】 B. 青少年性生活不需要避孕【错误】 C. 避孕套是安全有效的避孕方法【正确】 D. 女性一旦开始月经就可以进行性行为【错误】\n单选(2分) 关于性和生殖健康，以下哪项是错误的？ A. 没有性生活就没有妇科疾病【正确】 B. 计划怀孕时性交不使用避孕【错误】 C. 性行为基于双方同意【错误】 D. 首次性行为前应注射HPV疫苗【错误】\n单选(2分) 以下哪种疾病可以通过HPV疫苗预防？ A. 梅毒【错误】 B. 艾滋病【错误】 C. 生殖器疱疹【错误】 D. 宫颈癌【正确】\n单选(2分) 以下哪种方法是预防性传播感染，包括艾滋病的最有效方式？ A. 避孕药【错误】 B. 疫苗【错误】 C. 避孕套【正确】 D. 抗病毒药物【错误】\n单选(2分) 青少年比成人生殖健康风险更高的原因，除了 A. 自慰【正确】 B. 无保护性行为【错误】 C. 缺乏性知识【错误】 D. 性观念开放【错误】\n单选(2分) 以下哪项不是避孕措施？ A. 宫内节育器【错误】 B. 口服避孕药【错误】 C. 人工流产【正确】 D. 植入避孕器【错误】\n单选(2分) 产后不想要孩子的妇女，想使用IUD进行避孕，应告知的并发症，除了 A. 感染【错误】 B. 子宫穿孔【错误】 C. 异位IUDs【错误】 D. 血肿【正确】\n单选(2分) 以下哪种情况下的妇女可以使用口服避孕药以预防怀孕 A. 急性或慢性肝炎【错误】 B. 血栓性疾病【错误】 C. 哺乳期【错误】 D. 宫颈炎【正确】\n单选(2分) 以下哪种避孕方法最适合想要在六个月后怀孕的新婚夫妇？ A. 安全期避孕【错误】 B. 阴茎避孕套【正确】 C. 宫内节育器【错误】 D. 植入避孕器【错误】\n单选(2分) 在人工流产过程中，一位患者经历了腹痛、冷汗和血压下降，原因可能是 A. 人工流产综合征【正确】 B. 子宫穿孔【错误】 C. 出血性休克【错误】 D. 粗暴的手术操作【错误】\n多选(2分) 如果你是一位13岁女孩的母亲，她进入青春期时你应该如何教育她？ A. 注意保持阴道卫生【正确】 B. 经常更换内裤【正确】 C. 使用干净柔软的卫生用品并及时更换【正确】 D. 在月经期间避免游泳和沐浴，并适当锻炼【正确】\n多选(2分) 一个年轻健康的未婚女性想使用口服避孕药，建议包括 A. 黛安妈富隆【正确】 B. 美罗昂【正确】 C. 雅思明【正确】 D. 雅兹【正确】\n多选(2分) 一对不想要孩子的育龄夫妇，以下哪种避孕方法不推荐 A. 紧急避孕药【正确】 B. 植入避孕器【错误】 C. 安全期避孕【正确】 D. 体外射精【正确】\n多选(2分) 早期性行为或多性伴侣的风险有哪些？ A. 生殖道衣原体感染【正确】 B. 意外怀孕【正确】 C. HPV感染【正确】 D. 性观念开放【错误】\n多选(2分) 以下哪些群体容易感染艾滋病？ A. 有固定性伴侣的年轻人【错误】 B. 同性恋【正确】 C. 性工作者【正确】 D. 吸毒者【正确】","性与生殖的关系#性与生殖的关系":"单选(2分) 儿童性心理发展的特点不包括 A. 意识不完全【错误】 B. 持续性【正确】 C. 短暂性【错误】 D. 不完整性【错误】\n单选(2分) 性心理学是基于性生理学的，与（ ）相关的心理状态和心理过程 A. 性伴侣，性特征【错误】 B. 性欲望，性特征，性行为【正确】 C. 性欲望，性伴侣【错误】 D. 性行为，性伴侣【错误】\n单选(2分) 世界卫生组织(WHO)说：性心理健康是通过（ ）实现生理、情感、智力和社会方面性行为的完善和协调 A. 人际交往和社会关系【错误】 B. 个性和社会关系【错误】 C. 爱的方式，人际交往和个性【正确】 D. 社会关系和个性【错误】\n单选(2分) 性欲望的基础是 A. 生殖器官成熟【错误】 B. 适宜的环境因素【错误】 C. 性需求【错误】 D. 性激素【正确】\n单选(2分) 最常见的男性性功能障碍是 A. 性无能和早泄【正确】 B. 无射精和逆行射精【错误】 C. 勃起功能障碍【错误】 D. 遗精【错误】\n单选(2分) 性高潮障碍是指在足够的性刺激和唤醒后持续或重复出现（ ），给患者带来痛苦 A. 延迟性高潮【错误】 B. 高潮困难【错误】 C. 没有性高潮【错误】 D. 上述三者都是【正确】\n单选(2分) 性心理因素的生理基础是 A. 嗅觉，视觉，触觉，听觉【正确】 B. 视觉，幻觉，触觉和听觉【错误】 C. 触觉，幻觉，听觉，视觉【错误】 D. 幻觉，嗅觉，视觉和触觉【错误】\n多选(2分) 青少年性心理的表现包括 A. 性别认同【正确】 B. 性取向【正确】 C. 性经验【正确】 D. 性身份【正确】\n多选(2分) 老年心理调整包括 A. 家庭中性别角色的调整【正确】 B. 性行为的调整【正确】 C. 消除性活动障碍【正确】 D. 单身老年人的性调整【正确】\n多选(2分) 女性性功能障碍包括 A. 缺乏性兴趣【正确】 B. 高潮障碍【正确】 C. 性行为痛苦或插入障碍【正确】 D. 性唤醒障碍【正确】\n多选(2分) 异常性欲包括 A. 性欲过强【正确】 B. 异常性欲【正确】 C. 性欲低下【正确】 D. 无性欲【正确】\n多选(2分) 受精的前提条件包括 A. 卵子成熟，精子成熟，精子激活【正确】 B. 精子数量和质量能使300-500个最强壮的精子到达壶腹部【正确】 C. 在排卵后48小时内相遇【错误】 D. 在排卵后24小时内相遇【正确】\n判断(2分)\n性变态包括生理变态和心理变态。【正确】","性与生殖的未来#性与生殖的未来":"单选(2分) 以下哪种被分类为成体干细胞？ A. 生殖系干细胞【错误】 B. 间充质干细胞【错误】 C. 诱导多能干细胞【错误】 D. 胚胎干细胞【正确】\n单选(2分) 关于成体干细胞的以下陈述中哪个是错误的？ A. 多潜能性【错误】 B. 体外增殖能力【错误】 C. 移植后常见肿瘤发生【正确】 D. 组织和器官保持生长和衰减的动态平衡【错误】\n单选(2分) 以下哪种不是多能干细胞？ A. 诱导多能干细胞(iPSCs)【错误】 B. 胚胎干细胞【错误】 C. 胚胎生殖细胞【错误】 D. 毛囊干细胞【正确】\n单- 选(2分) 出生后，女性卵巢中的原始卵泡数量随年龄的增加而() A. 增加【错误】 B. 减少【正确】 C. 不变【错误】 D. 无关【错误】\n单选(2分) 为了使干细胞技术尽快造福人类，我们应该做到的，除了() A. 充分的基础研究【错误】 B. 足够的动物实验【错误】 C. 进行标准化临床试验【错误】 D. 早期临床应用【正确】\n多选(2分) 胚胎干细胞应用受限的原因是什么？ A. 胚胎干细胞来源有限【正确】 B. 伦理问题【正确】 C. 形成肿瘤的风险【正确】 D. 免疫排斥【正确】\n多选(2分) 干细胞治疗的原理是什么？ A. 修复受损的组织和细胞【正确】 B. 替换受损细胞【正确】 C. 刺激细胞再生【正确】 D. 免疫抑制【错误】\n多选(2分) 根据分化潜能，干细胞可分为 A. 全能干细胞【正确】 B. 多能干细胞【正确】 C. 单能干细胞【正确】 D. 成体干细胞【错误】\n多选(2分) 人工精子/卵细胞的主要争议包括 A. 高生物风险【正确】 B. “为人父母”概念受到攻击【正确】 C. 技术滥用【正确】 D. 昂贵【错误】\n多选(2分) 根据发育阶段，干细胞可分为和 A. 全能干细胞【错误】 B. 胚胎干细胞【正确】 C. 单能干细胞【错误】 D. 成体干细胞【正确】\n判断(2分)\n诱导多能干细胞具有类似胚胎干细胞的全能性。【正确】 卵巢组织中存在有分化成卵子潜能的生殖系干细胞。【正确】 在特定条件下，成体干细胞可以产生新的干细胞或分化为新的功能性细胞，从而维持和修复它们所在的特定组织。【正确】 诱导多能干细胞不涉及伦理问题，并且不存在肿瘤形成和免疫排斥的风险。【错误】 IPSCs是一种单能干细胞，通过向人体细胞内引入一系列转录体并进行重编程获得。【错误】","性关系#性关系":"单选(2分) 根据摩根的性别关系发展理论，家庭“经历几个连续发展阶段以成熟”的正确顺序是 A. 普纳鲁亚家庭、血缘家庭、配偶家庭、一夫一妻制家庭【错误】 B. 血缘家庭、普纳鲁亚家庭、配偶家庭、一夫一妻制家庭【正确】 C. 血缘家庭、配偶家庭、普纳鲁亚家庭、一夫一妻制家庭【错误】 D. 血缘家庭、配偶家庭、一夫一妻制家庭、普纳鲁亚家庭【错误】\n单选(2分) 集体婚姻的初始阶段是 A. 普纳鲁亚家庭【错误】 B. 血缘家庭【正确】 C. 配偶家庭【错误】 D. 一夫一妻制家庭【错误】\n单选(2分) 随着母系社会向父系社会的转变和私有制的出现，人类婚姻制度进入了以（ ）为基础的（ ）阶段，这是文明时代开始的标志之一。 A. 私有制、一夫一妻制【正确】 B. 私有制、一夫多妻制【错误】 C. 个体性别、一夫一妻制【错误】 D. 个体性别、一夫多妻制【错误】\n单选(2分) 现代医学将人类性生活定义为“由（ ）驱动，以（ ）结束，旨在给予和获得不同程度的性满足的一系列活动。” A. 性欲望，皮肤【错误】 B. 性欲望，生殖器官【错误】 C. 大脑性中枢兴奋，皮肤【正确】 D. 大脑性中枢兴奋，生殖器官【错误】\n单选(2分) 爱情的本质基于（ ）需求，使人们能够获得稳定持久的强烈身体和心理愉悦。 A. 生理、心理、文化【错误】 B. 心理、社会、文化【错误】 C. 生理、社会、文化【错误】 D. 生理、心理、社会【正确】\n多选(2分) 人类性具有丰富的内涵。它不仅是人类生命的源泉，也是生活中不可或缺的一部分。人类性的功能包括 A. 幸福【正确】 B. 文化【错误】 C. 生育【正确】 D. 健康【正确】\n多选(2分) 性行为有其广义和狭义之分。广义上的性行为指为满足性欲望和获得性愉悦可以观察到的一系列行为和情感反应，包括 A. 性交【正确】 B. 自慰【正确】 C. 亲吻【正确】 D. 拥抱【正确】\n多选(2分) 性欲望只是淫欲的生理基础，并不等同于爱情。人类的性爱应该包括 A. 奉献【正确】 B. 责任【正确】 C. 尊重【正确】 D. 理解【正确】\n多选(2分) 心理学家认为性意识成熟的标志包括 A. 对男女关系有正确理解并了解性的本质、功能和社会责任【正确】 B. 以社会接受的方式表达性冲动和需求；正常追求异性并发展关系【正确】 C. 有规律的性情感和意识；根据社会伦理和法律要求主动控制性行为【正确】 D. 学习发现和探索性行为，包括同性和异性性行为【错误】\n多选(2分) 树立科学的爱情观，推崇志趣相投的爱情。在真爱的关系中，我们可以 A. 建立自己的价值观【正确】 B. 坚持自己的价值观【正确】 C. 强加于人自己所不愿【错误】 D. 履行承诺【正确】\n判断(2分)\n古代为了生存，人类必须“以群体的联合力量和集体行动弥补个体的不足”。任何性别之间的排他性都必将削弱这种联合力和集体行动，因此只有有序的性关系才能适应当时生活条件的客观需要。【正确】 通过大脑引起的所有类型的性刺激属于性活动的范畴。亲密的身体接触是性活动的一种方式。【正确】 成熟的爱是因为我需要你所以我爱你。【正确】 由于爱情失败而自杀在大学生中排名第一，占比21.7%。【正确】","性心理是如何形成的#性心理是如何形成的？":"性心理的形成是一个复杂的过程，受到多种因素的影响，包括生理、教育和社会因素。\n生理基础： 遗传因素：人的性心理特质中有部分是由遗传决定的，比如性别身份的基本感知和性取向的某些方面。 生理发展：从出生到成年，人的性器官和内分泌系统（特别是性激素）的发展对性心理有重要影响。青春期的生理变化尤其关键，它们会激发性欲望和性身份的形成。 教育： 家庭教育：父母和家庭成员对性的态度和行为模式会深刻影响个体的性心理发展。例如，开放和健康的家庭性教育可以促进个体形成积极的性观念。 学校教育：学校中的性教育课程同样重要，它帮助青少年理解生理变化，认识到性健康和性权利的重要性。 社会基础： 文化因素：不同的文化对性有不同的看法和规范，这些文化差异会显著影响个体的性心理形成。 社会互动：与同伴、伴侣的交往，社会媒体和其他媒体的影响也在塑造个人的性心理方面发挥着作用。 综上所述，性心理的形成是一个多方面互动的结果，涉及个体的生理发展、教育背景以及社会文化环境。良好的性心理健康需要个体在这些方面都得到健康和平衡的发展。","性障碍的影响因素有哪些#性障碍的影响因素有哪些？":"性障碍的影响因素多样，可以从生理、心理和社会环境几个方面来考虑：\n生理因素：包括荷尔蒙失衡、疾病（如糖尿病、高血压）、药物副作用、酒精或药物滥用以及年龄等。 心理因素：情感问题、压力、焦虑、抑郁、性身份困扰、性史问题（如性虐待经历）等都可能影响性功能。 社会和文化因素：包括性教育的缺乏、文化和宗教信仰、社会对性的态度和预期等。 人际关系因素：伴侣之间的沟通问题、情感疏远、信任问题或其他关系冲突也可能导致性障碍。","描述男性与女性之间的性特征#描述男性与女性之间的性特征":"第一性征: 男女生殖器的不同。男性的生殖器包括阴茎、阴囊、生殖腺(睾丸) 、输精管道和附属腺；女性生殖器主要包括外阴、内阴、阴蒂、生殖腺(卵巢) 、子宫、阴道和附属腺。 第二性征: 指男女除了生殖器官以外的由性激素不同导致的外貌特征区别。男性的第二性征主要表现为体格高大，肌肉发达，喉结突出，汗毛多而密，长胡须；女性的第二性征主要表现为皮下脂肪多，皮肤细腻，骨盆宽大，乳腺发达。","环境精神心理生活方式对生殖健康的影响#环境、精神心理、生活方式对生殖健康的影响":"单选(2分) ()属于影响生殖健康的生物因素。 A. 高温环境【错误】 B. TORCH【正确】 C. 汞【错误】 D. 离子辐射【错误】\n单选(2分) ()对卵巢功能无害。 A. 接受腹腔化疗【错误】 B. 保持良好心情【正确】 C. 熬夜【错误】 D. 接受卵巢肿瘤手术【错误】\n单选(2分) ()是不健康的生活方式。 A. 熬夜【正确】 B. 早晨慢跑【错误】 C. 参加健康促进讲座【错误】 D. 远离烟草【错误】\n单选(2分) ()是健康的生活方式。 A. 熬夜玩游戏【错误】 B. 首次性行为\u003c 16岁【错误】 C. 每天过量饮酒【错误】 D. 早晨慢跑【正确】\n单选(2分) ()是计算BMI的正确方法。 A. 体重(kg) / 身高(m) ^ 2【正确】 B. 体重(kg) ^ 2 / 身高(cm)【错误】 C. 体重(kg) / 身高(cm) ^ 2【错误】 D. 身高(m) / 体重(kg)【错误】\n多选(2分) 影响生殖健康的物理因素包括() A. 重金属污染【正确】 B. 温度【正确】 C. 环境内分泌干扰物因素【正确】 D. 噪声【正确】\n多选(2分) TORCH感染方式包括() A. 食用生食(奶、蛋、肉)【正确】 B. 食用被猫粪便污染的水源(含有弓形虫卵囊)【正确】 C. 与动物密切接触【正确】 D. 食用被猫粪便污染的食物(含有弓形虫卵囊)【正确】\n多选(2分) 宫颈癌的主要原因包括() A. HPV感染【正确】 B. 多个性伴侣【正确】 C. 多次妊娠和分娩【正确】 D. 首次性行为 \u003c 16岁【正确】\n多选(2分) 高温的不良影响包括() A. 降低精子活力【正确】 B. 阻碍精子发生【正确】 C. 降低精子密度【正确】 D. 影响精子数量【正确】\n多选(2分) 与生殖健康相关的药物类型包括() A. 阿片类药物【正确】 B. 抗风湿药物【正确】 C. 抗肿瘤药物【正确】 D. 抗结核药物【正确】\n判断(2分)\n钢铁制造、焊接、烹饪、紧身裤、桑拿等高温条件会阻碍精子产生，导致精子数量减少。【正确】 淋病、梅毒等可以对女性生殖道和器官造成极大伤害，不仅降低生活质量，还影响胎儿的生长。【正确】 根据不同年龄青少年的生理特征，应开展性健康教育，以正确理解生理和身体变化，并引导他们树立正确的性和道德观念。【正确】 精神疾病不是导致勃起功能障碍(ED)的常见原因。【错误】","现代辅助生殖技术的概念#现代辅助生殖技术的概念":"单选(2分) 故意将精子引入女性的宫颈或子宫腔以实现怀孕的过程是() A. 体外受精和胚胎移植 (IVF-ET)【错误】 B. 人工授精（AI）【正确】 C. 种植前遗传诊断 (PGD)【错误】 D. 胞浆内单精子注射(ICSI)【错误】\n单选(2分) 第二代体外受精指的是() A. 胞浆内单精子注射 (ICSI)【正确】 B. 种植前遗传诊断 (PGD)【错误】 C. 人工授精【错误】 D. 体外受精 (IVF)【错误】\n单选(2分) 关于控制性卵巢超刺激，以下哪项是错误的() A. 使用GnRH激动剂来增加卵泡数量【正确】 B. 自然周期只有一个成熟的卵子，因此成功率低【错误】 C. 排卵时间不固定，不容易取卵【错误】 D. 个体化的控制性卵巢超刺激包括长方案、短方案、拮抗剂方案、温和刺激方案等【错误】\n单选(2分) 罗伯逊易位的定义是() A. 两个分离的端粒染色体的两个长臂丢失【错误】 B. 两个分离的端粒染色体的两个短臂融合，长臂丢失【错误】 C. 两个分离的中部着丝点染色体的两个长臂融合，短臂丢失【错误】 D. 两个分离的端粒染色体的两个长臂融合，短臂丢失【正确】\n单选(2分) 以下哪个不是X连锁隐性疾病() A. 威尔逊病【正确】 B. 血友病A【错误】 C. 进行性肌肉萎缩症【错误】 D. 红-绿色盲【错误】\n多选(2分) ICSI的主要适应症包括() A. 严重男性不育症【正确】 B. 梗阻性无精子症【正确】 C. 使用传统IVF先前受精失败【正确】 D. 排卵功能障碍【错误】\n多选(2分) 传统IVF程序包括() A. 使用促性腺激素刺激多个卵泡发育【正确】 B. 通过血液内分泌激素检测和B超监测卵泡发育【正确】 C. 当卵泡成熟时，注射绒毛膜促性腺激素以触发最终成熟【正确】 D. 36小时后，通过B超引导的穿刺进行成熟卵泡的获取【正确】\n多选(2分) PGT的适应症包括（） A. 染色体数目和结构异常患者【正确】 B. 具有染色体异常儿童或怀孕史【正确】 C. 高龄孕妇【正确】 D. HLA匹配，癌症易感基因携带者【正确】\n多选(2分) 以下关于遗传疾病的说法哪些是正确的？ A. 染色体平衡易位的携带者通常无表型，但生育时可能发生不育、流产和胎儿畸形【正确】 B. 遗传疾病患者必有家族史，不会出现新突变【错误】 C. X连锁隐性遗传病的致病基因位于X染色体上并且是显性的【错误】 D. 遗传疾病通过如微阵列和PCR等分子生物学技术在DNA分子层面进行诊断【正确】\n多选(2分) 根据不同的授精部位，人工授精可分为（） A. 阴道内授精【正确】 B. 宫颈内授精【正确】 C. 子宫内授精【正确】 D. 丈夫精液人工授精【错误】\n判断(2分)\n在胚胎冷冻过程中，非渗透保护剂，如单糖、果糖等，分子量较大，无法进入细胞，没有明显的细胞毒性。因此，长时间暴露于高浓度的这类物质不会影响胚胎的正常代谢和发育潜力。【错误】 线粒体DNA编码的基因突变会导致一系列母系遗传疾病。线粒体替换可以防止线粒体突变传给下一代。【正确】 平衡染色体易位是指染色体的一部分断裂并重新附着在另一个位置的情况。【正确】 遗传疾病是由染色体异常引起的疾病，不包括由基因突变引起的疾病。【错误】 PGT是IVF的重要技术之一，也被称为第三代IVF，对阻断遗传疾病的传播非常重要。【正确】","生殖障碍主要由女方引起-是否正确#生殖障碍主要由女方引起, 是否正确?":"不正确。生殖障碍不是主要由女方引起。实际上，生殖障碍可能由男性、女性或双方因素引起，甚至有些情况下原因不明确。原因大致归类为：\n男性因素：比如精子质量不佳、精子数量减少、性功能障碍等。 女性因素：如排卵障碍、子宫或输卵管的问题、激素失衡等。 双方因素：包括免疫学因素、性交问题等。 不明原因：在一些情况下，尽管进行了全面的检查，仍然无法确定具体原因。","男女基本生殖知识#男女基本生殖知识":"单选(2分) 什么是精子发生？ A. 精子细胞转化为精子的过程【错误】 B. 精原细胞形成精子的过程【正确】 C. 初级精细胞形成精子的过程【错误】 D. 精原细胞的减数分裂过程【错误】\n单选(2分) 精子主要储存在 A. 输精管和附睾【正确】 B. 睾丸【错误】 C. 前列腺【错误】 D. 精囊【错误】\n单选(2分) 以下哪种细胞分泌雄激素？ A. 精原细胞【错误】 B. 间质细胞【正确】 C. 陶特细胞【错误】 D. 精子【错误】\n单选(2分) 卵巢产生的激素是 A. 雌激素，孕酮【错误】 B. 孕酮，甲状腺素【错误】 C. 雌激素，促黄体激素【错误】 D. 雌激素，孕酮，雄激素【正确】\n单选(2分) 以下哪项陈述是正确的 A. 阴道是女性的性交器官【正确】 B. 阴道是女性的勃起器官【错误】 C. 男性有勃起器官而女性没有【错误】 D. 阴蒂是女性的性交器官【错误】\n单选(2分) 以下哪项陈述不属于更年期过渡期的表现 A. 骨质疏松【错误】 B. 易怒【错误】 C. 性欲消失【正确】 D. 经期紊乱【错误】\n单选(2分) 以下哪项陈述是正确的 A. 性欲从青春期开始【错误】 B. 更年期过渡期的女性不会产生性兴奋【错误】 C. 高性欲期是生育期【正确】 D. 乳房成熟是青春期的重要信号【错误】\n单选(2分) 精子和卵子在哪里相遇 A. 阴道【错误】 B. 子宫腔【错误】 C. 子宫颈【错误】 D. 输卵管【正确】\n多选(2分) 阴茎勃起的类型包括 A. 心理性勃起【正确】 B. 反射性勃起【正确】 C. 夜间勃起【正确】 D. 早晨勃起【错误】\n多选(2分) 阴茎勃起的三个要素是 A. 海绵体平滑肌的放松【正确】 B. 阴茎动脉血流增加【正确】 C. 静脉堵塞【正确】 D. 海绵体血管窦的收缩【错误】\n多选(2分) 以下哪些是女性内生殖器 A. 子宫【正确】 B. 大阴唇【错误】 C. 阴道【正确】 D. 子宫颈【正确】\n多选(2分) 以下哪些因素有助于受精 A. 女性在排卵前后性欲最高【正确】 B. 男性正常射精【正确】 C. 女性正常排卵【正确】 D. 正常性生活【正确】\n多选(2分) 以下哪些是女性的生理阶段 A. 胎儿期【正确】 B. 青春期【正确】 C. 成熟期【正确】 D. 新生儿期【正确】","男女性障碍和发生的原因是什么#男女性障碍和发生的原因是什么？":"男性性障碍\n勃起功能障碍： 原因：心血管疾病、糖尿病、神经系统疾病、激素失衡、吸烟和酗酒、药物副作用（如抗抑郁药物）、心理因素（如压力、焦虑或抑郁）、关系问题。 早泄： 原因：生理因素（如激素失衡、感染）、心理因素（如焦虑、过度兴奋）、神经过敏。 性欲低下： 原因：慢性疾病、药物副作用、睾酮水平下降、心理因素（如抑郁、压力）、关系问题。 女性性障碍\n性欲缺乏和性兴趣减退： 原因：激素变化（如更年期、避孕药）、药物副作用、心理因素（如抑郁、压力）、身体健康问题、关系问题。 性交疼痛（如阴道痉挛）： 原因：身体原因（如感染、阴道干涩）、心理因素（如性创伤、焦虑）、关系问题。 性唤起障碍： 原因：血液循环问题、神经系统问题、药物副作用、心理和情感问题。 共性的心理和社会因素\n心理因素：包括焦虑、抑郁、性创伤、性压力、性功能焦虑、身体形象问题。 关系问题：沟通不良、情感疏远、信任问题或冲突。 社会和文化因素：性教育缺乏、文化背景中对性的负面态度、社会压力。 共性的生理因素\n慢性疾病：心血管疾病、糖尿病、神经疾病。 药物副作用：某些抗抑郁药、高血压药、避孕药等。 激素失衡：包括性激素水平变化。","简答复习#简答复习":"","课程大纲#课程大纲":"男女生殖器官和生殖生理 1.1 男性生殖器官 1.2 男性生殖生理 1.3 女性生殖器官 1.4 女性生殖生理 受精与妊娠 2.1 体内的受精过程 2.2 妊娠过程 2.3 胎儿分娩 性与生殖的关系 3.1 性心理与性生理 3.2 性与生殖的关系 3.3 性与生殖障碍 性关系 4.1 两性关系的起源与发展 4.2 各阶段两性关系的特点 4.3 健康性关系的建立要素 性与生殖健康的维护 5.1 性与生殖健康的问题与挑战 5.2 性与生殖健康的维护 环境、精神心理、生活方式对生殖健康的影响 6.1 环境因素对生殖健康的影响 6.2 精神心理因素对生殖健康的影响 6.3 生活方式对生殖健康的影响 现代辅助生殖技术的概念 7.1 现代辅助生殖技术的起源 7.2 现代辅助生殖技术的目前开展情况 7.3 现代辅助生殖技术的伦理问题与挑战 性与生殖的未来 8.1 克隆技术 8.2 生殖干细胞技术 8.3 性与生殖的未来展望","辅助生殖技术art对传统两性关系的影响#辅助生殖技术（ART）对传统两性关系的影响":"夫妻之间是否还需要性生活？ 需要。即使在ART广泛应用的背景下，性生活仍然是夫妻情感交流、身体和心理满足的重要方式，是维持亲密关系和个人健康的重要部分。 婚姻是否还与性别有关？ 关联性正在减弱。随着同性婚姻的合法化和性别观念的变化，婚姻越来越多地被视为基于爱和承诺的关系。ART的发展为不同性别组合的伴侣提供了生育可能，进一步淡化了婚姻与传统性别角色的关联。 是否可以挑选、定制后代？ 技术上部分可行，但引发巨大伦理争议。ART如基因筛选提供了一定程度上选择后代遗传特征的可能性，但这引发了深刻的伦理和道德问题。关于“定制婴儿”的技术和法律限制因国家和地区而异。 男女能实现性别平等吗？ 技术有助推作用，但平等是系统工程。ART在一定程度上帮助平衡了因生理差异导致的性别不平等。然而，性别平等的实现不仅仅是技术问题，还涉及到社会文化、教育、法律等多个方面的深刻变革。","选择复习#选择复习":""},"title":"性与生殖"},"/docs/undergraduate/%E9%80%9A%E8%AF%86%E6%9D%82%E9%A1%B9/%E6%96%B0%E5%86%9C%E7%A7%91%E5%AE%9E%E8%B7%B5-%E7%94%9F%E6%B4%BB%E5%9B%AD%E8%89%BA/":{"data":{"1-园艺疗法的定义与历史#1 园艺疗法的定义与历史":"","11-园艺疗法的起源和发展#1.1 园艺疗法的起源和发展":"园艺疗法，或称为康复园艺，是一种利用园艺活动进行身心康复的疗法。它的起源可以追溯到古代文明，当时人们认识到与大自然的互动对人的健康有着积极的影响。\n进入19世纪，园艺疗法开始在医疗体系中得到正式的认可，被用于帮助受伤士兵的物理和心理康复。20世纪70年代，随着西方国家医学核心焦点从疾病向健康的转变，园艺疗法逐渐兴盛起来，通过利用植物栽培与园艺操作帮助人舒缓情绪、激发动力，改善体能和运动协调，同时治疗心理障碍，增加社会联系和道德感的价值[1]。","12-现代园艺疗法的价值与意义#1.2 现代园艺疗法的价值与意义":"在现代社会，园艺疗法被认为是一种整合性治疗方法，不仅关注身体健康，也关注心理和社会福祉。\n对于病人来说, 由于客观存在的缺陷, 他们与植物的关系被切断。然而生物本身是有变化的, 这些变化带给人们平静。有的人在长期与病魔做斗争的过程中, 突然发现窗外的植物在摇晃或发芽时, 看到了自然的丰富性和生命的喜悦, 从而获得了生活的力量[2]。\n研究表明，园艺疗法可以降低压力水平，改善情绪状态，增加自我效能感，并促进社会交往。","13-园艺疗法与病患的互动重要性#1.3 园艺疗法与病患的互动重要性":"与园艺环境的互动是园艺疗法成功的关键。患者通过播种、浇水、修剪和收获等活动，不仅能够感受到生命的成长和变化，也在无形中加强了自己对健康的掌控感。更重要的是，这种互动促使患者转移焦点，从疾病的负面影响中解脱出来，找到积极面对生活的动力。此外，通过与植物的互动，患者可以重新建立与周围世界的联系，这对于身处孤立环境的患者来说尤为重要。\n图1 园艺活动中感觉体验和动作体验的相互作用","2-绿色设计的生理与心理效应#2 绿色设计的生理与心理效应":"","21-花卉植物和颜色对病患情绪和生理状态的影响#2.1 花卉、植物和颜色对病患情绪和生理状态的影响":"花卉和植物自古以来就被用作提升心情和改善生理状态的天然治疗剂。现代研究也证实了这一点，指出特定的植物和花卉，如薰衣草和玫瑰，能够降低心率，减缓呼吸速率，从而减轻压力和焦虑。\n通过刺激人体包括视觉、嗅觉、听觉、味觉和触觉的五感以及园艺手作活动是植物在园艺疗法中作用于人体而达到积极治疗效果的主要方式。五感花园通过五感作用进而调节人体身心健康，分为视觉花园、嗅觉花园、听觉花园、味觉花园和触觉花园。\n图2 常见植物颜色的心理效应\n这里以视觉花园举例，如红色花卉如木瓜海棠、红掌等能增进人的食欲；赭色的花卉如鸡冠花、大丽花等可稳定低血压患者的血压；花卉的绿叶使人产生舒适的感觉，可解除焦虑，舒畅心情[3]。\n植物的气味、色彩以及整体的观感可以影响病患的内分泌系统，导致激素如皮质醇的水平变化，进而影响情绪状态。同时，植物通过释放负离子和净化空气的方式，改善环境质量，对患者的呼吸健康产生积极作用。","22-选择适当的园艺环境提升疗效#2.2 选择适当的园艺环境提升疗效":"选择合适的园艺环境对于疗效的提升至关重要。在设计园艺疗法环境时，应当综合考虑患者的具体需求、病状以及个人偏好。对于生理上需要恢复和放松的患者，使用色彩柔和、结构简单、维护要求低的植物和花卉。而对于需要情绪激励和心理刺激的患者，则可以选择色彩鲜明、花香浓郁的品种。\n图3 花卉园艺活动(花卉创意栽培、干花书签制作)\n此外，可以根据病房的光线条件、空间大小以及患者的活动能力，进行植物种类和布置方式的个性化选择。例如，对于光线不足的病房，可以选择耐阴植物；而对于有轮椅患者的病房，则应确保植物布置不会阻碍行动。\n在选择过程中，还应注意植物的安全性和无过敏反应。所有的园艺环境都应该定期进行维护，以保持其美观及疗效。","3-生物医学工程技术的应用#3 生物医学工程技术的应用":"生物医学工程作为一个工科专业，运用工程学的技术和手段，结合生物学知识，为人类的健康服务和为疾病的防治提供依据，最终解决医学问题[4]。但是在这里，我们可以更多的运用医疗器械的工科技术手段，来为园艺治疗的疗效提供更为精确的判断。","31-功能磁共振成像监测病患的行为和表情#3.1 功能磁共振成像：监测病患的行为和表情":"生物医学工程的一项重要应用是利用功能磁共振成像(functional magnetic resonance imaging,f MRI)技术来监测和评估病患的行为和表情。现阶段研究发现面部表情的加工过程主要与以下结构有关:视觉皮质(梭状回、枕叶的内侧及下方、舌回),边缘结构(杏仁核、海马、后扣带回),颞叶,颞顶区(顶叶、颞中回、岛叶),额叶,皮质下区(壳核)以及小脑[5]。因此，我们可以通过分析病患与园艺环境互动时的面部表情和身体语言来评价情绪状态和舒适度。该种系统可以安装在病房内，实时收集数据，通过机器学习算法分析病患的微表情和动作，以判断其情绪变化。\n这类技术的应用不仅能够帮助医疗人员了解患者在接触不同类型的植物时的心理变化，还可以监测长期趋势，从而调整治疗方案，使园艺疗法更加个性化和精确。","32-生理信号传感器监测园艺环境中的生理响应#3.2 生理信号传感器：监测园艺环境中的生理响应":"生理信号传感器如心电图(ECG)、心率变异性(HRV)和皮肤电导率(GSR)是生物医学工程中常用的工具，它们能够提供有关病患生理状态的详尽信息。在园艺疗法中，这些传感器可以用来监测患者在接受治疗前后的生理变化。\n通过ECG监测心率和节律，可以评估病患对园艺环境的心脏反应；HRV是反映压力水平和自主神经系统活动的重要指标；而GSR可以显示出患者的情绪激动程度。将这些传感器的数据进行综合分析，可以为病患提供更加定制化的园艺疗法方案，例如，当监测到某种植物使患者的生理指标变得更加稳定时，可以将此类植物更多地集成到病患的环境中。","4-园艺环境的设计与选择#4 园艺环境的设计与选择":"在建立了植物和颜色对病人心理和生理影响的基础，并通过生物医学工程技术精确监测了病人的反应之后，我们的重点转向如何根据所收集到的数据来设计和选择最适合的园艺环境。这涉及到综合患者的生理和心理需求，以及他们与环境的互动反应，来创建一个有助于恢复和治疗的空间。","41-基于病患的需求和反应的园艺作物选择#4.1 基于病患的需求和反应的园艺作物选择":"园艺环境的设计应以病患的需求为中心。选择园艺作物时，需要考虑植物的香气、大小、维护要求以及它们所能带来的心理和生理上的益处。生物医学工程可以通过分析患者的反应数据，为每位患者推荐适宜的植物类型。\n图4 病房内可选的观赏植物\n这种个性化的方法能够确保植物的选择最大限度地符合患者的治疗需求，如选择能够带来镇静效果的植物为失眠患者服务，或是挑选能够激发感官刺激的植物帮助促进神经系统疾病患者的康复[6]。","42-植物形态与颜色的综合应用#4.2 植物形态与颜色的综合应用":"通过对患者心情和生理状态监测的数据分析，可以确定哪些植物种类和颜色最能引起正面的反应。例如，如果数据显示某些颜色的花朵能够稳定心率或减轻焦虑，那么这些花朵应被优先考虑纳入病房的园艺设计中。选择园艺作物不仅仅是根据患者的喜好，更是基于科学数据对其健康影响的理解。","43-病患的参与度的考量#4.3 病患的参与度的考量":"为了提高病患的积极参与度，园艺环境的设计需要考虑如何让病患更容易地与植物互动。这可以是通过可移动的植物床，触感友好的植物选择，或是引入可进行简单园艺活动的空间设计。\n图5 武汉市武东医院园艺治疗师带领精神残疾患者及家属播种[7]\n利用生物医学工程技术收集的数据，可以识别出哪些互动类型对于患者的恢复更有帮助，并相应地调整园艺环境。","5-总结与未来展望#5 总结与未来展望":"研究表明，结合生物医学工程技术可以大幅提高园艺疗法的个性化水平和效果。通过实时监测和分析病患的生理和心理状态，医疗团队能够调整治疗环境，以最大化其恢复潜力。采用这种技术，园艺治疗变得更加精细化和动态化，从而为病患提供了一个更加支持性和有益的康复环境。\n展望未来，生物医学工程与园艺治疗结合的领域有巨大的发展潜力。随着技术的进步，监测设备将变得更加微型化、智能化和成本效益高。预期未来的研究将进一步优化数据分析算法，提高预测病患反应的准确性，从而实现更加个性化的治疗方案。同时，随着对患者心理和生理影响更深入的理解，园艺疗法的应用范围和效果都有望得到显著扩展。\n总体而言，生物医学工程的应用将为传统园艺疗法带来创新，增强科学性和有效性。未来，我期望这种跨学科的合作不仅能够为患者提供更优质的治疗环境，同时为医院设计提供新的理念，使得园艺治疗在医疗保健领域的重要性得到更广泛的认识和应用。\n参考文献\n[1] 薛滨夏,李同予.园艺疗法体系的组成、内涵与实施应用[J].园林,2023,40(04):51-57.\n[2] 李树华.园艺疗法的特征[J].园林,2013(11):12-17.\n[3] 贾俊丽,罗海蓉,梅雪莹等.花卉在园艺疗法中的应用[J].安徽农业科学,2022,50(22):114-118.\n[4] 黄素琴,高斌.生物医学工程专业研究生医工交叉培养方式探索[J].中国教育技术装备,2021(20):134-136.\n[5] 李晓凡,范国光.抑郁症病人面部表情刺激的脑功能磁共振研究现状与进展[J].国际医学放射学杂志,2010,33(05):421-423+450.\n[6] 杨婷婷,赵艺玥.基于园艺疗法的室内植物色彩选择[J].现代园艺,2023,46(05):172-175.DOI:10.14051/j.cnki.xdyy.2023.05.072.\n[7] 荆楚网.武汉市武东医院开展园艺疗法 用植物护佑特殊患者心理健康[EB/OL].(2023-3-24)[2023-11-3].https://baijiahao.baidu.com/s?id=1728174748473947119\u0026wfr=spider\u0026for=pc","新农科实践-生活园艺#新农科实践-生活园艺":"新农科实践-生活园艺","生物医学工程在园艺治疗中的应用#生物医学工程在园艺治疗中的应用":"摘要: 本文探讨了生物医学工程技术在医院园艺治疗应用中的可能应用，从园艺疗法的历史和现代价值出发，详细讨论了花卉和植物在改善患者生理和心理状态方面的重要作用，同时突出了生物医学工程技术在精确监测和评估这些效应方面的关键作用。通过结合计算机视觉和生理信号传感器的数据来指导园艺环境的设计和选择，从而设计提高病患的参与度和治疗效果。\n关键词: 生物医学工程; 园艺疗法；病患互动；治疗环境设计；个性化医疗","课程论文#课程论文":"课程号：(2023-2024-1)-1617N007-0093229-1\n开课学院： 农学院\n提交日期：2023年 11 月 5 日\n得分 评语 评卷人 课程论文的任务要求（选题范围、格式要求、评分标准等）：\n论文包括中文标题、摘要、关键词、正文、参考文献等； 正文字数：3000字以上，或根据任课教师要求； 论文字体：仿宋(小四号)； 论文行距：1.5倍； 论文页边距：左右2.5厘米； 论文层次的编号采用1, 1.1, 1.1.1；2, 2.1, 2.1.1的形式； 参考文献一律用阿拉伯数字分别依序连续编排序号，同时在文中相应处用右上标标出，如[1]、[2]等 参考文献格式如下： [1] 陈怀满, 郑荣春, 周东美. 关于我国土壤环境保护研究中一些值得关注的问题[J]. 农业环境科学学报, 2004(6): 1244-1245.\n[2] Zhang HZ, Luo YM, Xia JQ. Some thoughts of the comparison of risk based soil environmental standards between different countries[J]. Environmental Science, 2011, 32(3): 795-802."},"title":"新农科实践-生活园艺"},"/docs/undergraduate/%E9%80%9A%E8%AF%86%E6%9D%82%E9%A1%B9/%E7%A4%BE%E4%BC%9A%E4%B8%BB%E4%B9%89%E5%8F%91%E5%B1%95%E5%8F%B2/":{"data":{"社会主义发展史#社会主义发展史":"社会主义发展史四史之一，体验不错。","课程论文#课程论文":"社会主义发展史课程论文"},"title":"社会主义发展史"},"/docs/undergraduate/%E9%80%9A%E8%AF%86%E6%9D%82%E9%A1%B9/%E7%A4%BE%E4%BC%9A%E5%BF%83%E7%90%86%E5%AD%A6/":{"data":{"1-从直接脏话到谐音语的转变#1. 从直接脏话到谐音语的转变":"脏话的演变过程体现了语言适应社会文化和技术环境变化的能力。传统上，脏话作为一种直接、粗俗的语言形式，主要用于表达愤怒、侮辱或其他强烈情绪。然而，在新媒体环境中，直接脏话的使用受到了社会规范和技术监控的限制。为了适应这种环境，并保持脏话表达愤怒和侮辱情绪的功能，出现了谐音脏话的现象。从粗俗的\"操你妈\"到羊驼的\"草泥马”，这种演变可以被视为一种语言策略，使得用户能够在遵守社会规范和技术限制的同时，继续使用脏话进行情绪表达和社会评论。谐音脏话通过改变原有脏话的音节结构，创造出听起来相似但字面意义不同的词汇，从而在表达上获得一定程度的隐蔽性和安全性。","1-参与群体的拓展#1. 参与群体的拓展":"祖安文化最初局限于《英雄联盟》游戏玩家的小范围群体，但随着时间的推移，其参与群体迅速扩展。这一扩展不仅涉及更广泛的网络用户，如追星粉丝、各类网络社区成员，也包括更多通过社交媒体和其他新媒体平台接触到该文化的青少年。青年群体作为网络的主要使用者,富有创造力,充满个性化表达欲望,而网络脏话作为一种风格化、符号化的表达方式,具有新奇、怪异的特点,正是青年群体用来消解、对抗传统主流文化的一种方式。5","1-引言#1. 引言":"在当今社会，随着新媒体技术的快速发展，人们的沟通方式经历了显著的变革。特别是在青少年中，新媒体平台如社交网络、在线游戏和视频分享网站已成为日常交流的主要渠道。伴随这一变革，网络语言也经历了迅速的演变，其中包括脏话使用的新形式和趋势。脏话在网络环境中的普及不仅体现了语言的适应性和创新性，也反映了深层的社会心理因素和文化变迁。","1-新媒体平台的作用#1. 新媒体平台的作用":"班杜拉的社会认知理论(STC)认为,人的行为是个人、社会和行为相互作用的结果3。在新媒体的语境中，脏话的谐音、倒放及其他变体形式的产生并非偶然现象，而是新媒体平台特性的直接结果。新媒体平台，如社交网络和即时通讯工具，提供了一个相对自由和去中心化的沟通环境，其中信息的传播速度快，覆盖范围广。这种环境促进了语言形式的快速创新和演变，尤其是在年轻的互联网使用者中。脏话的谐音变体在这种环境中快速流行，反映了青少年群体对于传统语言规范的挑战以及对创新表达形式的追求。此外，新媒体平台的匿名性或半匿名性特点也为脏话变体的使用提供了一个相对安全的空间，如美国的4chan、日本的2ch、中国的NGA等，使得用户在表达强烈情绪或观点时更加大胆。","1-起源和直接因素#1. 起源和直接因素":"祖安文化起源于网络游戏《英雄联盟》中的虚拟地区\"祖安”，这一地区以其玩家的特定言行风格著称。祖安文化的直接因素包括了游戏内的角色设定、玩家之间的互动方式，以及游戏主播对这种文化的推广和演绎。这种文化最初表现为游戏内的一种特定沟通方式，随后逐渐扩散至游戏外的网络空间，成为一种独立的亚文化现象。它的形成体现了青少年群体如何通过共享的兴趣和活动，构建并认同特定的社会身份。\n祖安文化中脏话的使用初期较为直接和粗俗，但随着文化的传播和发展，其语言形式经历了显著的演变。这种演变体现在从直接脏话到创新的谐音、音译和合成词等形式的转变。这一变化不仅是对新媒体监管的适应，也是对传统语言表达方式的创新。在这个过程中，祖安文化的参与者展示了他们对语言的掌控力和创造力，通过改变传统脏话的形式来维持文化的独特性和辨识度。","2-后亚文化特征的分析#2. 后亚文化特征的分析":"祖安文化作为一种后亚文化，其核心特征包括去中心化和弱群体归属感。去中心化表现在该文化没有固定的领导者或组织结构，文化的发展和传播主要依靠参与者的自发行为。这种特征导致了文化内部多样性和复杂性的增加，使得不同参与者可能基于不同的理由加入并诠释该文化。弱群体归属感反映在参与者通常不形成紧密的社群结构，个体在文化中的身份相对松散，容易受到外部因素如流行趋势和商业化的影响。","2-文化意义的变化#2. 文化意义的变化":"随着祖安文化的发展和传播，其文化意义也经历了变化。最初，祖安文化主要表现为对抗传统和权威的一种方式，反映了青少年群体对现实世界的不满和对网络空间的逃避。然而，随着这种文化在更广泛的网络空间中流行，其原始的文化内核逐渐被模糊和商业化。这种文化意义的转变反映了亚文化在新媒体环境中的演化特点，即从边缘化的反叛表达到更为主流和商业化的娱乐形式。在社会心理学的视角下，这一变化揭示了青少年群体在文化认同、集体归属感以及对权威的态度等方面的心理动态。\n爱德华在《语言论》中指出：“语言不脱离文化而存在，就是说，不脱离社会流传下来的、决定我们生活面貌的风俗和信仰的总体。”4通过分析祖安文化的形成、发展和文化意义的变化，我们可以更深入地理解青少年后亚文化中脏话使用的社会心理学含义，包括身份认同、集体归属感以及对社会规范的态度等方面的心理动因。","2-社会管理者的干预#2. 社会管理者的干预":"广电以及其下游的各大社交媒体审核对网络言论的监管和干预，如实施禁言条例和内容过滤机制，也是推动脏话变体产生的重要因素。以网易《我的世界》举例，“给点草方块\"“联机开房间\"等游戏正常用语会被屏蔽为”***\"，就连连续的数字也因QQ号的嫌疑而被屏蔽。为了规避这些监管措施，网络用户常常创造和使用脏话的变体形式，以绕过自动审核系统，这种变通方式反映了用户对言论自由的追求和对权威的逆反心理。\n当青少年尝试避免在某些社交场合使用网络脏话时，可能会经历思维抑制(Thought Suppression)的心理过程。然而，根据思维抑制理论，越是尝试抑制某些思维，这些思维反而可能更强烈地回归，导致在不适当的情况下使用网络脏话的反弹效应。在这个过程中，脏话变体不仅作为一种规避机制，还逐渐发展成为一种独特的文化符号，具有特定的社会和心理含义。","2-网络脏话的定义和分类#2. 网络脏话的定义和分类":"脏话变体可定义为一种在音韵上类似于传统脏话，但在字面意义上有所差异的语言现象。2这种变体既保留了原有脏话的部分音韵特征，又通过创新实现了新的语义层次。脏话变体的分类包括但不限于以下几种类型：\n表3 脏话变体的分类\n分类类型 解释 举例 单字替换 通过替换脏话中的某个字或音节，创造出听起来相似但含义不同的词汇 翔、戳、吊 字母或拼音替换 在拼写或拼音上进行改动，使得词汇在视觉或听觉上与原脏话相似 TMD、NMSL、SB、SM、NT 特定群体侮辱类 针对特定群体或个人的谐音脏话，旨在通过变体表达对特定对象的侮辱或讽刺 玩原神玩的、遥遥领先 同音字替代 使用与原脏话同音或近音的字词替代，以达到类似的表达效果 草泥马、我去年买了个表 外文发音 借助外语单词的发音来创造与原脏话类似的音效，同时避免直接使用脏话 花Q、碧池 夸赞类 将原本具有负面含义的脏话转化为表扬或夸奖的用语，通常带有讽刺或幽默的意味 牛逼、碉堡了 表情包 使用表情包代替文字脏话，通过图像传达相似的情绪或意图 龙图、孙笑川图","2-脏话使用的社会和个人因素#2. 脏话使用的社会和个人因素":"通过分析青少年使用网络脏话的动机、态度以及脏话的社会文化含义，本研究试图回答一个核心问题：即新媒体环境下，青少年使用脏话的主要动因是什么？针对这个问题，笔者提出了以下六种假说：\n表1 针对青少年使用脏话的六种假说\n假说次序 假说内容 H1 网络环境的相对宽松和自由使得青少年更倾向于在此环境中使用脏话，与现实环境形成对比 H2 青少年认为网络与现实生活中的行为规范存在差异，导致他们在网络环境中使用脏话的行为与现实生活中的言语选择有所不同 H3 特定网络亚文化群体（如祖安文化）对青少年使用网络脏话的方式产生影响，促使他们模仿和采纳这些群体的语言习惯 H4 青少年使用网络脏话作为一种情感表达方式，尤其在特定的情感状态下，如愤怒或激动时 H5 青少年在其社交圈和社交媒体的影响下，使用网络脏话作为一种适应和融入社交环境的手段 H6 随着时间推移，青少年对网络脏话的敏感度逐渐下降，导致他们更容易接受甚至采用这类语言，包括脏话的谐音或倒放形式","3-媒介属性和受众特征#3. 媒介属性和受众特征":"新媒体的媒介属性，特别是其交互性、即时性和多样性，与脏话变体的产生和流行密切相关。这些属性使得新媒体成为一个动态且富有创造性的沟通环境，其中用户可以迅速响应社会事件和文化趋势，并以创新的语言形式进行表达。特别是在年轻的受众群体中，他们对新奇和另类表达方式的偏好驱动了脏话变体的创造和流行。这些年轻用户通常更加熟悉和活跃于新媒体平台，他们的交流方式和文化偏好在很大程度上塑造了新媒体环境中的语言趋势。因此，脏话变体的流行也可以看作是新媒体环境中年轻用户文化特征和心理需求的反映。\n综上所述，脏话谐音和其他变体在新媒体环境中的产生和流行是多种因素交织的结果，包括新媒体平台的特性、社会管理者的干预以及媒介属性和受众特征的互动。这些因素共同作用，推动了脏话变体作为一种特殊的社会心理现象在网络空间的发展和演变。","3-调查方法和结果#3. 调查方法和结果":"为了深入理解青少年在新媒体环境下使用网络脏话的原因，本研究设计了一份详细的问卷调查。调查采用了结构化的问卷格式，目的是探索浙江大学大学生对网络脏话使用的态度、频率以及背后的心理和社会动因，总共收集到了76份有效问卷。\n接下来，我们将使用列联表分析和卡方检验对假说进行验证分析。\n表2 大学生对于脏话变体接受度调查问卷的卡方分析\n题目 是 否 X² P 你认为网络环境相比于现实环境在言论表达上更宽松吗？ 64 12 20.15 0.000*** 在现实环境中使用脏话的频率是否高于网络生活？ 22 54 7.05 0.008** 你认为在朵朵与98的匿名帖中使用脏话没有平时的心理负担吗？ 39 37 0.026 0.871 你在朵朵与98的实名帖子中经常见到脏话吗？ 47 29 2.16 0.141 你是否认为网络与现实生活中的行为准则有所不同？ 52 24 5.34 0.021* 在网络环境中使用脏话是否会影响你在现实生活中的言语选择？ 49 27 3.25 0.071 你是否认为特定的网络亚文化群体（如祖安文化）影响了你使用网络脏话的方式？ 48 28 2.68 0.102 你是否因为在B站或抖音刷到\"阿米诺斯物业\"“otto\"的二创视频而提高了类似用语的使用频率？ 40 36 0.105 0.746 你是否因为属于你关注的博主/UP主而更倾向于使用网络脏话？ 35 41 0.237 0.626 你的社交圈子是否常用网络脏话？ 43 33 0.661 0.416 你经常浏览的社交媒体（微博/贴吧/小红书/知乎等）是否影响了你平时的用语方式？ 56 20 9.03 0.003** 你是否觉得在你的社交圈中使用网络脏话可以帮助你更好地融入？ 43 33 0.061 0.416 长期以来，你对网络脏话的接受程度是否有所改变？ 56 20 9.03 0.003** 你是否认为网络脏话现在更为普遍接受，不再那么令人反感？ 50 26 3.89 0.049* 你是否认为脏话的谐音或倒放不属于侮辱性语言，已经脱离脏话的概念了？ 49 27 3.25 0.071","4-原因分析#4. 原因分析":"问卷结果表明，网络环境的相对宽松和自由是青少年倾向于在此环境中使用脏话的一个重要原因（H1）。这一现象与现实环境形成了鲜明对比，青少年在网络中感受到更少的言语表达限制。特别是，关于网络环境言论表达宽松程度的问题，卡方检验显示了极其显著的差异（X²=20.15, p\u003c0.001），强调了这一点的重要性。\n此外，青少年认为网络与现实生活中的行为规范存在差异（H2），这导致他们在网络环境中的脏话使用与现实生活中的言语选择有所不同。这种感知的差异可能是由网络环境的匿名性和去中心化特性所引起的，通过卡方检验得到了统计学上的显著支持（X²=5.34, p=0.021）。\n网络亚文化群体的影响同样显著（H3），尤其是如B站上\"一个叫’阿米诺斯’的互联网民族\"“聊聊阿米诺斯，脏话屏蔽背后的权力与反抗\"等视频所折射的特定群体，对青少年使用网络脏话的方式产生了显著影响。这些群体通过其独特的语言习惯和沟通风格，为青少年提供了新的表达方式。虽然对这一假设的卡方检验未达到传统的显著性水平（χ²=2.68, p=0.102），但结果仍提供了一定程度的支持，推测是取样样本不够的原因。\n情感表达是青少年使用网络脏话的一个重要动机（H4）。在特定的情感状态下，如愤怒或激动时，脏话成为了一种有效的情感释放手段，如问卷中搜集到的词云图所示。Rassin和Muris由宣泄情绪这一原因认为，脏话可以帮助保持心理卫生，从而具有存在的必要。1\n图2 更倾向于使用网络脏话的情感状态词云图\n此外，青少年在其社交圈和社交媒体的影响下，使用网络脏话作为适应和融入社交环境的一种手段（H5），这表明脏话的使用与社交认同和归属感的建立有关。对这一假设的卡方检验虽未达显著水平（X²=0.661, p=0.416），但提供了对趋势的洞见。\n随着时间的推移，青少年对网络脏话的敏感度逐渐下降（H6），这导致他们更容易接受甚至采用这类语言，包括脏话的谐音或倒放形式。这种变化可能与新媒体环境中脏话使用的日益普及有关，反映了文化适应过程中语言和价值观的变迁。这一假设通过卡方检验得到了显著支持（X²=9.03, p=0.003）。","5-新媒体环境中脏话变体的社会心理学分析#5. 新媒体环境中脏话变体的社会心理学分析":"","6-青少年后亚文化#6. 青少年后亚文化":"","7-结论#7. 结论":"青少年在新媒体环境下使用网络脏话和参与特定亚文化的行为，反映了他们在构建个人身份和社会关系方面的心理需求。这种行为不仅是对传统社会规范的一种挑战，也是一种对个人自我表达和群体归属感的追求。特别是在消费主义和媒体饱和的社会背景下，这种行为表现了青少年对于自我认同和社会认同的探索。\n在社会心理学的视角下，网络脏话的使用和亚文化的参与，可以被视为青少年应对社会压力、寻求社会归属以及表达个体差异的策略。这些行为不仅是语言和文化的简单表现，而是深层的社会心理过程的体现。它们揭示了青少年在社会化过程中如何适应和反应社会规范，以及如何在新媒体环境中寻找和构建属于自己的社会空间。\n此外，新媒体环境中脏话的演变和亚文化的流行，也反映了青少年群体在心理发展过程中的逆反心理、从众心理以及对于新奇体验的追求。这些心理特征与新媒体环境的特性相结合，促成了独特的社会心理现象的出现。\n总之，本研究强调了青少年在新媒体环境中使用网络脏T话和参与亚文化活动的行为，不仅是社会文化现象的反映，更是复杂的社会心理过程的体现。这些发现对于理解青少年的心理发展、社会行为以及他们与社会环境的互动具有重要意义。\n参考文献\nRassin, E., \u0026 Murs, P. (2005). Why do women syear? An exploration of reasons for and perceived efficacy of swearing in Dutch female students. Personality and Individual Differences, 38(7), 1669-1674.\n徐勇. (2019). 新媒体环境中脏话谐音现象的原因探究. 东南传播, 2019(8), 114-116. DOI:10.13556/j.cnki.dncb.cn35-1274/j.2019.08.034.\n班杜拉. (1988). 社会学习心理学. 吉林: 吉林教育出版社.\n爱德华·萨丕尔 (原著), 陆卓元 (译). (1977). 语言论: 言语研究导论. 北京: 商务印书馆.\n李长健, 沈露依, \u0026 但思臣. (2019). 当代青年群体使用\"网络脏话\"动因的实证研究. 东南传播, 2019(9), 96-99. DOI:10.13556/j.cnki.dncb.cn35-1274/j.2019.09.031.\n华桦. (2020). “祖安文化\"的形成机制、文化特征及应对策略——基于青年后亚文化的理论解释与局限. 当代青年研究, 2020(6), 46-52.\n说明：\n第一页为封面页，保留第一页格式。正文从第二页开始。 打印装订：A4页面，双面打印，左侧装订 Rassin, E., \u0026 Murs, P. (2005). Why do women syear? An exploration of reasons for and perceived efficacy of swearing in Dutch female students. Personality and Individual Differences, 38(7), 1669-1674. ↩︎\n徐勇. (2019). 新媒体环境中脏话谐音现象的原因探究. 东南传播, 2019(8), 114-116. DOI:10.13556/j.cnki.dncb.cn35-1274/j.2019.08.034. ↩︎\n班杜拉. (1988). 社会学习心理学. 吉林: 吉林教育出版社. ↩︎\n爱德华·萨丕尔 (原著), 陆卓元 (译). (1977). 语言论: 言语研究导论. 北京: 商务印书馆. ↩︎\n李长健, 沈露依, \u0026 但思臣. (2019). 当代青年群体使用\"网络脏话\"动因的实证研究. 东南传播, 2019(9), 96-99. DOI:10.13556/j.cnki.dncb.cn35-1274/j.2019.09.031. ↩︎\n华桦. (2020). “祖安文化\"的形成机制、文化特征及应对策略——基于青年后亚文化的理论解释与局限. 当代青年研究, 2020(6), 46-52. ↩︎","一祖安文化的形成和发展#（一）祖安文化的形成和发展":"在探讨新媒体环境中脏话变体的社会心理学现象时，我们不得不提及特定的亚文化群体——祖安文化，它在网络脏话的使用和演变中扮演着重要角色。祖安文化及其之后所发展的亚文化不仅反映了脏话在新媒体中的创新表达方式，而且展示了青少年群体如何通过特定的语言和行为模式，在亚文化中寻求认同和归属感。","一脏话的演变与分类#（一）脏话的演变与分类":"","三消费主义下的文化狂欢#（三）消费主义下的文化狂欢":"任何后亚文化能够得以生存，某种程度上来说，都与当今社会主流的商业化消费观脱不了干系。如果说后亚文化是一个机体，那么它的某些个体要素如风格化的东西则会被转化成象征符号，然后再转化成商品的形式。6网络迷因（Memes）经常被广告商用来吸引年轻消费者，如伴随\"只因你太美\"而兴起的一系列淘宝小装饰和二创视频，为相当大部分创作者和电商提供了收益。在消费主义的推动下，亚文化经历了从小众圈子到更广泛流行文化的转变。这种转变伴随着商业平台的介入和推广，使得文化原有的反叛和边缘特征逐渐被商业化和娱乐化的元素所消解。\n从社会心理学的角度来看，祖安文化的发展和演变反映了青少年在新媒体环境中的身份建构、群体互动以及消费主义影响下的文化参与方式。通过理解这些特征，我们可以更深入地洞察青少年的心理需求和行为动机，以及他们在特定社会环境中的互动模式。","二去中心化和弱群体归属感#（二）去中心化和弱群体归属感":"","二脏话变体的产生原因#（二）脏话变体的产生原因":"","新媒体环境下青少年脏话使用的社会心理学探究#新媒体环境下青少年脏话使用的社会心理学探究":"摘要：本研究聚焦于新媒体环境中青少年使用网络脏话的社会心理学现象，探讨了脏话使用的动因、亚文化的形成及其对青少年心理和行为的影响。通过分析青少年在新媒体环境下脏话使用的社会和个人原因，本研究揭示了网络脏话的使用不仅仅是语言的简单使用，而是深层的社会心理过程的体现。研究还深入探讨了脏话变体（如谐音、倒放）的产生原因，包括新媒体平台的特性、社会管理者的干预及媒介属性和受众特征。此外，本文对祖安文化这一青少年后亚文化现象进行了分析，强调了去中心化、弱群体归属感和消费主义下的文化狂欢对青少年心理特征和行为动因的影响。通过这一研究，我们能更深入地理解青少年的社会心理需求，以及他们在特定社会环境中的互动模式。\n关键词：社会心理学；青少年；新媒体；网络脏话；亚文化","期-末-论-文#期 末 论 文":"根据本学期《社会心理学》课程的教学内容，描述某一社会现象，并运用社会心理学的相关理论加以分析。\n论文写作要求\n论文应包含题目，摘要，正文，参考文献四部分，其中正文字数不少于3000字 参考文献以近五年为主，APA 7th格式 格式规范，符合心理学论文的一般写作要求（具体可参考《心理学论文写作规范》） 不得抄袭，一旦发现抄袭以零分处理","社会心理学#社会心理学":"社会心理学","课程论文#课程论文":"浙江大学2023–2024学年 冬 学期\n《社会心理学》课程期末考查\n课程号：7913N001\n开课学院：心理与行为科学系\n考查形式：课程论文\n考查日期：2024年1月3日\n打分项目 论文结构 论文内容 文字表达 创新性 研究意义 总 分 得分 评卷人"},"title":"社会心理学"},"/docs/web/":{"data":{"":"Web 开发相关学习笔记。"},"title":"Web开发"},"/docs/web/algorithms/":{"data":{"":"算法相关学习笔记。"},"title":"算法"},"/docs/web/algorithms/dynamic-programming/":{"data":{"":"# 动态规划 ## 动态规划基础 ### 核心思想 将复杂问题分解为子问题，通过存储子问题的解来避免重复计算。 ### 适用条件 1. **最优子结构**：原问题的最优解包含子问题的最优解 2. **无后效性**：子问题的解不影响其他子问题 3. **重叠子问题**：子问题会被重复计算 ### 解题步骤 1. 定义状态（DP数组的含义） 2. 找出状态转移方程 3. 确定初始条件 4. 考虑边界情况 ## 经典问题 ### 1. 斐波那契数列 ```js // 递归（效率低） function fib(n) { if (n \u003c= 1) return n; return fib(n - 1) + fib(n - 2); } // 动态规划（高效） function fib(n) { if (n \u003c= 1) return n; const dp = new Array(n + 1); dp[0] = 0; dp[1] = 1; for (let i = 2; i \u003c= n; i++) { dp[i] = dp[i - 1] + dp[i - 2]; } return dp[n]; } // 空间优化 function fib(n) { if (n \u003c= 1) return n; let prev2 = 0, prev1 = 1; for (let i = 2; i \u003c= n; i++) { const current = prev1 + prev2; prev2 = prev1; prev1 = current; } return prev1; }","1-打印dp表#1. 打印DP表":"观察状态转移过程","1-滚动数组#1. 滚动数组":"空间优化：只保留前几行的结果","1-线性dp#1. 线性DP":"斐波那契数列 爬楼梯 打家劫舍","2-0-1背包问题#2. 0-1背包问题":"function knapsack(weights, values, capacity) { const n = weights.length; const dp = Array(n + 1).fill().map(() =\u003e Array(capacity + 1).fill(0)); for (let i = 1; i \u003c= n; i++) { for (let w = 1; w \u003c= capacity; w++) { if (weights[i - 1] \u003c= w) { dp[i][w] = Math.max( dp[i - 1][w], // 不选第i个物品 dp[i - 1][w - weights[i - 1]] + values[i - 1] // 选第i个物品 ); } else { dp[i][w] = dp[i - 1][w]; } } } return dp[n][capacity]; } // 空间优化 function knapsack(weights, values, capacity) { const n = weights.length; const dp = new Array(capacity + 1).fill(0); for (let i = 0; i \u003c n; i++) { for (let w = capacity; w \u003e= weights[i]; w--) { dp[w] = Math.max(dp[w], dp[w - weights[i]] + values[i]); } } return dp[capacity]; }","2-区间dp#2. 区间DP":"矩阵链乘法 石子合并 回文子串","2-检查边界#2. 检查边界":"特殊情况：空数组、单元素等","2-状态压缩#2. 状态压缩":"用二进制位表示状态，适用于集合相关问题","3-最长递增子序列-lis#3. 最长递增子序列 (LIS)":"function lengthOfLIS(nums) { if (nums.length === 0) return 0; const dp = new Array(nums.length).fill(1); for (let i = 1; i \u003c nums.length; i++) { for (let j = 0; j \u003c i; j++) { if (nums[i] \u003e nums[j]) { dp[i] = Math.max(dp[i], dp[j] + 1); } } } return Math.max(...dp); } // 贪心 + 二分查找优化 function lengthOfLIS(nums) { const tails = []; for (const num of nums) { let left = 0, right = tails.length; while (left \u003c right) { const mid = Math.floor((left + right) / 2); if (tails[mid] \u003c num) { left = mid + 1; } else { right = mid; } } if (left === tails.length) { tails.push(num); } else { tails[left] = num; } } return tails.length; }","3-树形dp#3. 树形DP":"二叉树最大路径和 树的最大独立集","3-记忆化搜索#3. 记忆化搜索":"自顶向下：递归 + 缓存","3-验证状态定义#3. 验证状态定义":"确保DP数组含义清晰","4-数字dp#4. 数字DP":"处理数字范围内的DP问题","4-时间空间优化#4. 时间空间优化":"根据具体问题优化复杂度","4-状态压缩dp#4. 状态压缩DP":"TSP问题 集合划分","4-编辑距离#4. 编辑距离":"function minDistance(word1, word2) { const m = word1.length; const n = word2.length; const dp = Array(m + 1).fill().map(() =\u003e Array(n + 1).fill(0)); // 初始化 for (let i = 0; i \u003c= m; i++) { dp[i][0] = i; } for (let j = 0; j \u003c= n; j++) { dp[0][j] = j; } // 填充DP表 for (let i = 1; i \u003c= m; i++) { for (let j = 1; j \u003c= n; j++) { if (word1[i - 1] === word2[j - 1]) { dp[i][j] = dp[i - 1][j - 1]; } else { dp[i][j] = Math.min( dp[i - 1][j - 1] + 1, // 替换 dp[i - 1][j] + 1, // 删除 dp[i][j - 1] + 1 // 插入 ); } } } return dp[m][n]; }","leetcode相关题目#LeetCode相关题目":"","不同类型的问题#不同类型的问题":"","中等#中等":"最长递增子序列 编辑距离 0-1背包","困难#困难":"正则表达式匹配 戳气球 俄罗斯套娃信封问题","常见技巧#常见技巧":"","总结#总结":"核心思路：分解问题，存储子解，避免重复计算 关键步骤：状态定义、转移方程、初始条件 常见类型：线性DP、区间DP、树形DP、状态压缩DP 优化技巧：滚动数组、记忆化搜索、贪心优化 实践重点：多做题，理解状态转移的本质","简单#简单":"爬楼梯 打家劫舍","调试技巧#调试技巧":""},"title":"dynamic-programming"},"/docs/web/algorithms/sorting/":{"data":{"1-快速排序-quick-sort#1. 快速排序 (Quick Sort)":"function quickSort(arr, left = 0, right = arr.length - 1) { if (left \u003c right) { const pivotIndex = partition(arr, left, right); quickSort(arr, left, pivotIndex - 1); quickSort(arr, pivotIndex + 1, right); } return arr; } function partition(arr, left, right) { const pivot = arr[right]; let i = left - 1; for (let j = left; j \u003c right; j++) { if (arr[j] \u003c= pivot) { i++; [arr[i], arr[j]] = [arr[j], arr[i]]; } } [arr[i + 1], arr[right]] = [arr[right], arr[i + 1]]; return i + 1; } 时间复杂度：平均 O(nlogn)，最坏 O(n²) 空间复杂度：O(logn) 稳定性：不稳定","1-快速排序的优化#1. 快速排序的优化":"三数取中选择枢轴 小数组使用插入排序 尾递归优化","2-外部排序#2. 外部排序":"多路归并排序 败者树优化","2-归并排序-merge-sort#2. 归并排序 (Merge Sort)":"function mergeSort(arr) { if (arr.length \u003c= 1) return arr; const mid = Math.floor(arr.length / 2); const left = mergeSort(arr.slice(0, mid)); const right = mergeSort(arr.slice(mid)); return merge(left, right); } function merge(left, right) { const result = []; let i = 0, j = 0; while (i \u003c left.length \u0026\u0026 j \u003c right.length) { if (left[i] \u003c= right[j]) { result.push(left[i++]); } else { result.push(right[j++]); } } return result.concat(left.slice(i)).concat(right.slice(j)); } 时间复杂度：O(nlogn) 空间复杂度：O(n) 稳定性：稳定","3-堆排序-heap-sort#3. 堆排序 (Heap Sort)":"function heapSort(arr) { const n = arr.length; // 建堆 for (let i = Math.floor(n / 2) - 1; i \u003e= 0; i--) { heapify(arr, n, i); } // 排序 for (let i = n - 1; i \u003e 0; i--) { [arr[0], arr[i]] = [arr[i], arr[0]]; heapify(arr, i, 0); } return arr; } function heapify(arr, n, i) { let largest = i; const left = 2 * i + 1; const right = 2 * i + 2; if (left \u003c n \u0026\u0026 arr[left] \u003e arr[largest]) { largest = left; } if (right \u003c n \u0026\u0026 arr[right] \u003e arr[largest]) { largest = right; } if (largest !== i) { [arr[i], arr[largest]] = [arr[largest], arr[i]]; heapify(arr, n, largest); } } 时间复杂度：O(nlogn) 空间复杂度：O(1) 稳定性：不稳定","3-稳定性重要性#3. 稳定性重要性":"按多个关键字排序时 对象排序保持原有顺序","4-冒泡排序-bubble-sort#4. 冒泡排序 (Bubble Sort)":"function bubbleSort(arr) { const n = arr.length; for (let i = 0; i \u003c n - 1; i++) { let swapped = false; for (let j = 0; j \u003c n - i - 1; j++) { if (arr[j] \u003e arr[j + 1]) { [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]]; swapped = true; } } if (!swapped) break; } return arr; } 时间复杂度：O(n²) 空间复杂度：O(1) 稳定性：稳定","4-实际应用场景#4. 实际应用场景":"数据库索引 文件排序 算法竞赛","5-插入排序-insertion-sort#5. 插入排序 (Insertion Sort)":"function insertionSort(arr) { const n = arr.length; for (let i = 1; i \u003c n; i++) { const key = arr[i]; let j = i - 1; while (j \u003e= 0 \u0026\u0026 arr[j] \u003e key) { arr[j + 1] = arr[j]; j--; } arr[j + 1] = key; } return arr; } 时间复杂度：O(n²) 空间复杂度：O(1) 稳定性：稳定","leetcode相关题目#LeetCode相关题目":"排序数组 数组中的第K个最大元素 合并区间 颜色分类","常见排序算法#常见排序算法":"","排序算法#排序算法":"排序算法","排序算法分类#排序算法分类":"","排序算法比较#排序算法比较":"算法 时间复杂度 空间复杂度 稳定性 适用场景 快速排序 O(nlogn) O(logn) 不稳定 大数据量，平均性能好 归并排序 O(nlogn) O(n) 稳定 稳定排序，外排序 堆排序 O(nlogn) O(1) 不稳定 大数据量，空间受限 冒泡排序 O(n²) O(1) 稳定 小数据量，教学演示 插入排序 O(n²) O(1) 稳定 基本有序的小数组","比较排序-vs-非比较排序#比较排序 vs 非比较排序":"比较排序：通过比较元素大小进行排序（最坏时间复杂度 O(nlogn)） 非比较排序：不通过比较元素大小（计数排序、基数排序等）","稳定排序-vs-不稳定排序#稳定排序 vs 不稳定排序":"稳定排序：相等元素的相对位置保持不变 不稳定排序：相等元素的相对位置可能改变","面试重点#面试重点":""},"title":"sorting"},"/docs/web/backend/":{"data":{"":"后端开发相关学习笔记。"},"title":"后端"},"/docs/web/backend/devops/":{"data":{"":"DevOps 相关学习笔记。"},"title":"DevOps"},"/docs/web/backend/devops/autodl/":{"data":{"autodl-使用指南#AutoDL 使用指南":"AutoDL 使用指南AutoDL 是一个 GPU 算力租用平台，提供云服务器和深度学习环境。其实类似的平台还有很多，这里也推荐谷歌的 Colab。","优势#优势":"一次配置，无限复用：节省数小时，如果原实例 GPU 被占用、到期或故障，我们可以快速切换到空闲 GPU，不中断工作流。 并行实验（手动多卡）：同一环境跑不同参数 / 模型，同时启动多个实例，并行验证。 备份恢复：克隆作备份，一键恢复到稳定状态。","使用方式#使用方式":"下载：选中网盘后可以浏览网盘中的文件，点击下载可以下载至本地（实例中）。固定下载到实例 /root/autodl-tmp 下，下载完成后可以自行移动。 上传：点击上传按钮则可以上传本地（实例中）的文件至网盘。 速度：上传下载速度一般为 5~20MB/s，具体与当时网络负载有关（百度网盘与会员有关）","公网网盘设置#公网网盘设置":"照着公网网盘去设置百度网盘的访问。\n:::info 个人开发者可用 官方文档提到\"百度网盘由于规则调整，不再支持个人认证及个人创建应用功能，需要企业认证才可开通应用，因此不再推荐使用百度网盘\"，但实际上个人开发者可以申请一个应用（企业可以10个），这是文档更新不及时导致的。 :::\n在 AutoPanel 点击添加授权，输入 AppKey 和 SecretKey 后，点击获取 Access Token 将打开百度网盘的网页，将其返回的 Token 粘贴过来即可完成授权。\n:::tip 全局授权 记得把这个授权也顺带放到控制台 -\u003e 账号 -\u003e 设置 -\u003e 公网网盘那个地方，这样就能够在每一个新创建的实例里面自动授权了。 :::","学术加速#学术加速":"autodl 自带学术加速：\n# 开启 source /etc/network_turbo # 取消 unset http_proxy \u0026\u0026 unset https_proxy 支持的加速域名：\ngithub.com：github 本体，git clone githubusercontent.com：GitHub 用来托管静态资源的域名，放在亚马逊 s3 上。repo 中的文件资源（图片、文档以及其他静态文件，readme 中的图片）；大文件或二进制文件，可能通过这个站点分发。比如 raw.githubusercontent.com/awesome-selfhosted/awesome-selfhosted/refs/heads/master/README.md githubassets.com：存储和提供 GitHub Pages 和仓库中的静态资源，如图片、CSS、JavaScript 文件 huggingface.co：模型下载 :::tip pip/conda 加速 至于 pip、conda 这些，参考 docs/self-study/backend/python-package-managers.md 里，一步是换 miniforge 或者 mamba，另一步是清华源，还没有遇见这两种方法解决不了的情况。 :::","学生认证#学生认证":"Autodl 有一个聊胜于无的学生认证，与充值成长值获得的【炼丹会员】权益是完全一致的。\n权益类别 具体内容 价值说明 基础折扣 GPU 算力租用 95 折（长期有效） 每小时 / 每天自动减免 5%，直接抵扣账户余额，不限消费次数 资源优先级 高峰时段优先调度 GPU 资源 避免 “排队等待”，更快获取算力（尤其热门显卡如 A100、3090） 专属镜像库 会员专属预置环境镜像 含学术常用框架（PyTorch/TensorFlow）、预训练模型，一键部署 技术支持 绿色通道，响应更快 学术问题优先处理，缩短排障时间 活动资格 新功能内测、学术合作优先参与 优先体验平台新特性，对接科研资源 续费 / 认证便利 学生认证 1 年有效，到期可续 学生无需充值即可长期享受全部权益","实例克隆#实例克隆":"Autodl 要善用实例克隆。","注意事项#注意事项":":::warning 数据盘内容 如果项目放到数据盘（/root/autodl-tmp）里的话，克隆时要记得勾选上相应的选项。 :::","清理系统盘#清理系统盘":"为什么系统盘手动删除后还是爆满状态？\n因为是 Ubuntu，有垃圾桶机制，需要手动清理：\nrm -rf /.Trash-0","磁盘分区说明#磁盘分区说明":"autodl 的分区结构：\n分区 路径 大小 类型 速度 系统盘 根目录 / 以及其下所有路径（除了下面的） 30GB 本地盘 快 数据盘 /root/autodl-tmp 50GB（可扩容） 本地盘 快 文件存储 /root/autodl-fs 免费 200GB 网络盘 一般 :::warning 系统盘炸盘问题 系统盘只有 30GB，容易爆满。建议：\n虚拟环境：放系统盘 项目本体：放数据盘（/root/autodl-tmp） :::","磁盘管理#磁盘管理":"在实例中查看磁盘使用情况请在终端中执行：\nsource /root/.bashrc"},"title":"autodl"},"/docs/web/backend/fastapi/":{"data":{"":"众所周知没有人会去用 Python 写后端，除非你想和 AI 交叉。\n大名鼎鼎的三大 Python 后端框架：Flask、Django 还有 FastAPI，这里就随便用一个 FastAPI 吧，尽管难度层面 Flask 和 FastAPI 可能差不多（或者说都没啥难度），但 FastAPI 货真价实帮了我两次，所以本章的目标也是以搭建出这样的项目来进行讲解。\nLangChain 上面是一个介绍Langchain的飞书文档，其实我想表达的就是，至少在本科阶段，我对LLM应用开发的印象就是类似这种调API套壳。 而现在哪怕有了Coze、Dify这种，其实相当于加入了一个Workflow，从而实现比以前套壳更强大的功能。\n欢迎来到 Dify+FastAPI+WebUI 实战项目文档。本项目旨在通过一个具体的实例，向您展示如何将 Dify 的强大 AI 工作流能力、FastAPI 的高性能后端框架以及现代化的前端 UI 结合起来，构建一个属于您自己的智能应用。\n在本系列文档中，我们将从零开始，一步步搭建一个智能学习助手。您将学到：\n如何设计和规划一个 AI 应用的系统架构。 如何利用 FastAPI 作为中间层，桥接前端与 Dify 服务。 如何配置和调用 Dify 的 API，特别是其强大的 DeepResearch 工作流。 如何将这一切整合起来，并进行本地部署和测试。","学习资源#学习资源":"","官方文档#官方文档":"FastAPI 官方文档 FastAPI 中文文档","实践#实践":"后端深入","导论#导论":"导论","部署#部署":"前端与部署"},"title":"FastAPI"},"/docs/web/backend/fastapi/01-introduction/":{"data":{"":"","1-项目目标与痛点#1. 项目目标与痛点":"在日常学习和研究中，我们常常需要花费大量时间来搜集、整理和消化信息。为了提高学习效率，我们希望构建一个能够理解我们需求、主动提供深度研究材料的智能助手。\n核心痛点:\n信息过载: 难以从海量信息中快速找到高质量、高相关性的内容。 学习路径不清晰: 对于一个新领域，不知道从何学起，缺乏结构化的学习路径。 工具链繁杂: 需要在不同的工具（搜索引擎、笔记软件、翻译工具等）之间频繁切换。 项目目标:\n一站式学习: 提供一个统一的交互界面，完成从提出问题到获取深度研究报告的全过程。 智能化研究: 利用大型语言模型（LLM）的能力，自动完成信息检索、筛选、整合和总结。 个性化服务: 能够根据用户的特定需求，动态调整研究方向和内容。","2-技术选型#2. 技术选型":"为了实现上述目标，我们选择了以下技术栈：\nDify: 一个开源的 LLM 应用开发平台。它提供了可视化的 Prompt 编排、运营、数据集管理等功能，让我们可以快速构建和迭代复杂的 AI 应用。在本项目中，我们主要使用其 DeepResearch 工作流。 FastAPI: 一个现代、高性能的 Python Web 框架。我们用它来构建后端服务，作为连接前端和 Dify 的桥梁。它提供了自动化的 API 文档、数据验证和异步支持，非常适合快速开发。 ChatNIO: 一个开源的、支持多种大模型后端的 WebUI。我们将其作为项目的前端界面，为用户提供一个美观、易用的对话窗口。","3-系统工作流#3. 系统工作流":"整个系统的工作流程如下图所示：\ngraph LR\rA[用户] -- 1. 发送学习请求 --\u003e B{ChatNIO 前端};\rB -- 2. 调用后端 API --\u003e C{FastAPI 后端};\rC -- 3. 提取研究主题 --\u003e C;\rC -- 4. 调用 Dify API --\u003e D[Dify Platform];\rD -- 5. 执行 DeepResearch 工作流 --\u003e D;\rD -- 6. 返回研究结果 --\u003e C;\rC -- 7. 返回给前端 --\u003e B;\rB -- 8. 展示给用户 --\u003e A; 流程解析:\n用户请求: 用户在 ChatNIO 界面输入学习需求，例如“我想了解一下 Transformer 的原理”。 前端交互: ChatNIO 将用户的输入打包，通过 HTTP 请求发送给 FastAPI 后端。 后端处理: FastAPI 接收到请求后，首先会调用 extract_research_theme 函数，从用户原始输入中提炼出核心的研究主题（例如“Transformer 原理”）。 然后，它将该主题作为参数，构造一个对 Dify API 的请求。 Dify 执行: Dify 平台接收到请求后，会触发预设的 DeepResearch 工作流。该工作流会自动在网络上搜索相关信息，进行筛选、整理，并生成一份结构化的研究报告。 结果返回: Dify 完成任务后，将结果返回给 FastAPI 后端，后端再将其转发给前端，最终呈现给用户。 通过这个工作流，我们实现了一个从用户简单输入到系统深度研究报告输出的闭环。在下一章节中，我们将深入探讨后端 FastAPI 的具体实现。\n然后，我们就来简单的介绍一下一个最最最基础的FastAPI如何搭建：","第一阶段基础语法#第一阶段：基础语法":"基本应用 from fastapi import FastAPI from pydantic import BaseModel app = FastAPI(title=\"我的 API\", version=\"1.0.0\") class User(BaseModel): id: int name: str email: str age: int | None = None @app.get(\"/\") async def root(): return {\"message\": \"Hello World\"} @app.get(\"/users/{user_id}\") async def get_user(user_id: int): return {\"user_id\": user_id, \"name\": \"包博文\"} @app.post(\"/users/\") async def create_user(user: User): return user 路径参数和查询参数 from typing import Optional from fastapi import Query, Path @app.get(\"/items/{item_id}\") async def read_item( item_id: int = Path(..., title=\"商品ID\", ge=1), q: Optional[str] = Query(None, max_length=50), skip: int = Query(0, ge=0), limit: int = Query(10, ge=1, le=100) ): return { \"item_id\": item_id, \"q\": q, \"skip\": skip, \"limit\": limit } 请求体模型 from pydantic import BaseModel, EmailStr, Field from typing import List class UserCreate(BaseModel): name: str = Field(..., min_length=1, max_length=50) email: EmailStr age: int = Field(..., ge=0, le=150) is_active: bool = True class UserResponse(BaseModel): id: int name: str email: str age: int is_active: bool class Config: from_attributes = True @app.post(\"/users/\", response_model=UserResponse) async def create_user(user: UserCreate): # 模拟创建用户 return UserResponse( id=1, name=user.name, email=user.email, age=user.age, is_active=user.is_active )"},"title":"01-introduction"},"/docs/web/backend/fastapi/02-backend-deep-dive/":{"data":{"":"import CollapsibleBlock from ‘@site/src/components/CollapsibleBlock’;","1-初始化项目环境#1. 初始化项目环境":"","11-创建工作目录与虚拟环境#1.1 创建工作目录与虚拟环境":"","12-安装依赖#1.2 安装依赖":"pip install fastapi uvicorn[standard] httpx python-dotenv 在项目根目录下创建 requirements.txt，便于团队成员同步环境：\nfastapi uvicorn[standard] httpx python-dotenv","2-准备配置与项目结构#2. 准备配置与项目结构":"新建目录结构：\n├── backend/ │ ├── app.py │ └── __init__.py ├── .env.example └── requirements.txt 在 backend/ 目录中放置 FastAPI 应用；__init__.py 可以暂时留空。\n将敏感配置写入 .env（不提交到 Git）：\nDIFY_API_KEY=your_dify_api_key_here DIFY_BASE_URL=https://api.dify.ai/v1 然后复制一份到仓库中的 env.example，以便读者了解必填项。","3-创建-fastapi-实例与中间件#3. 创建 FastAPI 实例与中间件":"核心应用入口位于 app.py，创建 FastAPI 实例并加上所需中间件：\napp = FastAPI( title=\"学习助手API\", description=\"基于Dify的智能学习助手，支持ChatNIO集成\", version=\"1.0.0\" ) # CORS配置 app.add_middleware( CORSMiddleware, allow_origins=[\"*\"], # 生产环境要限制 allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"], ) title (学习助手API)：决定 Swagger UI 顶部显示的名称，也会写入自动生成的 OpenAPI 文档。保持语义化命名有助于团队识别环境（例如可在测试环境加 -staging 后缀）。 description：作为接口文档的项目简介；在团队协作中，简介里可以列出支持的主要能力或依赖，以免使用者误解服务定位。 version (1.0.0)：用于接口版本管理。升级接口时，可结合语义化版本控制表达兼容性变化；在监控或健康检查中也能快速确认部署版本。 allow_origins=[\"*\"]：允许任意来源的前端访问，适合本地开发或内网调试。上线后务必改成具体域名列表，防止被非授权站点调用。 allow_credentials=True：保持浏览器能在跨域请求中携带 Cookie/认证信息；如果后端只处理匿名请求，可以关闭以降低风险。 allow_methods=[\"*\"] 与 allow_headers=[\"*\"]：默认放开所有 HTTP 方法与头信息，避免调试阶段频繁遇到预检失败；生产环境建议改成 [\"GET\",\"POST\"] 等最小集合，并只允许必要的自定义头。 安全建议：在生产配置中结合环境变量或独立配置文件控制 CORS 列表，避免代码提交时泄露实际域名；同时配合 API 网关或 WAF 做进一步访问控制。","4-全局配置与数据模型#4. 全局配置与数据模型":"利用环境变量保存 Dify 的访问凭证，同时定义本项目会用到的 Pydantic 数据模型：\n# 配置 DIFY_API_KEY = os.getenv(\"DIFY_API_KEY\") DIFY_BASE_URL = os.getenv(\"DIFY_BASE_URL\", \"https://api.dify.ai/v1\") class ChatMessage(BaseModel): message: str user_id: str = \"default\" conversation_id: Optional[str] = None model: str = \"learning-assistant\" temperature: float = 0.7 max_tokens: Optional[int] = None class LearningRequest(BaseModel): \"\"\"学习相关请求\"\"\" question: str subject: Optional[str] = None # 学科：math, physics, chemistry, etc. difficulty: Optional[str] = \"medium\" # easy, medium, hard user_level: Optional[str] = \"intermediate\" # beginner, intermediate, advanced conversation_id: Optional[str] = None 默认参数：ChatMessage 预设模型 ID、温度等，让前端即开即用。 场景化建模：LearningRequest 聚焦学习问答，覆盖学科、难度、用户水平等维度。","5-封装-dify-客户端#5. 封装 Dify 客户端":"DifyClient 是后端的核心，它负责与 Dify 通信、补齐工作流所需的上下文，并处理重试逻辑。\nclass DifyClient: def __init__(self): self.api_key = DIFY_API_KEY self.base_url = DIFY_BASE_URL async def send_message(self, message: str, user_id: str = \"default\", conversation_id: Optional[str] = None, inputs: Dict[str, Any] = None): \"\"\"发送消息到Dify\"\"\" if not self.api_key: raise HTTPException(status_code=500, detail=\"Dify API Key未配置\") url = f\"{self.base_url}/chat-messages\" headers = { \"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\" } # 为DeepResearch工作流提供必需的输入参数 research_inputs = inputs or {} if \"Research_Theme\" not in research_inputs: # 使用NER从消息中提取研究主题 research_theme = extract_research_theme(message) research_inputs[\"Research_Theme\"] = research_theme print(f\"[NER] 提取的研究主题: {research_theme}\") payload = { \"inputs\": research_inputs, \"query\": message, \"response_mode\": \"blocking\", \"user\": user_id, \"files\": [] } print(f\"[DIFY] 发送的请求参数: {payload}\") # 打印完整请求参数 if conversation_id: payload[\"conversation_id\"] = conversation_id # 添加重试机制 max_retries = 3 last_exception = None for attempt in range(max_retries): try: start_time = time.time() print(f\"[DIFY] 开始请求 (尝试 {attempt + 1}/{max_retries}): {message[:50]}...\") async with httpx.AsyncClient(timeout=120.0) as client: response = await client.post(url, json=payload, headers=headers) end_time = time.time() print(f\"[DIFY] 请求完成，耗时: {(end_time - start_time)*1000:.2f}ms\") if response.status_code == 200: data = response.json() print(f\"[DIFY] 原始响应: {data}\") # 打印完整响应 # 检查Dify API是否返回错误 if \"error\" in data: error_msg = data.get(\"error\", {}).get(\"message\", \"未知错误\") print(f\"[DIFY] API错误: {error_msg}\") raise HTTPException( status_code=500, detail=f\"Dify API错误: {error_msg}\" ) elif \"answer\" not in data: print(f\"[DIFY] 响应格式错误: {data}\") raise HTTPException( status_code=500, detail=\"Dify API响应格式错误\" ) elif not data.get(\"answer\"): print(f\"[DIFY] 警告: Dify返回空答案\") # 不抛出异常，但记录警告 return { \"answer\": data.get(\"answer\", \"未收到回复\"), \"conversation_id\": data.get(\"conversation_id\"), \"message_id\": data.get(\"message_id\"), \"metadata\": data.get(\"metadata\", {}) } else: error_text = response.text print(f\"[DIFY] HTTP错误 {response.status_code}: {error_text}\") raise HTTPException( status_code=response.status_code, detail=f\"Dify API错误: {error_text}\" ) except (httpx.ConnectError, httpx.TimeoutException) as e: last_exception = e if attempt \u003c max_retries - 1: wait_time = 2 ** attempt # 指数退避 print(f\"[DIFY] 连接失败，{wait_time}秒后重试: {e}\") await asyncio.sleep(wait_time) continue else: print(f\"[DIFY] 所有重试失败: {e}\") raise HTTPException(status_code=503, detail=\"无法连接到Dify服务\") NER 主题抽取：extract_research_theme 在缺省时自动填充 Research_Theme，确保工作流变量完整。 异步请求：httpx.AsyncClient + async/await 保持高并发性能。 指数退避：对网络波动更加友好，最后一次失败才抛出异常。 DifyClient 还提供 get_conversation_history，用于在 ChatNIO 中回放历史记录：\nasync def get_conversation_history(self, conversation_id: str): \"\"\"获取对话历史\"\"\" if not self.api_key: raise HTTPException(status_code=500, detail=\"Dify API Key未配置\") url = f\"{self.base_url}/messages\" headers = { \"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\" } params = { \"conversation_id\": conversation_id, \"first_id\": \"\", \"limit\": 20 } try: async with httpx.AsyncClient(timeout=120.0) as client: response = await client.get(url, headers=headers, params=params) if response.status_code == 200: data = response.json() return data.get(\"data\", []) else: raise HTTPException( status_code=response.status_code, detail=f\"获取对话历史失败: {response.text}\" ) except Exception as e: raise HTTPException(status_code=500, detail=f\"获取对话历史失败: {str(e)}\")","6-编排-api-端点#6. 编排 API 端点":"服务端暴露了一组围绕 ChatNIO 和学习助手的 REST API。我们按功能分层理解。","61-基础健康检查#6.1 基础健康检查":"@app.get(\"/\") async def root(): return {\"message\": \"学习助手API服务运行中\", \"status\": \"healthy\"} @app.get(\"/health\") async def health_check(): \"\"\"健康检查接口\"\"\" return { \"status\": \"healthy\", \"timestamp\": datetime.now().isoformat(), \"service\": \"learning-assistant-api\", \"version\": \"1.0.0\" } / 提供轻量级握手信息。 /health 返回时间戳与版本号，便于监控系统拉取。","62-聊天主流程#6.2 聊天主流程":"@app.post(\"/chat\") async def chat_direct(chat_message: ChatMessage): \"\"\"聊天接口 - 直接匹配ChatNIO请求路径\"\"\" try: result = await dify_client.send_message( chat_message.message, chat_message.user_id, chat_message.conversation_id ) return { \"success\": True, \"message\": \"\", \"data\": { \"message\": result[\"answer\"], \"keyword\": \"学习助手\", \"tokens\": len(result[\"answer\"]), \"quota\": 0, \"conversation_id\": result[\"conversation_id\"], \"message_id\": result[\"message_id\"] } } except HTTPException as e: return { \"success\": False, \"message\": e.detail, \"data\": None } 直接复用 ChatMessage，返回结构完全对齐 ChatNIO 预期。 /api/chat 只是同一逻辑的别名，保证历史兼容。","63-学习场景增强#6.3 学习场景增强":"@app.post(\"/api/learning/ask\") async def ask_learning_question(request: LearningRequest): \"\"\"学习问题解答接口\"\"\" try: # 构建学习上下文 context_message = f\"\"\" 学科: {request.subject or '通用'} 难度: {request.difficulty} 用户水平: {request.user_level} 问题: {request.question} 请作为学习助手，提供详细、易懂的解答。 \"\"\" result = await dify_client.send_message( context_message, f\"user_{request.user_level}\", request.conversation_id, inputs={ \"Research_Theme\": request.question, # 使用问题作为研究主题 \"subject\": request.subject, \"difficulty\": request.difficulty, \"user_level\": request.user_level } ) return { \"success\": True, \"message\": \"\", \"data\": { \"answer\": result[\"answer\"], \"conversation_id\": result[\"conversation_id\"], \"subject\": request.subject, \"difficulty\": request.difficulty, \"metadata\": result[\"metadata\"] } } 通过格式化上下文，让 LLM 精准理解用户水平。 inputs 同步传入 Dify 工作流，维持变量一致性。 /api/learning/study-plan 则输出多学科学习计划，复用同一 send_message 通道。","64-模型与对话管理#6.4 模型与对话管理":"@app.get(\"/api/models\") async def get_models(): \"\"\"获取可用模型列表 - ChatNIO需要这个接口\"\"\" return { \"success\": True, \"message\": \"\", \"data\": [ { \"id\": \"learning-assistant\", \"name\": \"学习助手\", \"description\": \"基于Dify的智能学习助手\", \"provider\": \"dify\", \"enabled\": True } ] } @app.post(\"/api/conversation\") async def create_conversation(request: Request): \"\"\"创建对话 - ChatNIO需要\"\"\" return { \"success\": True, \"message\": \"\", \"data\": { \"id\": f\"conv_{os.urandom(4).hex()}\", \"name\": \"新对话\", \"message\": [], \"model\": \"learning-assistant\" } } ChatNIO 在初始化时会先获取模型列表、创建对话 ID，然后再调用聊天接口。 /api/conversation/{conversation_id} 会走 get_conversation_history，把 Dify 返回的历史消息原样回传。","7-依赖注入与身份校验#7. 依赖注入与身份校验":"走到这里，我们已经有了一个能跑起来的学习助手 API，但要想在真实环境中上线，还得考虑「谁」在调用接口。FastAPI 的依赖注入（Dependency Injection, DI）机制就像给服务加上一层“入口守卫”，从而在不破坏核心业务代码的前提下，完成认证与授权。\nfrom fastapi import Depends, HTTPException, status from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials security = HTTPBearer() async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)): token = credentials.credentials # 验证 token 并返回用户信息 if token == \"invalid\": raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\" ) return {\"user_id\": 1, \"name\": \"包博文\"} @app.get(\"/me/\") async def read_users_me(current_user: dict = Depends(get_current_user)): return current_user 入口守卫：HTTPBearer 负责解析来自前端的 Authorization: Bearer xxx，我们只需要专注 token 的校验逻辑。 面向接口编程：任何需要当前用户信息的端点，都可以通过 Depends(get_current_user) 注入，避免在每个路由里重复粘贴验证代码。 扩展性：将来接入 JWT、OAuth2 或者自研 SSO，只要调整 get_current_user 即可，业务端点无须改动。","8-数据持久化与-sqlalchemy-整合#8. 数据持久化与 SQLAlchemy 整合":"有了用户体系，下一步自然是落地数据。FastAPI 与 SQLAlchemy 是常规搭档，下面这段代码展示了最小可用配置，方便你在实验环境中迅速验证数据流。\nfrom sqlalchemy import create_engine, Column, Integer, String, Boolean from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker, Session SQLALCHEMY_DATABASE_URL = \"sqlite:///./test.db\" engine = create_engine(SQLALCHEMY_DATABASE_URL) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) Base = declarative_base() class User(Base): __tablename__ = \"users\" id = Column(Integer, primary_key=True, index=True) name = Column(String, index=True) email = Column(String, unique=True, index=True) is_active = Column(Boolean, default=True) Base.metadata.create_all(bind=engine) def get_db(): db = SessionLocal() try: yield db finally: db.close() @app.post(\"/users/\", response_model=UserResponse) async def create_user(user: UserCreate, db: Session = Depends(get_db)): db_user = User( name=user.name, email=user.email, is_active=user.is_active ) db.add(db_user) db.commit() db.refresh(db_user) return db_user 轻量起步：sqlite 让我们免去部署数据库的负担，适合 demo 或集成测试。部署时可以无缝切换到 PostgreSQL/MySQL。 会话生命周期：get_db 利用 yield 在请求结束后自动释放连接，避免连接泄露。 模型收口：定义 UserResponse、UserCreate 等 Pydantic 模型，既能校验入参，又能控制返回字段，保持 API 的契约清晰。","9-中间件与全局异常处理#9. 中间件与全局异常处理":"当 API 稳步成长，观测性与错误处理就像是给汽车装上仪表盘。通过自定义中间件与异常处理器，我们可以在一处集中记录耗时、拦截错误，并输出一致的响应格式。\nfrom fastapi import Request from fastapi.responses import JSONResponse import time @app.middleware(\"http\") async def add_process_time_header(request: Request, call_next): start_time = time.time() response = await call_next(request) process_time = time.time() - start_time response.headers[\"X-Process-Time\"] = str(process_time) return response @app.exception_handler(HTTPException) async def http_exception_handler(request: Request, exc: HTTPException): return JSONResponse( status_code=exc.status_code, content={\"message\": exc.detail, \"status_code\": exc.status_code} ) 性能洞察：X-Process-Time 头部能快速告诉你哪条请求慢了，结合日志平台即可对症下药。 统一返回格式：异常处理器避免了「哪个接口返回了奇怪字段」的尴尬，前端可放心解析。 扩展思路：在企业级项目中，可以继续叠加链路追踪 ID、请求日志、告警埋点，让系统运行状况一目了然。 最后，文件尾部提供 uvicorn 启动入口，方便在开发环境直接运行。"},"title":"02-backend-deep-dive"},"/docs/web/backend/fastapi/03-frontend-and-deployment/":{"data":{"":"","1-前端集成chatnio#1. 前端集成：ChatNIO":"ChatNIO 是一个 Go 写的开源 WebUI。它支持通过标准 REST 接口与大模型后端通信，非常适合与我们在前面构建的 FastAPI 服务对接。\n集成关键点：\n契约对齐：后端 ChatMessage、LearningRequest Pydantic 模型已经定义了字段，ChatNIO 只需按以下结构向 /api/chat 发送请求：\n{ \"message\": \"请用高中生能理解的方式解释麦克斯韦方程组\", \"user_id\": \"demo-user\", \"conversation_id\": null } FastAPI 会调用 DifyClient.send_message，透传到 Dify Deep Research 工作流并返回格式化答复。\n路由映射：/chat 与 /api/chat 共用一套业务逻辑，ChatNIO 默认使用后者。初始化时还会请求 /api/models、/api/conversation，这些端点已在后端实现，可返回模型列表与新对话 ID。\n会话管理：ChatNIO 会在后续消息中携带 conversation_id。FastAPI 会把它提交给 Dify 并允许 /api/conversation/{conversation_id} 读取历史消息，实现 Chat 回放。\n健康监控：/、/health 提供基础握手，适合配置在 ChatNIO 的启动脚本或反向代理健康检查。\n返回模式：当前实现采用一次性阻塞返回（response_mode=\"blocking\"）。若改为流式 SSE，可参考 FastAPI 的 StreamingResponse，同时在 ChatNIO 中启用流式渲染。","2-端到端运行指南#2. 端到端运行指南":"","3-总结与展望#3. 总结与展望":"这个项目本来是我帮朋友写来去打华为某嵌入开发板比赛的，一开始需求是只用对接一个webui，后面发现连后端都没有要自己搓，再后来原来连Workflow和主题都还没定，等我写完距离报名结束只剩下一个晚上了，然后他大手一挥不打了xs\n后面还要调docker之类部署到开发板上过于麻烦，这里就不赘述了。","启动与本地调试#启动与本地调试":"","步骤一准备工作#步骤一：准备工作":"获取 Dify API 密钥：\n登录 Dify 账户。 进入 设置 \u003e API 密钥，创建一个新的 API 密钥并复制它。 同时，记下 Dify API 基础 URL (例如 https://api.dify.ai/v1)。","步骤三启动前端界面-chatnio#步骤三：启动前端界面 (ChatNIO)":"进入 ChatNIO 工程目录：\ncd frontend/chatnio 配置 .env：指向 FastAPI 服务端点，并设定默认模型。\nNEXT_PUBLIC_API_URL=http://localhost:8000/api NEXT_PUBLIC_EDGE_URL=http://localhost:8000/api NEXT_PUBLIC_SYSTEM_NAME=学习助手 NEXT_PUBLIC_ALLOW_REGISTER=false NEXT_PUBLIC_ALLOW_PASSWORD_CHANGE=false NEXT_PUBLIC_DEFAULT_MODEL=learning-assistant 安装依赖并启动：\ngo mod tidy go run main.go 首次启动会请求后端的 /api/models，若能返回 learning-assistant，说明前后端契约已打通。","步骤二启动后端服务-fastapi#步骤二：启动后端服务 (FastAPI)":"进入后端目录，创建虚拟环境并安装依赖：\n配置环境变量：复制 .env.example 为 .env，填入 Dify 凭证。\nDIFY_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx DIFY_BASE_URL=https://api.dify.ai/v1 启动服务：开发建议使用 uvicorn，便于观察 DifyClient 的调试日志。\nuvicorn app:app --host 0.0.0.0 --port 8000 --reload","步骤四开始使用#步骤四：开始使用":"完成以上步骤后，就可以在 ChatNIO 界面中与智能学习助手进行对话了，此时后端遵循一个开源的Deep Research工作流。","测试#测试":"利用 TestClient 可以在不启动服务器的情况下验证接口契约，以下测试覆盖了健康检查与学习问答接口。\nfrom fastapi.testclient import TestClient import pytest client = TestClient(app) def test_root(): response = client.get(\"/\") assert response.status_code == 200 assert response.json() == { \"message\": \"学习助手API服务运行中\", \"status\": \"healthy\" } @pytest.mark.asyncio async def test_learning_endpoint(monkeypatch): async def mock_send_message(*_args, **_kwargs): return {\"answer\": \"OK\", \"conversation_id\": \"conv_1\", \"metadata\": {}} monkeypatch.setattr(dify_client, \"send_message\", mock_send_message) payload = {\"question\": \"测试题目\", \"subject\": \"math\"} response = client.post(\"/api/learning/ask\", json=payload) assert response.status_code == 200 assert response.json()[\"data\"][\"answer\"] == \"OK\"","第三阶段实战应用#第三阶段：实战应用":"认证与配额控制：当项目需要对 ChatNIO 的访问做权限管理时，可以在 FastAPI 上接入 OAuth2。下面的示例利用 OAuth2PasswordBearer 和 JWT 生成访问令牌，你可以替换成真实的用户校验逻辑或对接企业统一认证。\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm from passlib.context import CryptContext import jwt from datetime import datetime, timedelta pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\") oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\") SECRET_KEY = \"your-secret-key\" ALGORITHM = \"HS256\" ACCESS_TOKEN_EXPIRE_MINUTES = 30 def create_access_token(data: dict): to_encode = data.copy() expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES) to_encode.update({\"exp\": expire}) encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM) return encoded_jwt @app.post(\"/token\") async def login(form_data: OAuth2PasswordRequestForm = Depends()): # TODO: 替换为真实的账号体系 if form_data.username != \"test\" or form_data.password != \"test\": raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Incorrect username or password\" ) access_token = create_access_token(data={\"sub\": form_data.username}) return {\"access_token\": access_token, \"token_type\": \"bearer\"} 实时推送：如果希望在前端显示 Dify 工作流的进度或任务状态，可以使用 WebSocket 与客户端保持长连接。\nfrom fastapi import WebSocket, WebSocketDisconnect class ConnectionManager: def __init__(self): self.active_connections: List[WebSocket] = [] async def connect(self, websocket: WebSocket): await websocket.accept() self.active_connections.append(websocket) def disconnect(self, websocket: WebSocket): self.active_connections.remove(websocket) async def send_personal_message(self, message: str, websocket: WebSocket): await websocket.send_text(message) async def broadcast(self, message: str): for connection in self.active_connections: await connection.send_text(message) manager = ConnectionManager() @app.websocket(\"/ws/{client_id}\") async def websocket_endpoint(websocket: WebSocket, client_id: int): await manager.connect(websocket) try: while True: data = await websocket.receive_text() await manager.send_personal_message(f\"You wrote: {data}\", websocket) await manager.broadcast(f\"Client #{client_id} says: {data}\") except WebSocketDisconnect: manager.disconnect(websocket) await manager.broadcast(f\"Client #{client_id} left the chat\")","运行服务#运行服务":"uvicorn app:app --host 0.0.0.0 --port 8000 --reload --reload 能在保存时自动重启，适合开发阶段。 终端会出现 [DIFY]、[NER] 等调试日志，便于观察请求耗时与主题抽取情况。 若部署到生产，请移除 --reload，改用 gunicorn + uvicorn.workers.UvicornWorker，并通过环境变量配置真实域名和密钥。","配置管理#配置管理":"将 Dify API 密钥、数据库连接等敏感信息放入 .env，通过 pydantic-settings 统一加载，可避免散落在代码中的硬编码。\nfrom pydantic_settings import BaseSettings class Settings(BaseSettings): app_name: str = \"学习助手API\" database_url: str | None = None secret_key: str debug: bool = False class Config: env_file = \".env\" settings = Settings()","验证接口#验证接口":"访问 http://127.0.0.1:8000/docs，确认 /chat、/api/learning/ask、/api/models 等端点已注册。\n使用 curl 或 httpie 调用 /api/chat，确保基础对话流程正常：\ncurl -X POST http://127.0.0.1:8000/api/chat ^ -H \"Content-Type: application/json\" ^ -d \"{\\\"message\\\":\\\"如何用积分求解函数 x^2 的面积？\\\",\\\"user_id\\\":\\\"demo-user\\\"}\" 测试 /api/learning/ask，检查学习上下文字段是否被传入 Dify：\ncurl -X POST http://127.0.0.1:8000/api/learning/ask ^ -H \"Content-Type: application/json\" ^ -d \"{\\\"question\\\":\\\"解释热力学第二定律\\\",\\\"subject\\\":\\\"physics\\\",\\\"difficulty\\\":\\\"hard\\\",\\\"user_level\\\":\\\"advanced\\\"}\" 返回体中的 metadata 字段会包含 Workflow 细节，便于调试。\n若缺少密钥或网络异常，FastAPI 会抛出 500 并输出具体错误，日志中也会记录重试情况。"},"title":"03-frontend-and-deployment"},"/docs/web/backend/gin/":{"data":{"":"Go 语言介绍Go（又称 Golang）是 Google 开发的一门开源编程语言。它是一门静态强类型、编译型语言，并且内置了强大的并发支持。Go 语言的设计哲学是“少即是多”，它语法简洁、上手快，同时性能卓越，被誉为“云时代的 C 语言”。\n在底层，Go 拥有一个高效的垃圾回收器和一个强大的运行时（runtime），能够轻松地将并发编程的威力发挥到极致。它的标准库非常丰富，特别是 net/http 包，为构建高性能的 Web 服务提供了坚实的基础。\n为什么选择 Go？ 初次接触 Go，你可能会被它的简洁所吸引。没有复杂的类继承，没有繁琐的注解，一切都显得那么直白。选择 Go 作为后端开发语言，原因有三：\n极致的性能：作为一门编译型语言，Go 的性能远超解释型语言如 Node.js 或 Python。它的原生并发模型（Goroutine）使得构建高并发服务变得异常轻松，能够充分利用多核处理器的性能。\n开发的简洁性：Go 的语法规则很少，学习曲线平缓。它强制性的代码格式化工具 gofmt 让团队协作变得更加愉快，告别了关于代码风格的无尽争论。\n强大的生态和工具链：虽然不像 Java 或 Node.js 那样历史悠久，但 Go 的生态系统已经非常成熟。从 Web 框架 Gin，到 ORM 库 Gorm，再到各种中间件，你几乎可以找到任何需要的轮子。\n说了这么多，让我们开始这段探索之旅吧！本文将主要包含以下内容：\n创建一个 Go 项目 使用 Gin 框架处理路由和请求 项目结构的最佳实践 使用 Gorm 连接 MySQL 并实现 CRUD 生成 Swagger 接口文档 创建一个 Go 项目 首先，确保你的操作系统上已经安装了 Go (版本 \u003e= 1.18)。与 Node.js 使用 npm 管理依赖不同，Go 使用 Go Modules 来管理项目依赖。\n打开你的终端，创建一个新的项目目录，然后输入以下命令来初始化项目：\n# 创建一个项目文件夹 mkdir go-gin-blog cd go-gin-blog # 初始化 Go Module，'go-gin-blog' 是你的模块名，通常是你的代码仓库地址 go mod init go-gin-blog 这个命令会生成一个 go.mod 文件，它类似于 Node.js 的 package.json，用于记录项目的依赖信息。\n项目文件介绍 现在，让我们在项目根目录下创建一个 main.go 文件。这是我们应用的入口。\npackage main import \"fmt\" func main() { fmt.Println(\"Hello, Go!\") } 此时的项目结构非常简单：\ngo-gin-blog/\r├── go.mod\r└── main.go go.mod: 定义了模块路径和依赖项。 main.go: 应用程序的入口文件。package main 和 func main() 表明这是一个可执行程序。 项目运行 在终端中输入以下命令，即可运行你的应用：\ngo run main.go 你会在控制台看到 “Hello, Go!” 的输出。这标志着我们的 Go 环境已经准备就绪。\n接下来，我们将引入流行的 Web 框架 Gin，让我们的应用能够处理 HTTP 请求。\n首先，获取 Gin 框架：\ngo get -u github.com/gin-gonic/gin go get 命令会自动下载依赖包，并将其版本信息记录在 go.mod 和 go.sum 文件中。\n修改 main.go，启动我们的第一个 Web 服务：\npackage main import ( \"net/http\" \"github.com/gin-gonic/gin\" ) func main() { // 创建一个默认的 Gin 引擎 r := gin.Default() // 定义一个 GET 路由 r.GET(\"/\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"message\": \"Hello, Gin!\", }) }) // 监听并启动服务，默认在 8080 端口 r.Run() // 监听并服务于 0.0.0.0:8080 } 再次运行 go run main.go，服务启动后，在浏览器或 Postman 中访问 http://localhost:8080，你将看到如下的 JSON 返回：\n{ \"message\": \"Hello, Gin!\" } 代码简析 从 main.go 中可以看出，我们使用 gin.Default() 创建了一个路由引擎。r.GET(\"/\", ...) 定义了一个处理根路径 / 的 GET 请求的处理器（Handler）。这个处理器是一个函数，它接收一个 *gin.Context 类型的参数。gin.Context 是 Gin 中最重要的部分，它封装了原始的 *http.Request 和 http.ResponseWriter，并提供了大量便捷的方法来处理请求和生成响应，例如我们用到的 c.JSON()。\n最后，r.Run() 启动了 HTTP 服务器，开始监听端口。\n新增一个用户模块 随着项目功能的增加，将所有代码都放在 main.go 中显然是不可持续的。我们需要对代码进行组织。与 NestJS 强大的 CLI 不同，Go 鼓励开发者手动创建文件和组织结构，这让你对项目的每一部分都有更清晰的掌控。\n让我们创建一个用户模块。首先，我们来规划一下项目结构。一个常见的 Go Web 项目结构如下：\ngo-gin-blog/\r├── cmd/ // 应用主入口\r│ └── main.go\r├── internal/ // 内部模块，外部无法导入\r│ ├── handlers/ // 存放 HTTP 处理器 (类似 Controller)\r│ ├── models/ // 存放数据模型 (类似 Entity)\r│ └── services/ // 存放业务逻辑 (类似 Service)\r├── go.mod\r└── go.sum 让我们按照这个结构进行重构。\n创建 cmd/main.go 并将入口代码移入。 创建 internal/handlers/user_handler.go 文件。 在 user_handler.go 中，我们编写一个简单的 HTTP 请求处理器：\n// internal/handlers/user_handler.go package handlers import ( \"net/http\" \"github.com/gin-gonic/gin\" ) // GetUser godoc // @Summary 获取用户信息 // @Description 这是一个获取用户信息的示例接口 // @Produce json // @Success 200 {object} map[string]interface{} // @Router /user [get] func GetUser(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"message\": \"Hello, Go User!\", }) } 然后，修改入口文件 cmd/main.go 来注册这个新的路由：\n// cmd/main.go package main import ( \"github.com/gin-gonic/gin\" \"go-gin-blog/internal/handlers\" // 导入我们自己的包 ) func main() { r := gin.Default() // 使用路由组来管理用户相关的路由 userGroup := r.Group(\"/user\") { userGroup.GET(\"\", handlers.GetUser) } r.Run() } 现在，我们通过 go run cmd/main.go 来启动应用。等待程序编译运行后，在浏览器输入 http://localhost:8080/user 访问即可看到结果。\nGin 的核心：路由与请求处理 控制器（在 Gin 中通常称为 Handler）负责处理传入的请求并向客户端返回响应。路由机制则控制哪个 Handler 接收哪些请求。\n路由组 (Route Group) 与 NestJS 使用 @Controller('user') 装饰器为整个控制器添加路径前缀类似，Gin 提供了路由组的功能。这对于组织结构清晰的路由非常有帮助。\n// 所有以 /v1 开头的路由 v1 := r.Group(\"/v1\") { // 匹配 GET /v1/users v1.GET(\"/users\", GetUsersHandler) // 匹配 POST /v1/users v1.POST(\"/users\", CreateUserHandler) } 获取请求参数 处理程序通常需要访问客户端请求的详细信息。gin.Context 提供了丰富的方法来获取这些信息。\n路由参数:\n当需要接收动态数据时，例如 GET /user/123，我们可以这样定义路由：\nr.GET(\"/user/:id\", func(c *gin.Context) { // 使用 c.Param() 获取路由参数 id := c.Param(\"id\") c.JSON(http.StatusOK, gin.H{\"message\": \"User ID is: \" + id}) }) 查询参数:\n对于 URL 中的查询字符串，例如 GET /search?name=john，可以这样获取：\nr.GET(\"/search\", func(c *gin.Context) { // 使用 c.Query() 获取查询参数 name := c.Query(\"name\") // c.DefaultQuery() 可以提供一个默认值 age := c.DefaultQuery(\"age\", \"20\") c.JSON(http.StatusOK, gin.H{ \"name\": name, \"age\": age, }) }) 请求体 (Request Payload):\n当处理 POST 或 PUT 请求时，我们通常需要解析请求体中的数据。Gin 可以轻松地将 JSON、XML 或表单数据绑定到 Go 的 struct 上。这引出了我们下一个重要概念：结构体 (Struct)。\n与 NestJS 中使用 DTO (数据传输对象) 类来定义数据结构类似，在 Go 中，我们使用 struct。让我们创建一个 CreateUser 结构体。\n// 通常放在 internal/models/user_model.go package models type CreateUser struct { // `json:\"name\"` 这种被称为 struct tag，用于指定 JSON 字段名 Name string `json:\"name\" binding:\"required\"` Age int `json:\"age\" binding:\"required,gt=0\"` } 这里的 binding:\"required\" 是 Gin 提供的验证功能，表示该字段为必填。\n现在，我们可以在 Handler 中使用 c.ShouldBindJSON() 来绑定和验证数据：\n// internal/handlers/user_handler.go import \"go-gin-blog/internal/models\" func CreateUser(c *gin.Context) { var user models.CreateUser // 将请求的 JSON 绑定到 user 结构体上 if err := c.ShouldBindJSON(\u0026user); err != nil { // 如果验证失败，返回 400 错误 c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) return } c.JSON(http.StatusOK, gin.H{ \"message\": \"User created successfully\", \"user\": user, }) } MySQL 操作与 GORM 作为后端项目，与数据库的交互是核心功能。在这里，我们选择 Go 社区最流行的 ORM 框架 GORM。\nGORM 介绍 GORM (Go Object Relational Mapping) 是一个功能强大、对开发者友好的 Go 语言 ORM 库。它提供了简洁的 API 来实现对数据库的增删改查，支持关联、事务、数据库迁移等高级功能。\n环境配置与连接 首先，安装 GORM 和 MySQL 驱动：\ngo get -u gorm.io/gorm go get -u gorm.io/driver/mysql 为了管理数据库配置，我们通常不会硬编码在代码中。一个好的实践是使用环境变量或配置文件。为了简单起见，我们先在代码中定义连接信息。\n创建一个 internal/database/db.go 文件来处理数据库的初始化：\n// internal/database/db.go package database import ( \"gorm.io/driver/mysql\" \"gorm.io/gorm\" \"log\" ) var DB *gorm.DB func InitDB() { var err error dsn := \"root:root@tcp(127.0.0.1:3306)/go_blog?charset=utf8mb4\u0026parseTime=True\u0026loc=Local\" DB, err = gorm.Open(mysql.Open(dsn), \u0026gorm.Config{}) if err != nil { log.Fatalf(\"failed to connect database: %v\", err) } log.Println(\"Database connection successful.\") } 请将 dsn 中的用户名、密码和数据库名替换为你自己的配置。\n然后在 cmd/main.go 中调用这个初始化函数：\n// cmd/main.go package main import ( \"github.com/gin-gonic/gin\" \"go-gin-blog/internal/database\" // 导入数据库包 \"go-gin-blog/internal/handlers\" ) func main() { // 在启动服务前，初始化数据库连接 database.InitDB() r := gin.Default() // ... 路由注册 r.Run() } 实现 CRUD 现在数据库已经连接，我们可以创建数据表对应的模型，并通过接口实现 CRUD 功能。\n创建 user.entity.go 模型 (在 Go 中通常叫 Model) // internal/models/user_model.go package models import \"gorm.io/gorm\" // User 模型对应数据库中的 users 表 type User struct { gorm.Model // 内嵌 gorm.Model，包含了 ID, CreatedAt, UpdatedAt, DeletedAt 字段 Name string `gorm:\"type:varchar(100);not null\"` Age int `gorm:\"not null\"` } GORM 会自动将结构体名 User 转换为蛇形的复数 users 作为表名。\n自动迁移 (Auto Migration) GORM 可以根据你的模型自动创建或更新数据库表结构。\n// internal/database/db.go import \"go-gin-blog/internal/models\" func InitDB() { // ... 连接代码 // 自动迁移，只会添加缺失的字段，不会删除或修改 err = DB.AutoMigrate(\u0026models.User{}) if err != nil { log.Fatalf(\"failed to migrate database: %v\", err) } } 创建 user.service.go 为了将业务逻辑和数据访问与 Handler 解耦，我们创建一个 Service 层。\n// internal/services/user_service.go package services import ( \"go-gin-blog/internal/database\" \"go-gin-blog/internal/models\" ) func CreateUser(user *models.User) error { result := database.DB.Create(user) return result.Error } func GetUserByID(id uint) (models.User, error) { var user models.User result := database.DB.First(\u0026user, id) return user, result.Error } func UpdateUser(user *models.User) error { result := database.DB.Save(user) return result.Error } func DeleteUser(id uint) error { result := database.DB.Delete(\u0026models.User{}, id) return result.Error } 在 user_handler.go 中调用 Service 最后，我们在 Handler 中调用 Service 层的方法来完成整个请求流程。\n// internal/handlers/user_handler.go // (仅展示 CreateUser，其他类似) import ( \"go-gin-blog/internal/models\" \"go-gin-blog/internal/services\" \"strconv\" // 用于字符串转换 ) // 创建用户 func CreateUserHandler(c *gin.Context) { var user models.User if err := c.ShouldBindJSON(\u0026user); err != nil { c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) return } if err := services.CreateUser(\u0026user); err != nil { c.JSON(http.StatusInternalServerError, gin.H{\"error\": \"Failed to create user\"}) return } c.JSON(http.StatusOK, user) } // 根据 ID 获取用户 func GetUserHandler(c *gin.Context) { idStr := c.Param(\"id\") id, err := strconv.ParseUint(idStr, 10, 32) if err != nil { c.JSON(http.StatusBadRequest, gin.H{\"error\": \"Invalid user ID\"}) return } user, err := services.GetUserByID(uint(id)) if err != nil { c.JSON(http.StatusNotFound, gin.H{\"error\": \"User not found\"}) return } c.JSON(http.StatusOK, user) } 不要忘记在 main.go 中注册这些新的 CRUD 路由。\n接口文档 最后，作为一个专业的后端服务，接口文档必不可少。在 Go 生态中，swaggo 是一个非常流行的工具，它通过解析代码注释来自动生成 Swagger 文档。\n安装依赖 go get -u github.com/swaggo/gin-swagger go get -u github.com/swaggo/swag/cmd/swag 添加通用 API 注释 在 cmd/main.go 的 main 函数上方，添加描述整个 API 的注释。\n// cmd/main.go // @title Go Gin Blog API // @version 1.0 // @description 这是一个使用 Go 和 Gin 构建的博客 API 示例. // @host localhost:8080 // @BasePath / func main() { // ... } 为 Handler 添加注释 我们之前在 user_handler.go 中已经为 GetUser 添加了注释，这就是 swaggo 的格式。\n生成文档并集成 在项目根目录运行 swag 命令：\nswag init -g cmd/main.go 这个命令会扫描你的代码并生成一个 docs 文件夹。\n最后，在 main.go 中引入 gin-swagger 中间件：\n// cmd/main.go import ( // ... swaggerFiles \"github.com/swaggo/files\" ginSwagger \"github.com/swaggo/gin-swagger\" _ \"go-gin-blog/docs\" // 重要：引入生成的 docs 包 ) func main() { // ... r := gin.Default() // 集成 Swagger r.GET(\"/swagger/*any\", ginSwagger.WrapHandler(swaggerFiles.Handler)) // ... 你的其他路由 r.Run() } 现在重启服务，访问 http://localhost:8080/swagger/index.html，你就能看到一个漂亮且可交互的 API 文档了。\nGo Web 开发自学笔记 (第二章): 走向生产级应用 在第一章中，我们成功搭建了一个基于 Gin 和 Gorm 的 CRUD 应用，并为它生成了 API 文档。这很棒，但距离一个健壮、高性能的后端服务还有一段路要走。真实世界的应用需要处理认证、日志、性能优化等一系列复杂问题。\n这一章，我们将深入探讨 Go 在 Web 开发中的三大进阶利器：\n中间件 (Middleware)：构建可复用的请求处理逻辑。 并发编程 (Concurrency)：利用 Goroutine 提升应用性能。 gRPC: 实现高性能的内部服务间通信。 Gin 中间件：应用的“守门人” 如果你熟悉 NestJS 的守卫（Guards）、拦截器（Interceptors）或 Express.js 的中间件，那么 Gin 的中间件概念对你来说会非常亲切。\n中间件本质上是一个在请求到达你的业务处理器（Handler）之前或之后执行的函数。它形成了一个处理链，请求会像穿过一层层洋葱一样经过它们。这使得我们可以将一些通用的逻辑，如日志记录、身份验证、异常恢复等，从业务代码中抽离出来，实现高度复用。\nGin 事实上已经内置了一些中间件。当我们使用 gin.Default() 时，它就默认集成了 Logger（日志）和 Recovery（异常恢复）这两个中间件。","1-改造我们的应用以读取配置#1. 改造我们的应用以读取配置":"首先，一个好的实践是让应用配置（比如数据库连接字符串）外部化，而不是硬编码在代码里。这样我们的镜像才能在不同环境（开发、测试、生产）中使用不同的配置。\n我们修改 internal/database/db.go，让它从环境变量中读取数据库 DSN (Data Source Name)。\n// internal/database/db.go package database import ( \"gorm.io/driver/mysql\" \"gorm.io/gorm\" \"log\" \"os\" // 导入 os 包 ) var DB *gorm.DB func InitDB() { var err error // 从环境变量 \"DB_DSN\" 中读取连接信息 dsn := os.Getenv(\"DB_DSN\") if dsn == \"\" { // 如果环境变量为空，提供一个默认值，方便没有 Compose 的情况下运行 dsn = \"root:root@tcp(127.0.0.1:3306)/go_blog?charset=utf8mb4\u0026parseTime=True\u0026loc=Local\" } DB, err = gorm.Open(mysql.Open(dsn), \u0026gorm.Config{}) // ... 后续代码不变 }","2-编写-docker-composeyml-文件#2. 编写 docker-compose.yml 文件":"在项目根目录下，创建一个名为 docker-compose.yml 的文件。\nversion: '3.8' # 指定 compose 文件版本 services: # 数据库服务 db: image: mysql:8.0 # 使用官方的 MySQL 8.0 镜像 container_name: go_blog_db restart: always # 容器退出时总是重启 environment: MYSQL_ROOT_PASSWORD: 'root_password' # 设置 root 用户的密码 MYSQL_DATABASE: 'go_blog' # 初始化时创建的数据库名 ports: - \"3306:3306\" # 将主机的 3306 端口映射到容器的 3306 端口，方便外部工具连接 volumes: - db_data:/var/lib/mysql # 将数据库数据持久化到名为 db_data 的数据卷中 # 我们的 Go 应用服务 app: build: . # 使用当前目录下的 Dockerfile 来构建镜像 container_name: go_blog_app restart: always ports: - \"8080:8080\" - \"50051:50051\" environment: # 关键：在这里注入数据库的 DSN # `db` 是上面定义的数据库服务的名称，Compose 会自动处理 DNS 解析 DB_DSN: \"root:root_password@tcp(db:3306)/go_blog?charset=utf8mb4\u0026parseTime=True\u0026loc=Local\" depends_on: - db # 告诉 Compose，app 服务依赖于 db 服务，需要先启动 db volumes: db_data: # 定义一个数据卷，用于持久化存储 #### 3. 启动和管理 现在，管理整个开发环境只需要两条简单的命令： ```bash # 在后台启动所有服务（数据库和应用） docker-compose up -d # 停止并移除所有相关的容器、网络 docker-compose down 有了 Docker Compose，任何新加入项目的开发者，只需要安装 Docker，然后运行 docker-compose up -d，就可以在几分钟内拥有一套完整的、与团队其他成员完全一致的本地开发环境。这极大地提升了开发效率和协作的一致性。","cicd-与-github-actions让部署自动化#CI/CD 与 GitHub Actions：让部署自动化":"我们已经能手动构建镜像了，但这还不够。理想的流程是：每当我们向代码仓库的主分支提交代码时，系统会自动运行测试、构建新的镜像，并将其推送到镜像仓库中。这就是 CI/CD (持续集成/持续部署)。\nGitHub Actions 是 GitHub 自带的 CI/CD 工具，它与代码仓库无缝集成，配置简单，对于个人项目和中小型团队来说是绝佳的选择。","gin-的核心路由与请求处理#Gin 的核心：路由与请求处理":"","go-web-开发自学笔记-第三章-可靠高效的应用交付#Go Web 开发自学笔记 (第三章): 可靠高效的应用交付":"在前两章，我们全心投入于代码的世界，构建了一个功能丰富、性能优越的 Go 应用。但代码的终点并非在我们的编辑器里，而是在服务器上稳定运行，为用户创造价值。这个从代码到服务的“最后一公里”，充满了挑战：环境不一致、依赖冲突、手动部署的繁琐与高风险……\n这一章，我们将聚焦于软件交付的工程实践，解决这些痛点。我们将学习：\n容器化与 Docker：将我们的应用打包成一个标准、可移植的“集装箱”。\nCI/CD 与 GitHub Actions：建立一条自动化的流水线，从代码提交到镜像构建，一气呵成。","go-web-开发自学笔记-第二章-走向生产级应用#Go Web 开发自学笔记 (第二章): 走向生产级应用":"","go-语言介绍#Go 语言介绍":"","gorm-介绍#GORM 介绍":"","grpc微服务时代的利器#gRPC：微服务时代的利器":"我们目前构建的是一个单体应用，通过 RESTful API 对外提供服务。但在现代的微服务架构中，一个复杂的系统会被拆分成许多个小而专的服务。这些服务之间需要一种高效、可靠的通信方式。\n虽然服务间也可以使用 REST API 通信，但 gRPC 是一个更优的选择。\n什么是 gRPC? 它是一个由 Google 开发的高性能、开源的远程过程调用（RPC）框架。\n为什么用 gRPC?\n性能：基于 HTTP/2，支持双向流、头部压缩，性能远超基于 HTTP/1.1 的 REST。 协议约定：使用 Protocol Buffers (Protobuf) 作为接口定义语言 (IDL)。它能生成强类型的客户端和服务端代码，让服务间的调用像调用本地函数一样简单，并且杜绝了因字段名写错、类型不匹配等问题导致的运行时错误。","k8s-核心概念入门#K8s 核心概念入门":"我们将使用两个最核心的 K8s 资源来部署我们的应用：\nDeployment：定义了我们应用期望的状态。比如，“我希望一直有 3 个我的 Go 应用容器在运行”。Deployment 会持续监控，确保实际状态与期望状态一致。\nService：为一组功能相同的容器（由 Deployment 管理）提供一个统一的、稳定的访问入口。它就像一个集群内部的负载均衡器。\n编写 Kubernetes 部署清单\n在项目根目录下创建一个 k8s 文件夹，并在其中创建 deployment.yml 文件。\n# k8s/deployment.yml # --- Deployment 定义 --- apiVersion: apps/v1 kind: Deployment metadata: name: go-blog-app-deployment # Deployment 的名称 spec: replicas: 3 # 期望的副本数量，K8s 会确保一直有 3 个实例在运行 selector: matchLabels: app: go-blog-app # 选择器，用于关联下方的 Pod 模板 template: metadata: labels: app: go-blog-app # Pod 的标签，必须与上面的 selector 匹配 spec: containers: - name: go-blog-app-container # 使用我们在 CI/CD 流程中推送到 Docker Hub 的镜像 image: your-dockerhub-username/go-gin-blog:latest ports: - containerPort: 8080 name: http - containerPort: 50051 name: grpc # 在真实生产环境中，数据库连接等配置应该使用 K8s 的 ConfigMap 或 Secret 来管理 # env: # - name: DB_DSN # value: \"your_production_db_dsn\" --- # --- Service 定义 --- apiVersion: v1 kind: Service metadata: name: go-blog-app-service # Service 的名称 spec: # type: LoadBalancer 会让云服务商 (如 AWS, GCP, Azure) 自动创建一个外部负载均衡器来暴露服务 # 如果在本地环境 (如 Minikube)，可以使用 NodePort type: LoadBalancer selector: app: go-blog-app # 选择器，将流量转发到带有此标签的 Pod ports: - protocol: TCP port: 80 # Service 暴露给外部的端口 targetPort: 8080 # 流量转发到容器的目标端口 name: http - protocol: TCP port: 50051 targetPort: 50051 name: grpc 部署到集群 要运行 K8s，你可以使用云服务商提供的托管 K8s 服务（如 GKE, EKS, AKS），或者在本地使用 Minikube 或 Docker Desktop 自带的 Kubernetes 功能来创建一个单节点集群。\n部署应用的命令非常简单：\n# 应用清单文件，K8s 会根据文件内容创建或更新资源 kubectl apply -f k8s/deployment.yml # 查看 Deployment 的状态 kubectl get deployment # 查看 Pod (我们的容器实例) 的状态 kubectl get pods # 查看 Service 的状态，并获取外部访问 IP kubectl get service 当 go-blog-app-service 的 EXTERNAL-IP 从 变为一个实际的 IP 地址后，你就可以通过这个 IP 地址访问你的应用了！Kubernetes 会自动将你的请求负载均衡到 3 个 Pod 实例中的一个。","mysql-操作与-gorm#MySQL 操作与 GORM":"","为-handler-添加注释#为 Handler 添加注释":"","为什么选择-go#为什么选择 Go？":"","为我们的应用添加-grpc-服务#为我们的应用添加 gRPC 服务":"让我们为用户服务添加一个 gRPC 接口，使得其他内部服务可以通过 gRPC 来获取用户信息。\n定义 Protobuf 文件\n在项目根目录创建一个 proto 文件夹，并新建 user.proto 文件：\n// proto/user.proto syntax = \"proto3\"; package proto; option go_package = \"./proto\"; // 定义 User 服务 service UserService { // 定义一个 GetUser 方法 rpc GetUser(UserRequest) returns (UserResponse); } // GetUser 的请求消息体 message UserRequest { uint32 id = 1; } // GetUser 的响应消息体 message UserResponse { uint32 id = 1; string name = 2; int32 age = 3; } 安装 gRPC 相关工具\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 确保你的 $GOPATH/bin 在系统 PATH 环境变量中。\n生成 Go 代码\n在项目根目录运行 protoc 命令：\nprotoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ proto/user.proto 这个命令会在 proto 目录下生成 user.pb.go 和 user_grpc.pb.go 两个文件。它们包含了消息体的 Go 结构体和我们需要实现的服务端接口。\n实现 gRPC 服务端\n创建一个 internal/grpc/server.go 文件：\n// internal/grpc/server.go package grpc import ( \"context\" \"go-gin-blog/internal/services\" \"go-gin-blog/proto\" // 导入生成的 proto 包 ) // UserServer 结构体实现了 proto.UnimplementedUserServiceServer // 这是为了向前兼容，我们只需实现自己需要的方法 type UserServer struct { proto.UnimplementedUserServiceServer } // 实现 GetUser 方法 func (s *UserServer) GetUser(ctx context.Context, req *proto.UserRequest) (*proto.UserResponse, error) { // 调用我们已有的 business service user, err := services.GetUserByID(uint(req.Id)) if err != nil { return nil, err } return \u0026proto.UserResponse{ Id: uint32(user.ID), Name: user.Name, Age: int32(user.Age), }, nil } 启动 gRPC 服务\n最后，修改 cmd/main.go，让我们的应用同时监听 HTTP 和 gRPC 请求。\n// cmd/main.go import ( // ... 其他 import \"log\" \"net\" \"go-gin-blog/internal/grpc_server\" // 注意路径变化 \"go-gin-blog/proto\" \"google.golang.org/grpc\" ) func main() { // ... 初始化数据库和 Gin // 使用 Goroutine 启动 gRPC 服务，避免阻塞主线程 go func() { lis, err := net.Listen(\"tcp\", \":50051\") // gRPC 服务监听 50051 端口 if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() proto.RegisterUserServiceServer(s, \u0026grpc_server.UserServer{}) log.Println(\"gRPC server listening at\", lis.Addr()) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve gRPC: %v\", err) } }() // 启动 Gin HTTP 服务 log.Println(\"HTTP server listening at :8080\") r.Run(\":8080\") } 现在，你的应用不仅是一个 REST API 服务器，还是一个 gRPC 服务器！其他内部服务可以通过 gRPC 客户端，以一种类型安全、高性能的方式来调用你的用户服务。","什么是容器化为什么是-docker#什么是容器化？为什么是 Docker？":"想象一下，你要搬家。你是选择把成千上万件零散的家具、电器、书籍一件件搬上卡车，还是先把它们分门别类打包进一个个标准化的箱子里，再进行运输？\n答案显而易见。容器化就是软件世界的“打包术”。\n它将你的应用程序，连同它的所有依赖（比如特定的库、配置文件、甚至是操作系统的一部分），一起打包到一个轻量、独立、可执行的软件包中。这个包，我们称之为容器镜像 (Container Image)。\nDocker 是目前最流行、事实上的容器化标准。它带来的好处是革命性的：\n环境一致性：彻底解决“在我电脑上明明是好的”这一世纪难题。一个镜像无论是在开发者的 Mac、测试服务器的 Linux，还是云端的生产环境，其运行表现都是完全一致的。\n快速部署与扩展：容器的启动速度是秒级的，远快于虚拟机。这使得应用的部署、更新和弹性伸缩变得极其迅速。\n隔离性：容器之间相互隔离，一个容器的崩溃不会影响到其他容器，提高了应用的稳定性和安全性。\n为我们的 Go 应用编写 Dockerfile Dockerfile 是一个文本文件，它包含了构建一个 Docker 镜像所需的所有指令和步骤。它就是我们应用的“打包清单”。\n由于 Go 是编译型语言，我们可以利用 Docker 的多阶段构建 (Multi-stage builds) 特性来创建一个极其精简的最终镜像。这是一种最佳实践，它能有效减小镜像体积，提升安全性。\n在你的项目根目录下，创建一个名为 Dockerfile 的文件：\n# ---- Stage 1: The Builder ---- # 使用官方的 Go 语言镜像作为构建环境 FROM golang:1.21-alpine AS builder # 设置工作目录 WORKDIR /app # 复制 go.mod 和 go.sum 文件，并下载依赖 # 这一步可以利用 Docker 的缓存机制，如果依赖没有变化，则无需重复下载 COPY go.mod go.sum ./ RUN go mod download # 复制所有源代码到工作目录 COPY . . # 编译应用。 # CGO_ENABLED=0: 禁用 CGO，构建一个纯 Go 的静态二进制文件 # GOOS=linux: 确保编译出的文件可以在 Linux 环境下运行（Docker 容器的基础环境） RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main ./cmd/main.go # ---- Stage 2: The Final Image ---- # 使用一个极简的基础镜像，比如 alpine FROM alpine:latest # 设置工作目录 WORKDIR /app # 从 builder 阶段拷贝编译好的二进制文件到当前阶段 COPY --from=builder /app/main . # （可选）如果应用需要加载 ca-certificates (例如访问 HTTPS API) RUN apk --no-cache add ca-certificates # 暴露我们的应用监听的端口 (REST 和 gRPC) EXPOSE 8080 EXPOSE 50051 # 容器启动时执行的命令 CMD [\"./main\"] 同时，创建一个 .dockerignore 文件，告诉 Docker 在构建时忽略哪些文件，这能减小构建上下文，加快构建速度。\n# .dockerignore\r.git/\r.idea/\r*.md\rDockerfile\r.dockerignore","代码简析#代码简析":"","使用-docker-compose-简化本地开发#使用 Docker Compose 简化本地开发":"当我们应用所依赖的服务越来越多（数据库、缓存、消息队列……），手动管理这些容器的启动顺序、网络和数据卷会变得异常繁琐。\nDocker Compose 就是为了解决这个问题而生的。它是一个用于定义和运行多容器 Docker 应用程序的工具。你只需要使用一个 YAML 文件来配置你的应用服务，然后使用一条命令，就可以创建并启动所有服务。","创建-github-actions-workflow#创建 GitHub Actions Workflow":"在你的项目根目录下，创建 .github/workflows 文件夹。\n在该文件夹下，创建一个 YAML 文件，例如 ci-cd.yml。\n# .github/workflows/ci-cd.yml name: Go App CI/CD # 定义触发工作流的事件 on: push: branches: [ \"main\" ] # 当有代码推送到 main 分支时触发 # 定义工作流中的任务 jobs: build-and-push: # 任务运行的环境 runs-on: ubuntu-latest # 任务的步骤 steps: # 步骤1: 检出代码 - name: Checkout code uses: actions/checkout@v4 # 步骤2: 登录到 Docker Hub # 需要在 GitHub 仓库的 Settings -\u003e Secrets and variables -\u003e Actions 中配置 DOCKERHUB_USERNAME 和 DOCKERHUB_TOKEN - name: Login to Docker Hub uses: docker/login-action@v3 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} # 步骤3: 设置 Docker Buildx - name: Set up Docker Buildx uses: docker/setup-buildx-action@v3 # 步骤4: 构建并推送 Docker 镜像 - name: Build and push uses: docker/build-push-action@v5 with: context: . file: ./Dockerfile push: true # 确认推送 tags: ${{ secrets.DOCKERHUB_USERNAME }}/go-gin-blog:latest,${{ secrets.DOCKERHUB_USERNAME }}/go-gin-blog:${{ github.sha }} # 我们会打两个标签: latest 和 当前 commit 的 SHA 值，方便版本追溯","创建-userentitygo-模型-在-go-中通常叫-model#创建 user.entity.go 模型 (在 Go 中通常叫 Model)":"","创建-userservicego#创建 user.service.go":"","创建一个-go-项目#创建一个 Go 项目":"","在-user_handlergo-中调用-service#在 user_handler.go 中调用 Service":"","安装依赖#安装依赖":"","实现-crud#实现 CRUD":"","并发编程用-goroutine-为你的-api-提速#并发编程：用 Goroutine 为你的 API 提速":"这是 Go 语言最闪耀的特性。Goroutine 是由 Go 运行时管理的轻量级线程。创建成千上万个 Goroutine 也是轻而易举的事，这使得 Go 在处理高并发任务时表现得游刃有余。\n在我们的 Web 应用中，什么时候会用到并发呢？想象一个场景：当一个新用户注册时，我们的 CreateUserHandler 除了要将用户信息存入数据库，可能还需要：\n发送一封欢迎邮件。 为该用户生成初始化的统计数据。 将注册事件上报给分析系统。 如果这些操作串行执行，特别是发送邮件、调用第三方 API 这种耗时较长的网络 I/O 操作，会让用户等待很长时间才能收到注册成功的响应，体验极差。\n这时，Goroutine 就派上用场了。我们可以将那些非核心、耗时的操作放到一个新的 Goroutine 中去执行，让主流程快速返回。\n让我们来改造一下 CreateUserHandler：\n// internal/handlers/user_handler.go // 模拟发送邮件（实际应用中会是网络请求） func sendWelcomeEmail(userName string) { log.Printf(\"开始为 %s 发送欢迎邮件...\", userName) time.Sleep(2 * time.Second) // 模拟耗时操作 log.Printf(\"用户 %s 的欢迎邮件已发送。\", userName) } // 模拟上报分析数据 func reportAnalytics(eventName string, userName string) { log.Printf(\"开始上报事件 '%s' for user %s\", eventName, userName) time.Sleep(1 * time.Second) // 模拟耗时操作 log.Printf(\"事件 '%s' for user %s 上报成功。\", eventName, userName) } func CreateUserHandler(c *gin.Context) { var user models.User if err := c.ShouldBindJSON(\u0026user); err != nil { c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) return } // 核心业务：创建用户，这个必须同步执行 if err := services.CreateUser(\u0026user); err != nil { c.JSON(http.StatusInternalServerError, gin.H{\"error\": \"Failed to create user\"}) return } // ---- 并发处理非核心业务 ---- // 使用 `go` 关键字开启一个新的 Goroutine go sendWelcomeEmail(user.Name) go reportAnalytics(\"UserRegistered\", user.Name) c.JSON(http.StatusOK, user) } 效果分析： 现在，当请求 /user 创建用户时，API 会在数据库操作成功后立即返回响应。而发送邮件和上报数据的操作会在后台的 Goroutine 中并发执行，完全不影响主请求的响应时间。用户的体验得到了极大的提升。\n这就是 Go 并发编程的魅力：用一个简单的 go 关键字，就能轻松地将任务并发化，榨干服务器的性能。","应用中间件#应用中间件":"我们可以在不同粒度上应用中间件：\n全局应用： 对所有路由生效。\n// cmd/main.go func main() { // ... r := gin.New() // 使用 gin.New() 创建一个没有任何默认中间件的引擎 r.Use(handlers.LoggerMiddleware()) // 全局使用我们的日志中间件 r.Use(gin.Recovery()) // 手动添加 Recovery 中间件 // ... 注册路由 r.Run() } 路由组应用： 只对某个路由组生效，非常适合用于需要权限验证的 API 组。\n// cmd/main.go func main() { // ... r := gin.Default() // 假设这是一个需要鉴权的 API 组 authorized := r.Group(\"/admin\") // authorized.Use(AuthMiddleware()) // 在这里应用认证中间件 { authorized.GET(\"/dashboard\", ...) } }","接口文档#接口文档":"","新增一个用户模块#新增一个用户模块":"","构建并运行容器#构建并运行容器":"现在，打开终端，确保你已经安装了 Docker。\n构建镜像：\n# -t 参数为镜像命名，格式为 : docker build -t go-gin-blog:latest . 运行容器：\n# -p 参数将主机的端口映射到容器的端口 : docker run --rm -p 8080:8080 -p 50051:50051 go-gin-blog:latest --rm 参数表示容器停止后自动删除，方便调试。\n现在，你的整个 Go 应用，包括 Gin 和 gRPC 服务，都在一个隔离的容器中运行了！你可以像之前一样通过 http://localhost:8080 访问 REST API，或者使用 gRPC 客户端连接到 localhost:50051。应用的打包和环境问题，已经得到了完美的解决。","添加通用-api-注释#添加通用 API 注释":"","环境配置与连接#环境配置与连接":"","生成文档并集成#生成文档并集成":"","自动迁移-auto-migration#自动迁移 (Auto Migration)":"","自定义一个日志中间件#自定义一个日志中间件":"虽然 Gin 的默认日志已经很好用，但让我们亲手打造一个更详细的日志中间件来理解其工作原理。这个中间件将记录每个请求的耗时、客户端 IP 和请求路径。\n在 internal/handlers 目录下创建一个 middleware.go 文件：\n// internal/handlers/middleware.go package handlers import ( \"log\" \"time\" \"github.com/gin-gonic/gin\" ) func LoggerMiddleware() gin.HandlerFunc { return func(c *gin.Context) { // 请求开始前的时间 startTime := time.Now() // 调用链中的下一个处理函数 // 如果这里不调用 next()，请求将被中断 c.Next() // 请求结束后的时间 endTime := time.Now() // 计算执行时间 latencyTime := endTime.Sub(startTime) // 获取请求信息 reqMethod := c.Request.Method reqUri := c.Request.RequestURI statusCode := c.Writer.Status() clientIP := c.ClientIP() // 格式化日志输出 log.Printf( \"[GIN] %v | %3d | %13v | %15s | %-7s %s\", endTime.Format(\"2006/01/02 - 15:04:05\"), statusCode, latencyTime, clientIP, reqMethod, reqUri, ) } } 代码简析：\n我们的中间件 LoggerMiddleware 返回一个 gin.HandlerFunc 类型的函数，这是 Gin 中间件的标准格式。\nc.Next() 是关键。它将控制权移交给调用链中的下一个中间件或最终的业务 Handler。\n在 c.Next() 之后的代码，会在 Handler 执行完毕后才执行。这使得我们能精确地计算出整个请求的处理耗时。","获取请求参数#获取请求参数":"","见证奇迹#见证奇迹":"现在，将你的代码（包括 Dockerfile 和 .github/workflows/ci-cd.yml）提交并推送到 GitHub 的 main 分支。\ngit add . git commit -m \"feat: Add Dockerfile and GitHub Actions CI/CD\" git push origin main 进入 GitHub 仓库的 “Actions” 标签页，你会看到一个新的工作流正在运行！它会自动执行你在 YAML 文件中定义的每一个步骤。如果一切顺利，几分钟后，一个以你的代码最新版本构建的 Docker 镜像就会被推送到你的 Docker Hub 仓库中。\n部署： 至此，持续集成的“CI”部分和持续交付的“CD”中的打包部分已经完成。真正的“部署”步骤，就是登录到你的服务器，执行以下命令：\n# 拉取最新的镜像 docker pull your-dockerhub-username/go-gin-blog:latest # 停止并删除旧的容器 (如果是生产环境，需要更优雅的停机更新策略) docker stop old_container_name docker rm old_container_name # 运行新版本的容器 docker run -d -p 8080:8080 -p 50051:50051 --name new_container_name your-dockerhub-username/go-gin-blog:latest -d 参数表示在后台运行容器。","路由组-route-group#路由组 (Route Group)":"","迈向终极目标kubernetes-k8s#迈向终极目标：Kubernetes (K8s)":"Docker Compose 非常适合本地开发和简单的单机部署，但当应用需要部署到生产环境的服务器集群时，我们就需要一个更强大的“指挥官”——Kubernetes。\nKubernetes（常简称为 K8s）是 Google 开源的容器编排引擎，是目前容器化部署领域的事实标准。它能做什么？\n自动化部署和扩缩容：根据流量自动增加或减少应用的容器实例。 服务发现和负载均衡：自动将请求分发到健康的容器实例上。 自愈能力：当某个容器或节点发生故障，K8s 会自动重启或替换它，保证服务的高可用性。 配置和密钥管理：集中管理应用的配置信息。","配置-secrets#配置 Secrets":"去 Docker Hub (或其他镜像仓库，如 Aliyun ACR, Harbor) 注册一个账号。\n在 Docker Hub 的“账户设置” -\u003e “安全”中创建一个新的访问令牌 (Access Token)。请务必复制并保存好这个令牌，它只会出现一次。\n在你的 GitHub 仓库页面，进入 “Settings” -\u003e “Secrets and variables” -\u003e “Actions”。\n点击 “New repository secret”，创建两个秘密变量：\nDOCKERHUB_USERNAME: 你的 Docker Hub 用户名。 DOCKERHUB_TOKEN: 刚刚创建的访问令牌。","项目文件介绍#项目文件介绍":"","项目运行#项目运行":""},"title":"Gin"},"/docs/web/backend/middleware/":{"data":{"":"中间件相关学习笔记。"},"title":"中间件"},"/docs/web/backend/nestjs/":{"data":{"crud-操作实现#CRUD 操作实现":"","http-方法装饰器#HTTP 方法装饰器":"Nest.js 为所有标准的 HTTP 方法提供装饰器：@Get()、@Post()、@Put()、@Delete()、@Patch()、@Options() 和 @Head()。此外，@All() 定义了一个端点来处理所有这些方法：\nimport { Controller, Delete, Get, Patch, Post, Put } from '@nestjs/common'; import { AppService } from './app.service'; @Controller() export class AppController { constructor(private readonly appService: AppService) {} @Get() getHello(): string { return this.appService.getHello(); } @Get('/list') getHelloList(): string { return '这是list页面'; } @Post('/list') create(): string { return 'post请求页面'; } @Put('/list') update(): string { return 'put请求页面'; } @Delete('/list') delete(): string { return 'delete请求页面'; } @Patch('/list') patch(): string { return 'patch请求页面'; } }","nestjs#Nest.js":"Nest.js","中间件与异常处理#中间件与异常处理":"","中间件应用#中间件应用":"使用模块类的 configure() 方法设置中间件，包含中间件的模块必须实现 NestModule 接口：\nimport { Module, NestModule, MiddlewareConsumer } from '@nestjs/common'; import { LoggerMiddleware } from './common/middleware/logger.middleware'; import { CatsModule } from './cats/cats.module'; @Module({ imports: [CatsModule], }) export class AppModule implements NestModule { configure(consumer: MiddlewareConsumer) { consumer .apply(LoggerMiddleware) .forRoutes('cats'); } }","中间件机制#中间件机制":"中间件是在路由处理程序之前调用的函数，中间件函数可以访问 request 和 response 对象，以及应用请求-响应周期中的 next() 中间件函数。中间件函数能够执行各种任务，如验证请求、解析请求体、处理错误等。\n你可以在函数中或在具有 @Injectable() 装饰器的类中实现自定义 Nest 中间件：\nimport { Injectable, NestMiddleware } from '@nestjs/common'; import { Request, Response, NextFunction } from 'express'; @Injectable() export class LoggerMiddleware implements NestMiddleware { use(req: Request, res: Response, next: NextFunction) { console.log('Request...'); next(); } }","为什么选择-mysql-和-typeorm#为什么选择 MySQL 和 TypeORM":"作为后端项目，数据库连接是必不可少的。MySQL 作为关系型数据库，应用广泛且非常稳定，拥有非常活跃的社区支持。TypeORM 作为 Node.js 中老牌的 ORM 框架，无论是接口定义还是代码实现方面都简单易懂、可读性高，也很容易对接多种数据源。\nTypeORM 使用 TypeScript 编写，在 NestJS 框架下运行得非常好，也是 NestJS 首推的 ORM 框架，有开箱即用的 @nestjs/typeorm 软件包支持。","为什么选择-nestjs#为什么选择 Nest.js":"选择 Nest.js 作为后端开发框架有多个重要原因。首先，它提供了严格的模块分类机制，让代码组织更加清晰有序。其次，Nest.js 的学习曲线相对平缓，上手容易，同时拥有丰富的学习资源和活跃的社区支持。最重要的是，它能够充分利用 TypeScript 的严格类型系统来规范代码质量，这对于构建大型应用来说至关重要。\n从开发效率角度来看，Nest.js 的热更新机制采用了增量更新策略，这意味着在开发过程中只有发生变化的文件会被重新编译，大大提升了开发时的响应速度。从个人开发体验来说，Nest.js 使用装饰器语法编写代码，这种风格与 Java 的注解系统非常相似，对于有 Java 背景的开发者来说会感到非常熟悉，能够快速适应并提高开发效率。","为什么选择-swagger#为什么选择 Swagger":"作为后端接口，接口文档是必不可少的。选择 Swagger 的原因一方面是 Nest.js 提供了专用的模块来使用它，其次它可以精确地展示每个字段的意义，只要注解写得到位，就能生成清晰完整的 API 文档。","使用-nestjstypeorm-连接数据库#使用 @nestjs/typeorm 连接数据库":"首先安装依赖：\nnpm i @nestjs/typeorm typeorm mysql2 -S 在根目录下创建 .env 文件存放数据库配置信息：\nDB_HOST=localhost DB_PORT=3306 DB_USER=root DB_PASSWD=root DB_DATABASE=blog 在 app.module.ts 中连接数据库：\nimport { TypeOrmModule } from '@nestjs/typeorm'; import { ConfigService, ConfigModule } from '@nestjs/config'; import { Module } from '@nestjs/common'; import { AppController } from './app.controller'; import { AppService } from './app.service'; import { UserModule } from './user/user.module'; import * as path from 'path'; const envPath = path.resolve('.env'); @Module({ imports: [ ConfigModule.forRoot({ isGlobal: true, envFilePath: [envPath], }), TypeOrmModule.forRootAsync({ imports: [ConfigModule], inject: [ConfigService], useFactory: async (configService: ConfigService) =\u003e ({ type: 'mysql', entities: [], host: configService.get('DB_HOST', 'localhost'), port: configService.get\u003cnumber\u003e('DB_PORT', 3306), username: configService.get('DB_USER', 'root'), password: configService.get('DB_PASSWORD', 'root'), database: configService.get('DB_DATABASE', 'blog'), timezone: '+08:00', synchronize: true, }), }), UserModule ], controllers: [AppController], providers: [AppService], }) export class AppModule {}","入口文件分析#入口文件分析":"启动服务后，首先需要了解入口文件 main.ts 的工作原理：\nimport { NestFactory } from '@nestjs/core'; import { AppModule } from './app.module'; async function bootstrap() { const app = await NestFactory.create(AppModule); await app.listen(process.env.PORT ?? 3000); } bootstrap(); 从这段代码可以看出，应用启动过程分为两个主要步骤：首先使用 Nest.js 的工厂函数 NestFactory 来创建一个 AppModule 实例，然后通过 app.listen 来监听指定的端口。端口号可以通过环境变量 PORT 来自定义，如果没有设置则默认使用 3000 端口。\n需要注意的是，如果 3000 端口被占用，可以在 main.ts 里面修改默认端口号，或者创建 .env 文件来自定义端口号。启动成功后，在浏览器中访问 http://localhost:3000 就能看到应用的运行状态。","全局管道#全局管道":"使用 useGlobalPipes 实现全局作用域管道，使其应用于整个应用中的每个路由处理程序：\nasync function bootstrap() { const app = await NestFactory.create(AppModule); app.useGlobalPipes(new ValidationPipe()); await app.listen(3000); } bootstrap();","共享模块与全局模块#共享模块与全局模块":"在 Nest.js 中，默认情况下模块是单例的，这意味着你可以轻松地在多个模块之间共享任何提供者的同一实例。每个模块自动成为共享模块，一旦创建就可以被任何模块重用。\n如果你想要在其他几个模块之间共享 UserService 的实例，需要将 UserService 提供者添加到模块的 exports 数组中：\nimport { Module } from '@nestjs/common'; import { UserService } from './user.service'; import { UserController } from './user.controller'; @Module({ controllers: [UserController], providers: [UserService], exports: [UserService] }) export class UserModule {} 对于需要在所有地方导入的相同模块集，可以使用 @Global() 装饰器使模块全局化。全局模块应该只注册一次，通常由根模块或核心模块注册。","启动命令#启动命令":"安装完成后，可以通过查看 package.json 文件来了解可用的启动命令。最基本的启动命令是：\nnpm run start 这个命令会启动应用，但不会监听文件变化。如果你希望在开发过程中实现自动重新编译和重新加载服务器，可以使用：\nnpm run start:dev 这个命令会启动文件监视模式，当代码发生变化时会自动重新编译并重启服务，大大提升了开发效率。","在-maints-中引入#在 main.ts 中引入":"在 main.ts 中引入方法即可：\nimport { NestFactory } from '@nestjs/core'; import { AppModule } from './app.module'; import { generateDocument } from './doc'; async function bootstrap() { const app = await NestFactory.create(AppModule); generateDocument(app); await app.listen(process.env.PORT ?? 3000); } bootstrap();","守卫与拦截器#守卫与拦截器":"","守卫机制#守卫机制":"守卫是一个用 @Injectable() 装饰器注释的类，它实现了 CanActivate 接口。守卫有单一的责任，它们根据运行时存在的某些条件（如权限、角色、ACL 等）确定给定请求是否将由路由处理程序处理，这通常称为授权。\n守卫在所有中间件之后、任何拦截器或管道之前执行，是处理身份验证和授权的理想选择。","安装与配置#安装与配置":"首先安装依赖：\nnpm i @nestjs/swagger -S 创建 doc.ts 文件来配置 Swagger：\nimport { SwaggerModule, DocumentBuilder } from '@nestjs/swagger'; import * as packageConfig from '../package.json' export const generateDocument = (app) =\u003e { const options = new DocumentBuilder() .setTitle(packageConfig.name) .setDescription(packageConfig.description) .setVersion(packageConfig.version) .build(); const document = SwaggerModule.createDocument(app, options); SwaggerModule.setup('/api/doc', app, document); } 为了方便，Swagger 的配置信息都从 package.json 中获取。默认情况下，TypeScript 项目中不能直接导入 .json 模块，所以需要在 tsconfig.json 中新增 resolveJsonModule: true 配置。","实体定义#实体定义":"通过 TypeORM 连接好数据库后，需要创建数据表实体，并通过接口实现 CRUD 功能。创建 user.entity.ts 实例：\nimport { Column, Entity, PrimaryGeneratedColumn } from \"typeorm\"; @Entity('user') export class User { @PrimaryGeneratedColumn() id: number; @Column({ default: null }) name: string; @Column({type: 'timestamp', default: () =\u003e \"CURRENT_TIMESTAMP\"}) create_time: Date; @Column({type: 'timestamp', default: () =\u003e \"CURRENT_TIMESTAMP\"}) update_time: Date; }","应用启动与运行#应用启动与运行":"","异常过滤器#异常过滤器":"异常过滤器负责处理应用中所有未处理的异常。当应用代码未处理异常时，该层会捕获异常，然后自动发送适当的用户友好响应。\nNest.js 提供了一个内置的 HttpException 类，可以抛出标准异常：\n@Get() async findAll() { throw new HttpException('Forbidden', HttpStatus.FORBIDDEN); } 如果内置的异常不能满足需求，可以创建自定义异常。通过扩展基类 HttpException，自定义异常将与内置异常处理程序无缝协作：\nexport class ForbiddenException extends HttpException { constructor() { super('Forbidden', HttpStatus.FORBIDDEN); } }","总结与展望#总结与展望":"通过本文的学习，我们已经掌握了 Nest.js 框架的核心概念和基本使用方法。从项目创建、模块系统、控制器路由、数据库集成到接口文档生成，形成了一个完整的开发流程。\n这只是 Nest.js 学习之旅的开始，后面还会有更多复杂的场景等待探索，比如微服务架构、GraphQL 集成、测试策略、部署监控等。Nest.js 作为一个成熟的企业级框架，提供了丰富的功能和良好的扩展性，值得深入学习和实践。\n继续努力，加油！","拦截器系统#拦截器系统":"拦截器是用 @Injectable() 装饰器注释并实现 NestInterceptor 接口的类。每个拦截器都实现了 intercept() 方法，它有两个参数：第一个是 ExecutionContext 实例，第二个参数是 CallHandler。\nCallHandler 接口实现了 handle() 方法，你可以使用它在拦截器中的某个点调用路由处理程序方法。如果在 intercept() 方法的实现中不调用 handle() 方法，则根本不会执行路由处理程序方法。","接口文档与-swagger#接口文档与 Swagger":"","接口标签配置#接口标签配置":"在对应的 controller 文件中加入 @ApiTags 装饰器：\nimport { Controller, Get, Post, Body, Patch, Param, Delete, Put, } from '@nestjs/common'; import { ApiTags } from '@nestjs/swagger'; import { UserService } from './user.service'; @ApiTags('用户') @Controller('user') export class UserController { // ... 其他代码 } 通过上述操作后，启动服务，在浏览器输入 http://localhost:3000/api/doc 就可以看到生成的 API 文档了。","控制器与路由系统#控制器与路由系统":"","控制器基础#控制器基础":"控制器负责处理传入的请求并向客户端返回响应。控制器的目的是接收应用的特定请求，路由机制控制哪个控制器接收哪些请求。通常，每个控制器都有多条路由，不同的路由可以执行不同的操作。\n@Controller() 装饰器是定义基本控制器所必需的，该装饰器可以传入一个路径参数，作为访问这个控制器的主路径。如果不传参数，则表示使用默认路径。","控制器层实现#控制器层实现":"在 user.controller.ts 中添加相关路由：\nimport { Controller, Get, Post, Body, Patch, Param, Delete, Put, } from '@nestjs/common'; import { UserService } from './user.service'; @Controller('user') export class UserController { constructor(private readonly userService: UserService) {} @Post() async create(@Body() createUserDto) { return await this.userService.create(createUserDto); } @Get(':id') async findById(@Param('id') id: string) { return await this.userService.findById(id); } @Put(':id') async update(@Param('id') id: string, @Body() updateUserDto) { return await this.userService.updateById(id, updateUserDto); } @Delete(':id') async remove(@Param('id') id: string) { return await this.userService.remove(id); } }","数据库连接方式#数据库连接方式":"NestJS 使用 TypeORM 的方式有两种。一种是使用 NestJS 提供的 @nestjs/typeorm 集成包，另一种是直接使用 typeorm 自由封装 Providers。","数据库集成与-typeorm#数据库集成与 TypeORM":"","服务层实现#服务层实现":"修改 user.service.ts 文件，实现 CRUD 操作：\nimport { HttpException, Injectable } from '@nestjs/common'; import { InjectRepository } from '@nestjs/typeorm'; import { Repository } from 'typeorm'; import { User } from './entities/user.entity'; @Injectable() export class UserService { constructor( @InjectRepository(User) private readonly userRepository: Repository\u003cUser\u003e, ) {} async create(createUserDto: Partial\u003cUser\u003e): Promise\u003cUser\u003e { const { name } = createUserDto; if (!name) { throw new HttpException('名字不能为空', 401); } const userInfo = await this.userRepository.findOne({ where: { name } }); if (userInfo) { throw new HttpException('名称不能重复', 401); } return await this.userRepository.create(createUserDto); } async findById(id): Promise\u003cUser\u003e { return await this.userRepository.findOne(id); } async updateById(id, user): Promise\u003cUser\u003e { const existUser = await this.userRepository.findOne(id); if (!existUser) { throw new HttpException('用户不存在', 401); } const updateUser = this.userRepository.merge(existUser, user); return this.userRepository.save(updateUser); } async remove(id) { const existUser = await this.userRepository.findOne(id); if (!existUser) { throw new HttpException('用户不存在', 401); } return await this.userRepository.remove(existUser); } }","根模块结构#根模块结构":"从 main.ts 中可以看出，应用只导入了 AppModule，让我们深入查看 app.module.ts 文件：\nimport { Module } from '@nestjs/common'; import { AppController } from './app.controller'; import { AppService } from './app.service'; import { UserController } from './user/user.controller'; @Module({ imports: [], controllers: [AppController, UserController], providers: [AppService], }) export class AppModule {} AppModule 是应用程序的根模块，它提供了启动应用的引导机制，可以包含多个功能模块。每个 .module 文件都需要使用 @Module() 装饰器，这个装饰器接收四个重要属性：\nproviders: 提供者数组，包含服务、工厂等可注入的类 controllers: 控制器数组，定义该模块中可用的控制器 imports: 导入模块列表，用于引入其他模块导出的提供者 exports: 导出数组，指定该模块提供的、可以被其他模块使用的提供者","框架概述#框架概述":"Nest (NestJS) 是一个专为构建高效、可扩展的 Node.js 服务器端应用而设计的框架。它采用了渐进式 JavaScript 的设计理念，完全支持 TypeScript 开发，同时保持了使用纯 JavaScript 编码的灵活性。Nest.js 巧妙地将面向对象编程(OOP)、函数式编程(FP)和函数式反应式编程(FRP)的元素融合在一起，为开发者提供了一个结构清晰、易于维护的开发框架。\n在底层架构上，Nest.js 构建在强大的 HTTP 服务器框架之上。默认情况下使用 Express 作为底层引擎，但通过配置也可以无缝切换到 Fastify。这意味着开发者可以根据项目需求选择使用 Express 的 API 或者 Fastify 的 API，享受不同框架的优势。","模块创建与组织#模块创建与组织":"为了保持代码的清晰界限，Nest.js 提供了强大的模块化能力。你可以使用 CLI 命令快速生成不同类型的模块：\n使用 nest g mo 生成模块来组织代码 使用 nest g co 生成控制器来定义 CRUD 路径 使用 nest g s 生成服务来表示和隔离业务逻辑 使用 nest g resource 生成完整的 CRUD 功能模块 值得注意的是，使用命令创建 UserController 的同时，CLI 会自动在 app.module.ts 里面注册对应的 Controller，减少了手动配置的工作量。","模块系统详解#模块系统详解":"","模块配置#模块配置":"在 user.module.ts 中加上相关引入：\nimport { Module } from '@nestjs/common'; import { UserService } from './user.service'; import { UserController } from './user.controller'; import { TypeOrmModule } from '@nestjs/typeorm'; import { User } from './entities/user.entity'; @Module({ imports: [TypeOrmModule.forFeature([User])], controllers: [UserController], providers: [UserService], }) export class UserModule {}","环境准备#环境准备":"在开始使用 Nest.js 之前，首先需要确保你的操作系统上安装了 Node.js，版本要求不低于 16。Node.js 的安装相对简单，可以从官网下载安装包进行安装。","环境配置#环境配置":"NestJS 自带了多环境配置方法，使用 @nestjs/config 会默认从项目根目录载入并解析一个 .env 文件。首先需要安装依赖：\nnpm i @nestjs/config -S 安装完毕后，在 app.module.ts 中添加 ConfigModule 模块：\nimport { Module } from '@nestjs/common'; import { AppController } from './app.controller'; import { AppService } from './app.service'; import { UserModule } from './user/user.module'; import { ConfigModule } from '@nestjs/config'; @Module({ imports: [ConfigModule.forRoot(), UserModule], controllers: [AppController], providers: [AppService], }) export class AppModule {}","管道与验证#管道与验证":"","管道基础#管道基础":"管道是用 @Injectable() 装饰器注释的类，它实现了 PipeTransform 接口。管道有两个典型的用例：转型和验证。转型将输入数据转换为所需的形式，验证评估输入数据，如果有效则传递，否则抛出异常。","管道绑定#管道绑定":"要使用管道，需要将管道类的实例绑定到适当的上下文。如果希望将管道与特定的路由处理程序方法相关联，可以在方法参数级别绑定管道：\n@Get(':id') async findOne(@Param('id', ParseIntPipe) id: number) { return this.catsService.findOne(id); }","自定义管道#自定义管道":"每个管道都必须实现 transform() 方法来履行 PipeTransform 接口契约。这个方法有两个参数：value 参数是当前处理的方法参数，metadata 是当前处理的方法参数的元数据。","请求处理与装饰器#请求处理与装饰器":"处理程序通常需要访问客户端请求的详细信息。通过将 @Req() 装饰器添加到处理程序的签名中，可以指示 Nest.js 注入请求对象来访问请求信息：\nimport { Controller, Get, Req } from '@nestjs/common'; import { Request } from 'express'; @Controller('user') export class UserController { @Get() getHello(@Req() request: Request): string { console.log(request); return 'Hello Nest.js!'; } } 请求对象表示 HTTP 请求，具有请求查询字符串、参数、HTTP 标头和正文等属性。在大多数情况下，没有必要手动获取这些属性，Nest.js 提供了开箱即用的专用装饰器，如 @Body() 或 @Query()。","路由参数与请求负载#路由参数与请求负载":"当需要接受动态数据作为请求的一部分时（例如，GET /cats/1 获取 ID 为 1 的 cat），可以使用路由参数。在路由的路径中添加路由参数标记，使用 @Param() 装饰器访问这些参数：\n@Get(':id') findOne(@Param() params: any): string { console.log(params.id); return `This action returns a #${params.id} cat`; } 对于 POST 路由处理程序，如果客户端需要传递参数，可以使用 @Body() 装饰器。但首先需要确定 DTO（数据传输对象）架构。DTO 是定义数据如何通过网络发送的对象，可以通过 TypeScript 接口或类来确定：\nexport class CreateCatDto { name: string; age: number; breed: string; } @Post() async create(@Body() createCatDto: CreateCatDto) { return 'This action adds a new cat'; }","项目创建与环境搭建#项目创建与环境搭建":"","项目初始化#项目初始化":"使用 Nest.js 的命令行接口来创建新项目非常简单，只需要在终端中输入以下命令：\nnpm i -g @nestjs/cli nest new project-name 第一条命令会全局安装 Nest.js 的 CLI 工具，第二条命令会创建一个新的项目目录并自动安装所有必要的依赖。CLI 工具会自动生成项目的基础结构和配置文件，让你能够立即开始开发。","项目结构解析#项目结构解析":"创建完成后，在 src 目录下会生成一些 Nest.js 标准的文件，这些文件构成了应用的基础架构：\napp.controller.spec.ts: 这是 app.controller 的单元测试文件，用于编写控制器相关的测试用例 app.controller.ts: 控制器文件，主要负责处理 HTTP 请求和调用 service 层的处理方法，路由管理通常在这里进行 app.module.ts: 根模块文件，用于处理其他类的引用与共享，通常以 module 文件夹来组织不同的功能模块 app.service.ts: 服务文件，封装通用的业务逻辑、与数据层的交互（例如数据库操作）、以及其他第三方服务的请求 main.ts: 应用程序的入口文件，使用 NestFactory 来创建 Nest 应用实例"},"title":"NestJS"},"/docs/web/backend/package-managers/":{"data":{"anaconda一站式数据科学平台#Anaconda：一站式数据科学平台":"Anaconda 是面向数据科学的完整发行版，预装了 Conda、Python 以及几百个常用的数据科学包，比如 numpy、pandas、tensorflow 等。它的优势是开箱即用，不需要手动安装基础包。\n但 Anaconda 的缺点也很明显：体积巨大（GB 级），包含很多可能用不到的包，而且默认使用的源（defaults）更新较慢，部分包可能不是最新版本。Anaconda 适合零基础入门数据科学的新手，但不推荐用于生产环境或容器环境。\n# 创建环境 conda create -n myenv python=3.11 # 激活环境 conda activate myenv # 安装包 conda install numpy pandas # 从 environment.yml 创建环境 conda env create -f environment.yml","conda-libmamba-solverconda-的提速插件#conda-libmamba-solver：Conda 的提速插件":"如果不想安装 Mamba，但又想让 Conda 提速，可以使用 conda-libmamba-solver。这是 Conda 官方的求解器插件，把 Mamba 的求解器集成到 Conda 里，让在不改变使用习惯的情况下获得速度提升。\n从 Conda 23.10 版本开始，官方已经默认推荐使用 libmamba 求解器。安装和启用都很简单，只需要两行命令。\n# 安装插件 conda install -n base conda-libmamba-solver # 全局启用 conda config --set solver libmamba # 之后使用 conda 命令就会自动使用快速求解器 conda install numpy pandas","conda-生态跨语言环境管理#Conda 生态：跨语言环境管理":"如果做的是 AI、数据科学或需要 CUDA 的项目，PyPI 生态的工具就不够用了。这时候需要 Conda 生态的工具，它们不仅能管理 Python 包，还能处理 C++ 库、CUDA 驱动等非 Python 依赖。\n在深入之前，我们先理清几个概念。Anaconda、Miniconda、Miniforge 是「Conda 发行版」，它们预装了不同组件。Conda 和 Mamba 是「核心工具」，负责实际的包管理和环境管理。Micromamba 则是「精简版」，适合容器和轻量场景。","conda核心工具#Conda：核心工具":"Conda 是 Conda 生态的核心工具，所有发行版都自带。它既是包管理器，也是环境管理器，能处理 Python、C++、CUDA、Linux 库等非 Python 依赖。\nConda 的环境隔离是全局级的，不同环境的 Python 版本和 CUDA 版本可以完全独立，互不干扰。它提供的都是预编译的二进制包，不需要手动编译，这对 habitat-sim、bullet 这样的复杂库特别友好。\nConda 的主要缺点是原生求解器速度慢。如果需要安装复杂的依赖（比如 habitat-sim），可能需要等很长时间。建议配合 conda-libmamba-solver 使用，或者直接用 Mamba。\n# 创建包含特定 Python 版本的环境 conda create -n myenv python=3.11 # 安装多个包 conda install numpy scipy matplotlib # 导出环境配置 conda env export \u003e environment.yml # 从配置文件创建环境 conda env create -f environment.yml","mambaconda-的高速替代#Mamba：Conda 的高速替代":"Mamba 是 Conda 的 C++ 重写版，核心解决 Conda 解析慢的问题。Mamba 的命令和 Conda 完全兼容，只需要把 conda 换成 mamba 即可，求解器速度能提升 10-100 倍。\nMamba 的优点是极速解析、完全兼容 Conda。缺点是它需要依赖 Conda 环境，所以需要先安装 Miniconda 或 Miniforge。如果经常需要安装复杂依赖（比如 habitat-sim、PyTorch+CUDA），Mamba 能大大节省的时间。\n# 安装 Mamba（需要先有 Conda） conda install mamba -n base -c conda-forge # 使用 Mamba 创建环境（命令和 Conda 一样） mamba create -n myenv python=3.11 # 安装包（速度比 Conda 快很多） mamba install numpy pandas pytorch # 安装复杂依赖（如 habitat-sim） mamba install -c conda-forge habitat-sim","micromamba极致轻量版#Micromamba：极致轻量版":"Micromamba 是 Mamba 的超轻量版，没有 Python 依赖，体积极小（MB 级），启动速度很快。它不需要预装 Conda 或 Python，兼容 Mamba 和 Conda 的命令，特别适合容器和嵌入式场景。\nMicromamba 的优点是极致轻量、启动快、无依赖。缺点是命令略有差异（比如用 micromamba create 而不是 mamba create），新手可能需要适应一下。如果在 Docker 容器中工作，或者需要极致轻量的环境，Micromamba 是最佳选择。\n# 安装 Micromamba（Linux/Mac） curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba # 创建环境 micromamba create -n myenv python=3.11 # 激活环境 micromamba activate myenv # 安装包 micromamba install numpy pandas","miniconda精简版-anaconda#Miniconda：精简版 Anaconda":"Miniconda 是 Anaconda 的瘦身版，只预装 Conda、Python 和基础依赖，没有多余的包。它的体积只有几百 MB，但保留了 Conda 的所有核心功能。\nMiniconda 的缺点是默认使用 Anaconda 官方源（defaults），这个源的包更新较慢，而且部分开源包可能缺失。另外，原生 Conda 的求解器是用 Python 写的，解析复杂依赖时速度很慢。Miniconda 适合需要 Conda 但想减少体积的场景，或者维护依赖 Miniconda 的老项目。\n# 创建环境 conda create -n myenv python=3.11 # 安装包（使用 conda-forge 源，更快更全） conda install -c conda-forge numpy pandas # 列出所有环境 conda env list","miniforge社区版的最佳选择#Miniforge：社区版的最佳选择":"Miniforge 是社区维护的 Conda 发行版，可以看作是 Miniconda 的改进版。它的默认源是 conda-forge，这是开源社区维护的源，包最全、更新最快。Miniforge 的体积和 Miniconda 相当，而且没有商业许可限制。\nMiniforge 现在基本是 Conda 生态的首选发行版，尤其是对于 AI 和跨语言项目。它轻量、源更优、完全开源，几乎没有明显缺点。\n# 创建环境 conda create -n myenv python=3.11 # 安装包（默认使用 conda-forge） conda install numpy pandas pytorch # 安装 CUDA 版本的 PyTorch conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia","pipenv早期的尝试#pipenv：早期的尝试":"pipenv 是早期为了解决「pip + 虚拟环境」痛点而诞生的工具。它把 pip 和 virtualenv 整合在一起，自动创建虚拟环境，并生成 Pipfile 和 Pipfile.lock 来管理依赖。\npipenv 比纯 pip 更规范，它提供了环境隔离和版本锁定功能。但它的依赖解析速度很慢，功能也比较简陋。现在基本上已经被 Poetry 和更新的工具替代了，除非维护的是老项目，否则不推荐使用。\n# 安装 pipenv pip install pipenv # 创建虚拟环境并安装依赖 pipenv install requests # 激活虚拟环境 pipenv shell # 安装开发依赖 pipenv install pytest --dev","pip最基础的选择#pip：最基础的选择":"pip 是 Python 官方内置的包管理器，也是整个 PyPI 生态的基石。几乎所有其他工具最终都会调用 pip 来安装包。pip 的特点就是极简：直接安装、直接卸载，没有多余的功能。\n但 pip 的局限性也很明显。它默认会把包安装到全局 Python 环境中，这意味着不同项目可能会因为依赖版本冲突而互相影响。而且 pip 没有锁文件机制，只能手动维护 requirements.txt，无法保证在不同机器上复现完全相同的环境。\n如果只是临时安装一个包来测试，或者写一个极简的脚本，pip 完全够用。但对于生产环境或复杂的项目，建议使用更现代的工具。\n# 安装包 pip install requests # 从 requirements.txt 安装 pip install -r requirements.txt # 生成 requirements.txt pip freeze \u003e requirements.txt","poetry现代-python-项目的标准#Poetry：现代 Python 项目的标准":"Poetry 是专为 Python 项目开发和发布而设计的工具。它不仅管理依赖，还能帮打包和发布到 PyPI。Poetry 的依赖解析能力很强，能很好地解决版本冲突问题，而且完全符合 PEP 标准，使用 pyproject.toml 作为配置文件。\nPoetry 的优点是规范、稳定，特别适合需要发布到 PyPI 的库项目。但它对复杂二进制依赖（比如 CUDA）的支持较弱，而且速度不如新一代工具。如果做的是纯 Python 项目，尤其是需要打包发布的库，Poetry 是个不错的选择。\n# 安装 Poetry curl -sSL https://install.python-poetry.org | python3 - # 创建新项目 poetry new my-project cd my-project # 添加依赖 poetry add requests poetry add pytest --group dev # 安装所有依赖 poetry install # 打包发布 poetry build poetry publish","pypi-生态专注-python-包管理#PyPI 生态：专注 Python 包管理":"如果做的是纯 Python 项目，不需要处理 CUDA 或其他系统级依赖，那么 PyPI 生态的工具就足够了。这类工具的核心优势是简单、快速，而且 PyPI 上有超过 400 万个包，几乎能想到的 Python 库都能找到。","python-包管理器生态全览#Python 包管理器生态全览":"Python 包管理器生态全览我刚开始接触 Python 的时候是某个大二暑假自己去跑眼科图片的 CNN 分类，被各种包管理器搞得一头雾水。pip、conda、poetry、uv、mamba…\n为什么会有这么多工具？它们之间有什么区别？我应该用哪个？\n其实 pip 走天下就可以了（bushi）\n在深入之前，我们先明确一个核心概念：Python 包管理器大致可以分为两大阵营。PyPI 生态专注于纯 Python 包的管理，它们基于 pip，只能处理从 PyPI 下载的 Python 包。而 Conda 生态则更强大，不仅能管理 Python 包，还能处理 CUDA、C++ 库等非 Python 依赖，是环境管理和包管理的二合一解决方案。","ryepoetry-的升级版#Rye：Poetry 的升级版":"Rye 是 Poetry 作者开发的继任者，目标是解决 Poetry 的速度和复杂度问题。Rye 整合了 Python 版本管理、虚拟环境、依赖管理和打包功能，是一个真正的「一站式」解决方案。\nRye 最大的亮点是内置了 Python 版本下载功能，不需要手动安装 Python，Rye 会自动帮管理。它的依赖解析速度比 Poetry 更快，而且兼容 pyproject.toml 标准。不过 Rye 的生态还比较新，部分老工具的兼容性可能略差。\n如果想要一个功能全面、速度更快的工具，而且不介意使用相对较新的技术，Rye 值得尝试。\n# 安装 Rye curl -sSf https://rye-up.com/get | bash # 创建新项目 rye init my-project cd my-project # 添加依赖 rye add requests rye add pytest --dev # 同步依赖 rye sync # 运行项目 rye run python main.py","uv2024-年的新星#uv：2024 年的新星":"uv 是 2024 年爆火的新一代包管理器，用 Rust 重写，目标是替代 pip、Poetry 和 Rye。uv 的最大优势就是速度：它的依赖解析速度是毫秒级的，比 Poetry 快 100 倍以上。\nuv 完全兼容 pip 的 requirements.txt 和 Poetry 的 pyproject.toml，这意味着可以无缝迁移现有项目。它支持虚拟环境、锁文件、依赖安装更新和打包等所有功能。uv 的缺点就是太新了，极少数边缘场景可能兼容性略差，但这些问题基本可以忽略。\n如果做的是纯 Python 项目，我强烈推荐优先考虑 uv。它的速度优势在大型项目中会非常明显，而且使用体验也很棒。\n# 安装 uv curl -LsSf https://astral.sh/uv/install.sh | sh # 创建虚拟环境 uv venv # 安装包 uv pip install requests # 从 requirements.txt 安装 uv pip install -r requirements.txt # 使用 pyproject.toml（兼容 Poetry） uv sync","如何选择按场景选型#如何选择：按场景选型":"如果做的是纯 Python 项目，没有 CUDA 或其他非 Python 依赖，优先选择 uv。它的速度最快，使用体验最好。如果 uv 在的场景下有问题，可以退而求其次选择 Rye 或 Poetry。\n如果做的是AI 或跨语言项目，需要 CUDA、habitat-sim、bullet 等依赖，那么 Miniforge + Mamba 是最佳组合。Miniforge 提供轻量级的 Conda 环境，Mamba 提供极速的依赖解析。如果需要在容器中使用，可以用 Micromamba 替代。\n如果是完全的新手，刚入门数据科学，Miniforge 是个好选择，它轻量、开源、源更优。如果觉得手动安装包太麻烦，也可以先用 Anaconda，等熟悉了再迁移到 Miniforge。\n如果需要维护老项目，而且项目依赖 Miniconda 和 defaults 源，那就继续用 Miniconda，但建议安装 conda-libmamba-solver 来提速。","总结关键差异一览#总结：关键差异一览":"最后，让我们快速总结一下这些工具的关键差异。\n从速度来看，uv 是最快的，其次是 Mamba 和 Micromamba，然后是 Rye 和 Poetry，pipenv 和原生 Conda 比较慢，pip 最慢。\n从跨语言支持来看，Conda、Mamba、Micromamba 都能处理非 Python 依赖，而 PyPI 生态的工具（uv、Poetry 等）只能处理 Python 包。\n从包源丰富度来看，PyPI（uv/pip 使用）有超过 400 万个包，是最丰富的。conda-forge（Miniforge 默认使用）次之，Anaconda defaults 最少。\n从体积来看，Micromamba 只有几 MB，Miniforge 和 Miniconda 是几百 MB，Anaconda 则是 GB 级。\n简单来说，纯 Python 项目用 uv，需要 CUDA 或非 Python 依赖用 Miniforge+Mamba，容器环境用 Micromamba，这样选基本不会错。","系统包管理器避免混淆#系统包管理器：避免混淆":"最后提一下系统包管理器，比如 apt（Debian/Ubuntu）、yum/dnf（RHEL/CentOS）、pacman（Arch）。这些工具是 Linux 系统级的包管理器，管理的是全局系统组件，没有环境隔离功能。\n系统包管理器装的包是全局生效的，无法解决 Python 项目的版本冲突问题。应该用它们来安装系统工具（比如 git、gcc）。"},"title":"包管理器生态全览"},"/docs/web/backend/springboot/":{"data":{"mysql-操作#MySQL 操作":"后端应用离不开数据库。我们将使用 Spring Data JPA 来简化数据库操作。","restcontroller--requestmapping#@RestController \u0026amp; @RequestMapping":"@RestController 是一个组合注解，它结合了 @Controller 和 @ResponseBody。@Controller 用于标识这是一个控制器类，而 @ResponseBody 则告诉 Spring，这个类中所有方法的返回值都应直接写入 HTTP 响应体中，而不是解析为视图（比如 JSP）。这对于构建 RESTful API 至关重要。\n@RequestMapping(\"/user\") 定义了这个控制器下所有请求的基础路径。","spring-boot#Spring Boot":"Spring Boot最好还是从(https://github.com/wuyouzhuguli/SpringAll)学习科班知识，实际上很多都可以去https://mrbird.cc/tags/这个人的博客上学","spring-boot-介绍#Spring Boot 介绍":"Spring Boot 是一个用于构建高效、可独立运行的、生产级别的 Spring 应用程序的框架。它基于 Spring 框架，但极大地简化了初始搭建和开发过程。Spring Boot 遵循\"约定优于配置\"的原则，通过大量的自动配置，让开发者可以快速启动并运行一个项目，而无需深陷于繁杂的 XML 配置中。\n在底层，Spring Boot 内嵌了诸如 Tomcat、Jetty 或 Undertow 这样的 Web 服务器，这意味着你不需要将应用打包成 WAR 文件部署到外部服务器，而是可以直接运行一个 JAR 文件。它完美地整合了 Spring 生态系统中的各种项目（如 Spring MVC, Spring Data），并为第三方库提供了大量的\"starter\"依赖，极大地简化了依赖管理。\n详情可以查阅 Spring Boot 官方文档。","spring-boot-控制器-controller#Spring Boot 控制器 (Controller)":"控制器负责处理传入的 HTTP 请求，并将响应返回给客户端。它的核心任务是接收、解析请求，然后调用相应的服务来处理业务逻辑，最后封装并返回结果。路由机制决定了哪个控制器的哪个方法来处理特定请求。","spring-核心依赖注入-di-与-ioc-容器#Spring 核心：依赖注入 (DI) 与 IoC 容器":"Spring 的核心是控制反转 (Inversion of Control, IoC) 和 依赖注入 (Dependency Injection, DI)。简单来说，IoC 意味着你不再需要自己去创建和管理对象的生命周期 (new UserSerivce())，而是将这个控制权\"反转\"给了 Spring 容器。DI 则是实现 IoC 的一种方式：容器会主动将一个对象所依赖的其他对象\"注入\"给它。\n模块化是通过组件 (Component) 和依赖注入来实现的。在 Spring 中，任何被 @Component 或其衍生注解（@Service, @Repository, @Controller）标记的类，都会被 Spring 容器扫描并管理。\n假设我们需要一个 UserService 来处理业务逻辑。\npackage com.example.demo.user; import org.springframework.stereotype.Service; @Service public class UserService { public String findUserById(Long id) { // 模拟从数据库查找用户 return \"Found user with id: \" + id; } } @Service 注解表明这个类是一个业务逻辑组件。现在，我们可以在 UserController 中注入并使用它。推荐使用构造函数注入：\n@RestController @RequestMapping(\"/user\") public class UserController { private final UserService userService; // 通过构造函数注入 UserService public UserController(UserService userService) { this.userService = userService; } @GetMapping(\"/{id}\") public String findOne(@PathVariable Long id) { return userService.findUserById(id); } } 当 Spring 创建 UserController 实例时，它会发现构造函数需要一个 UserService 类型的参数，于是它会自动从容器中找到 UserService 的实例并传入。这就是依赖注入。","为什么选择-spring-boot#为什么选择 Spring Boot":"选择一个框架，往往是基于其生态、效率和稳定性。Spring Boot 在这些方面都表现出色：\n庞大的生态与社区：作为 Java 世界事实上的标准，Spring 拥有无与伦比的社区支持和丰富的学习资源。遇到任何问题，你几乎都能找到解决方案。\n开发效率：Spring Initializr 让我们可以在几秒钟内创建一个功能完备的项目骨架。“约定优于配置\"和自动配置让我们能专注于业务逻辑，而不是框架的配置。\n稳定与成熟：在无数企业级应用中得到了验证，Spring Boot 的稳定性和性能毋庸置疑。对于构建大型、复杂的系统来说，这是一个非常可靠的选择。\n说了这么多，接下来让我们正式踏上 Spring Boot 的学习之旅吧！本文将主要涵盖以下内容：","什么是-spring-data-jpa#什么是 Spring Data JPA?":"JPA (Java Persistence API) 是 Java 的一个官方规范，用于对象关系映射 (ORM)。它定义了一套 API，但没有提供实现。Hibernate 是最流行的 JPA 实现。Spring Data JPA 则是 Spring 提供的一个模块，它在 JPA 的基础上再次进行了封装，提供了 Repository 编程模型，让你几乎不用写任何 SQL 语句就能完成大部分数据操作。","修改-service-和-controller#修改 Service 和 Controller":"现在，将 UserRepository 注入到 UserService 中，并实现完整的 CRUD 逻辑。\n@Service public class UserService { private final UserRepository userRepository; public UserService(UserRepository userRepository) { this.userRepository = userRepository; } public User createUser(CreateUserDto dto) { User user = new User(); user.setName(dto.getName()); user.setEmail(dto.getEmail()); return userRepository.save(user); } public User findUserById(Long id) { return userRepository.findById(id) .orElseThrow(() -\u003e new ResourceNotFoundException(\"User not found with id \" + id)); } public void deleteUser(Long id) { userRepository.deleteById(id); } // ... 其他更新方法 } 最后，在 UserController 中调用这些服务方法，并连接到对应的 HTTP 端点。","全局异常处理器-exception-filter#全局异常处理器 (Exception Filter)":"这是处理应用中所有未捕获异常的最佳实践。通过创建一个带有 @ControllerAdvice 注解的类，我们可以定义处理特定异常的方法。\nimport org.springframework.http.HttpStatus; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.ControllerAdvice; import org.springframework.web.bind.annotation.ExceptionHandler; @ControllerAdvice public class GlobalExceptionHandler { @ExceptionHandler(Exception.class) public ResponseEntity\u003cString\u003e handleAllExceptions(Exception ex) { // 在这里可以记录日志 return new ResponseEntity\u003c\u003e(\"An unexpected error occurred: \" + ex.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR); } @ExceptionHandler(ResourceNotFoundException.class) public ResponseEntity\u003cString\u003e handleResourceNotFoundException(ResourceNotFoundException ex) { return new ResponseEntity\u003c\u003e(ex.getMessage(), HttpStatus.NOT_FOUND); } } 当任何控制器中抛出 ResourceNotFoundException 时，handleResourceNotFoundException 方法就会被调用，并返回一个 404 响应。","切面编程拦截器过滤器与异常处理#切面编程：拦截器、过滤器与异常处理":"在 Web 开发中，我们经常需要处理一些横切关注点，如日志记录、权限验证、异常捕获等。Spring 提供了强大的机制来处理这些任务，其思想与 Nest.js 中的中间件、守卫、管道等类似。","创建一个-spring-boot-项目#创建一个 Spring Boot 项目":"首先，确保你的系统上安装了 Java Development Kit (JDK)（推荐版本 \u003e= 11）和构建工具 Maven 或 Gradle。创建 Spring Boot 项目最便捷的方式是使用 Spring Initializr。\n你可以直接访问 start.spring.io，或者通过你的 IDE（如 IntelliJ IDEA Ultimate）内置的 Spring Initializr 功能来创建。\n我们选择以下配置：\nProject: Maven Project Language: Java Spring Boot: 选择一个稳定的版本 (如 3.x.x) Project Metadata: 填写你自己的 Group 和 Artifact Dependencies: 添加 Spring Web（用于构建 Web 应用）、Spring Data JPA（用于数据库操作）、MySQL Driver（连接 MySQL 数据库）和 Lombok（简化代码的工具）。 点击 “Generate”，下载并解压项目，然后用你的 IDE 打开。","创建仓库-repository#创建仓库 (Repository)":"创建一个接口，继承 JpaRepository。Spring Data JPA 会在运行时自动为这个接口提供实现。\nimport org.springframework.data.jpa.repository.JpaRepository; public interface UserRepository extends JpaRepository\u003cUser, Long\u003e { // 你还可以在这里定义自定义查询方法，比如 // User findByName(String name); }","创建实体-entity#创建实体 (Entity)":"实体是一个映射到数据库表的 Java 类。我们使用 JPA 注解来定义映射关系。\nimport jakarta.persistence.*; import lombok.Data; import java.util.Date; @Entity @Table(name = \"user\") @Data public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(nullable = false) private String name; private String email; @Temporal(TemporalType.TIMESTAMP) private Date createTime; @PrePersist protected void onCreate() { createTime = new Date(); } }","安装依赖#安装依赖":"在 pom.xml 中添加依赖：\norg.springdoc springdoc-openapi-starter-webmvc-ui 2.5.0","实现-crud#实现 CRUD":"","总结#总结":"至此，我们已经从零开始，循序渐进地学习了 Spring Boot 的核心概念，并搭建起一个包含 Web 层、业务逻辑层、数据访问层以及 API 文档的完整后端应用。这仅仅是一个开始，Spring Boot 的世界还有更多强大的功能等待你去探索。","拦截器-interceptor#拦截器 (Interceptor)":"拦截器是 Spring MVC 提供的机制，它比过滤器更贴近业务，可以访问到将要处理请求的控制器方法 (Handler) 等上下文信息。它非常适合做权限验证、日志等。\n要使用拦截器，需要实现 HandlerInterceptor 接口，并通过 WebMvcConfigurer 进行注册。","接口文档#接口文档":"最后，一个专业的后端服务必须有清晰的接口文档。我们可以使用 SpringDoc OpenAPI (集成了 Swagger UI) 来自动生成交互式 API 文档。","新增一个用户模块#新增一个【用户模块】":"遵循分层架构的思想，我们来手动创建一个简单的用户模块。一个典型的模块包含 Controller（控制器）、Service（服务）和 Repository（数据访问）层。\n首先，我们通过命令行或在 IDE 中创建对应的包和类文件：\n创建一个 user 包。 在 user 包下，创建一个 UserController.java。 然后，在 UserController 中编写一个简单的 HTTP 请求处理方法。 package com.example.demo.user; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController @RequestMapping(\"/user\") public class UserController { @GetMapping public String getHello() { return \"Hello Spring Boot!\"; } } 等待程序自动重新加载（如果你的 IDE 配置了），或者重启应用。在浏览器中访问 http://localhost:8080/user，你将看到页面上显示的 “Hello Spring Boot!\"。","添加描述信息#添加描述信息":"为了让文档更清晰，我们可以使用注解来添加描述。\nimport io.swagger.v3.oas.annotations.Operation; import io.swagger.v3.oas.annotations.tags.Tag; @Tag(name = \"用户管理\", description = \"用户相关的所有API\") @RestController @RequestMapping(\"/user\") public class UserController { // ... @Operation(summary = \"根据ID查找用户\", description = \"返回单个用户的详细信息\") @GetMapping(\"/{id}\") public User findOne(@PathVariable Long id) { // ... } } 刷新 Swagger 页面，你会看到这些描述信息已经更新上去了。","管道-pipes---数据校验-validation#管道 (Pipes) -\u0026gt; 数据校验 (Validation)":"在 Spring Boot 中，数据校验通常通过 spring-boot-starter-validation (它引入了 Hibernate Validator) 来实现。我们可以在 DTO 中使用 JSR 303/380 注解来声明校验规则。\nimport jakarta.validation.constraints.*; import lombok.Data; @Data public class CreateUserDto { @NotEmpty(message = \"Name cannot be empty\") private String name; @Min(value = 18, message = \"Age must be at least 18\") private int age; @Email(message = \"Invalid email format\") private String email; } 然后在控制器的方法参数上加上 @Valid 注解。\n@PostMapping public String create(@Valid @RequestBody CreateUserDto createUserDto) { return \"User created successfully!\"; } 如果传入的 JSON 不符合规则，Spring 会自动抛出一个 MethodArgumentNotValidException，我们可以用前面提到的全局异常处理器来捕获它，并返回一个友好的错误信息。","请求方法-resources#请求方法 (Resources)":"Spring 为所有标准的 HTTP 方法提供了专门的注解，使得代码更具可读性：@GetMapping、@PostMapping、@PutMapping、@DeleteMapping、@PatchMapping。\nimport org.springframework.web.bind.annotation.*; @RestController @RequestMapping(\"/api/users\") public class UserController { // 匹配 GET /api/users @GetMapping public String getAllUsers() { return \"获取所有用户\"; } // 匹配 POST /api/users @PostMapping public String createUser() { return \"创建新用户\"; } // 匹配 PUT /api/users @PutMapping public String updateUser() { return \"更新用户\"; } // 匹配 DELETE /api/users @DeleteMapping public String deleteUser() { return \"删除用户\"; } }","请求负载-request-body#请求负载 (Request Body)":"对于 POST 或 PUT 请求，客户端通常会在请求体中发送数据（例如 JSON）。我们可以使用 @RequestBody 注解来处理。\n首先，我们需要定义一个 DTO (数据传输对象)，它是一个简单的 Java 类 (POJO)，用于映射传入的 JSON 数据。\n// 使用 Lombok 简化代码 import lombok.Data; @Data public class CreateUserDto { private String name; private int age; private String email; } 然后，在控制器的方法中使用 @RequestBody 来接收这个 DTO。\n@PostMapping public String create(@RequestBody CreateUserDto createUserDto) { return \"Creating a new user with name: \" + createUserDto.getName(); } Spring Boot 会自动使用内置的 Jackson 库将请求体中的 JSON 字符串反序列化为 CreateUserDto 对象。","路由参数-path-variables#路由参数 (Path Variables)":"当需要从 URL 路径中捕获动态数据时（例如 GET /users/1 来获取 ID 为 1 的用户），可以使用路径变量。通过在路径中使用 {} 占位符，并使用 @PathVariable 注解来将这部分值绑定到方法参数上。\n@GetMapping(\"/{id}\") public String findOne(@PathVariable Long id) { System.out.println(\"Fetching user with id: \" + id); return \"This action returns user #\" + id; }","过滤器-filter#过滤器 (Filter)":"过滤器是 Java Servlet规范的一部分，它工作在请求处理的最外层，可以在请求到达 DispatcherServlet 之前或之后执行。它非常适合用于处理编码、日志记录、CORS 等底层任务。\nimport jakarta.servlet.*; import org.springframework.stereotype.Component; import java.io.IOException; @Component public class LoggingFilter implements Filter { @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"Request received...\"); chain.doFilter(request, response); // 将请求传递给下一个过滤器或处理器 System.out.println(\"Response sent...\"); } }","运行#运行":"重新启动应用。就是这么简单！Spring Boot 的自动配置会为你搞定一切。现在访问 http://localhost:8080/swagger-ui.html，你就能看到一个漂亮的、可交互的 API 文档页面了。","配置#配置":"首先，在 application.properties 文件中添加数据库连接信息：\n# MySQL settings spring.datasource.url=jdbc:mysql://localhost:3306/blog?useSSL=false\u0026serverTimezone=UTC spring.datasource.username=root spring.datasource.password=your_password spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # JPA settings spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=true spring.jpa.hibernate.ddl-auto=update 会让 Hibernate 根据你的实体类自动更新数据库表结构，这在开发阶段非常方便。","项目文件介绍#项目文件介绍":"除去 Maven 的标准目录结构和配置文件之外，在 src/main/java 目录下，我们会看到一些 Spring Boot 的核心文件：\nsrc\r└── main\r├── java\r│ └── com\r│ └── example\r│ └── demo\r│ └── DemoApplication.java // 主程序入口\r└── resources\r├── application.properties // 核心配置文件\r├── static\r└── templates\rpom.xml // Maven 依赖与构建配置 文件名 文件描述 DemoApplication.java 应用程序的入口文件。它包含一个 main 方法，使用 SpringApplication.run() 来启动整个应用。 application.properties Spring Boot 的核心配置文件，用于配置应用端口、数据库连接、日志级别等各种属性。 pom.xml Maven 项目的配置文件，定义了项目信息、依赖库（starters）和构建插件。","项目运行#项目运行":"Maven 依赖下载完成后，你可以直接在 IDE 中找到 DemoApplication.java 文件，右键点击并选择 “Run”。或者在项目根目录下打开终端，输入以下命令：\nmvn spring-boot:run 此命令会启动内嵌的 Tomcat 服务器并运行你的应用。服务启动后，我们来分析一下入口文件 DemoApplication.java：\npackage com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 代码非常简洁。核心是 @SpringBootApplication 这个注解，它是一个复合注解，包含了三个关键功能：\n@EnableAutoConfiguration: 启用 Spring Boot 的自动配置机制，它会根据你添加的依赖（比如 spring-boot-starter-web）来猜测并配置你可能需要的 Bean。\n@ComponentScan: 自动扫描该类所在包及其子包下的所有组件（如 @Component, @Service, @Controller 等）并注册到 Spring 容器中。\n@Configuration: 允许在上下文中注册额外的 Bean 或导入其他配置类。\nmain 方法中的 SpringApplication.run() 则负责引导和启动整个 Spring 应用。"},"title":"Spring Boot"},"/docs/web/client/":{"data":{"":"客户端开发相关学习笔记。"},"title":"客户端"},"/docs/web/client/intro/":{"data":{"":"","1-原生-app-开发-native-app-development---旗舰总店#1. 原生 App 开发 (Native App Development) - “旗舰总店”":"原生安卓和 iOS 开发 。它好比在最繁华的商业街（苹果 App Store 和安卓应用市场）开设的官方旗舰店。\niOS 开发 :\n语言 : 主要使用 Swift (现代、高效) 或 Objective-C (较老)。 工具 : 苹果官方的 Xcode 。 特点 : 只能在 Mac 电脑上开发。应用性能最好，体验最流畅，能调用所有系统底层功能，UI/UX 完全符合苹果的设计规范。 Android 开发 :\n语言 : 主要使用 Kotlin (Google 官方推荐) 或 Java。 工具 : Google 官方的 Android Studio 。 特点 : 可以在 Windows, macOS, Linux 上开发。安卓系统开放性高，应用发布渠道多。同样能实现最佳性能和系统功能调用。 “旗舰店”的优缺点： 优点 : 性能天花板，用户体验最佳，品牌形象最专业。\n缺点 : 需要为 iOS 和 Android 两套系统分别组建“装修队”（开发团队），使用不同的“建材”（语言和工具），成本最高，工期最长。","2-跨平台开发-cross-platform-development---标准化连锁店#2. 跨平台开发 (Cross-Platform Development) - “标准化连锁店”":"这就是 UniApp 所属的领域。目标是用一套“装修图纸”（代码）开出能在不同商业街（iOS/Android）营业的连锁店。\n除了 UniApp (基于 Vue)，主流的还有：\nReact Native : 由 Facebook (Meta) 推出，使用 React 。开发者可以用写网页的方式写出原生 App。\nFlutter : 由 Google 推出，使用 Dart 语言。性能非常接近原生，势头很猛。 “连锁店”的优缺点：\n优点 : 一套代码，多端运行，大大降低了开发成本和时间。\n缺点 :\n性能和体验通常会比“旗舰店”稍逊一筹。 一些最新的系统功能可能无法第一时间使用。 在处理复杂动画或高强度计算时可能会遇到瓶颈。","3-小程序开发-mini-program-development---商场里的专柜#3. 小程序开发 (Mini-Program Development) - “商场里的专柜”":"这个在国内非常流行。它好比您不在外面单独开店，而是在 微信、支付宝、抖音 这些巨型购物中心（Super App）里租一个“专柜”。\n技术栈 : 每个平台都有一套自己的开发规范，但思想上都借鉴了网页技术。通常是 WXML (类似 HTML) + WXSS (类似 CSS) + JavaScript 。\n特点 :\n用完即走，无需安装 ：这是最大的优势。 依赖宿主 : 功能受限于“购物中心”提供的能力，比如无法实现某些原生 App 的复杂功能。 自带流量 : 可以方便地在微信、支付宝内分享和传播，获客成本低。 “专柜”的优缺点： 优点 : 开发快，发布快，推广方便，完美结合宿主 App 的生态（如微信支付、社交分享）。\n缺点 : 功能限制多，性能一般，命脉掌握在平台手中。","4-桌面客户端开发-desktop-client-development---pc总店办公楼#4. 桌面客户端开发 (Desktop Client Development) - “PC总店/办公楼”":"除了手机，电脑上的软件也是客户端，比如 QQ、VS Code、微信电脑版。\n传统方式 : 针对 Windows (用 C#/.NET, C++) 和 macOS (用 Swift/Objective-C) 分别开发。 现代跨平台方式 : 使用网页技术来开发桌面应用，比如 Electron (VS Code, Slack, Discord 都是用它开发的) 和 Tauri (一个更轻量、更安全的新选择)。 C# 绝对属于客户端开发。 并且是好几种客户端的“主力语言”： Windows 桌面开发 ：这是 C# 的“龙兴之地”。使用 WPF、WinUI 等框架，可以开发出非常漂亮和强大的 Windows 桌面应用。这是最传统的客户端开发。 游戏开发 ： 这正好能解释您舍友的行为！ 全球最流行的游戏引擎之一 Unity ，其官方开发语言就是 C#。无数我们熟知的游戏（无论是端游、手游）都是用 Unity 开发的。 我舍友用 C# 写 Mod，极大概率就是那款游戏本身是用 Unity 引擎开发的。Mod 作者通过编写 C# 脚本，可以调用游戏开放的接口，从而添加新功能、新角色、修改游戏规则。这本质上就是在做“游戏客户端”的二次开发。 借助 ASP.NET Core 框架，C# 是构建高性能、高可靠性后端服务（API、网站）的顶级选择之一，在企业级应用中非常普遍，完全可以和 Java、Go 分庭抗礼。"},"title":"intro"},"/docs/web/client/uniapp/":{"data":{"":"","说明#说明":"这里整理 UniApp 的环境搭建、组件使用与多端发布经验，后续内容可持续补充。"},"title":"uniapp"},"/docs/web/data-structures/":{"data":{"":"数据结构相关学习笔记。"},"title":"数据结构"},"/docs/web/data-structures/arrays-strings/":{"data":{"1-两数之和#1. 两数之和":"function twoSum(nums, target) { const map = new Map(); for (let i = 0; i \u003c nums.length; i++) { const complement = target - nums[i]; if (map.has(complement)) { return [map.get(complement), i]; } map.set(nums[i], i); } return []; }","2-移动零#2. 移动零":"function moveZeroes(nums) { let left = 0; for (let right = 0; right \u003c nums.length; right++) { if (nums[right] !== 0) { [nums[left], nums[right]] = [nums[right], nums[left]]; left++; } } }","3-最长公共前缀#3. 最长公共前缀":"function longestCommonPrefix(strs) { if (strs.length === 0) return \"\"; let prefix = strs[0]; for (let i = 1; i \u003c strs.length; i++) { while (strs[i].indexOf(prefix) !== 0) { prefix = prefix.substring(0, prefix.length - 1); if (prefix === \"\") return \"\"; } } return prefix; }","kmp算法#KMP算法":"function buildKMPTable(pattern) { const table = new Array(pattern.length).fill(0); let i = 1, j = 0; while (i \u003c pattern.length) { if (pattern[i] === pattern[j]) { j++; table[i] = j; i++; } else { if (j !== 0) { j = table[j - 1]; } else { table[i] = 0; i++; } } } return table; }","不可变性#不可变性":"字符串在大多数语言中是不可变的，每次修改都会创建新字符串。\n# Python字符串操作 s = \"hello\" s[0] = \"H\" # 错误！字符串不可变 # 正确方式 s = \"H\" + s[1:] # \"Hello\"","字符串匹配算法#字符串匹配算法":"","字符串基础#字符串基础":"","常用操作#常用操作":"// JavaScript数组操作 const arr = [1, 2, 3, 4, 5]; // 访问元素 console.log(arr[0]); // 1 // 添加元素 arr.push(6); // [1, 2, 3, 4, 5, 6] // 删除元素 arr.pop(); // [1, 2, 3, 4, 5] arr.shift(); // [2, 3, 4, 5]","数组与字符串#数组与字符串":"数组与字符串","数组基础#数组基础":"","时间复杂度#时间复杂度":"访问：O(1) 插入/删除：O(n)（非末尾） 搜索：O(n)","高频面试题#高频面试题":""},"title":"arrays-strings"},"/docs/web/data-structures/linked-lists/":{"data":{"1-反转链表#1. 反转链表":"function reverseList(head) { let prev = null; let curr = head; while (curr !== null) { const next = curr.next; curr.next = prev; prev = curr; curr = next; } return prev; }","1-快慢指针#1. 快慢指针":"检测环 寻找中点 删除倒数第N个节点","2-双指针遍历#2. 双指针遍历":"反转链表 合并有序链表 移除元素","2-检测环#2. 检测环":"function hasCycle(head) { if (!head || !head.next) return false; let slow = head; let fast = head; while (fast \u0026\u0026 fast.next) { slow = slow.next; fast = fast.next.next; if (slow === fast) { return true; } } return false; }","3-环的入口#3. 环的入口":"function detectCycle(head) { if (!head || !head.next) return null; let slow = head; let fast = head; // 找到相遇点 while (fast \u0026\u0026 fast.next) { slow = slow.next; fast = fast.next.next; if (slow === fast) { // 找到环的入口 let ptr1 = head; let ptr2 = slow; while (ptr1 !== ptr2) { ptr1 = ptr1.next; ptr2 = ptr2.next; } return ptr1; } } return null; }","4-合并两个有序链表#4. 合并两个有序链表":"function mergeTwoLists(list1, list2) { const dummy = new ListNode(); let current = dummy; while (list1 \u0026\u0026 list2) { if (list1.val \u003c= list2.val) { current.next = list1; list1 = list1.next; } else { current.next = list2; list2 = list2.next; } current = current.next; } current.next = list1 || list2; return dummy.next; }","5-删除倒数第n个节点#5. 删除倒数第N个节点":"function removeNthFromEnd(head, n) { const dummy = new ListNode(0, head); let fast = dummy; let slow = dummy; // fast先走n+1步 for (let i = 0; i \u003c= n; i++) { fast = fast.next; } // 一起走直到fast为null while (fast !== null) { fast = fast.next; slow = slow.next; } // 删除slow.next slow.next = slow.next.next; return dummy.next; }","双指针技巧#双指针技巧":"","常见坑点#常见坑点":"空指针检查：操作前检查head是否为空 边界情况：只有一个节点、两个节点等 环检测：使用快慢指针，避免死循环 内存管理：注意垃圾回收，防止内存泄漏","时间复杂度#时间复杂度":"访问：O(n) 插入/删除：O(1)（知道位置的情况下） 搜索：O(n)","链表#链表":"链表","链表-vs-数组#链表 vs 数组":"特性 数组 链表 访问 O(1) O(n) 插入/删除 O(n) O(1) 内存分配 连续 不连续 缓存友好 是 否","链表基础#链表基础":"","链表类型#链表类型":"单向链表：每个节点只有一个指向下一个节点的指针 双向链表：每个节点有两个指针（前驱和后继） 循环链表：尾节点指向头节点","链表节点定义#链表节点定义":"// JavaScript链表节点 class ListNode { constructor(val = 0, next = null) { this.val = val; this.next = next; } } // Python链表节点 class ListNode: def __init__(self, val=0, next=None): self.val = val self.next = next","高频面试题#高频面试题":""},"title":"linked-lists"},"/docs/web/database/":{"data":{"":"数据库相关学习笔记。"},"title":"数据库"},"/docs/web/database/intro/":{"data":{"":"","一orm-框架-object-relational-mapping#一、ORM 框架 (Object-Relational Mapping)":"ORM 的选择通常和 编程语言 强相关。我按您接触过的语言来分类：\nNode.js / TypeScript 生态 (目前最“卷”的领域) 当红炸子鸡 \u0026 强烈推荐学习: Prisma\n特点 : 类型安全 是它的王牌。它能根据您的数据库结构自动生成 TypeScript 类型，提供极致的自动补全和编译时错误检查，写数据库代码就像调用普通函数一样顺滑。 热门原因 : 彻底改变了 Node.js 世界的数据库开发体验，从“手写 SQL 字符串”或“复杂的链式调用”变成了完全类型安全的代码，极大提升了开发效率和代码质量。 老牌劲旅 \u0026 用得最广: TypeORM\n特点 : 采用经典的“装饰器”语法来定义实体 (Entity)，设计思想深受 Java Hibernate/JPA 的影响，功能非常全面，支持多种数据库。 现状 : 依然有巨大的存量市场，很多老项目在用。但因为其 API 设计和类型推导不如 Prisma 现代，新项目的热度正在被 Prisma 超越。 轻量新星: Drizzle ORM\n特点 : 如果说 Prisma 是 ORM，Drizzle 更像一个“类型安全的 SQL 查询构建器”。它让您更贴近 SQL 本身，但又提供了完整的 TypeScript 类型支持。 热门原因 : 性能开销极小，非常适合对性能敏感或在 Serverless/Edge 环境下运行的应用。 2. Go 生态 事实标准 \u0026 用得最广: Gorm\n特点 : 功能大而全，开箱即用，学习曲线平缓，符合 Go 社区追求“快速开发”的习惯。 现状 : 依然是 Go 社区最流行的 ORM，拥有庞大的用户群和丰富的文档。 热门新思路: sqlc\n特点 : 它不是一个传统的 ORM。您只需要写标准的 .sql 文件， sqlc 会在编译时自动为您生成类型安全的 Go 代码来执行这些 SQL。 热门原因 : 实现了“SQL 优先”和“类型安全”的完美结合。数据库专家可以写出最高效的 SQL，而业务开发者可以调用类型安全的 Go 函数，两全其美。 3. Java 生态 绝对霸主: Spring Data JPA (底层是 Hibernate )\n特点 : 在 Spring Boot 项目中，您几乎不需要写任何实现代码，只需要定义一个接口并继承 JpaRepository ，增删改查、分页、排序等功能就全部自动拥有了。 现状 : 在 Java 企业级开发中是无可争议的王者，是 Java 后端工程师的必备技能。 关系型数据库 (SQL) - 基石 最热门 \u0026 强烈推荐: PostgreSQL (简称 PG)\n特点 : 功能极其强大，开源，严格遵循 SQL 标准，并且拥有强大的扩展能力（比如支持 JSON、地理空间数据、全文搜索）。很多新的数据库都是基于 PG 构建的。 热门原因 : 社区活跃，功能迭代快，被誉为“最先进的开源关系型数据库”。对于新项目，技术圈内“无脑推荐 PG”的呼声越来越高。 用得最广: MySQL\n特点 : 简单易用，性能出色，拥有全世界最广泛的用户基础和最丰富的文档资料。 现状 : 依然是互联网行业的绝对主力，尤其是在中小型公司和传统 LAMP 架构中。 2. NoSQL 数据库 - 扩展 文档数据库霸主: MongoDB\n特点 : 使用类似 JSON 的 BSON 格式存储数据，灵活的 Schema 设计对敏捷开发非常友好。 现状 : 事实上的文档数据库标准，非常适合需要快速迭代、数据结构多变的业务场景。 内存数据库霸主: Redis\n特点 : 数据存储在内存中，读写速度极快。 现状 : 不仅仅是数据库，更是后端架构的“瑞士军刀”。主要用于 缓存 、 分布式锁 、 消息队列 、 排行榜 等场景，是高级后端工程师的必备技能。 3. 热门新兴数据库 - 未来趋势 向量数据库 (Vector DB): Pinecone , Milvus , Chroma\n热门原因 : AI 时代的刚需 。专门用于存储和高效检索由 AI 模型生成的“向量嵌入”（Embeddings）。所有大语言模型应用（如 RAG）、以图搜图、推荐系统等都离不开它。这是目前最火热的赛道。 时序数据库 (Time Series DB): TimescaleDB , InfluxDB\n热门原因 : 物联网 (IoT) 和监控系统 的标配。专门为处理带有时间戳的数据（如服务器指标、传感器读数、股票价格）而优化，写入和查询性能极高。 NewSQL 数据库: TiDB , CockroachDB\n热门原因 : 试图将“SQL 的事务一致性”和“NoSQL 的高扩展性”结合起来。它们兼容 MySQL 协议，但底层是分布式的，可以无限水平扩展。主要用于对数据一致性和扩展性要求都极高的场景，如金融、大型电商。"},"title":"intro"},"/docs/web/database/milvus/":{"data":{"ai-应用场景#AI 应用场景":"语义搜索: 文本和图像检索 推荐系统: 个性化推荐 异常检测: 模式识别 多模态搜索: 跨模态检索","milvus-自学笔记#Milvus 自学笔记":"Milvus 自学笔记","向量数据库#向量数据库":"向量存储: 高维向量数据管理 相似性搜索: 基于距离的检索 索引算法: HNSW、IVF、ANNOY 等 分布式架构: 可扩展的集群部署","学习目标#学习目标":"掌握 Milvus 向量数据库的核心概念和操作技能，能够构建高性能的向量搜索和 AI 应用。","学习资源#学习资源":"Milvus 官方文档 向量数据库理论 AI 应用案例 开源项目实践 本页面内容正在完善中…","学习路径#学习路径":"","实践项目#实践项目":"语义搜索引擎 图像检索系统 推荐系统后端 多模态搜索平台","应用实例#应用实例":"智能问答系统 图像相似性搜索 商品推荐引擎 文档检索系统 视频内容分析","核心概念#核心概念":"","第一阶段基础入门#第一阶段：基础入门":"Milvus 安装和配置 集合和分区管理 向量数据插入和查询 基本 API 使用","第三阶段集群部署#第三阶段：集群部署":"分布式架构设计 负载均衡配置 数据备份和恢复 监控和运维","第二阶段索引优化#第二阶段：索引优化":"索引类型选择 参数调优策略 查询性能优化 内存和存储管理","第四阶段ai-集成#第四阶段：AI 集成":"向量化模型集成 实时推理服务 多模态数据处理 端到端应用开发"},"title":"milvus"},"/docs/web/database/mongodb/":{"data":{"mongodb-自学笔记#MongoDB 自学笔记":"MongoDB 自学笔记","nosql-基础#NoSQL 基础":"文档数据库: JSON/BSON 文档存储 集合和文档: 数据组织结构 动态模式: 灵活的数据结构 水平扩展: 分片和副本集","学习目标#学习目标":"掌握 MongoDB 文档数据库的核心概念和操作技能，能够进行 NoSQL 数据库设计和应用开发。","学习资源#学习资源":"MongoDB 官方文档 NoSQL 数据库理论 实战项目教程 社区最佳实践 本页面内容正在完善中…","学习路径#学习路径":"","实践项目#实践项目":"博客系统后端 实时数据分析 地理位置服务 用户行为追踪","应用场景#应用场景":"内容管理系统 实时分析应用 物联网数据存储 社交网络应用 日志和事件数据","数据操作#数据操作":"CRUD 操作: 创建、读取、更新、删除 查询语言: MongoDB 查询语法 聚合管道: 复杂数据处理 索引优化: 查询性能提升","核心概念#核心概念":"","第一阶段基础入门#第一阶段：基础入门":"MongoDB 安装和配置 数据库和集合操作 文档的增删改查 数据类型和结构设计","第三阶段性能优化#第三阶段：性能优化":"索引策略和设计 查询性能分析 数据建模最佳实践 内存和存储优化","第二阶段查询进阶#第二阶段：查询进阶":"复杂查询条件 聚合框架使用 地理空间查询 全文搜索功能","第四阶段集群部署#第四阶段：集群部署":"副本集配置 分片集群搭建 数据备份和恢复 监控和运维管理"},"title":"mongodb"},"/docs/web/database/mysql/":{"data":{"mysql-自学笔记#MySQL 自学笔记":"MySQL 自学笔记","学习目标#学习目标":"掌握 MySQL 数据库的核心概念和操作技能，能够进行数据库设计、查询优化和管理维护。","学习资源#学习资源":"MySQL 官方文档 数据库设计理论 性能优化实战 运维最佳实践 本页面内容正在完善中…","学习路径#学习路径":"","实践项目#实践项目":"电商数据库设计 数据分析和报表 性能测试和优化 高可用架构搭建","数据库基础#数据库基础":"关系型数据库: ACID 特性和事务处理 SQL 语言: 数据定义、操作、查询和控制 存储引擎: InnoDB、MyISAM 等引擎特性 索引机制: B+ 树索引和查询优化","核心概念#核心概念":"","第一阶段基础操作#第一阶段：基础操作":"MySQL 安装和配置 数据库和表的创建管理 基本 CRUD 操作 数据类型和约束","第三阶段性能优化#第三阶段：性能优化":"索引设计和优化 查询执行计划分析 慢查询日志分析 数据库监控和调优","第二阶段查询进阶#第二阶段：查询进阶":"复杂查询和连接 子查询和窗口函数 聚合函数和分组 正则表达式和全文搜索","第四阶段运维管理#第四阶段：运维管理":"备份和恢复策略 用户权限管理 主从复制配置 集群部署和维护","高级特性#高级特性":"视图和存储过程: 复杂业务逻辑封装 触发器: 数据变更自动处理 分区表: 大数据量处理策略 复制和集群: 高可用架构"},"title":"mysql"},"/docs/web/frontend/":{"data":{"":"前端开发相关学习笔记。"},"title":"前端"},"/docs/web/frontend/css/":{"data":{"":"CSS属于前端里最牢的知识了，有点像编程界的C++，你永远没法说自己精通了CSS，而且前端开发和UI设计本身也在合流，这样一来就更牢了。 这里大纲先这样写着，后面再来补充。\ngraph TD\rA[CSS 学习指南] --\u003e B[核心基础];\rA --\u003e C[现代布局];\rA --\u003e D[响应式设计];\rA --\u003e E[视觉与交互];\rA --\u003e F[生态与工具];\rB --\u003e B1[\"CSS 是什么 \u0026 如何工作\"];\rB --\u003e B2[选择器];\rB --\u003e B3[盒模型];\rC --\u003e C1[Flexbox];\rC --\u003e C2[Grid];\rD --\u003e D1[媒体查询];\rD --\u003e D2[流式单位];\rE --\u003e E1[CSS 变量];\rE --\u003e E2[过渡];\rE --\u003e E3[动画];\rF --\u003e F1[\"预处理器 (Sass/SCSS)\"];\rF --\u003e F2[\"CSS 框架 (Tailwind)\"]; 核心基础 现代布局 响应式设计 视觉与交互 生态与工具"},"title":"CSS"},"/docs/web/frontend/css/bedrock/":{"data":{"":"","第一部分css-核心基础#\u003cstrong\u003e第一部分：CSS 核心基础\u003c/strong\u003e":"CSS 是什么 \u0026 如何工作\n简介：CSS 的角色——网页的“化妆师”。 三种引入方式：内联样式、内部样式表、外部样式表。 核心概念：层叠 (Cascade), 优先级 (Specificity), 和 继承 (Inheritance) —— 理解“谁的样式说了算”。 选择器 (Selectors): 精准定位你的目标\n基础选择器：标签、类 (.class)、ID (#id)。 组合选择器：后代 ( )、子代 (\u003e)、相邻兄弟 (+)、通用兄弟 (~)。 伪类与伪元素： 伪类 (Pseudo-classes): :hover, :focus, :nth-child(), :not() 等，用于描述元素在特定状态下的样式。 伪元素 (Pseudo-elements): ::before, ::after, ::first-line 等，用于在元素内容之外添加样式。 盒模型 (The Box Model): 万物皆为盒\ncontent, padding, border, margin 的可视化解释。 关键属性 box-sizing: border-box：为什么它能让布局更直观。"},"title":"bedrock"},"/docs/web/frontend/css/ecosystem/":{"data":{"":"","第五部分生态与工具#\u003cstrong\u003e第五部分：生态与工具\u003c/strong\u003e":"预处理器 (Pre-processors): Sass/SCSS\n变量、嵌套、混合 (Mixins)。 模块化：@import 与 @use。 CSS 框架：Tailwind CSS\n原子化 CSS (Atomic CSS) 的理念。 即时编译 (Just-in-Time) 引擎。 与传统 CSS (如 Bootstrap) 的对比。"},"title":"ecosystem"},"/docs/web/frontend/css/interactivity/":{"data":{"":"","第四部分视觉效果与交互性#\u003cstrong\u003e第四部分：视觉效果与交互性\u003c/strong\u003e":"CSS 变量 (Variables): 让样式可复用\n定义 (--main-color: #...) 与使用 (var(--main-color))。 全局与局部作用域。 过渡 (Transitions): 平滑的状态变化\ntransition 属性：property, duration, timing-function, delay。 动画 (Animations): 让页面动起来\n@keyframes 规则：定义动画的关键帧。 animation 属性：绑定动画到元素。"},"title":"interactivity"},"/docs/web/frontend/css/layouts/":{"data":{"":"","第二部分现代布局技术#\u003cstrong\u003e第二部分：现代布局技术\u003c/strong\u003e":"Flexbox：一维布局的瑞士军刀\n核心概念：容器 (Container) 与 项目 (Items)。 主轴与交叉轴：flex-direction, justify-content, align-items。 应用场景：导航栏、垂直居中、项目对齐与分布。 Grid：二维布局的终极解决方案\n核心概念：网格容器与网格项。 定义网格：grid-template-columns, grid-template-rows, gap。 放置项目：通过行/列号或命名区域。 应用场景：复杂的页面布局、卡片式画廊。"},"title":"layouts"},"/docs/web/frontend/css/responsive/":{"data":{"":"","第三部分响应式设计#\u003cstrong\u003e第三部分：响应式设计\u003c/strong\u003e":"媒体查询 (Media Queries): 适应不同屏幕\n语法：@media (min-width: ...) 和 (max-width: ...)。 移动优先 (Mobile-First) vs. 桌面优先 (Desktop-First) 策略。 流式单位 (Fluid Units): 让元素“活”起来\n相对单位：%, vw, vh, em, rem。 clamp() 函数：在特定范围内动态调整大小。"},"title":"responsive"},"/docs/web/frontend/javascript/":{"data":{"":"","heading#\ufffd\ufffd\ufffd\ufffd\ufffd﷨":"const add = (a, b) =\u003e a + b; const sayHello = name =\u003e { console.log(`Hello, ${name}!`); }; ֻ��һ������ʱ����ʡ�����ţ��޲�����ʹ�� () =\u003e�� ����ʽ��Ĭ�Ϸ��ظñ���ʽ��ֵ������Ҫ��������������ʽʹ�û����Ų�ͨ�� return ���ء�","this-ĳ#this \ufffdĲ\ufffd\ufffd\ufffd":"д�� this �󶨹��� ���ͳ��� ��ͨ���� function () {} �ɵ��÷����������� call/apply/bind �޸� ���캯�������󷽷�����Ҫ��̬�л������� ��ͷ���� () =\u003e {} ����ʱ���������� this���޷������°��� ����������Promise �ص����¼������еıհ� const teacher = { name: 'Bubblevan', students: ['Alice', 'Bob'], callRoll() { this.students.forEach(student =\u003e { console.log(`${this.name} ������${student}`); }); }, }; teacher.callRoll(); ������ forEach ��ʹ����ͨ��������Ҫ�ֶ����� this����������ʱ this Ϊ undefined���ϸ�ģʽ����ָ�� window�����ϸ�ģʽ����","ʺ벻ʺϵĳ#\ufffdʺ\ufffd\ufffd벻\ufffdʺϵĳ\ufffd\ufffd\ufffd":"? �ʺϣ��̳����� this �Ļص�����ʽ���á����̺�������ʽ�������ͺ����� ? ���ʺϣ���Ҫ��Ϊ���캯�������� arguments ����������ͨ�� bind/call/apply ��̬�ı� this �ĳ�����","ͷ-this#\ufffd\ufffdͷ\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd this":"","ϊҫͷ#Ϊ\ufffd\ufffd\ufffd\ufffdҪ\ufffd\ufffdͷ\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd":"��ͳ������ this ָ���������÷�ʽ���ص������г����� “const self = this” ����ʽ bind ��д������ͷ������Arrow Function���Դʷ������򲶻� this��ʹ�ص����������ࡢ�������ȶ���","ѧϰ#ѧϰ\ufffd\ufffd\ufffd\ufffd":"ǿ��\"�ڹ�\"���ԣ��¼�ѭ����Promise��async/await ������ JavaScript ����ʱ���ƣ��� TypeScript ������ϵͳ�໥�����������߲���ѧϰ������������֪��Щ�����Ա�������������Щ�����Ͳ��ĸ����� ͨ���ԣ���Щ֪ʶ������ǰ�ˣ��������������ز����٣��ں��ˣ�Node.js��Deno �ȣ�ͬ�����á������ɶ���“JS ��������\"�ĵ��������첽��ִ�л�������ʱ����ʱ���ݡ� ѧϰ·�������������� JavaScript ��ͬ��/�첽ԭ������ѧϰ TypeScript ����Ϊ Promise��async ��������������Ϣ����ѧϰ·�߸�ƽ����","ھ#\ufffd\ufffd\ufffd\ufffd\ufffdھ\ufffd":"��ͷ����������ʱȷ�� this����ͨ����������ʱ���� this��\n�������ϲ��죬������ React �ȿ����и���������д�¼��������첽�ص��Լ��Զ��� Hook �ڲ����߼���","ݴ#\ufffd\ufffd\ufffdݴ\ufffd\ufffd\ufffd":"graph TD\rA[JS ��������] --\u003e B[���Ի��������л���];\rA --\u003e C[�첽���̺���];\rA --\u003e D[�ִ��﷨��];\rA --\u003e E[ʵս����];\rB --\u003e B1[�����������򡢱հ�];\rB --\u003e B2[ԭ��������������];\rB --\u003e B3[\"ģ�黯 (ESM)\"];\rC --\u003e C1[�¼�ѭ��];\rC --\u003e C2[\"Promise, async/await\"];\rC --\u003e C3[��������΢����];\rD --\u003e D1[\"��ͷ����, �⹹, չ��\"];\rD --\u003e D2[\"ģ���ַ���, ��ѡ��, ��ֵ�ϲ�\"];\rE --\u003e E1[��������������];\rE --\u003e E2[�����Ż����ڴ�����];\rE --\u003e E3[�� TypeScript Э��]; ������Ŀ���������ƣ���Ϊ�� React��Node.js �ȼ���ջ�лع� JS ���ĵ����ڡ�"},"title":"JavaScript"},"/docs/web/frontend/n_xtjs/":{"data":{"":"���� ���ļ��� ���������� ���� Ӧ�÷��ڣ� Next.js React + Node.js �� React Ӧ�þ߱� SSR/SSG��·�ɡ�API ������ ǰ��ȫջ���� docs/self-study/frontend/ Nuxt.js Vue + Node.js �� Vue Ӧ�þ߱� SSR/SSG��·�ɡ�API ������ ǰ��ȫջ���� docs/self-study/frontend/ NestJS Node.js + TypeScript ������Ч������չ�Ĵ����˷��� �����˿��� docs/self-study/backend/ ��ʱ�ȸ��⣬֪����Щ��һ�����Ÿ���ɡ�"},"title":"Next.js / Nuxt.js"},"/docs/web/frontend/react/":{"data":{"":"作为如今前端两大著名框架之一，React 最吸引人的地方，在于它把 UI 看作状态的函数。我们只需要描述\"在某个状态下界面长什么样\"，其余的更新、渲染、协调工作都交给 React 背后的虚拟 DOM 与 Fiber 架构处理。\nBTW 它最烦的一点在于版本更新 Page Router 还有 APP Router 不一样，当初在那个 chatgpt 还不存在的时候看英文博客差点给我看似了。","react-入门与核心概念#React 入门与核心概念":"React 基础","入门准备#入门准备":"在正式深入学习之前，先搭建好开发环境并熟悉基础工具。","创建第一个-react-项目#创建第一个 React 项目":"打开终端执行：\nnpm create vite@latest my-react-app -- --template react 进入项目目录并安装依赖：\ncd my-react-app npm install 安装完成后，你的项目结构应该如下所示：\nmy-react-app/\r├── node_modules/\r├── public/\r│ └── vite.svg\r├── src/\r│ ├── App.css\r│ ├── App.jsx\r│ ├── index.css\r│ ├── main.jsx\r│ └── assets/\r│ └── react.svg\r├── .gitignore\r├── index.html\r├── package.json\r└── vite.config.js 现在，让我们启动开发服务器：\nnpm run dev Vite 将会启动一个本地开发服务器，你可以在浏览器中打开 http://localhost:5173 (端口号可能会不同) 来查看你的应用。","在线课程#在线课程":"（待补充）","学习路线导航#学习路线导航":"下面的文档按照\"入门 → 进阶 → 状态管理 → 实战复用\"的顺序排列，可按章节顺序阅读，也可以根据需求跳读。","官方文档#官方文档":"React 官方文档 React 中文文档","实战组件与项目案例#实战组件与项目案例":"实战项目","实践项目#实践项目":"GitHub 上的开源项目、个人作品集网站、企业管理系统……","延伸资源#延伸资源":"","状态管理选型指南#状态管理选型指南":"状态管理","环境要求#环境要求":"Node.js ≥ 14.0.0 包管理器：npm / Yarn / pnpm（任选其一） 我们将使用 Vite 来快速搭建 React 项目，它提供极速的开发体验。","组件进阶与状态提升#组件进阶与状态提升":"React 进阶"},"title":"React"},"/docs/web/frontend/react/advanced/":{"data":{"":"现在，让我们把组件之间的互动、数据流转、渲染策略梳理得更系统：先解决通讯，再掌握列表与 key，最后处理跨组件共享和副作用。","使用-map-渲染列表#使用 \u003ccode\u003emap\u003c/code\u003e 渲染列表":"可以利用 JavaScript 的 map() 方法把数组转换成一组元素：\nconst numbers = [1, 2, 3, 4, 5]; const listItems = numbers.map((number) =\u003e \u003cli\u003e{number}\u003c/li\u003e ); ReactDOM.render( \u003cul\u003e{listItems}\u003c/ul\u003e, document.getElementById('root') ); 详细说明可参考《JavaScript：箭头函数与 this》。","使用-props-传递数据#使用 Props 传递数据":"Props（properties 的缩写）是从父组件传递给子组件的数据。这让我们的组件更加灵活和可复用。\n让我们来创建一个 Greeting 组件，它接收一个 name prop 并显示一条问候消息：\nfunction Greeting(props) { return \u003ch1\u003e你好, {props.name}！\u003c/h1\u003e; } export default Greeting; 现在，我们可以在 App.jsx 中使用这个组件，并传递一个 name prop 给它：\nimport Greeting from './components/Greeting'; function App() { return ( \u003cdiv\u003e \u003cGreeting name=\"张三\" /\u003e \u003cGreeting name=\"李四\" /\u003e \u003c/div\u003e ); } export default App; Props 是只读的，一个组件永远不能修改它自己的 props，只能根据传入的值去表现。","使用逻辑与--运算符#使用逻辑与 \u003ccode\u003e\u0026amp;\u0026amp;\u003c/code\u003e 运算符":"如果条件为 true，\u0026\u0026 右侧的 JSX 会被渲染，否则直接跳过：\nfunction Mailbox(props) { const unreadMessages = props.unreadMessages; return ( \u003cdiv\u003e \u003ch1\u003e你好！\u003c/h1\u003e {unreadMessages.length \u003e 0 \u0026\u0026 \u003ch2\u003e 你有 {unreadMessages.length} 条未读消息。 \u003c/h2\u003e } \u003c/div\u003e ); }","列表渲染与-keys#列表渲染与 Keys":"","副作用与生命周期#副作用与生命周期":"函数组件通过 useEffect 处理数据获取、订阅、计时器等副作用。\nimport { useEffect, useState } from 'react'; function OnlineStatus() { const [isOnline, setIsOnline] = useState(true); useEffect(() =\u003e { const handleFocus = () =\u003e setIsOnline(true); const handleBlur = () =\u003e setIsOnline(false); window.addEventListener('focus', handleFocus); window.addEventListener('blur', handleBlur); return () =\u003e { window.removeEventListener('focus', handleFocus); window.removeEventListener('blur', handleBlur); }; }, []); return \u003cspan\u003e{isOnline ? '在线' : '离线'}\u003c/span\u003e; } export default OnlineStatus; 依赖数组：第二个参数控制副作用何时执行。 [] 表示仅在挂载/卸载时触发。 [foo, bar] 表示当依赖变化时重新执行。 清理函数：在返回的函数里释放资源，避免内存泄漏或重复订阅。 掌握 props、条件渲染、列表与 key、状态提升和 useEffect，便能搭出大部分业务场景所需的组件交互模型。","条件渲染#条件渲染":"应用中经常需要根据不同条件显示或隐藏某些内容。React 提供了多种写法来实现条件渲染。\n最直接的方式是使用 if 语句：\nfunction UserGreeting() { return \u003ch1\u003e欢迎回来！\u003c/h1\u003e; } function GuestGreeting() { return \u003ch1\u003e请先登录。\u003c/h1\u003e; } function Greeting(props) { const isLoggedIn = props.isLoggedIn; if (isLoggedIn) { return \u003cUserGreeting /\u003e; } return \u003cGuestGreeting /\u003e; } 对于更简单的条件，可以使用三元运算符：\nfunction Greeting(props) { const isLoggedIn = props.isLoggedIn; return ( \u003cdiv\u003e {isLoggedIn ? \u003cUserGreeting /\u003e : \u003cGuestGreeting /\u003e } \u003c/div\u003e ); }","状态提升与共享数据#状态提升与共享数据":"当多个组件需要共享同一份状态时，可以将 state 提升到它们最近的共同父组件，再通过 props 将数据和回调下发。\nimport { useState } from 'react'; function TemperatureInput({ label, value, onChange }) { return ( \u003clabel\u003e {label} \u003cinput value={value} onChange={(event) =\u003e onChange(event.target.value)} /\u003e \u003c/label\u003e ); } function TemperatureConverter() { const [celsius, setCelsius] = useState(''); const [fahrenheit, setFahrenheit] = useState(''); const handleCelsiusChange = (next) =\u003e { setCelsius(next); setFahrenheit(next === '' ? '' : (Number(next) * 9) / 5 + 32); }; const handleFahrenheitChange = (next) =\u003e { setFahrenheit(next); setCelsius(next === '' ? '' : ((Number(next) - 32) * 5) / 9); }; return ( \u003cdiv\u003e \u003cTemperatureInput label=\"摄氏度\" value={celsius} onChange={handleCelsiusChange} /\u003e \u003cTemperatureInput label=\"华氏度\" value={fahrenheit} onChange={handleFahrenheitChange} /\u003e \u003c/div\u003e ); } export default TemperatureConverter; 这样可以确保两侧输入框的值始终保持同步。","组件之间的通讯#组件之间的通讯":"","维护稳定的-key#维护稳定的 Key":"Keys 帮助 React 识别哪些元素被改变、添加或删除，应该稳定且唯一：\nconst todoItems = todos.map((todo) =\u003e \u003cli key={todo.id}\u003e {todo.text} \u003c/li\u003e ); Key 只需要在兄弟节点之间保持唯一，不必全局唯一。稳定的 key 是构建复杂交互界面的基础。"},"title":"组件交互与数据传递"},"/docs/web/frontend/react/basics/":{"data":{"":"OK 啊宝鸡们，我们已经拥有了一个可运行的 Vite + React 项目。接下来顺着目录往里走，从项目结构到第一份组件代码，再到 JSX 的来龙去脉，完整理解 React 的基础。","1-xml-extensible-markup-language---可扩展标记语言#1. XML (eXtensible Markup Language - 可扩展标记语言)":"是什么？ XML 是一种用来存储和传输数据的标记语言。它的设计初衷是“让数据能被机器和人同时轻松读懂”。\n核心特点：\n自定义标签：你可以根据数据结构创建任何你想要的标签，比如 , , 。 语法严格：所有标签都必须闭合，必须有唯一的根元素。这保证了数据结构的严谨性。 纯数据，无表现：XML 只关心“数据是什么”，不关心“数据长什么样”。它本身不带任何样式。 生活中的例子： 想象一下，XML 就像一份非常规范的个人档案表格。","2-jsx-javascript-xml#2. JSX (JavaScript XML)":"是什么？ JSX 是 JavaScript 的一个语法扩展，它允许您在 JavaScript 代码中编写类似 HTML 的结构。它由 React 推广开来，但现在也被 Vue 等其他框架支持。\n核心特点：\n在 JS 中写 UI：它让开发者可以用声明式、类似 HTML 的语法来描述用户界面应该长什么样，而不是用繁琐的命令式代码（如 document.createElement）去一步步创建。 不是真正的 HTML：虽然看起来像，但它会被编译器（如 Babel）转换成常规的 JavaScript 函数调用。 强大的 JavaScript 能力：你可以在 JSX 中无缝地使用 JavaScript 的变量、函数和逻辑（通过 {} 包裹）。 生活中的例子： JSX 就像一个“宜家家具的图纸”。\n// 这不是字符串，也不是 HTML，这是 JSX const user = \"Bubblevan\"; const myComponent = ( \u003cdiv className=\"profile\"\u003e \u003ch1\u003eWelcome, {user}!\u003c/h1\u003e \u003cp\u003eThis is your personalized dashboard.\u003c/p\u003e \u003c/div\u003e ); 这张“图纸”清晰地描述了一个组件的样子，并且还能动态地把 {user} 这个变量“组装”进去。最终，React 这个“工匠”会根据这张图纸，把它变成浏览器里真正的网页元素。","3-mdx-markdown-for-the-component-era#3. MDX (Markdown for the Component Era)":"是什么？ MDX 是 Markdown 的超集。它允许您在写 Markdown 文档的同时，无缝地导入和使用 JSX 组件。\n核心特点：\nMarkdown + JSX：您可以像往常一样用 # 写标题，用 * 写列表，但同时，您可以像写 React 代码一样，直接在文档里插入一个交互式图表、一个视频播放器或者任何您创建的组件。 内容与交互的融合：它打破了静态内容（文章）和动态应用（组件）之间的墙。 本站（Docusaurus）正是一个大量使用 MDX 的例子：\n--- title: My Awesome Document --- import { Chart } from '@site/src/components/Chart'; import { VideoPlayer } from '@site/src/components/VideoPlayer'; # 我的神奇文档 这是一段普通的 Markdown 文本。 我们甚至可以直接在这里插入一个动态图表： 甚至可以放一个视频播放器： --- 特性 XML JSX MDX 核心用途 结构化数据 在 JS 中描述 UI 交互式文档 本质 标记语言 JS 的语法扩展 Markdown 的超集 运行环境 任何能解析 XML 的程序 JavaScript（需编译） Markdown 解析器 + JS 框架（需编译） 好比 数据的蓝图 UI 的蓝图 文章的蓝图 虽然 JSX 看起来很像 HTML，但它实际上是 JavaScript。在编译时，Babel 会将 JSX 转换为 React.createElement() 的调用。","入门准备#入门准备":"熟悉 src 目录结构，搞清入口、组件、样式文件分别在哪。 了解 React 函数组件的基本写法与运行流程。 理解 JSX 为什么能在 JavaScript 里写出类似 HTML 的结构。","深入理解-jsx#深入理解 JSX":"JSX (JavaScript XML) 是 React 的一个语法扩展，它允许我们在 JavaScript 中编写看起来像 HTML 的代码。这让我们可以将 UI 逻辑和渲染逻辑放在同一个地方，使得代码更加直观和易于维护。","第一个-react-组件从函数开始#第一个 React 组件：从函数开始":"React 的核心思想就是组件化。组件是独立的、可复用的代码块，它负责渲染 UI 的一部分。\n让我们来修改 App.jsx，创建我们自己的第一个组件。\n打开 src/App.jsx 文件，并将其内容替换为：\nfunction App() { return ( \u003cdiv\u003e \u003ch1\u003e你好，React！\u003c/h1\u003e \u003cp\u003e这是一个简单的 React 组件。\u003c/p\u003e \u003c/div\u003e ); } export default App; 保存文件后，浏览器中的页面会自动更新。\n我们刚刚创建了一个名为 App 的函数式组件。它返回了一段类似 HTML 的代码，这就是 JSX。","让组件拥有状态#让组件拥有状态":"静态的组件是不够的。在实际应用中，我们需要处理用户的交互，并根据数据的变化来更新 UI。这就是 State 的用武之地。\nState 是组件内部私有的数据。当 State 发生变化时，React 会自动重新渲染组件，以反映最新的数据。\n让我们来创建一个计数器组件来理解 State。\n在 src 目录下创建一个新文件 Counter.jsx，并添加以下代码：\nimport React, { useState } from 'react'; function Counter() { const [count, setCount] = useState(0); return ( \u003cdiv\u003e \u003cp\u003e你点击了 {count} 次\u003c/p\u003e \u003cbutton onClick={() =\u003e setCount(count + 1)}\u003e 点我 \u003c/button\u003e \u003c/div\u003e ); } export default Counter; 这里我们用到了 useState，这是 React 提供的一个 Hook。\nHook 顾名思义就是钩子函数，把它理解成用你的钩子去转一个组件里面的密码锁齿轮，让这个齿轮的 State 变换，就像 Java 里面一个类单独暴露出一个 setValue 方法来一样\nHook 让你可以在函数式组件中使用 React 的特性，比如 State。\nuseState(0) 初始化了一个名为 count 的 state 变量，其初始值为 0。 useState 返回一个数组，包含两个元素：当前的 state 值 (count) 和一个更新该 state 的函数 (setCount)。 当我们点击按钮时，onClick 事件会触发，调用 setCount(count + 1)。这会更新 count 的值，并告诉 React 重新渲染 Counter 组件。 现在，让我们在 App.jsx 中使用这个新组件。\nimport Counter from './components/Counter'; function App() { return ( \u003cdiv\u003e \u003ch1\u003e你好，React！\u003c/h1\u003e \u003cp\u003e这是一个简单的 React 组件。\u003c/p\u003e \u003chr /\u003e \u003ch2\u003e这是一个计数器：\u003c/h2\u003e \u003cCounter /\u003e \u003c/div\u003e ); } export default App; 现在，你可以在页面上点击按钮，看到数字不断增加。这就是 React 响应式更新！\n让我们小结一下：\n组件是 React 的最小单位，函数返回 JSX 就能定义界面。 JSX 只是语法糖，最终都会被编译成 React.createElement。 useState 让组件拥有内部状态，配合事件即可响应用户交互。","项目结构速览#项目结构速览":"在 src 目录下，有几个核心文件：\nmain.jsx: 这是应用的入口文件。它使用 ReactDOM.createRoot() 来告诉 React 将应用渲染到 index.html 中的 元素上。 App.jsx: 这是我们的第一个 React 组件。它是一个简单的函数，返回了一些 JSX。 index.css 和 App.css: 这些是应用的样式文件。"},"title":"核心概念：组件、JSX 和 State"},"/docs/web/frontend/react/projects/":{"data":{"browserwindow-组件#BrowserWindow 组件":"BrowserWindow 组件用于模拟一个浏览器窗口，通常用于展示项目截图或嵌入其他内容，提供一个更具沉浸感的视觉体验。","highlightblock-高亮提示组件#HighlightBlock 高亮提示组件":"HighlightBlock 是文档中常用的高亮提示组件，可用于强调注意事项、成功提示或警告信息，为读者提供更清晰的阅读导航。","marquee-跑马灯组件#Marquee 跑马灯组件":"Marquee 是一个高度可定制的跑马灯组件，用于创建平滑滚动的动画效果，常用于展示客户 Logo、特性列表或任何需要循环播放的内容。","代码解析-srccomponentsbrowserwindowindextsx#代码解析 (\u003ccode\u003esrc/components/BrowserWindow/index.tsx\u003c/code\u003e)":"import React from 'react'; import styles from './styles.module.css'; function BrowserWindow({ children, minHeight, url }) { return ( \u003cdiv className={styles.browserWindow} style={{ minHeight }}\u003e \u003cdiv className={styles.browserWindowHeader}\u003e \u003cdiv className={styles.buttons}\u003e \u003cspan className={styles.dot} style={{ background: '#f25f58' }} /\u003e \u003cspan className={styles.dot} style={{ background: '#fbbe3c' }} /\u003e \u003cspan className={styles.dot} style={{ background: '#58cb42' }} /\u003e \u003c/div\u003e \u003cdiv className={styles.browserWindowAddressBar}\u003e{url}\u003c/div\u003e \u003cdiv className={styles.browserWindowMenuIcon}\u003e \u003cdiv\u003e \u003cspan className={styles.bar} /\u003e \u003cspan className={styles.bar} /\u003e \u003cspan className={styles.bar} /\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003cdiv className={styles.browserWindowBody}\u003e{children}\u003c/div\u003e \u003c/div\u003e ); } export default BrowserWindow; Props： children：任何可渲染的 React 节点，将显示在窗口主体中。 minHeight：窗口的最小高度，通过内联样式设置。 url：显示在地址栏中的 URL 字符串。 结构：组件由头部 (browserWindowHeader) 和主体 (browserWindowBody) 组成。头部包含了模拟的窗口控件和地址栏，而主体则通过 {children} 渲染传入的内容。 样式：组件的样式由 styles.module.css 文件定义，实现了 CSS 模块化，避免了全局样式冲突。","代码解析-srccomponentshighlightblockindextsx#代码解析 (\u003ccode\u003esrc/components/HighlightBlock/index.tsx\u003c/code\u003e)":"import React from 'react'; import clsx from 'clsx'; import styles from './styles.module.css'; export default function HighlightBlock({ type = 'info', title, icon, children, className }) { const getDefaultIcon = () =\u003e { switch (type) { case 'warning': return '⚠️'; case 'success': return '✅'; case 'error': return '❌'; case 'note': return '📝'; case 'tip': return '💡'; case 'question': return '❓'; default: return 'ℹ️'; } }; const displayIcon = icon || getDefaultIcon(); return ( \u003cdiv className={clsx(styles.highlightBlock, styles[type], className)}\u003e \u003cdiv className={styles.header}\u003e \u003cspan className={styles.icon}\u003e{displayIcon}\u003c/span\u003e {title \u0026\u0026 \u003cspan className={styles.title}\u003e{title}\u003c/span\u003e} \u003c/div\u003e \u003cdiv className={styles.content}\u003e{children}\u003c/div\u003e \u003c/div\u003e ); }","代码解析-srccomponentsmagicuimarqueetsx#代码解析 (\u003ccode\u003esrc/components/magicui/marquee.tsx\u003c/code\u003e)":"import { cn } from '../../lib/utils'; interface MarqueeProps { className?: string; reverse?: boolean; pauseOnHover?: boolean; children?: React.ReactNode; vertical?: boolean; repeat?: number; [key: string]: any; } export default function Marquee({ className, reverse, pauseOnHover = false, children, vertical = false, repeat = 4, ...props }: MarqueeProps) { return ( \u003cdiv {...props} className={cn( 'group flex overflow-hidden p-2 [--duration:40s] [--gap:1rem] [gap:var(--gap)]', { 'flex-row': !vertical, 'flex-col': vertical, }, className, )} \u003e {Array(repeat) .fill(0) .map((_, i) =\u003e ( \u003cdiv key={i} className={cn('flex shrink-0 justify-around [gap:var(--gap)]', { 'animate-marquee flex-row': !vertical, 'animate-marquee-vertical flex-col': vertical, 'group-hover:[animation-play-state:paused]': pauseOnHover, '[animation-direction:reverse]': reverse, })} \u003e {children} \u003c/div\u003e ))} \u003c/div\u003e ); } Props： vertical：如果为 true，则垂直滚动；否则水平滚动。 reverse：如果为 true，则反向滚动。 pauseOnHover：如果为 true，鼠标悬停时动画暂停。 repeat：内容重复的次数，默认为 4。 核心逻辑： 组件使用 Array(repeat).fill(0).map(...) 来动态创建内容的多个副本。 cn 工具函数（通常来自 clsx 或类似库）用于根据 props 动态地组合 CSS 类。 动画本身由 CSS 类（如 animate-marquee）和 CSS 变量（如 --duration）控制，实现了表现与逻辑的分离。","使用示例#使用示例":"你可以在 MDX 文件中这样使用它，来展示一张图片：","使用示例-1#使用示例":"你可以用它来展示一系列技术图标：\nconst icons = [ // ... an array of icon components or images ]; \u003cMarquee\u003e {icons.map(icon =\u003e \u003cdiv className=\"mx-4\"\u003e{icon}\u003c/div\u003e)} \u003c/Marquee\u003e 具体可参考首页下技术栈的跑马灯效果。","使用示例-2#使用示例":"在复杂的教程或流程中加入提示块，可以让读者快速抓住重点或避坑信息。","功能与用途#功能与用途":"模拟浏览器 UI：包含窗口按钮（关闭、最小化、最大化）、地址栏和菜单图标，创造出逼真的浏览器外观。 内容嵌入：通过 children prop，可以在窗口主体内嵌入任何 React 内容，如图片、文本或交互式组件。 可定制性：支持通过 minHeight 和 url props 自定义最小高度和地址栏中显示的链接。","功能与用途-1#功能与用途":"无限滚动：通过复制内容并使用 CSS 动画，实现无缝的循环滚动。 方向控制：支持水平和垂直滚动。 交互性：可以配置为当鼠标悬停时暂停动画。 可重复性：可以指定内容重复的次数，以确保动画的连续性。","功能与用途-2#功能与用途":"多种类型：通过 type prop 快速切换 info、warning、success、tip 等风格，内置默认图标。 自定义标题与图标：可以传入 title 与 icon，满足不同语气与场景需求。 模块化样式：样式隔离在 styles.module.css 中，便于维护。","学习小贴士#学习小贴士":"结合项目需求先挑选最核心的功能组件，再逐步扩展，可以让学习路径更清晰。","实战项目#实战项目":"实战项目用 React 可以做很多厉害的东西，像是笔者之前参加的创赛，其考古报告提取的前端我记得就是用 React 写的，然后一些入门的项目（像是 XLAB 万年不变的评论区）我也写过，不过最近的实践貌似都集中在一些博客/知识库网站构建的应用上，这里就以Docusaurus所用到的组件来进行说明。","注意#注意":"高亮块会占据额外的垂直空间，别忘了在文档中控制使用频率。\n这些都是你在构建自己的 React 应用时可以借鉴的宝贵经验。希望通过这些组件，你能更深入地理解 React 的核心思想，并能将这些知识应用到你未来的项目中。"},"title":"projects"},"/docs/web/frontend/react/state-management/":{"data":{"":"我们已经学过组件内的 useState，现在需要进一步回答这些问题：什么时候单纯的本地 state 就够用？什么时候要把 state 提升或抽到全局？副作用怎么处理？常见的第三方状态库应该如何选择？","react-内建状态工具箱#React 内建状态工具箱":"除了 useState 与状态提升，React 还提供了几种内建手段应对更复杂的场景：\nuseReducer：适用于状态更新逻辑复杂、涉及多步操作或需要可预测状态转换的组件。 Context：用来在组件树中向深层传递数据，避免多层级 props drilling。 自定义 Hook：把状态逻辑抽离成独立函数，在多个组件间复用。 这些工具组合使用，通常能覆盖中小型项目的绝大多数需求。","为什么需要状态管理#为什么需要状态管理":"状态描述了 UI 的当前展示形态。React 把视图看成「状态 → UI」的映射，所以管理好状态就等于掌控了界面。\nuseState 会返回一个包含两个元素的数组：\n第一个元素：当前的状态值。 第二个元素：更新状态的函数。 const [count, setCount] = useState(0); // React：请给我一个名为 count 的状态，初始值为 0； // 同时返回 setCount，以便我更新 count 并触发重新渲染。 对于单个组件来说，这已经足够。但一旦多个组件需要共享数据，就需要新的策略。","使用-useeffect-处理副作用#使用 \u003ccode\u003euseEffect\u003c/code\u003e 处理副作用":"副作用是指在组件渲染之外执行的操作，例如：\n数据获取（fetching data） 设置订阅（subscriptions） 手动更改 DOM useEffect 让函数组件可以在渲染完成后执行这些操作：\nimport React, { useState, useEffect } from 'react'; function Example() { const [count, setCount] = useState(0); // 相当于 componentDidMount 和 componentDidUpdate: useEffect(() =\u003e { // 使用浏览器 API 更新文档标题 document.title = `你点击了 ${count} 次`; }); return ( \u003cdiv\u003e \u003cp\u003e你点击了 {count} 次\u003c/p\u003e \u003cbutton onClick={() =\u003e setCount(count + 1)}\u003e 点我 \u003c/button\u003e \u003c/div\u003e ); }","依赖项数组#依赖项数组":"默认情况下，useEffect 在每次渲染后都会运行。通过传递依赖项数组可以控制触发时机：\nuseEffect(() =\u003e { document.title = `你点击了 ${count} 次`; }, [count]); // 仅在 count 更改时更新 如果依赖项数组为空 []，则 effect 只会在组件挂载和卸载时运行一次。\n代码 含义 比喻 useEffect(fn) 每次渲染后都运行 强迫症保安，每次都检查 useEffect(fn, []) 只在首次渲染后运行 挂画，只做一次 useEffect(fn, [dep]) 首次渲染，以及 dep 变化后运行 智能空调，按需启动 return cleanup 组件卸载或 effect 重跑前运行 搬家时取画，换新空调前拆旧的","本地状态共享状态提升#本地状态共享：状态提升":"当兄弟组件需要读取同一份数据时，可以把 state 提升到它们的最近公共父组件，再通过 props 下发。这就是 Lifting State Up。\n想象一个温度转换组件，需要同时展示摄氏度和华氏度：\nfunction TemperatureInput(props) { // ... } class Calculator extends React.Component { // 1. 状态被“提升”到了父组件 Calculator 里 this.state = {temperature: '', scale: 'c'}; render() { // 2. 父组件根据统一的 state，计算出两个需要的值 const celsius = scale === 'f' ? tryConvert(temperature, toCelsius) : temperature; const fahrenheit = scale === 'c' ? tryConvert(temperature, toFahrenheit) : temperature; return ( \u003cdiv\u003e {/* 3. 将计算好的值，通过 props 传给子组件去显示 */} \u003cTemperatureInput scale=\"c\" temperature={celsius} /\u003e \u003cTemperatureInput scale=\"f\" temperature={fahrenheit} /\u003e \u003c/div\u003e ); } }","清除-effect#清除 Effect":"有些副作用需要清除。例如设置订阅、计时器等，都要在组件卸载时取消。\nuseEffect 的回调可以返回一个清理函数：\nuseEffect(() =\u003e { const subscription = props.source.subscribe(); return () =\u003e { // 清除订阅 subscription.unsubscribe(); }; });"},"title":"状态管理与副作用"},"/docs/web/frontend/typescript/":{"data":{"heading#\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd":"��: ���Ķ����ͼ̳� �ӿ�: �ӿڶ�����ʵ�� ����: ���Ͳ�����","heading-1#\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd֯":"ģ�黯: ������֯����ģ�� �����淶: ͳһ�������淶 �ĵ�ע��: ���Ƶ�����ע��","typescript#TypeScript":"TypeScript","ģϵͳ#ģ\ufffd\ufffdϵͳ":"ES6 ģ��: import/export �﷨ �����ռ�: namespace �ؼ��� ģ������: ģ�����Ҳ���","ĸ#\ufffd\ufffd\ufffdĸ\ufffd\ufffd\ufffd":"","ż#\ufffd\ufffd\ufffd\ufffd\ufffdŻ\ufffd":"�����Ż�: ���ñ���ѡ�� ���ͼ���: �Ż����ͼ������� �����Ż�: ���������Ż�","ʵ#\ufffd\ufffd\ufffd\ufffdʵ\ufffd\ufffd":"","ʵŀ#ʵ\ufffd\ufffd\ufffd\ufffdĿ":"��Ҫ��nestjs������Ӧ������","ͱȫ#\ufffd\ufffd\ufffdͰ\ufffdȫ":"�ϸ�ģʽ: �����ϸ����ͼ��� �����ƶ�: �������������ƶ� ���Ͷ���: ����ʹ�����Ͷ���","σʵսӧ#\ufffd\ufffd\ufffd\ufffd\ufffd׶Σ\ufffdʵսӦ\ufffd\ufffd":"React + TypeScript // ���� Props ���� interface UserCardProps { user: User onEdit?: (user: User) =\u003e void onDelete?: (id: number) =\u003e void } const UserCard: React.FC\u003cUserCardProps\u003e = ({ user, onEdit, onDelete }) =\u003e { return ( \u003cdiv className=\"user-card\"\u003e \u003ch3\u003e{user.name}\u003c/h3\u003e \u003cp\u003e{user.email}\u003c/p\u003e {onEdit \u0026\u0026 \u003cbutton onClick={() =\u003e onEdit(user)}\u003e�༭\u003c/button\u003e} {onDelete \u0026\u0026 \u003cbutton onClick={() =\u003e onDelete(user.id)}\u003eɾ��\u003c/button\u003e} \u003c/div\u003e ) } // Hook ���� function useLocalStorage\u003cT\u003e(key: string, initialValue: T) { const [storedValue, setStoredValue] = useState\u003cT\u003e(() =\u003e { try { const item = window.localStorage.getItem(key) return item ? JSON.parse(item) : initialValue } catch (error) { return initialValue } }) const setValue = (value: T | ((val: T) =\u003e T)) =\u003e { try { const valueToStore = value instanceof Function ? value(storedValue) : value setStoredValue(valueToStore) window.localStorage.setItem(key, JSON.stringify(valueToStore)) } catch (error) { console.error(error) } } return [storedValue, setValue] as const } Node.js + TypeScript // Express Ӧ�� import express, { Request, Response, NextFunction } from 'express' interface UserRequest extends Request { user?: User } const app = express() // �м������� const authMiddleware = (req: UserRequest, res: Response, next: NextFunction) =\u003e { const token = req.headers.authorization?.split(' ')[1] if (!token) { return res.status(401).json({ message: 'δ��Ȩ' }) } // ��֤ token �������û���Ϣ req.user = { id: 1, name: '������', email: 'test@example.com' } next() } // ·�ɴ��� app.get('/api/users/:id', authMiddleware, (req: UserRequest, res: Response) =\u003e { const userId = parseInt(req.params.id) const user = req.user res.json({ user, requestedId: userId }) })","ϵͳ#\ufffd\ufffd\ufffd\ufffdϵͳ":"��̬���ͼ���: ����ʱ���ͼ��� �����ƶ�: �Զ��ƶϱ������� ����ע��: ��ʽ��������","ѧϰ#ѧϰ·\ufffd\ufffd":"","ѧϰŀ#ѧϰĿ\ufffd\ufffd":"���� TypeScript �ĺ��ĸ����Ϳ������ܣ��ܹ�ʹ�� TypeScript ����ǰ�˺ͺ��˿�����","ѧϰդ#ѧϰ\ufffd\ufffdԴ":"","һσ#\ufffd\ufffdһ\ufffd׶Σ\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd﷨":"�������� // �������� let name: string = '������' let age: number = 25 let isStudent: boolean = true let hobbies: string[] = ['����', '�Ķ�', '�˶�'] let tuple: [string, number] = ['������', 25] // ö�� enum Color { Red = 'red', Green = 'green', Blue = 'blue' } // �������� let id: string | number = '123' id = 123 // ���ͱ��� type UserId = string | number type User = { id: UserId name: string age: number } �������� // ��������ע�� function add(a: number, b: number): number { return a + b } // ��ͷ���� const multiply = (a: number, b: number): number =\u003e a * b // ��ѡ������Ĭ�ϲ��� function greet(name: string, greeting?: string): string { return greeting ? `${greeting}, ${name}!` : `Hello, ${name}!` } // �������� function process(x: number): number function process(x: string): string function process(x: number | string): number | string { if (typeof x === 'number') { return x * 2 } return x.toUpperCase() } �ӿ� // �ӿڶ��� interface User { id: number name: string email: string age?: number readonly createdAt: Date } // �ӿ�ʵ�� class UserImpl implements User { constructor( public id: number, public name: string, public email: string, public readonly createdAt: Date ) {} } // �ӿ���չ interface Employee extends User { department: string salary: number } // �����ӿ� interface SearchFunc { (source: string, subString: string): boolean }","ٷĵ#\ufffdٷ\ufffd\ufffdĵ\ufffd":"TypeScript �ٷ��ĵ� TypeScript �����ֲ�","ڶσ#\ufffdڶ\ufffd\ufffd׶Σ\ufffd\ufffd߼\ufffd\ufffd\ufffd\ufffd\ufffd":"���� // ���ͺ��� function identity\u003cT\u003e(arg: T): T { return arg } // ���ͽӿ� interface GenericIdentityFn\u003cT\u003e { (arg: T): T } // ������ class GenericNumber\u003cT\u003e { zeroValue: T add: (x: T, y: T) =\u003e T } // ����Լ�� interface Lengthwise { length: number } function loggingIdentity\u003cT extends Lengthwise\u003e(arg: T): T { console.log(arg.length) return arg } �߼����� // �������� type Combined = User \u0026 Employee // �������� type Status = 'loading' | 'success' | 'error' // ӳ������ type Partial\u003cT\u003e = { [P in keyof T]?: T[P] } type Required\u003cT\u003e = { [P in keyof T]-?: T[P] } // �������� type NonNullable\u003cT\u003e = T extends null | undefined ? never : T // ģ������������ type EmailLocaleIDs = \"welcome_email\" | \"email_heading\" type FooterLocaleIDs = \"footer_title\" | \"footer_sendoff\" type AllLocaleIDs = `${EmailLocaleIDs | FooterLocaleIDs}_id` װ���� // ��װ���� function sealed(constructor: Function) { Object.seal(constructor) Object.seal(constructor.prototype) } @sealed class Greeter { greeting: string constructor(message: string) { this.greeting = message } greet() { return \"Hello, \" + this.greeting } } // ����װ���� function log(target: any, propertyKey: string, descriptor: PropertyDescriptor) { const method = descriptor.value descriptor.value = function (...args: any[]) { console.log(`Calling ${propertyKey} with:`, args) const result = method.apply(this, args) console.log(`Result:`, result) return result } } class Calculator { @log add(a: number, b: number): number { return a + b } }"},"title":"TypeScript"},"/docs/web/frontend/vue/":{"data":{"":"当初学 React 的时候真的是Fly Bitch那种样子，甚至不得不去油管上看印度人 30min 的讲解视频，学长学姐也都抱着一副【啊这应该会啊】的态度，给我留下了很深的心理阴影，于是本科后面的前端基本上都是选用 Vue，毕竟其中文文档相比 React 还是丰富多了，并且个人开发者体验下来没有太大的差别。Vue 的组合式 API 和单文件组件 (SFC) 提供了灵活的工程化能力。通过拆解学习路径与实践项目，我们可以在掌握响应式系统的同时，快速构建现代化的前端应用。\n我超喜欢 SFC 的思想，最讨厌 Tailwind CSS 那种把样式写模版里的情况","学习路径#学习路径":"核心语法与响应式基础 组件开发 高级特性","官方文档#官方文档":"Vue 官方英文文档 Vue 官方中文文档","实战项目#实战项目":"项目实践"},"title":"Vue"},"/docs/web/frontend/vue/advanced/":{"data":{"":"","1-script-的三种形态#1. \u003ccode\u003e\u0026lt;script\u0026gt;\u003c/code\u003e 的三种形态":"a) 经典","2-vue-2-vs-vue-3-的核心区别#2. Vue 2 vs. Vue 3 的核心区别":"特性 Vue 2 Vue 3 核心 API 选项式 API：逻辑按选项分散，不利于复杂逻辑的组织。 组合式 API：逻辑按功能组织，代码更内聚、更易复用。 性能 通过 Object.defineProperty 实现响应式，对对象属性的增删无法直接检测。 通过 Proxy 重写响应式系统，性能更好，且能直接监听对象和数组的动态变化。 模板根节点 模板必须有一个根元素。 支持多个根元素 (Fragments)，代码结构更灵活。 v-model 一个组件上只能使用一个 v-model。 支持在同一个组件上绑定多个 v-model，如 v-model:title 和 v-model:content。","样式隔离与预处理器#样式隔离与预处理器":"","组合式-api-composition-api#组合式 API (Composition API)":"在 Vue 2 中，我们习惯于 选项式 API (Options API)，它通过 data、methods、computed 等选项来组织代码。这种方式结构清晰，对初学者非常友好。但随着组件功能的增多，同一个逻辑（比如，处理用户数据）的代码会散落在不同的选项里，难以维护。\n组合式 API 正是为了解决这个问题而生。它允许你将相关的逻辑代码组织在一起，形成一个高内聚的“功能单元”。\n让我们看一个简单的生命周期钩子示例：\nimport { onMounted, onUnmounted } from 'vue' // 这段代码通常在组件的","自定义-hooks-composables#自定义 Hooks (Composables)":"组合式 API 最大的魅力在于它能够通过 自定义 Hooks (官方称之为 Composables) 轻松实现逻辑复用。一个 “Composable” 本质上就是一个“钩入”Vue 响应式系统的函数。\n想象一下，你有一个计数器逻辑，想在多个组件中使用。与其每次都重写，不如将它提取到一个自定义 Composable 中。\n按照惯例，我们将这些文件放在 src/composables 目录下。\n1. 创建你的 Composable:\n// useCounter.js import { ref } from 'vue' // Composable 函数名通常以 \"use\" 开头 export function useCounter(initialValue = 0) { const count = ref(initialValue) const increment = () =\u003e count.value++ const decrement = () =\u003e count.value-- const reset = () =\u003e { count.value = initialValue } // 暴露状态和方法，供组件使用 return { count, increment, decrement, reset } } 2. 在组件中使用它:\n现在，任何组件都可以像调用一个普通函数一样，轻松地使用这个计数器逻辑。\n\u003ctemplate\u003e \u003cdiv\u003e \u003cp\u003eCount: {{ count }}\u003c/p\u003e \u003cbutton @click=\"increment\"\u003eIncrement\u003c/button\u003e \u003cbutton @click=\"decrement\"\u003eDecrement\u003c/button\u003e \u003cbutton @click=\"reset\"\u003eReset\u003c/button\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { useCounter } from '../composables/useCounter.js' // 使用自定义 Hook，并传入初始值 10 const { count, increment, decrement, reset } = useCounter(10) \u003c/script\u003e 通过自定义 Composables，你可以构建一个可重用的、响应式的逻辑库，在整个应用中轻松共享，让你的组件保持精简并专注于其 UI 呈现。","高级特性#高级特性":"随着你对 Vue 越来越熟悉，你会渴望更高效、更优雅的方式来组织代码，尤其是在构建复杂应用时。\nVue 3 提供了强大的 组合式 API (Composition API) 和 自定义 Hooks (Composables)。"},"title":"第三阶段 高级特性"},"/docs/web/frontend/vue/basics/":{"data":{"":"","reactive#\u003ccode\u003ereactive\u003c/code\u003e":"import { ref, reactive } from 'vue'; const user = reactive({ name: 'Bubblevan', age: 21, }); // 适合对象、数组 console.log(user.name); // 输出: Bubblevan // 要想改变盒子里面的值，你需要通过 .value 属性 user.age++; console.log(user.age); // 输出: 22 特性 ref reactive 适用场景 基本类型 (Number, String, Boolean) 或需要重新分配整个对象的场景 对象、数组 (Object, Array) 访问方式 在脚本中通过 .value 访问 直接访问属性 模板中访问 直接访问，Vue 会自动解包 直接访问属性 返回类型 返回一个包含 .value 属性的 RefImpl 对象 返回一个响应式的 Proxy 对象","ref#\u003ccode\u003eref\u003c/code\u003e":"import { ref, reactive } from 'vue'; const count = ref(0); // 适合基本类型 console.log(count.value); // 输出: 0 // 要想改变盒子里面的值，你需要通过 .value 属性 count.value++; console.log(count.value); // 输出: 1 为什么需要 .value？ ref 会将你的值包装在一个特殊的对象里，这样 Vue 才能“监视”到它的变化。在 JavaScript 逻辑中，你必须通过 .value 来访问或修改这个值。 好消息是，在模板 () 中使用 count 时，你不需要写 .value。Vue 会自动帮你“拆箱”。","vue-应用如何启动#Vue 应用如何启动":"Vue 将所有功能打包进应用实例，再挂载到某个 DOM 节点上：\nimport { createApp } from 'vue'; import App from './App.vue'; createApp(App).mount('#app'); 这段代码看起来很简单，但它做了三件至关重要的事：\nimport { createApp } from 'vue': 我们从 Vue 的核心库中取出了 createApp 这个工具，专门用来生产我们的 Vue 应用。 import App from './App.vue': App.vue 是我们应用的“根组件”。你可以把它想象成一棵树的树根，所有的枝叶（其他组件）都将从这里生长出来。它定义了应用最外层的结构和内容。 createApp(App).mount('#app'): createApp(App)：我们调用“工厂”函数，告诉它：“请用 App.vue 这个蓝图来创建我的应用吧！” .mount('#app')：创建好应用后，我们需要告诉 Vue 把它显示在哪里。这行代码的意思是：“找到 HTML 页面里那个 ID 是 app 的元素，然后把我的整个应用都渲染到那里去！” 小贴士：Vue 3 推荐以组件为中心组织代码，根组件 App.vue 就是我们渲染的起点。","入门准备#入门准备":"npm init vue@latest my-vue-app cd my-vue-app npm install npm run dev 你可以对比一下./src下面的核心入口文件，React是App.jsx、App.css、index.css、main.jsx，Vue是App.vue、main.js。这其实很好的体现了 SFC（Single File Component）的优势，将组件的模板、样式、逻辑都写在一个文件中，方便维护和管理。","响应式数据ref-与-reactive#响应式数据：\u003ccode\u003eref\u003c/code\u003e 与 \u003ccode\u003ereactive\u003c/code\u003e":"静态的页面是无趣的。Vue 的“响应式系统”就是当你的数据变化时，页面上依赖这些数据的地方会自动更新。在组合式 API (Composition API) 中，我们有两个方法来创建这种“会响应”的数据：ref 和 reactive。","模板语法与事件绑定#模板语法与事件绑定":"我们已经学会了如何创建“活”数据，现在是时候让用户与这些数据互动了。 在 Vue 中，HTML 结构写在 块里，而交互逻辑则放在","计算属性声明式派生状态#计算属性：声明式派生状态":"有时候，你需要根据已有的状态计算出新的值。例如，根据用户的姓和名，得到完整的姓名。你当然可以每次都在模板里拼接字符串，但这样既不优雅，也效率低下。\n更好的方法是使用 computed（计算属性）。\n想象一下，你有一个购物车，里面有几件商品，每件商品都有价格和数量。你希望实时显示总价。总价就是一个“派生状态”，它完全依赖于商品的价格和数量。\n\u003ctemplate\u003e \u003cdiv\u003e \u003cp\u003e商品A: 10元/件, 买了 {{ productA.quantity }} 件\u003c/p\u003e \u003cbutton @click=\"productA.quantity++\"\u003e增加A\u003c/button\u003e \u003cp\u003e商品B: 20元/件, 买了 {{ productB.quantity }} 件\u003c/p\u003e \u003cbutton @click=\"productB.quantity++\"\u003e增加B\u003c/button\u003e \u003chr\u003e \u003ch3\u003e总价：{{ totalPrice }} 元\u003c/h3\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { reactive, computed } from 'vue'; const productA = reactive({ price: 10, quantity: 1 }); const productB = reactive({ price: 20, quantity: 2 }); // 定义一个计算属性 totalPrice const totalPrice = computed(() =\u003e { console.log('正在重新计算总价...'); return productA.price * productA.quantity + productB.price * productB.quantity; }); \u003c/script\u003e computed 带来了两大好处：\n声明式与可读性：你只需声明 totalPrice 是如何由其他数据计算得来的。代码意图清晰明了，totalPrice 就像一个普通的响应式数据一样可以在模板中使用。\n智能缓存：computed 是懒加载且带缓存的。只有当它的依赖（productA.quantity 或 productB.quantity）发生变化时，它才会重新执行计算函数。如果你多次访问 totalPrice 而依赖没有变，它会直接返回上一次缓存的结果，避免了不必要的计算开销。你可以打开浏览器的控制台，点击按钮，看看 “正在重新计算总价…” 是不是只在必要时才打印。\n将 computed 和普通函数做个对比：\n普通函数：每次在模板中调用 {{ someFunction() }}，它都会在每次渲染时重新执行一遍，无论依赖变不变。 计算属性：只有在依赖更新后，第一次访问时才会重新计算，之后返回缓存值。 因此，当你需要从响应式数据派生出新值时，始终优先考虑使用 computed。"},"title":"Vue 核心语法与响应式基础"},"/docs/web/frontend/vue/components/":{"data":{"":"","emit从子到父的事件通知#Emit：从子到父的事件通知":"我们已经解决了“从上到下”的数据流，那如果子组件想和父组件“对话”呢？比如，点击 UserCard 里的一个按钮，想通知父组件发生了某件事。\n这就是 emit 的职责。子组件通过 emit 发出一个自定义事件，就像在说：“嘿，我这里发生了一件事！”，而父组件可以选择监听这个事件并做出响应。\n让我们给 UserCard.vue 添加一个“选择”按钮。\n\u003ctemplate\u003e \u003cdiv class=\"user-card\"\u003e \u003ch3\u003e{{ user.name }}\u003c/h3\u003e \u003cp\u003eEmail: {{ user.email }}\u003c/p\u003e \u003cbutton @click=\"selectUser\"\u003e选择该用户\u003c/button\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { defineProps, defineEmits } from 'vue'; const props = defineProps({ user: { type: Object, required: true } }); // 1. 声明该组件会触发一个名为 'user-selected' 的事件 const emit = defineEmits(['user-selected']); const selectUser = () =\u003e { // 2. 触发事件，并把 user 对象作为参数传递出去 emit('user-selected', props.user); }; \u003c/script\u003e \u003cstyle scoped\u003e /* ... */ \u003c/style\u003e 现在，当按钮被点击时，UserCard 会向外发出一个 user-selected 事件，并附带上当前卡片的 user 数据。\n父组件 App.vue 如何接收这个信号呢？\n\u003ctemplate\u003e \u003cdiv\u003e \u003ch1\u003e我的好友列表\u003c/h1\u003e \u003cp v-if=\"selectedUser\"\u003e当前已选择: {{ selectedUser.name }}\u003c/p\u003e \u003cUserCard :user=\"user1\" @user-selected=\"handleUserSelection\" /\u003e \u003cUserCard :user=\"user2\" @user-selected=\"handleUserSelection\" /\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { reactive, ref } from 'vue'; import UserCard from './components/UserCard.vue'; const user1 = reactive({ id: 1, name: 'Alice', email: 'alice@example.com' }); const user2 = reactive({ id: 2, name: 'Bob', email: 'bob@example.com' }); const selectedUser = ref(null); // 3. 定义一个方法来处理子组件传来的事件 const handleUserSelection = (userFromChild) =\u003e { console.log('子组件传来了数据:', userFromChild); selectedUser.value = userFromChild; }; \u003c/script\u003e 我们在父组件中通过 @user-selected=\"handleUserSelection\" 来监听子组件的自定义事件。当 UserCard emit user-selected 事件时，父组件的 handleUserSelection 方法就会被调用，并且能接收到子组件传递过来的 user 数据。\n这就是 Vue 组件通信的黄金法则：Props down, emits up (数据通过 props 向下传递，事件通过 emits 向上通知)。这个单向数据流的模式让应用的状态变化更容易被理解和追踪。","props从父到子的数据传递#Props：从父到子的数据传递":"props (properties 的缩写) 是父组件向子组件传递数据的“专属通道”。它就像你在调用一个函数时传入的参数。\n我们来改造一下 UserCard.vue，让它能接收一个 user 对象作为 prop。\n\u003ctemplate\u003e \u003cdiv class=\"user-card\"\u003e \u003ch3\u003e{{ user.name }}\u003c/h3\u003e \u003cp\u003eEmail: {{ user.email }}\u003c/p\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { defineProps } from 'vue'; // 1. 使用 defineProps 声明该组件期望接收一个名为 'user' 的 prop const props = defineProps({ user: { type: Object, // 期望的类型是对象 required: true, // 这个 prop 是必需的 }, }); // 2. 在模板中，你可以直接使用 props.user，或者更简单地，直接用 user // Vue 在模板中会自动处理，让你感觉 user 就像本地定义的一样 \u003c/script\u003e \u003cstyle scoped\u003e /* 样式保持不变 */ \u003c/style\u003e 现在，UserCard 组件变成了一个灵活的模板。任何父组件都可以使用它，并传入不同的 user 数据。\n假设我们有一个 App.vue 父组件：\n\u003ctemplate\u003e \u003cdiv\u003e \u003ch1\u003e我的好友列表\u003c/h1\u003e \u003cUserCard :user=\"user1\" /\u003e \u003cUserCard :user=\"user2\" /\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { reactive } from 'vue'; import UserCard from './components/UserCard.vue'; // 1. 导入子组件 const user1 = reactive({ name: 'Alice', email: 'alice@example.com' }); const user2 = reactive({ name: 'Bob', email: 'bob@example.com' }); \u003c/script\u003e 看到 :user=\"user1\" 了吗？\n: 是 v-bind: 的缩写，它告诉 Vue：“我要动态地绑定一个 prop。” user 是子组件 UserCard 中定义的 prop 名称。 \"user1\" 是父组件 App.vue 中要传递的数据源。 这样，我们就实现了父组件向子组件的数据传递，并且成功复用了 UserCard 组件！","定义你的第一个组件#定义你的第一个组件":"一个 Vue 组件就是一个带有 .vue 后缀的文件，它通常由三部分组成：\n：定义组件的 HTML 结构。","组件开发#组件开发":"随着应用变得越来越复杂，把所有代码都塞在一个文件里会变成一场噩梦。Vue 的核心思想就是组件化。它允许你将用户界面拆分成一个个独立、可复用的“积木块”，每个积木块就是一个组件。\n想象一下你在刷朋友圈，整个页面可以被拆分成：一个顶部导航栏组件、一个好友列表组件、一个动态卡片组件、一个底部菜单组件……每个组件都有自己的 HTML、CSS 和 JavaScript，它们各司其职，最后再拼装成一个完整的应用。"},"title":"第二阶段 组件开发"},"/docs/web/frontend/vue/projects/":{"data":{"":"","实战项目#实战项目":"我看看放区块链那个还是TIMEBOX那个合适……"},"title":"实战项目"},"/docs/web/frontend/vue/vue-basics-example/":{"data":{"":"","emit从子到父的事件通知#Emit：从子到父的事件通知":"我们已经解决了“从上到下”的数据流，那如果子组件想和父组件“对话”呢？比如，点击 UserCard 里的一个按钮，想通知父组件发生了某件事。\n这就是 emit 的职责。子组件通过 emit 发出一个自定义事件，就像在说：“嘿，我这里发生了一件事！”，而父组件可以选择监听这个事件并做出响应。\n让我们给 UserCard.vue 添加一个“选择”按钮。\n\u003ctemplate\u003e \u003cdiv class=\"user-card\"\u003e \u003ch3\u003e{{ user.name }}\u003c/h3\u003e \u003cp\u003eEmail: {{ user.email }}\u003c/p\u003e \u003cbutton @click=\"selectUser\"\u003e选择该用户\u003c/button\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { defineProps, defineEmits } from 'vue'; const props = defineProps({ user: { type: Object, required: true } }); // 1. 声明该组件会触发一个名为 'user-selected' 的事件 const emit = defineEmits(['user-selected']); const selectUser = () =\u003e { // 2. 触发事件，并把 user 对象作为参数传递出去 emit('user-selected', props.user); }; \u003c/script\u003e \u003cstyle scoped\u003e /* ... */ \u003c/style\u003e 现在，当按钮被点击时，UserCard 会向外发出一个 user-selected 事件，并附带上当前卡片的 user 数据。\n父组件 App.vue 如何接收这个信号呢？\n\u003ctemplate\u003e \u003cdiv\u003e \u003ch1\u003e我的好友列表\u003c/h1\u003e \u003cp v-if=\"selectedUser\"\u003e当前已选择: {{ selectedUser.name }}\u003c/p\u003e \u003cUserCard :user=\"user1\" @user-selected=\"handleUserSelection\" /\u003e \u003cUserCard :user=\"user2\" @user-selected=\"handleUserSelection\" /\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { reactive, ref } from 'vue'; import UserCard from './components/UserCard.vue'; const user1 = reactive({ id: 1, name: 'Alice', email: 'alice@example.com' }); const user2 = reactive({ id: 2, name: 'Bob', email: 'bob@example.com' }); const selectedUser = ref(null); // 3. 定义一个方法来处理子组件传来的事件 const handleUserSelection = (userFromChild) =\u003e { console.log('子组件传来了数据:', userFromChild); selectedUser.value = userFromChild; }; \u003c/script\u003e 我们在父组件中通过 @user-selected=\"handleUserSelection\" 来监听子组件的自定义事件。当 UserCard emit user-selected 事件时，父组件的 handleUserSelection 方法就会被调用，并且能接收到子组件传递过来的 user 数据。\n这就是 Vue 组件通信的黄金法则：Props down, emits up (数据通过 props 向下传递，事件通过 emits 向上通知)。这个单向数据流的模式让应用的状态变化更容易被理解和追踪。","props从父到子的数据传递#Props：从父到子的数据传递":"props (properties 的缩写) 是父组件向子组件传递数据的“专属通道”。它就像你在调用一个函数时传入的参数。\n我们来改造一下 UserCard.vue，让它能接收一个 user 对象作为 prop。\n\u003ctemplate\u003e \u003cdiv class=\"user-card\"\u003e \u003ch3\u003e{{ user.name }}\u003c/h3\u003e \u003cp\u003eEmail: {{ user.email }}\u003c/p\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { defineProps } from 'vue'; // 1. 使用 defineProps 声明该组件期望接收一个名为 'user' 的 prop const props = defineProps({ user: { type: Object, // 期望的类型是对象 required: true, // 这个 prop 是必需的 }, }); // 2. 在模板中，你可以直接使用 props.user，或者更简单地，直接用 user // Vue 在模板中会自动处理，让你感觉 user 就像本地定义的一样 \u003c/script\u003e \u003cstyle scoped\u003e /* 样式保持不变 */ \u003c/style\u003e 现在，UserCard 组件变成了一个灵活的模板。任何父组件都可以使用它，并传入不同的 user 数据。\n假设我们有一个 App.vue 父组件：\n\u003ctemplate\u003e \u003cdiv\u003e \u003ch1\u003e我的好友列表\u003c/h1\u003e \u003cUserCard :user=\"user1\" /\u003e \u003cUserCard :user=\"user2\" /\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { reactive } from 'vue'; import UserCard from './components/UserCard.vue'; // 1. 导入子组件 const user1 = reactive({ name: 'Alice', email: 'alice@example.com' }); const user2 = reactive({ name: 'Bob', email: 'bob@example.com' }); \u003c/script\u003e 看到 :user=\"user1\" 了吗？\n: 是 v-bind: 的缩写，它告诉 Vue：“我要动态地绑定一个 prop。” user 是子组件 UserCard 中定义的 prop 名称。 \"user1\" 是父组件 App.vue 中要传递的数据源。 这样，我们就实现了父组件向子组件的数据传递，并且成功复用了 UserCard 组件！","定义你的第一个组件#定义你的第一个组件":"一个 Vue 组件就是一个带有 .vue 后缀的文件，它通常由三部分组成：\n：定义组件的 HTML 结构。","深入探索组合式-api-与自定义-hooks#深入探索：组合式 API 与自定义 Hooks":"随着你对 Vue 越来越熟悉，你会遇到更强大的功能来帮助你组织代码，尤其是在大型应用中。让我们来探索其中的两个：组合式 API (Composition API) 和 自定义 Hooks。","组件像搭乐高一样构建应用#组件：像搭乐高一样构建应用":"随着应用变得越来越复杂，把所有代码都塞在一个文件里会变成一场噩梦。Vue 的核心思想就是组件化。它允许你将用户界面拆分成一个个独立、可复用的“积木块”，每个积木块就是一个组件。\n想象一下你在刷朋友圈，整个页面可以被拆分成：一个顶部导航栏组件、一个好友列表组件、一个动态卡片组件、一个底部菜单组件……每个组件都有自己的 HTML、CSS 和 JavaScript，它们各司其职，最后再拼装成一个完整的应用。","组合式-api一种更灵活的代码组织方式#组合式 API：一种更灵活的代码组织方式":"Vue 3 引入了 组合式 API 作为传统 选项式 API (Options API) 的替代方案。虽然选项式 API 以其清晰的结构（data、methods、computed）对初学者非常友好，但组合式 API 提供了更大的灵活性，允许你将相关的逻辑组合在一起。\n选项式 API vs. 组合式 API 在 Vue 2 和早期 Vue 3 中，你通常会用不同的选项块来组织组件：\n选项式 API: data, methods, computed, watch 等。这对于小型组件来说很直观，但当一个逻辑功能被分割到多个选项中时，会变得很麻烦。\n组合式 API: 通过","自定义-hooks像专家一样重用逻辑#自定义 Hooks：像专家一样重用逻辑":"组合式 API 最大的优势之一就是它能够通过 自定义 Hooks 轻松实现逻辑复用。一个 “Hook” (通常在 Vue 生态中称为 “Composable”) 只是一个“钩入”Vue 响应式系统的函数。\n想象一下，你有一个计数器逻辑，想在多个组件中使用。与其每次都重写逻辑，不如将它提取到一个自定义 Hook (Composable) 中。\n让我们创建一个 useCounter.js 文件 (通常放在 src/composables 目录下)：\n// useCounter.js import { ref } from 'vue' export function useCounter(initialValue = 0) { const count = ref(initialValue) const increment = () =\u003e count.value++ const decrement = () =\u003e count.value-- const reset = () =\u003e { count.value = initialValue } // 暴露状态和方法 return { count, increment, decrement, reset } } 现在，任何组件都可以使用这个计数器逻辑：\n\u003ctemplate\u003e \u003cdiv\u003e \u003cp\u003eCount: {{ count }}\u003c/p\u003e \u003cbutton @click=\"increment\"\u003eIncrement\u003c/button\u003e \u003cbutton @click=\"decrement\"\u003eDecrement\u003c/button\u003e \u003cbutton @click=\"reset\"\u003eReset\u003c/button\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import { useCounter } from '../composables/useCounter.js' // 使用自定义 Hook，并传入初始值 10 const { count, increment, decrement, reset } = useCounter(10) \u003c/script\u003e 通过自定义 Hooks (Composables)，你可以构建一个可重用的、响应式的逻辑库，在整个应用中轻松共享，让你的组件保持精简并专注于其表现层。"},"title":"vue-basics-example"},"/docs/web/languages/":{"data":{"":"编程语言相关学习笔记。"},"title":"编程语言"},"/docs/web/network/":{"data":{"":"计算机网络相关学习笔记。"},"title":"计算机网络"},"/docs/web/network/tcp-ip/":{"data":{"arp-address-resolution-protocol#ARP (Address Resolution Protocol)":"工作原理：\n主机广播ARP请求\"谁是IP地址192.168.1.1？\" 目标主机单播回复\"我是，MAC地址是XX:XX:XX:XX:XX:XX\" 请求主机缓存MAC地址","http-hypertext-transfer-protocol#HTTP (HyperText Transfer Protocol)":"","http状态码#HTTP状态码":"1xx - 信息响应\n100 Continue：继续发送请求 2xx - 成功响应\n200 OK：请求成功 201 Created：资源创建成功 204 No Content：无内容返回 3xx - 重定向\n301 Moved Permanently：永久重定向 302 Found：临时重定向 304 Not Modified：资源未修改 4xx - 客户端错误\n400 Bad Request：请求错误 401 Unauthorized：未授权 403 Forbidden：禁止访问 404 Not Found：资源不存在 429 Too Many Requests：请求过于频繁 5xx - 服务端错误\n500 Internal Server Error：服务器内部错误 502 Bad Gateway：网关错误 503 Service Unavailable：服务不可用","http请求方法#HTTP请求方法":"方法 描述 安全性 幂等性 GET 获取资源 是 是 POST 创建资源 否 否 PUT 更新资源 否 是 DELETE 删除资源 否 是 PATCH 部分更新 否 否 HEAD 获取头部 是 是 OPTIONS 预检请求 是 是","ip-internet-protocol#IP (Internet Protocol)":"","ipv4头部结构#IPv4头部结构":"0 1 2 3\r0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r|Version| IHL |Type of Service| Total Length |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Identification |Flags| Fragment Offset |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Time to Live | Protocol | Header Checksum |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Source Address |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Destination Address |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Options | Padding |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+","ip地址分类#IP地址分类":"类别 地址范围 网络位 主机位 用途 A类 0.0.0.0 ~ 127.255.255.255 8 24 大型网络 B类 128.0.0.0 ~ 191.255.255.255 16 16 中型网络 C类 192.0.0.0 ~ 223.255.255.255 24 8 小型网络 D类 224.0.0.0 ~ 239.255.255.255 - - 组播 E类 240.0.0.0 ~ 255.255.255.255 - - 保留","socket编程#Socket编程":"# Python TCP服务器 import socket def tcp_server(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind(('localhost', 8080)) server_socket.listen(5) print(\"服务器启动，监听端口8080...\") while True: client_socket, address = server_socket.accept() print(f\"接受连接：{address}\") data = client_socket.recv(1024) print(f\"接收数据：{data.decode()}\") client_socket.send(\"Hello from server!\".encode()) client_socket.close() # Python TCP客户端 def tcp_client(): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect(('localhost', 8080)) client_socket.send(\"Hello from client!\".encode()) response = client_socket.recv(1024) print(f\"服务器响应：{response.decode()}\") client_socket.close()","tcp-transmission-control-protocol#TCP (Transmission Control Protocol)":"","tcpip分层模型#TCP/IP分层模型":"应用层(Application Layer)\r传输层(Transport Layer)\r网络层(Network Layer)\r链路层(Link Layer)\r物理层(Physical Layer)","tcpip协议#TCP/IP协议":"TCP/IP协议","tcp头部结构#TCP头部结构":"0 1 2 3\r0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Source Port | Destination Port |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Sequence Number |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Acknowledgment Number |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Data | |U|A|P|R|S|F| |\r| Offset| Reserved |R|C|S|S|Y|I| Window |\r| | |G|K|H|T|N|N| |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Checksum | Urgent Pointer |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| Options | Padding |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\r| data |\r+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+","tcp拥塞控制#TCP拥塞控制":"拥塞控制算法：\n慢启动：指数增长拥塞窗口 拥塞避免：线性增长拥塞窗口 快速重传：收到3个重复ACK立即重传 快速恢复：收到重复ACK后进入快速恢复状态 class TCPConnection { constructor() { this.cwnd = 1; // 拥塞窗口 this.ssthresh = 16; // 慢启动阈值 this.state = 'slow_start'; } onPacketLoss() { this.ssthresh = Math.max(this.cwnd / 2, 2); this.cwnd = 1; this.state = 'slow_start'; } onAck() { if (this.state === 'slow_start') { this.cwnd *= 2; if (this.cwnd \u003e= this.ssthresh) { this.state = 'congestion_avoidance'; } } else { this.cwnd += 1; } } }","tcp流量控制#TCP流量控制":"滑动窗口机制：\n发送窗口：已发送但未确认的数据 接收窗口：可以接收的数据量 // 简单滑动窗口实现 class SlidingWindow { constructor(size) { this.size = size; this.window = []; this.sent = 0; this.acked = 0; } canSend() { return this.sent - this.acked \u003c this.size; } send(data) { if (this.canSend()) { this.window.push(data); this.sent++; return true; } return false; } ack(sequence) { while (this.window.length \u003e 0 \u0026\u0026 this.window[0].seq \u003c= sequence) { this.window.shift(); this.acked++; } } }","tcp状态机#TCP状态机":"客户端状态：\nCLOSED → SYN_SENT → ESTABLISHED → FIN_WAIT_1 → FIN_WAIT_2 → TIME_WAIT → CLOSED 服务端状态：\nCLOSED → LISTEN → SYN_RCVD → ESTABLISHED → CLOSE_WAIT → LAST_ACK → CLOSED","tcp连接建立-三次握手#TCP连接建立 (三次握手)":"sequenceDiagram\rparticipant Client\rparticipant Server\rClient-\u003e\u003eServer: SYN (seq=x)\rServer-\u003e\u003eClient: SYN+ACK (seq=y, ack=x+1)\rClient-\u003e\u003eServer: ACK (ack=y+1) 三次握手的目的：\n确认双方的发送和接收能力正常 交换初始序列号 防止旧的重复连接请求","tcp连接释放-四次挥手#TCP连接释放 (四次挥手)":"sequenceDiagram\rparticipant Client\rparticipant Server\rClient-\u003e\u003eServer: FIN\rServer-\u003e\u003eClient: ACK\rServer-\u003e\u003eClient: FIN\rClient-\u003e\u003eServer: ACK","传输层协议#传输层协议":"","子网掩码和cidr#子网掩码和CIDR":"192.168.1.0/24 表示：\r网络地址：192.168.1.0\r子网掩码：255.255.255.0\r可用主机：192.168.1.1 ~ 192.168.1.254\r广播地址：192.168.1.255","常见攻击类型#常见攻击类型":"DDoS攻击：分布式拒绝服务 SYN洪水：利用TCP三次握手漏洞 IP欺骗：伪造源IP地址 中间人攻击：窃听和篡改通信","应用层协议#应用层协议":"","网络安全#网络安全":"","网络层协议#网络层协议":"","网络编程基础#网络编程基础":"","防护措施#防护措施":"防火墙：过滤网络流量 入侵检测系统：监控异常活动 加密通信：使用HTTPS/TLS 访问控制：限制IP和端口访问"},"title":"tcp-ip"},"/docs/web/os/":{"data":{"":"操作系统相关学习笔记。"},"title":"操作系统"},"/posts/first-post/":{"data":{"":"这是你的第一篇文章！\nHugo 是一个快速、灵活的静态网站生成器，而 Hextra 是一个功能强大的现代化主题。","开始使用#开始使用":"你可以开始创建更多内容了！","特性#特性":"快速构建 - Hugo 可以在几秒内生成整个网站 现代化设计 - Hextra 主题提供了美观的界面 响应式布局 - 支持各种设备 深色模式 - 自动适配系统主题"},"title":"第一篇文章"},"/showcase/":{"data":{"":"项目作品展示\r智载千古：AI赋能考古研究基于深度学习的考古报告自动化处理平台，荣获2024年中国国际大学生创新大赛国赛铜奖并发表专利\r悦动体育从B端到C端的全栈综合性的体育健身平台，提供场馆预约、活动赛事举办与约球等功能"},"title":"作品展示"},"/showcase/archaeological-reports/":{"data":{"":"智载千古：AI赋能考古研究的探索与实践在这个数字化浪潮席卷全球的时代，古老的文明遗存正通过前沿技术的触角焕发出全新的生命力。我们的创赛项目智载千古（Intelligent Codex of Treasures）便是在这样的背景下应运而生。本项目致力于通过人工智能技术赋能考古研究，实现从良渚到世界的文明点亮，探索中华文明探源工程的数字化路径。凭借深厚的技术积淀与跨学科的创新应用，项目在 2024 年\"建行杯\"中国国际大学生创新大赛中脱颖而出，最终荣获中国国际大学生创新大赛（2024）国赛铜奖。","传统考古的现实痛点与行业挑战#传统考古的现实痛点与行业挑战":"尽管考古学科在不断发展，但传统考古研究方法在面对海量数据时依然存在明显的瓶颈。在实际的田野调查与报告整理中，考古研究人员常常面临数据零散、质量参差不齐以及分析难度大等痛点。\n首先，考古数据的记录和处理在很大程度上依然依赖人工。无论是器物线图的绘制、拓片的数字化整理，还是各类考古报告的编写，其自动化程度极低。这种低效率的作业模式不仅耗费了大量研究人员的时间，也使得大量珍贵的考古材料长期处于碎片化状态，难以进行大规模的横向对比和深度分析。其次，数据标准的不统一导致跨遗址、跨区域的联合研究极度困难。面对即将展开的第四次全国文物普查，传统的作业方式已难以满足现代考古对精准度和效率的需求。","技术架构与全流程设计#技术架构与全流程设计":"项目的底层工程结构遵循模块化原则设计，以确保海量考古报告处理时的稳定性。整个系统被划分为 PDF 转换模块、OCR 文本识别模块、YOLO 视觉检测模块、以及基于大语言模型的语义提取模块。\n# 项目核心目录结构逻辑 # D:\\PADDLEOCR\\PROJECT # ├─source # 原始考古报告 PDF 存放地 # ├─picture # PDF 转图片后的中间产物 # ├─ocr_result # PaddleOCR 生成的坐标与文本记录 # ├─llm_result # DeepSeek API 处理后的结构化 Excel 表格 # └─utils # 图像预处理与坐标匹配工具库 处理流程的首要环节是将数百页的考古报告（如《良渚遗址群考古报告之四：庙前》）转化为高分辨率的图像。由于 OCR 和 YOLO 对图像像素密度的敏感性，我采用了 PyMuPDF（fitz）库，并设定了 2.0 的水平与垂直缩放因子，以保证在后续处理中即使是微小的器物序号也能清晰可辨。\nimport fitz def pdf2img(pdf_path, img_dir): doc = fitz.open(pdf_path) for page in doc: # 设置缩放因子以提升 OCR 识别精度 zoom_x = 2.0 zoom_y = 2.0 mat = fitz.Matrix(zoom_x, zoom_y) pix = page.get_pixmap(matrix=mat) pix.save(r\"{}page-{}.jpg\".format(img_dir, page.number)) if __name__ == '__main__': pdf2img(\"./source/庙前.pdf\", \"./picture/\") [图片占位符：引用《线图提取.pdf》第 2 页，展示 PDF 转换后的原图与初步分割效果]","文明探源与时代使命#文明探源与时代使命":"考古工作不仅是发现过去，更是为了通过实证来理解历史、传承价值。项目立足于国家文化自信的战略高度，旨在通过 AI 技术还原历史真相，存续文化基因。在研究过程中，我们深刻认识到考古学对于增强民族凝聚力和筑梦中华民族复兴的重要意义。因此，智载千古不仅仅是一个技术工具的集合，它更是一次跨越千年的科技对话，试图向世界展示中华文明的灿烂成就。","智载千古ai赋能考古研究的探索与实践#智载千古：AI赋能考古研究的探索与实践":"","智载千古新一代科技考古一体化平台#智载千古：新一代科技考古一体化平台":"为了应对上述挑战，我们开发了智载千古——下一代科技考古一体化平台。项目集成了 YOLO 目标检测、大型语言模型（LLM）、文字识别（OCR）等多种先进技术，旨在解决传统考古中数据处理慢、分析难的核心问题。\n平台通过智能提取器物特征、自动化分析墓葬数据以及结构化考古报告，大幅提升了研究效率。例如，通过计算机视觉模型，我们可以快速从复杂的考古报告中精准检测并分割出器物线图，并结合版面分析算法实现自动化的图注匹配。这一平台的应用，不仅能够帮助研究人员从繁琐的机械劳动中解脱出来，更能通过数据挖掘发现隐藏在历史细节背后的社会网络关系和文化变迁规律。","极客精神在工程实践中的迭代与反思#极客精神：在工程实践中的迭代与反思":"没有任何一个 AI 模型是完美的。在处理《庙前》和《文家山》等真实考古数据时，我遇到了\"指鹿为马\"（OCR 错误识别数字）、\"眼盲\"（YOLO 漏检细小序号）等典型工程问题。\n针对图像质量导致的识别率低，我回溯到了传统计算机视觉领域。参考学长经验，我构建了一套图像预处理流水线：先进行灰度化与中值滤波去噪，再利用 Otsu 自适应阈值算法进行二值化，最后通过 Canny 边缘检测和 CLAHE 对比度增强来突出文字轮廓。这种\"传统 CV 预处理 + 深度学习识别“的组合拳，显著提升了在模糊扫描件上的表现。\n同时，我完成了从 YOLOv8 到 YOLOv10 的架构迁移。在训练过程中，我针对识别率较低的”玉管“等细长器物增加了训练集比重，并使用 YOLOv10m 和 YOLOv10x 不同的预训练模型进行对比实验。最终，全流程的自动化命名成功率大幅提升，将原本需要数天的人工整理工作缩短到了小时级别。","核心技术实现#核心技术实现":"我主要负责基于深度学习的线图自动化提取与结构化处理。这是一个典型的多模态任务，不仅需要精确的计算机视觉定位，还需要逻辑严密的后处理算法将视觉坐标与文本语义进行精准匹配。为了实现这一目标，我们构建了一套完整的技术流水线，涵盖了从原始 PDF 文档转换到最终器物卡片生成的全部流程。","视觉中枢基于-yolo-的多目标检测与空间匹配#视觉中枢：基于 YOLO 的多目标检测与空间匹配":"这是我投入精力最多的部分。考古报告的版面极其复杂，一页往往包含数十个器物线图，且图注和序号的排布没有统一规律。传统的 OCR 无法理解\"哪个序号对应哪个器物”，我选择通过 YOLOv8（后升级至 YOLOv10）来建立空间索引。\n我定义了五类核心标注对象：线图整体（zhengti）、单个器物（qiwu）、序号框（xuhao）、图注（tuzhu）以及墓葬元素（muzang）。其中，\"整体框“的设计是解决版面分析难题的关键。通过先定位”图注单元\"，再在该单元内寻找器物与序号，有效避免了跨区域的命名污染。\n在后处理脚本中，我实现了一套基于 IOU（交并比）的空间归属算法。系统会计算每一个\"序号框\"与\"器物框\"的重合度，当 IOU 超过 0.5 时，系统自动建立二者的逻辑链接。这一逻辑不仅解决了\"谁是谁\"的问题，更为后续的自动化命名打下了基础。\n# 部署 YOLO 模型进行全自动预测 yolo predict model=weights/best.pt source=./picture save_txt=True hide_labels=False","语义大脑大模型驱动的结构化卡片生成#语义大脑：大模型驱动的结构化卡片生成":"在视觉信息提取完毕后，如何将 OCR 识别出的零散文字转化为专业的考古器物卡片是另一个难点。考古报告中的文字描述极其密集且具有高度的专业性，简单的正则匹配无法处理复杂的语义关系。\n通过分段处理文本块（Chunks），系统能够自动提取遗迹号、尺寸、颜色、器型细分类、材质以及完整程度等十余个表头字段。针对 OCR 产生的冗余信息（如页眉、页码），我设计了基于位置坐标的过滤机制，大幅提升了 LLM 的处理效率和准确率。","跨学科深度协作与名师引领#跨学科深度协作与名师引领":"本项目的诞生离不开顶尖科研力量的支持。在良渚古城的发现者刘斌老师的引领下，项目团队深入良渚遗址一线，推动考古研究从简单的遗迹发现走向深度的价值挖掘。\n同时，项目依托浙江大学计算机辅助设计与图形学（CAD\u0026CG）国家重点实验室，以及艺术与考古图像数据实验室的强大平台。这种\"艺术+科技“的跨学科协作模式，为解决考古中的实际难题提供了坚实的技术保障。通过将潘云鹤院士等顶尖学者的学术成果应用于文化遗产保护，我们成功打破了学科壁垒，开启了 AI+考古的新范式。"},"title":"archaeological-reports"},"/showcase/yuedong-sports/":{"data":{"主要功能#主要功能":"运动记录: 记录用户的运动数据和轨迹 健身指导: 提供个性化的健身计划和指导 社区互动: 用户之间的社交和分享功能 数据统计: 运动数据的分析和可视化 课程管理: 在线健身课程和直播","悦动体育项目#悦动体育项目":"悦动体育项目","技术亮点#技术亮点":"使用 Uniapp 实现跨平台开发 实现了实时运动数据同步 开发了智能推荐算法 集成了第三方支付和地图服务","技术栈#技术栈":"前端: Vue.js, TypeScript, Element UI 后端: SpringBoot, Java 数据库: MySQL, Redis 移动端: Uniapp 部署: Docker, Nginx","项目成果#项目成果":"还未上线","项目概述#项目概述":"悦动体育是一个综合性的体育健身平台，为用户提供运动记录、健身指导、社区互动等功能。","项目特色#项目特色":"全平台支持: Web端、移动端、小程序全覆盖 数据驱动: 基于用户数据的个性化推荐 社交属性: 强化的社区互动功能 专业指导: 与专业教练合作的内容"},"title":"yuedong-sports"}}