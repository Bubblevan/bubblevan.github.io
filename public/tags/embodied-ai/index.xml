<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Embodied-Ai</title>
    <link>http://localhost:1313/tags/embodied-ai/</link>
    <description>Recent content in Embodied-Ai on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/embodied-ai/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>VLN 正交分析法寻找创新点</title>
      <link>http://localhost:1313/blog/2025/2025-11-23-vln-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-23-vln-matrix/</guid>
      <description>
        
        
        &lt;h1&gt;VLN 正交分析法寻找创新点&lt;/h1&gt;&lt;h2&gt;框架一：【表征-推理】矩阵 (Representation-Reasoning Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架一表征-推理矩阵-representation-reasoning-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%b8%80%e8%a1%a8%e5%be%81-%e6%8e%a8%e7%90%86%e7%9f%a9%e9%98%b5-representation-reasoning-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：解决&amp;quot;机器人怎么看世界&amp;quot;和&amp;quot;机器人怎么做决策&amp;quot;的匹配问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：推理范式 (Reasoning) \ &lt;strong&gt;横轴&lt;/strong&gt;：环境表征 (Representation)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;推理范式 \ 环境表征&lt;/th&gt;
          &lt;th&gt;A. 纯视觉流&lt;/th&gt;
          &lt;th&gt;B. 2D 语义地图&lt;/th&gt;
          &lt;th&gt;C. 3D 场景图&lt;/th&gt;
          &lt;th&gt;D. 拓扑/文本图&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. End-to-End RL / IL&lt;/td&gt;
          &lt;td&gt;已拥挤&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;较少&lt;/td&gt;
          &lt;td&gt;较少&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Modular + LLM Prompting&lt;/td&gt;
          &lt;td&gt;难点&lt;/td&gt;
          &lt;td&gt;拥挤&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. System 1 + System 2&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. World Model / Generative&lt;/td&gt;
          &lt;td&gt;前沿&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 1 (A-3)&lt;/strong&gt;: 目前 End-to-End 模型（如 NaVid）反应快但缺乏长程逻辑，而 LLM 反应慢。能否设计一个机制，平时用小模型看视频流走路（System 1），遇到&amp;quot;迷路&amp;quot;或&amp;quot;歧义&amp;quot;时，动态唤醒 LLM 分析当前视频帧（System 2）？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 2 (C-4)&lt;/strong&gt;: 现在的 Scene Graph 都是用来做当前状态的 Prompt。能否基于 Scene Graph 做&amp;quot;世界模型&amp;quot;？ 即：让 LLM 预测&amp;quot;如果我向左走，场景图会变成什么样？&amp;quot;，从而在图空间里做 Model-Based Planning，而不是由 LLM 直接瞎猜。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;框架二：【反馈-修正】矩阵 (Feedback-Correction Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架二反馈-修正矩阵-feedback-correction-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%ba%8c%e5%8f%8d%e9%a6%88-%e4%bf%ae%e6%ad%a3%e7%9f%a9%e9%98%b5-feedback-correction-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：针对 2024 年后的趋势——从&amp;quot;如何走对&amp;quot;转向&amp;quot;走错了如何修正&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：修正机制 (Correction) \ &lt;strong&gt;横轴&lt;/strong&gt;：错误源 (Source of Error)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;修正机制 \ 错误源&lt;/th&gt;
          &lt;th&gt;A. 感知幻觉&lt;/th&gt;
          &lt;th&gt;B. 空间迷失&lt;/th&gt;
          &lt;th&gt;C. 指令歧义&lt;/th&gt;
          &lt;th&gt;D. 动态障碍/变化&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. Passive (被动重规划)&lt;/td&gt;
          &lt;td&gt;传统方法&lt;/td&gt;
          &lt;td&gt;传统方法&lt;/td&gt;
          &lt;td&gt;无解&lt;/td&gt;
          &lt;td&gt;传统 DWA/TEB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Active Perception (主动探索)&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. Dialogue / Interaction&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
          &lt;td&gt;已拥挤&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. Self-Reflexion (自省)&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 3 (B-4)&lt;/strong&gt;: 现在的 Self-Reflexion 大多是在想&amp;quot;我是不是理解错指令了&amp;quot;。很少有工作做&amp;quot;空间自省&amp;quot;——即 LLM 结合历史轨迹图，反思&amp;quot;我现在的视觉观测和我记忆中的地图不一致，我是不是已经走到错误的房间了？&amp;quot;（Spatial Consistency Check via LLM）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 4 (A-3)&lt;/strong&gt;: 当 VLM 觉得前面是&amp;quot;椅子&amp;quot;但又不确定时（Confidence score 低），目前的做法是硬着头皮走。创新点可以是：主动发起一轮对话确认，或者主动移动相机去验证（Active Perception for VLM uncertainty）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;框架三：【多模态融合-时空】矩阵 (Fusion-Spatiotemporal Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架三多模态融合-时空矩阵-fusion-spatiotemporal-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%b8%89%e5%a4%9a%e6%a8%a1%e6%80%81%e8%9e%8d%e5%90%88-%e6%97%b6%e7%a9%ba%e7%9f%a9%e9%98%b5-fusion-spatiotemporal-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：针对 CoRL/ICRA 等机器人会议，关注&amp;quot;具体怎么融合特征&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：融合阶段 (Fusion Stage) \ &lt;strong&gt;横轴&lt;/strong&gt;：时间维处理 (Temporal)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;融合阶段 \ 时间维处理&lt;/th&gt;
          &lt;th&gt;A. Frame-wise (单帧)&lt;/th&gt;
          &lt;th&gt;B. Feature Buffer&lt;/th&gt;
          &lt;th&gt;C. Explicit Map&lt;/th&gt;
          &lt;th&gt;D. Neural Memory&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. Early Fusion&lt;/td&gt;
          &lt;td&gt;基础&lt;/td&gt;
          &lt;td&gt;计算量大&lt;/td&gt;
          &lt;td&gt;VLMaps&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Late Fusion&lt;/td&gt;
          &lt;td&gt;CLIP-Nav&lt;/td&gt;
          &lt;td&gt;NaVid&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;IVLN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. LLM-in-the-loop&lt;/td&gt;
          &lt;td&gt;GPT-4V Nav&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;UniGoal&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. Cross-Attention (Query-based)&lt;/td&gt;
          &lt;td&gt;传统 Transformer&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gap 5 (D-3)&lt;/strong&gt;: 目前的 LLM 导航要么看单张图，要么看 3D 场景图。很少有结合 Mamba 或 SSM (State Space Models) 的工作。 创新点：利用 Mamba 这种长序列处理能力极强的架构，作为 VLN 的&amp;quot;隐式记忆体&amp;quot;，替代显式的地图构建，实现无图但有长记忆的导航（Mamba-VLN）。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>VLN 综述以及后续文献</title>
      <link>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</guid>
      <description>
        
        
        &lt;h1&gt;VLN 系列&lt;/h1&gt;&lt;p&gt;从 Poing Navigation 到 Object Navigation，这也太难了，找Idea真的太难了。&lt;/p&gt;
&lt;p&gt;然后秋冬学期的一半，也就是大四上的一半已经过去了，马上就要寒假了，寒假做什么，实习还是论文？真能憋出论文吗？&lt;/p&gt;
&lt;h2&gt;Survey&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;survey&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#survey&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;研发能够与人类及其周边环境进行交互的&lt;strong&gt;具身智能体&lt;/strong&gt;（embodied agents），是&lt;strong&gt;人工智能&lt;/strong&gt;（AI）领域长期以来的核心目标之一。这类 AI 系统在现实世界中具有巨大的应用潜力，可作为日常生活中的多功能助手，例如&lt;strong&gt;家用机器人&lt;/strong&gt;、&lt;strong&gt;自动驾驶汽车&lt;/strong&gt;以及&lt;strong&gt;个人助手&lt;/strong&gt;。推动这一研究方向的一个正式问题设定是&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（Vision-and-Language Navigation, &lt;strong&gt;VLN&lt;/strong&gt;）—— 这是一项多模态协作任务，要求智能体遵循人类指令、探索&lt;strong&gt;三维&lt;/strong&gt;（3D）环境，并在存在各类歧义的场景下开展情境化通信。多年来，研究者已在&lt;strong&gt;照片级真实感模拟器&lt;/strong&gt;和&lt;strong&gt;真实环境&lt;/strong&gt;中对 VLN 展开探索，由此形成了一系列&lt;strong&gt;基准数据集&lt;/strong&gt;，每个数据集的问题表述略有不同。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/law-Challenge.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类&lt;/strong&gt;（Human）：给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;rdquo;；在智能体询问 &amp;ldquo;左边的房间还是前面的房间？&amp;rdquo; 时回复 &amp;ldquo;左边&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;物理环境&lt;/strong&gt;（Physical Environment）：智能体感知的视觉场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体&lt;/strong&gt;（VLN Agent）：接收指令后进行 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;（Grounding &amp;amp; Reasoning）、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;（Planning）、&lt;strong&gt;&amp;ldquo;对话&amp;rdquo;&lt;/strong&gt;（Dialogue），执行 &lt;strong&gt;&amp;ldquo;导航动作&amp;rdquo;&lt;/strong&gt;（Navigation Execution），并生成&lt;strong&gt;语言响应&lt;/strong&gt;（Language Response）；过程中可能产生疑问，如 &amp;ldquo;…… 进入房间。左边？右边？&amp;ldquo;&amp;ldquo;左边的房间还是前面的房间？&amp;quot;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心模块&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;（World Model）、&lt;strong&gt;人类模型&lt;/strong&gt;（Human Model），分别支撑智能体的环境理解与人类意图解读。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其&lt;a href=&#34;https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;仓库&lt;/a&gt;提到了一些工作内容，但是不全。&lt;/p&gt;
&lt;h3&gt;背景与任务基础&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;背景与任务基础&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%83%8c%e6%99%af%e4%b8%8e%e4%bb%bb%e5%8a%a1%e5%9f%ba%e7%a1%80&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;人类及其他具备导航能力的动物，很早就展现出对环境导航的理解与策略。例如，加利斯特尔（Gallistel）提出了两种基础机制：其一为&lt;strong&gt;引导法&lt;/strong&gt;（piloting），即利用环境地标计算距离与角度；其二为&lt;strong&gt;路径积分&lt;/strong&gt;（path integration），即通过自运动感知计算位移与方向变化。理解空间导航的核心是&lt;strong&gt;认知地图假说&lt;/strong&gt;（cognitive map hypothesis）—— 该假说认为，大脑会形成统一的空间表征，以支持记忆存储并指导导航行为。例如，托尔曼（Tolman）观察到，当熟悉的路径被阻断且地标消失时，大鼠仍能选择正确的新路径。神经科学家还发现了&lt;strong&gt;海马体位置细胞&lt;/strong&gt;（hippocampal place cells），这表明存在一种以&lt;strong&gt;异中心视角&lt;/strong&gt;（allocentrically）编码地标与目标的空间坐标系。&lt;/p&gt;
&lt;p&gt;传统上，&lt;strong&gt;&amp;ldquo;遵循自然语言导航指令&amp;rdquo;&lt;strong&gt;的任务多采用地图等&lt;/strong&gt;符号化世界表征&lt;/strong&gt;（symbolic world representations）进行建模。然而，本综述聚焦于采用视觉环境的模型，重点探讨&lt;strong&gt;多模态理解与接地&lt;/strong&gt;（grounding）的相关挑战。与此相对，关于&lt;strong&gt;视觉导航&lt;/strong&gt;和&lt;strong&gt;移动机器人导航&lt;/strong&gt;的综述文献已十分丰富，这类综述主要关注视觉感知与物理具身性，但若涉及 &lt;strong&gt;&amp;ldquo;语言在导航任务中的作用&amp;rdquo;&lt;/strong&gt;，则讨论较为简略，建议读者参考这些文献以获取相关背景。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;接地（Grounding）指将抽象的语言符号与具体的物理世界或感知数据建立对应关系的过程。在 VLN 中，接地是将自然语言指令映射到视觉场景中的具体位置、物体或动作。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;尽管在讨论 VLN 时，我们难免会将范围拓展到导航之外的领域（如移动操作、对话），但本综述的核心焦点仍是&lt;strong&gt;导航任务&lt;/strong&gt;，并将针对该任务提供详细的文献梳理。此外，以往的 VLN 综述多采用 &lt;strong&gt;&amp;ldquo;自下而上&amp;rdquo;&lt;/strong&gt; 的总结方式，聚焦于基准数据集与建模创新；而本综述则采用 &lt;strong&gt;&amp;ldquo;自上而下&amp;rdquo;&lt;/strong&gt; 的视角，并以&lt;strong&gt;基础模型&lt;/strong&gt;的角色为核心，将现有研究成果从 &lt;strong&gt;&amp;ldquo;世界模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;VLN 智能体&amp;rdquo;&lt;/strong&gt; 三个维度，归类为&lt;strong&gt;三大核心挑战&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;典型的 &lt;strong&gt;VLN 智能体&lt;/strong&gt;会在指定位置接收人类指令者给出的（一系列）&lt;strong&gt;语言指令&lt;/strong&gt;。该智能体以&lt;strong&gt;自我为中心的视觉视角&lt;/strong&gt;（egocentric visual perspective）在环境中导航，其核心任务是遵循指令生成&lt;strong&gt;轨迹&lt;/strong&gt; —— 轨迹可基于一系列离散视角，也可基于低层级动作与控制指令（例如 &amp;ldquo;前进 0.25 米&amp;rdquo;），最终抵达&lt;strong&gt;目标终点&lt;/strong&gt;。若智能体最终位置与目标终点的距离在指定范围内（例如 3 米），则判定为&lt;strong&gt;导航成功&lt;/strong&gt;。此外，智能体在导航过程中可与指令者交互：既可以请求帮助，也可进行自由形式的语言沟通。近年来，研究者对 VLN 智能体的期望进一步提升，要求其在导航的同时整合附加任务，例如&lt;strong&gt;操作任务&lt;/strong&gt;与&lt;strong&gt;目标检测任务&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-benchmark-2024.png&#34; alt=&#34;vln-benchmark-2024&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如上表，现有（2024）VLN &lt;strong&gt;基准数据集&lt;/strong&gt;可分为以下四类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航发生的 &amp;ldquo;世界&amp;rdquo;&lt;/strong&gt;：包括领域（室内或室外）与具体环境（如模拟器或真实场景）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类交互类型&lt;/strong&gt;：包括交互轮次（单轮或多轮）、通信格式（自由对话、受限对话或多指令）、语言粒度（动作导向或目标导向）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体属性&lt;/strong&gt;：包括智能体类型（如家用机器人、自动驾驶车辆、自主无人机）、动作空间（图基、离散或连续）、附加任务（操作与目标检测）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集收集方式&lt;/strong&gt;：包括文本收集（人类生成或模板生成）与路线演示（人类执行或规划器生成）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;研究者主要采用三类指标评估 VLN 智能体的&lt;strong&gt;导航寻路性能&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航误差&lt;/strong&gt;（Navigation Error, &lt;strong&gt;NE&lt;/strong&gt;）：智能体最终位置与目标终点之间最短路径距离的平均值&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成功率&lt;/strong&gt;（Success Rate, &lt;strong&gt;SR&lt;/strong&gt;）：最终位置足够接近目标终点的任务占比&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;路径长度加权成功率&lt;/strong&gt;（Success Rate Weighted Path Length, &lt;strong&gt;SPL&lt;/strong&gt;）：通过轨迹长度对成功率进行归一化，平衡 &lt;strong&gt;&amp;ldquo;抵达正确终点的成功率&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;路径效率&amp;rdquo;&lt;/strong&gt; 两大指标&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，还有一类指标用于衡量 &lt;strong&gt;&amp;ldquo;指令遵循的忠实度&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;预测轨迹和真实轨迹的一致性&amp;rdquo;&lt;/strong&gt;，例如：
4. &lt;strong&gt;长度加权覆盖得分&lt;/strong&gt;（Coverage Weighted by Length Score, &lt;strong&gt;CLS&lt;/strong&gt;）：衡量智能体轨迹与参考路径的贴合程度，通过 &lt;strong&gt;&amp;ldquo;参考路径覆盖范围&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;轨迹长度效率&amp;rdquo;&lt;/strong&gt; 两个维度平衡智能体性能
5. &lt;strong&gt;归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping, &lt;strong&gt;nDTW&lt;/strong&gt;）：对偏离真实轨迹的行为进行惩罚
6. &lt;strong&gt;成功率加权归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping Weighted by Success Rate, &lt;strong&gt;sDTW&lt;/strong&gt;）：在惩罚轨迹偏离的同时，还会结合导航成功率综合评估&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-challenges-and-soluions.png&#34; alt=&#34;vln-challenges-and-soluions&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;该图反映的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心模块关联&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;历史与记忆&amp;rdquo;&lt;/strong&gt;，&lt;strong&gt;人类模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;模糊指令&amp;rdquo;&lt;/strong&gt;，两者均涉及 &lt;strong&gt;&amp;ldquo;泛化能力&amp;rdquo;&lt;/strong&gt;；&lt;strong&gt;VLN 智能体&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;基础模型适配为智能体&amp;rdquo;&lt;/strong&gt; 三大方法&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基础模型角色&lt;/strong&gt;：根据基础模型承担的功能，将方法分为四类 —— &lt;strong&gt;数据与知识处理&lt;/strong&gt;（预处理 / 增强 / 合成数据、利用预训练常识）、&lt;strong&gt;表征学习&lt;/strong&gt;（通用文本 / 视觉表征、历史记忆处理）、&lt;strong&gt;决策制定&lt;/strong&gt;（导航规划器、信息寻求对话管理器、通用决策智能体）、&lt;strong&gt;任务学习&lt;/strong&gt;（具身推理、语言接地、少样本 / 上下文 / 微调学习具身任务）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;交互示例&lt;/strong&gt;：人类给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;quot;&amp;ldquo;去卫生间&amp;rdquo;；智能体通过提问（&amp;ldquo;走廊在哪里？&amp;ldquo;&amp;ldquo;哪个房间？&amp;quot;）寻求信息，人类回复（&amp;ldquo;左边的房间还是前面的房间？&amp;ldquo;&amp;ldquo;左边&amp;rdquo;）后，智能体执行动作（&amp;ldquo;前进&amp;quot;&amp;ldquo;左转&amp;rdquo;）并生成轨迹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战与未来方向&lt;/strong&gt;：&lt;strong&gt;基准数据集&lt;/strong&gt;（数据与任务局限）、&lt;strong&gt;世界模型&lt;/strong&gt;（从 2D 世界到 3D 世界）、&lt;strong&gt;人类模型&lt;/strong&gt;（从指令到对话）、&lt;strong&gt;智能体模型&lt;/strong&gt;（LLM 与 VLM 适配）、&lt;strong&gt;部署&lt;/strong&gt;（从仿真到真实机器人）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;三大解决方案&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三大解决方案&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e5%a4%a7%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;World Model: Learning and Representing the Visual Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-learning-and-representing-the-visual-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-learning-and-representing-the-visual-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;能够帮助 VLN 智能体理解周边环境、预测自身动作对世界状态的改变，并使自身感知与动作与语言指令对齐。现有研究中，学习世界模型主要面临两大挑战：一是将当前任务段内的&lt;strong&gt;视觉观测历史&lt;/strong&gt;编码为&lt;strong&gt;记忆&lt;/strong&gt;，二是实现对未见过环境的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;History and Memory&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;history-and-memory&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#history-and-memory&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;与&lt;strong&gt;视觉问答&lt;/strong&gt;（Visual Question Answering, &lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;视觉蕴含&lt;/strong&gt;（Visual Entailment）等其他视觉-语言任务不同，VLN 智能体需将过去动作与观测的&lt;strong&gt;历史信息&lt;/strong&gt;融入当前步骤的输入中以决策动作，而非仅依赖单一步骤的图像与文本。在 VLN 中应用基础模型之前，研究者通常采用 &lt;strong&gt;LSTM 隐藏状态&lt;/strong&gt;作为支持智能体导航决策的隐式记忆，并进一步设计不同的&lt;strong&gt;注意力机制&lt;/strong&gt;或&lt;strong&gt;辅助任务&lt;/strong&gt;，以提升编码历史与指令的对齐程度。&lt;/p&gt;
&lt;p&gt;目前已有多种基于基础模型的&lt;strong&gt;导航历史编码技术&lt;/strong&gt;，核心可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）基于令牌更新或序列建模的编码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer 初始化&lt;/strong&gt;：以基于域内指令-轨迹数据预训练的模型（如 &lt;strong&gt;Prevalent&lt;/strong&gt;）为基础，构建&lt;strong&gt;多模态 Transformer&lt;/strong&gt;，将编码后的指令与导航历史作为输入以实现决策。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;循环状态令牌编码&lt;/strong&gt;：部分方法通过循环更新的&lt;strong&gt;状态令牌&lt;/strong&gt;编码导航历史。例如，利用上一步的单个 &lt;strong&gt;[CLS] 令牌&lt;/strong&gt;编码历史信息；或设计&lt;strong&gt;变长记忆框架&lt;/strong&gt;，将过去步骤的多个动作激活值存储在记忆库中，作为历史编码。但这类方法需逐步骤更新令牌，难以高效检索导航轨迹中任意步骤的历史编码，限制了预训练的可扩展性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;全景与历史分层编码&lt;/strong&gt;：另一类方法直接通过多模态 Transformer 将导航历史编码为序列。例如，对轨迹中每一步的单视角图像进行编码；或进一步提出 &lt;strong&gt;&amp;ldquo;全景编码器 + 历史编码器&amp;rdquo;&lt;/strong&gt; 的分层设计 —— &lt;strong&gt;全景编码器&lt;/strong&gt;处理每一时间步的全景视觉观测，&lt;strong&gt;历史编码器&lt;/strong&gt;则编码所有过往观测。这种设计可分离全景视图中的空间关系与导航历史中跨全景的时间动态性，且无需依赖循环更新的状态令牌，便于基于指令-路径对进行高效、大规模的预训练。后续研究分别用 &lt;strong&gt;&amp;ldquo;图像均值池化&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;前视图像编码&amp;rdquo;&lt;/strong&gt; 替代全景编码器，均保持了良好的导航性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）基于 LLM 的文本化历史编码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随着基于 &lt;strong&gt;LLM&lt;/strong&gt; 的导航智能体兴起，&lt;strong&gt;&amp;ldquo;将视觉环境转换为文本描述&amp;rdquo;&lt;/strong&gt; 成为主流趋势。此时导航历史被编码为 &lt;strong&gt;&amp;ldquo;图像描述序列 + 相对空间信息&amp;rdquo;&lt;/strong&gt;（如朝向、高度、距离）的组合。例如，&lt;strong&gt;HELPER&lt;/strong&gt; 设计了 &lt;strong&gt;&amp;ldquo;语言-程序对&amp;rdquo;&lt;/strong&gt; 的外部记忆，通过检索增强的 LLM 提示，将人类与机器人的自由形式对话解析为动作程序。&lt;/p&gt;
&lt;p&gt;另一类研究通过融入&lt;strong&gt;图信息&lt;/strong&gt;增强导航历史建模，核心思路是利用&lt;strong&gt;结构化图表征&lt;/strong&gt;环境几何与空间关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拓扑图与结构化编码&lt;/strong&gt;：部分方法采用&lt;strong&gt;结构化 Transformer 编码器&lt;/strong&gt;捕捉环境中的几何线索。除编码中使用的&lt;strong&gt;拓扑图&lt;/strong&gt;外，许多研究还将&lt;strong&gt;俯视图信息&lt;/strong&gt;（如&lt;strong&gt;网格图&lt;/strong&gt;、&lt;strong&gt;语义图&lt;/strong&gt;、&lt;strong&gt;局部度量图&lt;/strong&gt;）与&lt;strong&gt;局部邻域图&lt;/strong&gt;纳入导航过程中的观测历史建模。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM 与图的结合&lt;/strong&gt;：近期基于 LLM 的导航智能体在记忆构建中引入了创新性的图应用。例如，提出一种基于&lt;strong&gt;地图引导的 GPT 智能体&lt;/strong&gt;，利用语言化形式的地图存储和管理拓扑图信息；&lt;strong&gt;MC-GPT&lt;/strong&gt; 则将拓扑图作为记忆结构，记录视角、物体及其空间关系的信息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;跨环境泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;跨环境泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%a8%e7%8e%af%e5%a2%83%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;VLN 的核心挑战之一是：如何从有限的可用环境中学习，并泛化到新的、未见过的环境中。现有研究表明，以下方法可提升智能体对未见过环境的泛化性能：&lt;strong&gt;学习语义分割特征&lt;/strong&gt;、&lt;strong&gt;利用训练过程中环境的 dropout 信息&lt;/strong&gt;、&lt;strong&gt;最大化不同环境中语义对齐图像对的相似度&lt;/strong&gt;。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;方法&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.1 预训练视觉表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;传统视觉编码器&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究采用在 ImageNet 上预训练的 &lt;strong&gt;ResNet&lt;/strong&gt; 提取视觉表征&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基于 VL 基础模型的表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;用 &lt;strong&gt;CLIP 视觉编码器&lt;/strong&gt;替代 ResNet——CLIP 通过图文对的对比损失预训练，可自然实现图像与指令的更好对齐，显著提升 VLN 性能&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;视频预训练表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;探索将从视频数据中学习的视觉表征迁移到 VLN 任务中，证实视频中的&lt;strong&gt;时间信息&lt;/strong&gt;对导航至关重要&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.2 环境增强&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;静态环境修改&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;EnvEdit&lt;/strong&gt;、&lt;strong&gt;EnvMix&lt;/strong&gt;、&lt;strong&gt;KED&lt;/strong&gt; 与 &lt;strong&gt;FDA&lt;/strong&gt; 通过修改 Matterport3D 中的现有环境生成合成数据，具体手段包括混合不同环境的房间、改变环境外观与风格、对环境高频特征进行插值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;动态环境合成&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;Pathdreamer&lt;/strong&gt; 与 &lt;strong&gt;SE3DS&lt;/strong&gt; 进一步实现 &lt;strong&gt;&amp;ldquo;基于当前观测合成未来步骤环境&amp;rdquo;&lt;/strong&gt;，并探索将合成视图作为 VLN 训练的增强数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.3 学习范式的转变&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;前基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究直接用自动收集的新环境增强训练环境，并微调基于 LSTM 的 VLN 智能体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;预训练&lt;/strong&gt;被证实对基础模型至关重要，因此 &lt;strong&gt;&amp;ldquo;在预训练阶段从收集的环境中学习&amp;rdquo;&lt;/strong&gt; 成为 VLN 的标准做法。基于增强域内数据的&lt;strong&gt;大规模预训练&lt;/strong&gt;，已成为缩小智能体与人类性能差距的关键；且域内预训练的多模态 Transformer，被证实比从 VLMs（如 &lt;strong&gt;Oscar&lt;/strong&gt;、&lt;strong&gt;LXMERT&lt;/strong&gt;）初始化的多模态 Transformer 更有效&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Human Model: Interpreting and Communicating with Humans&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;human-model-interpreting-and-communicating-with-humans&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#human-model-interpreting-and-communicating-with-humans&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;除学习和建模世界外，VLN 智能体还需一个 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; —— 该模型能根据具体场景理解人类提供的自然语言指令，从而完成导航任务。这一过程主要面临两大挑战：一是解决&lt;strong&gt;指令的模糊性&lt;/strong&gt;，二是实现 &lt;strong&gt;&amp;ldquo;接地指令&amp;rdquo;&lt;/strong&gt; 在不同视觉环境中的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;模糊指令&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;模糊指令&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a8%a1%e7%b3%8a%e6%8c%87%e4%bb%a4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;模糊指令&lt;/strong&gt;主要出现在&lt;strong&gt;单轮导航场景&lt;/strong&gt;中：智能体仅遵循初始指令执行任务，无法通过进一步人类交互获取澄清。这类指令缺乏灵活性，难以训练智能体根据动态环境调整自身的&lt;strong&gt;语言理解&lt;/strong&gt;与&lt;strong&gt;视觉感知能力&lt;/strong&gt;。例如，指令中可能包含 &lt;strong&gt;&amp;ldquo;当前视角不可见的地标&amp;rdquo;&lt;/strong&gt;，或 &lt;strong&gt;&amp;ldquo;从多个视角观察均难以区分的地标&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在基础模型应用于 VLN 之前，模糊指令问题几乎未得到有效解决。尽管 &lt;strong&gt;LEO 模型&lt;/strong&gt;尝试通过整合 &lt;strong&gt;&amp;ldquo;从不同视角描述同一轨迹的多条指令&amp;rdquo;&lt;/strong&gt; 来缓解该问题，但仍依赖人工标注的指令。而基础模型所具备的 &lt;strong&gt;&amp;ldquo;全面感知上下文&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;常识知识&amp;rdquo;&lt;/strong&gt;，使智能体既能利用外部知识解读模糊指令，也能向其他 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; 寻求协助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CLIP&lt;/strong&gt; 等大规模跨模态预训练模型具备&lt;strong&gt;视觉语义与文本的匹配能力&lt;/strong&gt;，这使得 VLN 智能体可利用 &lt;strong&gt;&amp;ldquo;当前感知到的视觉物体及其状态&amp;rdquo;&lt;/strong&gt; 来解决指令模糊性问题，在单轮导航场景中尤为有效。具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Trans 模型&lt;/strong&gt;：通过 CLIP 提取 &lt;strong&gt;&amp;ldquo;可见且具有辨识度的物体&amp;rdquo;&lt;/strong&gt;，构建易于遵循的子指令；并预训练一个 &lt;strong&gt;&amp;ldquo;转换器&amp;rdquo;&lt;/strong&gt;（Translator），将原始模糊指令转换为易于理解的子指令表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LANA+ 模型&lt;/strong&gt;：利用 CLIP，以视觉全景观测为输入，查询 &lt;strong&gt;&amp;ldquo;地标语义标签文本列表&amp;rdquo;&lt;/strong&gt;，并选取排名靠前的检索文本线索作为 &lt;strong&gt;&amp;ldquo;待跟随显著地标的表征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;KERM 模型&lt;/strong&gt;：提出一种 &lt;strong&gt;&amp;ldquo;知识增强推理模型&amp;rdquo;&lt;/strong&gt;，可检索 &lt;strong&gt;&amp;ldquo;以语言描述形式存储的导航视角相关知识事实&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavHint 方法&lt;/strong&gt;：构建一个提示数据集，提供详细的视觉描述，帮助 VLN 智能体全面理解视觉环境，而非仅聚焦于指令中提及的物体。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另一方面，&lt;strong&gt;LLM&lt;/strong&gt; 的&lt;strong&gt;常识推理能力&lt;/strong&gt;可用于 &lt;strong&gt;&amp;ldquo;澄清或修正指令中的模糊地标&amp;rdquo;&lt;/strong&gt;，并将指令拆解为可执行步骤。例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 LLM 提供 &lt;strong&gt;&amp;ldquo;开放世界中地标共现的常识&amp;rdquo;&lt;/strong&gt;，并结合 CLIP 实现地标探测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayCan 方法&lt;/strong&gt;：将指令拆解为 &lt;strong&gt;&amp;ldquo;预定义可执行动作的排序列表&amp;rdquo;&lt;/strong&gt;，并结合一个 &lt;strong&gt;&amp;ldquo;效用函数&amp;rdquo;&lt;/strong&gt; —— 该函数对当前场景中出现的物体赋予更高权重。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管可通过&lt;strong&gt;视觉感知&lt;/strong&gt;与&lt;strong&gt;场景上下文&lt;/strong&gt;解决模糊指令问题，但更直接的方法是向 &lt;strong&gt;&amp;ldquo;通信伙伴&amp;rdquo;&lt;/strong&gt;（即生成指令的人类）寻求帮助。这类研究主要面临三大核心挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断 &lt;strong&gt;&amp;ldquo;何时请求帮助&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;生成 &lt;strong&gt;&amp;ldquo;信息寻求问题&amp;rdquo;&lt;/strong&gt;（如询问下一步动作、物体位置、方向等）&lt;/li&gt;
&lt;li&gt;设计 &lt;strong&gt;&amp;ldquo;信息提供方&amp;rdquo;&lt;/strong&gt;（oracle）—— 可为真实人类、规则与模板或神经模型&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 与 &lt;strong&gt;VLM&lt;/strong&gt; 在该框架中可承担两种角色：一是 &lt;strong&gt;&amp;ldquo;信息寻求模型&amp;rdquo;&lt;/strong&gt;，二是 &lt;strong&gt;&amp;ldquo;人类助手的代理&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;信息提供模型&amp;rdquo;&lt;/strong&gt;。已有初步研究探索将 LLM 用作信息寻求模型，解决 &lt;strong&gt;&amp;ldquo;何时问&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;问什么&amp;rdquo;&lt;/strong&gt; 的问题 —— 这需借助 &lt;strong&gt;&amp;ldquo;保形预测&amp;rdquo;&lt;/strong&gt;（conformal prediction, &lt;strong&gt;CP&lt;/strong&gt;）或 &lt;strong&gt;&amp;ldquo;上下文学习&amp;rdquo;&lt;/strong&gt;（in-context learning, &lt;strong&gt;ICL&lt;/strong&gt;）等技术实现。&lt;/p&gt;
&lt;p&gt;对于 &lt;strong&gt;&amp;ldquo;信息提供&amp;rdquo;&lt;/strong&gt; 角色，基础模型需扮演 &lt;strong&gt;&amp;ldquo;掌握信息提供方专属信息的助手&amp;rdquo;&lt;/strong&gt; —— 例如知晓目标位置、环境地图等任务执行者无法获取的信息。近期相关研究包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Copilot 方法&lt;/strong&gt;：使智能体在遇到困惑时主动寻求协助，其中 LLM 扮演 &lt;strong&gt;&amp;ldquo;副驾驶&amp;rdquo;&lt;/strong&gt; 角色，为导航提供支持。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;GPT-3&lt;/strong&gt; 可逐步拆解训练数据中的真实响应，这有助于利用预训练的 &lt;strong&gt;SwinBert&lt;/strong&gt; 视频-语言模型训练信息提供方模型；同时，&lt;strong&gt;mPLUG-Owl&lt;/strong&gt; 等大型视觉-语言模型可作为 &lt;strong&gt;&amp;ldquo;现成的强零样本信息提供方&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自驱动通信智能体&lt;/strong&gt;：通过学习 &lt;strong&gt;&amp;ldquo;信息提供方给出肯定答案的置信度&amp;rdquo;&lt;/strong&gt; 实现，可采用 &lt;strong&gt;&amp;ldquo;自我问答&amp;rdquo;&lt;/strong&gt; 模式，在推理阶段无需依赖信息提供方。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;接地指令的泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地指令的泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e6%8c%87%e4%bb%a4%e7%9a%84%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;导航数据在规模与多样性上的局限，是影响 VLN 智能体 &lt;strong&gt;&amp;ldquo;理解多样语言表达、有效遵循指令&amp;rdquo;&lt;/strong&gt; 的另一重要问题 —— 在未见过的导航环境中该问题尤为突出。尽管&lt;strong&gt;语言风格&lt;/strong&gt;本身在 &lt;strong&gt;&amp;ldquo;见过与未见过的环境&amp;rdquo;&lt;/strong&gt; 中具备良好泛化能力，但受限于训练指令的规模，&lt;strong&gt;&amp;ldquo;如何将指令与未见过的环境进行接地&amp;rdquo;&lt;/strong&gt; 仍是一项难题。基础模型通过 &lt;strong&gt;&amp;ldquo;预训练表征&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;指令生成数据增强&amp;rdquo;&lt;/strong&gt; 两种方式，为解决这些问题提供了支持。&lt;/p&gt;
&lt;p&gt;在基础模型出现前，多数研究依赖 &lt;strong&gt;LSTM&lt;/strong&gt; 等文本编码器表征文本指令。而基础模型通过&lt;strong&gt;预训练表征&lt;/strong&gt;，显著提升了 VLN 智能体的&lt;strong&gt;语言泛化能力&lt;/strong&gt;，具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRESS 方法&lt;/strong&gt;：对预训练语言模型 &lt;strong&gt;BERT&lt;/strong&gt; 进行微调，获得对 &lt;strong&gt;&amp;ldquo;未见过指令&amp;rdquo;&lt;/strong&gt; 泛化性更强的文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer&lt;/strong&gt;：为 &lt;strong&gt;VLN-BERT&lt;/strong&gt;、&lt;strong&gt;PREVALENT&lt;/strong&gt; 等方法提供支撑 —— 这些方法通过在 &lt;strong&gt;&amp;ldquo;从网络收集的大规模图文对&amp;rdquo;&lt;/strong&gt; 上预训练，获得更通用的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Airbert 模型&lt;/strong&gt;：训练一个类 &lt;strong&gt;ViLBERT&lt;/strong&gt; 架构，从 &lt;strong&gt;&amp;ldquo;互联网收集的图像-标题对&amp;rdquo;&lt;/strong&gt; 中学习文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLEAR 方法&lt;/strong&gt;：学习 &lt;strong&gt;&amp;ldquo;跨语言语言表征&amp;rdquo;&lt;/strong&gt;，捕捉指令背后的&lt;strong&gt;视觉概念&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProbES 方法&lt;/strong&gt;：通过采样轨迹实现&lt;strong&gt;环境自探索&lt;/strong&gt;，并利用 CLIP 检测到的 &lt;strong&gt;&amp;ldquo;动作与物体短语&amp;rdquo;&lt;/strong&gt; 填充指令模板，自动构建对应指令；同时借助 &lt;strong&gt;&amp;ldquo;基于提示的学习&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;语言嵌入&lt;/strong&gt;的快速适配。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT-2 模型&lt;/strong&gt;：探索利用 &lt;strong&gt;&amp;ldquo;预训练 VLMs&amp;rdquo;&lt;/strong&gt;（如结合 &lt;strong&gt;Flan-T5&lt;/strong&gt; 或 &lt;strong&gt;Vicuna&lt;/strong&gt; 的 &lt;strong&gt;InstructBLIP&lt;/strong&gt;）的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;，提升&lt;strong&gt;导航策略学习&lt;/strong&gt;与&lt;strong&gt;导航推理能力&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;提升智能体泛化能力的另一方法是 &lt;strong&gt;&amp;ldquo;合成更多指令&amp;rdquo;&lt;/strong&gt;。相关研究可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）离线指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;早期研究采用 &lt;strong&gt;&amp;ldquo;说话者-跟随者（Speaker-Follower）框架&amp;rdquo;&lt;/strong&gt;：利用人工标注的 &lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 训练一个 &lt;strong&gt;&amp;ldquo;离线说话者（指令生成器）&amp;rdquo;&lt;/strong&gt;，再让其基于 &lt;strong&gt;&amp;ldquo;给定轨迹上的全景序列&amp;rdquo;&lt;/strong&gt; 生成新指令。但发现这类方法生成的指令质量较低，在人类寻路评估中表现不佳。&lt;/p&gt;
&lt;p&gt;后续改进方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Marky 模型&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;多语言 T5 模型的多模态扩展版本&amp;rdquo;&lt;/strong&gt;，结合 &lt;strong&gt;&amp;ldquo;文本对齐的视觉地标对应关系&amp;rdquo;&lt;/strong&gt;，在未见过环境的 R2R 风格路径上生成 &lt;strong&gt;&amp;ldquo;接近人类质量&amp;rdquo;&lt;/strong&gt; 的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PASTS 模型&lt;/strong&gt;：引入 &lt;strong&gt;&amp;ldquo;进度感知的时空 Transformer 说话者&amp;rdquo;&lt;/strong&gt;，更好地利用 &lt;strong&gt;&amp;ldquo;有序的多视觉与动作特征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SAS 方法&lt;/strong&gt;：利用环境的 &lt;strong&gt;&amp;ldquo;语义与结构线索&amp;rdquo;&lt;/strong&gt;，生成包含丰富&lt;strong&gt;空间信息&lt;/strong&gt;的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SRDF 方法&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;迭代自训练&amp;rdquo;&lt;/strong&gt; 构建一个性能强劲的指令生成器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）导航中实时指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;部分近期研究不再训练离线指令生成器，而是在&lt;strong&gt;导航过程中实时生成指令&lt;/strong&gt;。例如，&lt;strong&gt;LANA 模型&lt;/strong&gt;提出一种 &lt;strong&gt;&amp;ldquo;具备语言能力的导航智能体&amp;rdquo;&lt;/strong&gt; —— 该智能体不仅能执行导航指令，还可生成路线描述。&lt;/p&gt;
&lt;h4&gt;VLN Agent: Learning an Embodied Agent for Reasoning and Planning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管&lt;strong&gt;世界模型&lt;/strong&gt;与&lt;strong&gt;人类模型&lt;/strong&gt;为智能体赋予了&lt;strong&gt;视觉与语言理解能力&lt;/strong&gt;，但 VLN 智能体仍需培养&lt;strong&gt;具身推理&lt;/strong&gt;（embodied reasoning）与&lt;strong&gt;规划能力&lt;/strong&gt;，以支撑自身决策。从这一角度出发，我们将探讨两大挑战：&lt;strong&gt;接地与推理&lt;/strong&gt;、&lt;strong&gt;规划&lt;/strong&gt;；同时还将研究 &lt;strong&gt;&amp;ldquo;直接以基础模型作为 VLN 智能体核心骨干&amp;rdquo;&lt;/strong&gt; 的方法。&lt;/p&gt;
&lt;h5&gt;接地与推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地与推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e4%b8%8e%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;视觉-语言领域的其他任务（如&lt;strong&gt;视觉问答&lt;/strong&gt;（&lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;图像描述生成&lt;/strong&gt;）主要聚焦于 &lt;strong&gt;&amp;ldquo;图像与对应文本描述之间的静态对齐&amp;rdquo;&lt;/strong&gt;，而 VLN 智能体则需基于自身动作，对 &lt;strong&gt;&amp;ldquo;指令与环境中的时空动态信息&amp;rdquo;&lt;/strong&gt; 进行推理。具体而言，智能体需考虑&lt;strong&gt;过往动作&lt;/strong&gt;、识别&lt;strong&gt;待执行的子指令片段&lt;/strong&gt;，并将文本与视觉环境进行&lt;strong&gt;接地&lt;/strong&gt;（grounding），从而执行相应动作。&lt;/p&gt;
&lt;p&gt;传统方法主要通过 &lt;strong&gt;&amp;ldquo;显式语义建模&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;辅助任务设计&amp;rdquo;&lt;/strong&gt; 获取上述能力；但随着基础模型的兴起，&lt;strong&gt;&amp;ldquo;基于特定设计任务的预训练&amp;rdquo;&lt;/strong&gt; 已成为主流方案。&lt;/p&gt;
&lt;p&gt;传统研究通过 &lt;strong&gt;&amp;ldquo;视觉与语言模态的显式语义建模&amp;rdquo;&lt;/strong&gt; 提升智能体的&lt;strong&gt;显式接地能力&lt;/strong&gt;，具体方向包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;建模动作与地标&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;利用指令中的句法信息&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;建模空间关系&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前，&lt;strong&gt;&amp;ldquo;基于基础模型实现 VLN 智能体显式接地&amp;rdquo;&lt;/strong&gt; 的研究仍较少。例如，提出 &lt;strong&gt;&amp;ldquo;动作原子概念学习&amp;rdquo;&lt;/strong&gt;，并将视觉观测映射为&lt;strong&gt;多模态对齐特征&lt;/strong&gt;，以辅助接地。&lt;/p&gt;
&lt;p&gt;除显式语义建模外，传统研究还通过 &lt;strong&gt;&amp;ldquo;辅助推理任务&amp;rdquo;&lt;/strong&gt; 提升智能体的接地能力。但在基于基础模型的 VLN 智能体中，这类方法较少被探索 —— 因为基础模型的预训练过程已使其在导航前就具备了对 &lt;strong&gt;&amp;ldquo;时空语义&amp;rdquo;&lt;/strong&gt; 的通用理解。&lt;/p&gt;
&lt;p&gt;现有研究通过设计&lt;strong&gt;特定预训练任务&lt;/strong&gt;，进一步提升智能体的接地能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;设计专门针对 &lt;strong&gt;&amp;ldquo;场景与物体接地&amp;rdquo;&lt;/strong&gt; 的预训练任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOViS&lt;/strong&gt;：提出两项专项预训练任务，分别增强智能体的 &lt;strong&gt;&amp;ldquo;方向感知&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;视觉信息理解&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HOP&lt;/strong&gt;：提出 &lt;strong&gt;&amp;ldquo;历史与顺序感知预训练范式&amp;rdquo;&lt;/strong&gt;，重点强调&lt;strong&gt;历史信息&lt;/strong&gt;与&lt;strong&gt;轨迹顺序&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;&amp;ldquo;增强智能体的未来视角语义预测能力&amp;rdquo;&lt;/strong&gt; 有助于提升其在&lt;strong&gt;长路径导航&lt;/strong&gt;中的性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设计 &lt;strong&gt;&amp;ldquo;掩码路径建模目标&amp;rdquo;&lt;/strong&gt; —— 给定随机掩码的子路径，重建原始完整路径&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;提出 &lt;strong&gt;&amp;ldquo;实体感知预训练&amp;rdquo;&lt;/strong&gt;，通过预测&lt;strong&gt;接地实体&lt;/strong&gt;并将其与文本对齐实现接地能力提升&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;动态规划&lt;/strong&gt;能让 VLN 智能体实时适应环境变化、优化导航策略。目前，规划方法主要分为两类：一类是 &lt;strong&gt;&amp;ldquo;利用全局图信息增强局部动作空间&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;图基规划器&lt;/strong&gt;；另一类是随基础模型（尤其是 &lt;strong&gt;LLM&lt;/strong&gt;）兴起的 &lt;strong&gt;LLM 基规划器&lt;/strong&gt; —— 这类规划器借助 LLM 的&lt;strong&gt;海量常识&lt;/strong&gt;与&lt;strong&gt;先进推理能力&lt;/strong&gt;，生成动态规划方案，提升决策效果。&lt;/p&gt;
&lt;p&gt;近期 VLN 研究的核心方向之一，是通过 &lt;strong&gt;&amp;ldquo;全局图信息&amp;rdquo;&lt;/strong&gt; 增强导航智能体的规划能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;已访问节点的图边界&amp;rdquo;&lt;/strong&gt; 中的&lt;strong&gt;全局动作步骤&lt;/strong&gt;，增强&lt;strong&gt;局部导航动作空间&lt;/strong&gt;，以实现更优&lt;strong&gt;全局规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过 &lt;strong&gt;&amp;ldquo;高层规划（区域选择）+ 低层规划（节点选择）&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;分层策略&lt;/strong&gt;，进一步优化导航决策&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;&amp;ldquo;基于图边界的全局与局部动作空间&amp;rdquo;&lt;/strong&gt; 中融入 &lt;strong&gt;&amp;ldquo;网格级动作&amp;rdquo;&lt;/strong&gt;，提升&lt;strong&gt;动作预测精度&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在&lt;strong&gt;连续环境&lt;/strong&gt;中，规划方法进一步演进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;采用&lt;strong&gt;分层规划思路&lt;/strong&gt; —— 通过 &lt;strong&gt;&amp;ldquo;从预测的局部可导航性图中选择局部航点&amp;rdquo;&lt;/strong&gt;，用&lt;strong&gt;高层动作空间&lt;/strong&gt;替代&lt;strong&gt;低层动作空间&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CM2&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;在局部地图中实现指令接地&amp;rdquo;&lt;/strong&gt;，辅助&lt;strong&gt;轨迹规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拓展上述策略，构建&lt;strong&gt;全局拓扑图&lt;/strong&gt;或&lt;strong&gt;网格图&lt;/strong&gt;，支持 &lt;strong&gt;&amp;ldquo;基于地图的全局规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;视频预测模型&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;神经辐射表征模型&amp;rdquo;&lt;/strong&gt; 预测多个&lt;strong&gt;未来航点&lt;/strong&gt;，并基于 &lt;strong&gt;&amp;ldquo;预测候选航点的长期影响&amp;rdquo;&lt;/strong&gt; 规划最优动作&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与此同时，部分研究借助 LLM 的&lt;strong&gt;常识知识&lt;/strong&gt;生成 &lt;strong&gt;&amp;ldquo;基于文本的规划方案&amp;rdquo;&lt;/strong&gt;，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-Planner&lt;/strong&gt;：生成由 &lt;strong&gt;&amp;ldquo;子目标&amp;rdquo;&lt;/strong&gt; 构成的详细规划，并根据&lt;strong&gt;预定义程序模式&lt;/strong&gt;整合检测到的物体，实时动态调整规划&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mic&lt;/strong&gt; 与 &lt;strong&gt;A²Nav&lt;/strong&gt;：专注于将导航任务拆解为详细文本指令 —— Mic 从&lt;strong&gt;静态与动态双视角&lt;/strong&gt;生成分步规划，A²Nav 则利用 &lt;strong&gt;GPT-3&lt;/strong&gt; 将指令解析为可执行子任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ThinkBot&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;思维链推理&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought Reasoning），生成 &lt;strong&gt;&amp;ldquo;与交互物体相关的缺失动作&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VL-Map&lt;/strong&gt;：基于 &lt;strong&gt;&amp;ldquo;代码化 LLM&amp;rdquo;&lt;/strong&gt;（遵循 &lt;strong&gt;Code-as-Policy&lt;/strong&gt; 框架），将导航指令拆解为 &lt;strong&gt;&amp;ldquo;代码格式的时序化目标相关函数&amp;rdquo;&lt;/strong&gt;，并利用 &lt;strong&gt;&amp;ldquo;动态构建的可查询地图&amp;rdquo;&lt;/strong&gt; 指导目标执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayNav&lt;/strong&gt;：构建 &lt;strong&gt;&amp;ldquo;已探索环境的 3D 场景图&amp;rdquo;&lt;/strong&gt;，将其作为 LLM 输入，为导航器生成 &lt;strong&gt;&amp;ldquo;可行且符合上下文的高层规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;作为 VLN 智能体的基础模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;作为-vln-智能体的基础模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bd%9c%e4%b8%ba-vln-%e6%99%ba%e8%83%bd%e4%bd%93%e7%9a%84%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;主流方案以 &lt;strong&gt;&amp;ldquo;单流 VL 模型&amp;rdquo;&lt;/strong&gt; 作为 VLN 智能体的核心结构：这类模型在每个时间步同时处理 &lt;strong&gt;&amp;ldquo;语言、视觉、历史令牌（token）&amp;rdquo;&lt;/strong&gt; 输入，通过对&lt;strong&gt;跨模态令牌&lt;/strong&gt;的自注意力运算捕捉 &lt;strong&gt;&amp;ldquo;文本-视觉对应关系&amp;rdquo;&lt;/strong&gt;，进而推断动作概率。&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;零样本 VLN&lt;/strong&gt; 场景中，&lt;strong&gt;CLIP-NAV&lt;/strong&gt; 利用 CLIP 获取 &lt;strong&gt;&amp;ldquo;描述目标物体的自然语言指称表达式&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;序贯导航决策&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;此外，&lt;strong&gt;VLN-CE&lt;/strong&gt;（连续环境 VLN）智能体与 &lt;strong&gt;VLN-DE&lt;/strong&gt;（离散环境 VLN）智能体的核心差异在于&lt;strong&gt;动作空间&lt;/strong&gt;：前者在连续环境中执行&lt;strong&gt;低层控制&lt;/strong&gt;，而非后者 &lt;strong&gt;&amp;ldquo;基于图的高层视角选择动作&amp;rdquo;&lt;/strong&gt;。尽管早期研究采用 &lt;strong&gt;LSTM&lt;/strong&gt; 推断低层动作，但 &lt;strong&gt;&amp;ldquo;航点预测器&amp;rdquo;&lt;/strong&gt;（waypoint predictor）的引入实现了 &lt;strong&gt;&amp;ldquo;从 DE 到 CE 的方法迁移&amp;rdquo;&lt;/strong&gt; —— 所有这些方法均通过航点预测器获取 &lt;strong&gt;&amp;ldquo;局部可导航性图&amp;rdquo;&lt;/strong&gt;，使 DE 场景中的基础模型能适配连续环境。具体而言，航点检测过程主要通过 &lt;strong&gt;&amp;ldquo;视觉观测&amp;rdquo;&lt;/strong&gt;（如全景 RGBD 图像），从智能体当前位置预测 &lt;strong&gt;&amp;ldquo;可导航的相邻候选航点&amp;rdquo;&lt;/strong&gt; 作为潜在目标，再由智能体选择其一作为当前目的地。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 具备强大的&lt;strong&gt;推理能力&lt;/strong&gt;与&lt;strong&gt;世界语义抽象能力&lt;/strong&gt;，且在 &lt;strong&gt;&amp;ldquo;未知大规模环境&amp;rdquo;&lt;/strong&gt; 中表现出优异的&lt;strong&gt;泛化性&lt;/strong&gt; —— 因此，近期 VLN 研究开始直接将 LLM 作为智能体执行导航任务。其核心流程为：将&lt;strong&gt;视觉观测&lt;/strong&gt;转换为&lt;strong&gt;文本描述&lt;/strong&gt;，与指令一同输入 LLM，由 LLM 完成&lt;strong&gt;动作预测&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;代表性创新方案包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT&lt;/strong&gt; 与 &lt;strong&gt;MapGPT&lt;/strong&gt;：验证了&lt;strong&gt;零样本导航&lt;/strong&gt;的可行性 —— NavGPT 利用 &lt;strong&gt;GPT-4&lt;/strong&gt; 自主生成动作，MapGPT 将&lt;strong&gt;拓扑图&lt;/strong&gt;转换为&lt;strong&gt;全局探索提示&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiscussNav&lt;/strong&gt;：拓展上述思路，部署 &lt;strong&gt;&amp;ldquo;多领域专用 VLN 专家&amp;rdquo;&lt;/strong&gt;（包括&lt;strong&gt;指令分析专家&lt;/strong&gt;、&lt;strong&gt;视觉感知专家&lt;/strong&gt;、&lt;strong&gt;完成度估计专家&lt;/strong&gt;、&lt;strong&gt;决策测试专家&lt;/strong&gt;），减少导航任务中的人工参与：通过将任务分配给专用智能体，减轻单一模型负担，实现 &lt;strong&gt;&amp;ldquo;任务专属优化处理&amp;rdquo;&lt;/strong&gt;，并借助&lt;strong&gt;多大型模型的协同优势&lt;/strong&gt;提升&lt;strong&gt;鲁棒性&lt;/strong&gt;、&lt;strong&gt;透明度&lt;/strong&gt;与&lt;strong&gt;整体性能&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MC-GPT&lt;/strong&gt;：利用 &lt;strong&gt;&amp;ldquo;记忆拓扑图&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;人类导航示例&amp;rdquo;&lt;/strong&gt; 丰富&lt;strong&gt;策略多样性&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;InstructNav&lt;/strong&gt;：将导航拆解为&lt;strong&gt;子任务&lt;/strong&gt;，并结合 &lt;strong&gt;&amp;ldquo;多源价值图&amp;rdquo;&lt;/strong&gt; 实现高效执行&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与 &lt;strong&gt;&amp;ldquo;零样本使用&amp;rdquo;&lt;/strong&gt; 不同，部分研究通过 &lt;strong&gt;&amp;ldquo;微调 LLM&amp;rdquo;&lt;/strong&gt;，使其能更有效地处理&lt;strong&gt;具身导航任务&lt;/strong&gt;。另有研究融入 &lt;strong&gt;&amp;ldquo;思维链&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought, &lt;strong&gt;CoT&lt;/strong&gt;）推理机制提升推理过程，例如 &lt;strong&gt;Nav-CoT&lt;/strong&gt; 将 LLM 转化为 &lt;strong&gt;&amp;ldquo;世界模型与导航推理智能体&amp;rdquo;&lt;/strong&gt;，通过模拟未来环境简化决策 —— 这一方案证实了 &lt;strong&gt;&amp;ldquo;微调语言模型&amp;rdquo;&lt;/strong&gt; 在仿真与真实场景中的&lt;strong&gt;灵活性&lt;/strong&gt;与&lt;strong&gt;实用潜力&lt;/strong&gt;，较传统应用实现了显著突破。&lt;/p&gt;
&lt;h3&gt;挑战与未来方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;挑战与未来方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8c%91%e6%88%98%e4%b8%8e%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;尽管基础模型为&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（&lt;strong&gt;VLN&lt;/strong&gt;）提供了创新性解决方案，但仍有若干局限尚未得到充分探索，同时新的挑战也随之出现。在本节中，我们将从&lt;strong&gt;基准数据集&lt;/strong&gt;、&lt;strong&gt;世界模型&lt;/strong&gt;、&lt;strong&gt;人类模型&lt;/strong&gt;、&lt;strong&gt;智能体模型&lt;/strong&gt;及&lt;strong&gt;真实机器人部署&lt;/strong&gt;五个维度，梳理 VLN 领域的挑战与未来研究方向。&lt;/p&gt;
&lt;h4&gt;基准数据集：数据与任务的局限&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基准数据集数据与任务的局限&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e5%87%86%e6%95%b0%e6%8d%ae%e9%9b%86%e6%95%b0%e6%8d%ae%e4%b8%8e%e4%bb%bb%e5%8a%a1%e7%9a%84%e5%b1%80%e9%99%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;当前 &lt;strong&gt;VLN 数据集&lt;/strong&gt;在&lt;strong&gt;质量&lt;/strong&gt;、&lt;strong&gt;多样性&lt;/strong&gt;、&lt;strong&gt;偏差&lt;/strong&gt;及&lt;strong&gt;可扩展性&lt;/strong&gt;方面存在明显局限。例如，在 &lt;strong&gt;R2R&lt;/strong&gt; 数据集中，&lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 偏向于&lt;strong&gt;最短路径&lt;/strong&gt;，无法准确反映现实世界的导航场景。下文将探讨 VLN 基准数据集的改进趋势与建议方向：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;统一且贴近现实的任务与平台&lt;/strong&gt;：构建可靠的基准数据集并确保结果可复现，是评估真实场景下 VLN 性能的关键。现实世界的复杂性要求基准数据集需全面覆盖各类导航挑战，因此需要一个通用的 &lt;strong&gt;&amp;ldquo;仿真到现实&amp;rdquo;&lt;/strong&gt; 评估平台（如 &lt;strong&gt;OVMM&lt;/strong&gt;），以实现仿真与真实场景下的标准化测试。此外，任务与活动设计需贴近现实且源于人类需求，例如 &lt;strong&gt;BEHAVIOR-1K&lt;/strong&gt; 基准数据集，在虚拟、交互式且具生态性的环境中构建&lt;strong&gt;日常家庭活动场景&lt;/strong&gt;，以满足对 &lt;strong&gt;&amp;ldquo;多样性&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;真实性&amp;rdquo;&lt;/strong&gt; 的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;动态环境&lt;/strong&gt;：现实世界环境本质上具有&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;动态性&lt;/strong&gt; —— &lt;strong&gt;移动物体&lt;/strong&gt;、&lt;strong&gt;行人&lt;/strong&gt;，以及&lt;strong&gt;光照&lt;/strong&gt;、&lt;strong&gt;天气&lt;/strong&gt;等环境变化，均可能引发&lt;strong&gt;突发情况&lt;/strong&gt;。这些因素会干扰导航系统的&lt;strong&gt;视觉感知&lt;/strong&gt;，使其难以维持稳定性能。近期部分研究（如 &lt;strong&gt;HAZARD&lt;/strong&gt;、&lt;strong&gt;Habitat 3.0&lt;/strong&gt;、&lt;strong&gt;HA-VLN&lt;/strong&gt;）已开始关注&lt;strong&gt;动态环境&lt;/strong&gt;，为后续研究提供了良好起点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从室内到室外&lt;/strong&gt;：适用于室外环境的 VLN 智能体（如&lt;strong&gt;自动驾驶车辆&lt;/strong&gt;、&lt;strong&gt;无人机&lt;/strong&gt;）正逐渐获得更多关注，相关&lt;strong&gt;语言引导数据集&lt;/strong&gt;也已陆续开发。早期研究尝试将 LLM 融入&lt;strong&gt;室外 VLN 任务&lt;/strong&gt;，具体方式包括&lt;strong&gt;提示工程&lt;/strong&gt;，或通过&lt;strong&gt;微调 LLM&lt;/strong&gt; 实现 &lt;strong&gt;&amp;ldquo;预测下一步动作&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;规划未来轨迹&amp;rdquo;&lt;/strong&gt;。为使现成的 VLN 模型适配&lt;strong&gt;室外导航场景&lt;/strong&gt;，研究者利用&lt;strong&gt;真实驾驶视频&lt;/strong&gt;、&lt;strong&gt;仿真驾驶数据&lt;/strong&gt;或两者结合进行&lt;strong&gt;指令微调&lt;/strong&gt;，使基础模型能够学习预测未来的&lt;strong&gt;油门与转向角度&lt;/strong&gt;。此外，研究者还在基于基础模型的驾驶智能体中集成了额外的&lt;strong&gt;推理与规划模块&lt;/strong&gt;。关于室外 VLN 的详细综述，建议读者参考相关综述文献与立场论文。&lt;/p&gt;
&lt;h4&gt;世界模型：从二维（2D）到三维（3D）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;世界模型从二维2d到三维3d&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%96%e7%95%8c%e6%a8%a1%e5%9e%8b%e4%bb%8e%e4%ba%8c%e7%bb%b42d%e5%88%b0%e4%b8%89%e7%bb%b43d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;构建有效的&lt;strong&gt;世界表征&lt;/strong&gt;是&lt;strong&gt;具身感知&lt;/strong&gt;、&lt;strong&gt;推理&lt;/strong&gt;与&lt;strong&gt;规划&lt;/strong&gt;领域的核心研究主题。VLN 本质上是一项 &lt;strong&gt;3D 任务&lt;/strong&gt; —— 智能体需以 3D 形式感知真实世界环境。尽管当前研究已能通过强大的通用 &lt;strong&gt;2D 表征&lt;/strong&gt;描述世界，但这类表征无法充分支持 3D 场景下的&lt;strong&gt;空间语言理解&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;以往研究已提出多种显式 &lt;strong&gt;3D 表征方式&lt;/strong&gt;，包括各类&lt;strong&gt;语义同步定位与地图构建&lt;/strong&gt;（semantic &lt;strong&gt;SLAM&lt;/strong&gt;）、&lt;strong&gt;体素表征&lt;/strong&gt;、&lt;strong&gt;深度信息&lt;/strong&gt;、&lt;strong&gt;鸟瞰图&lt;/strong&gt;（Bird&amp;rsquo;s-Eye-View）表征（如&lt;strong&gt;网格图&lt;/strong&gt;）及&lt;strong&gt;局部度量图&lt;/strong&gt;。但这些表征存在局限：它们将物体集合限定为 &lt;strong&gt;&amp;ldquo;封闭集合&amp;rdquo;&lt;/strong&gt;，无法适配自然语言对应的 &lt;strong&gt;&amp;ldquo;开放词汇场景&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;部分研究尝试构建 &lt;strong&gt;&amp;ldquo;可查询的地图/场景表征&amp;rdquo;&lt;/strong&gt;，例如将 CLIP 提取的&lt;strong&gt;多视角图像特征&lt;/strong&gt;整合到 &lt;strong&gt;3D 体素网格&lt;/strong&gt;或&lt;strong&gt;俯视特征图&lt;/strong&gt;中，或利用&lt;strong&gt;场景图&lt;/strong&gt;表征&lt;strong&gt;空间关系&lt;/strong&gt;。然而，&lt;strong&gt;&amp;ldquo;如何将大规模数据中学习到的 3D 表征适配于 VLN 智能体，以提升其 3D 环境感知能力&amp;rdquo;&lt;/strong&gt; 仍是待探索的问题。近期兴起的 &lt;strong&gt;3D 基础模型&lt;/strong&gt; —— 包括 &lt;strong&gt;3D 重建模型&lt;/strong&gt; 与 &lt;strong&gt;3D 多模态表征模型&lt;/strong&gt; —— 有望为 VLN 领域提供关键支撑。&lt;/p&gt;
&lt;h4&gt;人类模型：从指令到对话&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;人类模型从指令到对话&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%ba%e7%b1%bb%e6%a8%a1%e5%9e%8b%e4%bb%8e%e6%8c%87%e4%bb%a4%e5%88%b0%e5%af%b9%e8%af%9d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;以往研究多采用 &lt;strong&gt;&amp;ldquo;说话者-倾听者范式&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;受限问答对话&amp;rdquo;&lt;/strong&gt; —— 这类方式仅允许智能体主动请求帮助。近年来，涌现出一批以 &lt;strong&gt;&amp;ldquo;开放式对话指令&amp;rdquo;&lt;/strong&gt; 为核心的新基准数据集，支持智能体在模糊或困惑场景下进行完全&lt;strong&gt;自由形式的通信&lt;/strong&gt;，包括&lt;strong&gt;提问&lt;/strong&gt;、&lt;strong&gt;提议&lt;/strong&gt;、&lt;strong&gt;解释&lt;/strong&gt;、&lt;strong&gt;建议&lt;/strong&gt;、&lt;strong&gt;澄清&lt;/strong&gt;与&lt;strong&gt;协商&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;然而，当前方法仍依赖 &lt;strong&gt;&amp;ldquo;基于规则的对话模板&amp;rdquo;&lt;/strong&gt; 应对上述复杂场景，即便部分方法包含基础模型组件，也未充分发挥其能力。通过 &lt;strong&gt;&amp;ldquo;人类对话数据 + 仿真导航视频&amp;rdquo;&lt;/strong&gt; 对&lt;strong&gt;视频-语言模型&lt;/strong&gt;进行&lt;strong&gt;对话调优&lt;/strong&gt;，使模型在导航过程中具备更强的&lt;strong&gt;对话生成能力&lt;/strong&gt;。未来研究需重点关注两方面：一是将基础模型融入 &lt;strong&gt;&amp;ldquo;情境化任务导向对话管理&amp;rdquo;&lt;/strong&gt;；二是探索现有基础模型在 &lt;strong&gt;&amp;ldquo;任务导向对话&amp;rdquo;&lt;/strong&gt; 中的应用潜力。&lt;/p&gt;
&lt;h4&gt;智能体模型：基础模型在 VLN 中的适配&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;智能体模型基础模型在-vln-中的适配&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%99%ba%e8%83%bd%e4%bd%93%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%9c%a8-vln-%e4%b8%ad%e7%9a%84%e9%80%82%e9%85%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管基础模型具有强大的&lt;strong&gt;泛化能力&lt;/strong&gt;，但将其融入导航任务仍面临挑战：&lt;strong&gt;LLM&lt;/strong&gt; 本质上缺乏对真实环境的&lt;strong&gt;视觉感知能力&lt;/strong&gt;，且易产生 &lt;strong&gt;&amp;ldquo;幻觉&amp;rdquo;&lt;/strong&gt;；下文还将探讨 LLM 在规划与推理方面的能力局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺乏具身经验&lt;/strong&gt;：这一局限可能导致 LLM 在任务规划与推理中仅依赖&lt;strong&gt;预设常识&lt;/strong&gt;，无法满足真实场景的特定需求。部分研究通过 &lt;strong&gt;&amp;ldquo;将视觉观测转换为文本描述，作为 LLM 的提示&amp;rdquo;&lt;/strong&gt; 解决该问题，但这种方式可能丢失关键&lt;strong&gt;视觉语义&lt;/strong&gt;。与 LLM 相比，&lt;strong&gt;VLM&lt;/strong&gt;（视觉-语言模型）智能体虽展现出 &lt;strong&gt;&amp;ldquo;感知视觉世界与规划&amp;rdquo;&lt;/strong&gt; 的潜力，但其训练数据主要源于互联网，缺乏&lt;strong&gt;具身经验&lt;/strong&gt;，需通过&lt;strong&gt;微调&lt;/strong&gt;实现稳健的智能体决策。未来需进一步研究 &lt;strong&gt;&amp;ldquo;如何将基础模型智能体中的常识知识迁移到具身场景中&amp;rdquo;&lt;/strong&gt;。近期提出的 &lt;strong&gt;&amp;ldquo;具身基础模型&amp;rdquo;&lt;/strong&gt;（如 &lt;strong&gt;EmbodieGPT&lt;/strong&gt;、&lt;strong&gt;PaLM-E&lt;/strong&gt;、&lt;strong&gt;Octopus&lt;/strong&gt;）为解决该问题提供了可行方向：这些模型通过在多类具身任务上微调基础模型，缩小智能体在 &lt;strong&gt;&amp;ldquo;视觉-语言-具身动作&amp;rdquo;&lt;/strong&gt; 理解上的差距，提升其基于多模态输入的理解与执行能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;幻觉问题&lt;/strong&gt;：LLM 与 VLM 可能生成 &lt;strong&gt;&amp;ldquo;不存在的物体&amp;rdquo;&lt;/strong&gt;，导致&lt;strong&gt;信息失真&lt;/strong&gt;。例如，LLM 在任务规划时可能生成 &lt;strong&gt;&amp;ldquo;向前走并在沙发处左转&amp;rdquo;&lt;/strong&gt; 的指令，即便房间内并无沙发。这种偏差可能导致智能体执行错误或无法完成的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM 在规划与推理中的能力局限&lt;/strong&gt;：已有文献针对 LLM 的&lt;strong&gt;零样本推理与规划能力&lt;/strong&gt;展开评估（尤其是结合 &lt;strong&gt;PlanBench&lt;/strong&gt; 与 &lt;strong&gt;CogEval&lt;/strong&gt;），结果表明 LLM 在&lt;strong&gt;复杂规划任务&lt;/strong&gt;中存在明显局限。这些研究在 &lt;strong&gt;&amp;ldquo;规划生成、最优性、稳健性、推理&amp;rdquo;&lt;/strong&gt; 等挑战性场景下评估 LLM，发现其不仅易产生幻觉，还可能无法理解复杂规划问题背后的&lt;strong&gt;关系结构&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在 VLN 场景中，由于室内环境固定且导航动作集合有限，&lt;strong&gt;动作空间&lt;/strong&gt;与&lt;strong&gt;规划需求&lt;/strong&gt;相对受限。这种 &lt;strong&gt;&amp;ldquo;有界场景&amp;rdquo;&lt;/strong&gt; 使 LLM 能够生成 &lt;strong&gt;&amp;ldquo;粗粒度方向的分步指令&amp;rdquo;&lt;/strong&gt; —— 已有研究证实该方式的有效性。需强调的是，在 VLN 任务中，LLM 并非主导整个规划过程，而是通过 &lt;strong&gt;&amp;ldquo;结构化拆解指令&amp;rdquo;&lt;/strong&gt; 提供辅助；智能体的实际决策仍主要依赖&lt;strong&gt;感知&lt;/strong&gt;、&lt;strong&gt;运动控制&lt;/strong&gt;等其他组件。因此，LLM 的规划功能更多是 &lt;strong&gt;&amp;ldquo;补充性指导&amp;rdquo;&lt;/strong&gt;，而非唯一决策依据。&lt;/p&gt;
&lt;h4&gt;部署：从仿真到真实机器人&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;部署从仿真到真实机器人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%83%a8%e7%bd%b2%e4%bb%8e%e4%bb%bf%e7%9c%9f%e5%88%b0%e7%9c%9f%e5%ae%9e%e6%9c%ba%e5%99%a8%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;仿真环境往往缺乏真实世界的&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;多样性&lt;/strong&gt;，且低质量渲染图像会进一步加剧这一问题。具体而言，当前部署面临三大瓶颈：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知差距&lt;/strong&gt;：仿真与真实场景的&lt;strong&gt;视觉差异&lt;/strong&gt;导致智能体性能与精度下降，因此需构建更稳健的感知系统。例如，尝试利用&lt;strong&gt;语义地图&lt;/strong&gt;与 &lt;strong&gt;3D 特征场&lt;/strong&gt;为单目机器人提供&lt;strong&gt;全景感知&lt;/strong&gt;，显著提升了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具身差距与数据稀缺&lt;/strong&gt;：仿真环境的&lt;strong&gt;物理规则&lt;/strong&gt;与真实机器人的&lt;strong&gt;具身特性&lt;/strong&gt;不匹配，且真实场景下 VLN 数据收集成本高、规模有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据规模化解决方案&lt;/strong&gt;：&lt;strong&gt;机器人远程操控&lt;/strong&gt;的兴起为解决数据稀缺提供了新思路 —— 通过人类远程控制机器人，可在真实人机交互场景中规模化收集 VLN 数据，为基础模型训练提供支撑。&lt;/p&gt;
&lt;h3&gt;仓库论文链接&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;仓库论文链接&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bb%93%e5%ba%93%e8%ae%ba%e6%96%87%e9%93%be%e6%8e%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：本表格按分类和子分类组织所有 VLN 相关论文，便于浏览和筛选。分类包括：Survey（综述）、World Model（世界模型）、Human Model（人类模型）、VLN Agent（VLN 智能体）、Behavior Analysis（行为分析）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;分类&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;子分类&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Survey&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.12667&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/eric-ai-lab/awesome-vision-language-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s10462-022-10174-9&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual language navigation: A survey and open challenges&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11544&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation: A Survey and Taxonomy&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;World Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.13451&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.03561&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.14158&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Volumetric Environment Representation for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/DefaultRui/VLN-VER&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2023/0204.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision Language Navigation with Knowledge-driven Environmental Dreamer&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;IJCAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/hekj/FDA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.19195&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.02960&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple and Effective Synthesis of Indoor 3D Scenes&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research/se3ds&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Navigational Visual Representations with Semantic Map Supervision&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.11984&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning vision-and-language navigation from youtube videos&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.12907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GridMM: Grid Memory Map for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/GridMM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.04385&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BEVBert: Multimodal Map Pre-training for Language-guided Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/VLN-BEVBert&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15644&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scaling Data Generation in Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/ScaleVLN/tree/main?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.03112&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clin1223/MTVM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.15685&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EnvEdit: Environment Editing for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960375.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.06383&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Much Can CLIP Benefit Vision-and-Language Tasks?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clip-vil/CLIP-ViL&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.11742&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/cshizhe/VLN-DUET&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.13309&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;History Aware Multimodal Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://cshizhe.github.io/projects/vln_hamt.html&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.08756&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pathdreamer: A World Model for Indoor Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Pashevich_Episodic_Transformer_for_Vision-and-Language_Navigation_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Episodic Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.09105&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbert: In-domain Pretraining for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://airbert-vln.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.07876&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation with Random Environmental Mixup&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LCFractal/VLNREM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Human Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene Map-based Prompt Tuning for Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.11142&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/NavRAG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.08467&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/VLN-SRDF&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.15087&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigation Instruction Generation with BEV Perception and Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/FanScy/BEVInstructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.07433&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Controllable Navigation Instruction Generation with Chain of Thought Prompting&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/refkxh/C-Instructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.acl-long.734.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/gmuraleekrishna/SAS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.18721&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Correctable Landmark Discovery via Large Models for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/CONSOLE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.02559&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavHint: Vision and Language Navigation Agent with a Hint Generator&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/NavHint&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10359152&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Follow and Generate Instructions for Language-Capable Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.08409&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lana: A Language-Capable Navigator for Instruction Following and Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wxh1996/LANA-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/xiangyangli-cn/KERM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.11918&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.00852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.09230&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Trans: Translator for the Vision and Language Navigation Agent&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/VLN-trans&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.04006&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/liangcici/Probes-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.14973&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Less is More: Generating Grounded Navigation Instructions from Landmarks&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research-datasets/RxR/tree/main/marky-mT5&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.10504&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Evaluation of Vision-and-Language Navigation Instructions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://say-can.github.io/assets/palm_saycan.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do As I Can, Not As I Say:Grounding Language in Robotic Affordances&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://say-can.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN Agent&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.05552&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/SAME&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.18800&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniVLN: Efficient Vision-and-Language Navigation byProgressive Knowledge Distillation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.06072&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.12587&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CSir1996/VLN-GELA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/chengaopro/AZHP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.04758&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bird&amp;rsquo;s-Eye-View Scene Graph for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14268&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Masked Path Modeling for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.04907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10006384&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2207.11201&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Target-Driven Structured Transformer Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YushengZhao/TD-STP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9880046&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/HOP-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.505.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;COLING&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/LOViS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.12944&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene-Intuitive Agent for Remote Embodied Visual Grounding&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.14143&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YuankaiQi/ORIST&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN BERT: A Recurrent Vision-and-Language BERT for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Recurrent-VLN-BERT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.10638&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/weituo12321/PREVALENT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;VLN-CE&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.02549&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.22548&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://miv-xjtu.github.io/JanusVLN.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23468&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Feliciaxyao/NavMorph&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.05890&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.01943&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/HNR-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03047v2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;PAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/ETPNav?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2408.10388&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Narrowing the Gap between Vision and Action in Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02764&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Discrete-Continuous-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.02857&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jacobkrantz/VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Zero-shot)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10611565&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/DiscussNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.07314&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://chen-judge.github.io/MapGPT/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.10620&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04882&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/InstructNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.16986&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March in Chat: Interactive Prompting for Remote Embodied Referring Expression&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/MiC&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.10822&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision and Language Navigation in the Real World via Online Visual Language Mapping&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://peihaochen.github.io/files/publications/A2Nav.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS Workshop&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.16649&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Fine-tuning)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.01551&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/EvolveNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.findings-naacl.60.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LangNav: Language as a Perceptual Representation for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/pbw-Berwin/LangNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07376&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/NavCoT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.02010&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generalist Model for Embodied Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LaVi-Lab/NaviLLM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2407.12366&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.15852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;RSS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Behavior Analysis&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.16394&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Visual Imaginations Improve Vision-and-Language Navigation Agents?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.17313&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/zehao-wang/navnuances&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Behavioral Analysis of Vision-and-Language Navigation Agents&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Yoark/vln-behave&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.438.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diagnosing Vision-and-Language Navigation: What Really Matters&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/VegB/Diagnose_VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;后续工作&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;后续工作&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%90%8e%e7%bb%ad%e5%b7%a5%e4%bd%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里夸奖一下Gemini3和qwen的deep Research，真的救我狗命。&lt;/p&gt;
&lt;p&gt;重点精读部分就看下面Gemini3提供的一份经过深度调研、严格筛选的 &lt;strong&gt;2023–2025&lt;/strong&gt; 年间顶会（CVPR, ICCV, ECCV, ICLR, NeurIPS, CoRL, RSS, ICRA, IROS）&lt;strong&gt;已接收 (Accepted)&lt;/strong&gt; 且 &lt;strong&gt;已公开代码&lt;/strong&gt; 的 VLN / ObjectNav / Zero-Shot / LLM-assisted Navigation 相关论文列表。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;简介&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;关键词&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CVPR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;UniGoal: Towards Universal Zero-shot Goal-oriented Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了基于场景图（Scene Graph）和 LLM 的通用导航框架，统一了 Object, Image, Text 三种目标导航任务，解决 Zero-Shot 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Zero-Shot, Scene Graph, LLM, Universal Goal&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;专注于提升 VLM 的空间理解能力，通过构建空间感知的指令微调数据集，大幅提升了机器人在 3D 环境中的导航和操作能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/RoboSpatial/RoboSpatial&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Spatial Reasoning, VLM, Robotics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-and-Language Navigation via Causal Learning (VLN-GOAT)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入因果推断（Causal Inference）消除数据偏差，提升 VLN 模型的泛化性&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CrystalSixone/VLN-GOAT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Causal Learning, Deconfounding&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;首个基于视频的大模型（Video-based VLM）端到端导航器，无需构建显式地图，直接从视频流规划动作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jzhzhang/NaVid-VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Video VLM, Mapless, End-to-End&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AIGeN: An Adversarial Approach for Instruction Generation in VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用对抗生成网络生成高质量的导航指令，用于数据增强&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/jialuli-luka/AIGeN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Iterative Vision-and-Language Navigation (IVLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了&amp;quot;迭代式导航&amp;quot;新基准，要求机器人在同一环境中持续执行多条指令，考察记忆能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/JacobKrantz/IVLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Continuous Navigation, Memory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics (VLN-SIG)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过生成未来视角的语义图像来辅助当前决策&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://jialuli-luka.github.io/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;模拟人类认知过程（感知-推理-决策），利用 LLM 进行常识推理和空间推理，解决 ObjectNav 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://yhancao.github.io/CogNav/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page &amp;amp; Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Cognitive Modeling, LLM, ObjectNav&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Learning Vision-and-Language Navigation from YouTube Videos (YouTube-VLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用大规模 YouTube 房屋导览视频进行预训练，学习真实世界先验&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入&amp;quot;心理规划&amp;quot;机制，在执行前在潜在空间预演路径&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/HanqingWangAI/DreamWalker&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ECCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN-Copilot: LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出&amp;quot;副驾驶&amp;quot;概念，当导航智能体困惑时，LLM 提供详细的指导和推理辅助&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zun-Wang/VLN-Copilot&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;LLM Agent, Coarse-grained VLN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过微调适配，激发通用多模态大模型（VLM）的导航推理能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/WZMIAOMIAO/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-Language Navigation with Energy-Based Policy (ENP)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出基于能量的模型（Energy-Based Model）来建模导航策略，更好地模拟专家轨迹分布&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://neurips.cc/virtual/2025/poster/93232&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS Page/GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;构建在线 3D 场景图作为 Prompt，实现无需训练的 Zero-Shot 导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/SG-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;InstructNav: Zero-shot Vision-and-Language Navigation with Instruction Tuning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;这是一个通用的导航大模型框架，统一了 VLN 和 ObjectNav&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;(查看作者 Hao Dong 的 GitHub 或 Project Page)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;PanoGen: Text-Conditioned Panoramic Environment Generation for VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;使用生成式模型根据文本生成全景环境，用于 VLN 的数据增强和训练&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CoRL&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;GC-VLN: Graph-Constrained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;UniGoal&lt;/em&gt; 团队新作，将导航建模为图约束优化问题，无需训练即可在连续环境中导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Training-free, Graph Constraints&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LeLaN: Learning a Language-Conditioned Navigation Policy from In-the-Wild Video&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;直接从野外（In-the-Wild）视频数据中学习语言条件的导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://lelan-video.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OpenVLA: An Open-Source Vision-Language-Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;虽然主要针对操作（Manipulation），但其架构和预训练模型被大量用于导航任务的底座&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/openvla/openvla&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 VLM 进行零样本 3D 视觉定位，是导航的关键前置任务&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/desdemonawang/VLM-Grounder&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OVSG: Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;基于开放词汇 3D 场景图的实体定位与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/ovsg-code/ovsg&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICRA&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation with Open-Source LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;探索使用 Llama 等开源模型替代 GPT-4 进行 Zero-Shot 导航，提出时空思维链&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/Open-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;MonoTransmotion&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;涉及单目视觉下的运动规划与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vita-epfl/MonoTransmotion&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 CLIP 和前沿点（Frontier）地图，指导机器人探索语义目标&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bdaiinstitute/vlfm&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLMaps: Visual Language Maps for Robot Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将 VLM 特征融合进 3D 地图，允许使用自然语言索引地图位置&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vlmaps/vlmaps&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;IROS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LLM3: Large Language Model-based Task and Motion Planning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 LLM 进行任务和运动规划，虽然偏 TAMP，但也包含导航组件&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zju-Robotics-Lab/LLM3&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RSS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Unified Video Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;统一的视频动作模型，涵盖导航和操作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page/Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;雖然偏向操作，但其 Policy 蒸馏方法正被用于加速导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/DLR-RM/Consistency-Policy&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICLR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 Web 教程合成智能体轨迹，辅助导航和任务执行&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/xduan7/AgentTrek&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>世界模型的三种路线</title>
      <link>http://localhost:1313/blog/2025/2025-11-18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-18/</guid>
      <description>
        
        
        &lt;h1&gt;世界模型的三种路线&lt;/h1&gt;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzYzNDE2OTYxMw%3D%3D&amp;amp;mid=2247483737&amp;amp;idx=1&amp;amp;sn=085711a5575f31c0e7ebf14e06a49759&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这则推文&lt;/a&gt;给了我很大的启发，事实上，选择这条道路的一个原因就是 Embodied AI 这条道路并未收敛，而真正落地的成果，以 AI 领域的卷度读个博的4、5年时间以内应该就能吃上红利。而且这个时候恰如上个世纪的物理学界，哪怕一个三流的物理学家在那个年代也能做出一流的发现，我也打算依靠这股浪潮。&lt;/p&gt;
&lt;p&gt;好了，接下来进入正文，引用这篇推文的三个问题：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;什么是世界模型：
是一个供人类观看的东西，一个供智能体训练的场所，还是一个图标内部的黑箱——是系统其他部分需要咨询的实际内部模型？
它的输出是静态资产、实时帧、还是主要驱动和预测和控制的潜在状态？
如果撞倒一个虚拟花瓶，系统中的任何部分是否会记住——并利用该记忆来更新其未来的预期——持续超过一帧？&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;所以得从实际应用出发，从具体的原始论文来熟悉这3类工作。&lt;/p&gt;
&lt;h2&gt;World Model as Interface: Marble&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-interface-marble&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-interface-marble&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;World Model as Simulator: Genie/SIMA 2&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-simulator-geniesima-2&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-simulator-geniesima-2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;World Model as Cognition: Prof. Lecun&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-cognition-prof-lecun&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-cognition-prof-lecun&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;等待更新中&amp;hellip;&amp;hellip;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>具身导航调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-14-navigation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-14-navigation/</guid>
      <description>
        
        
        &lt;h1&gt;具身导航调研&lt;/h1&gt;&lt;p&gt;对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。&lt;/p&gt;
&lt;p&gt;本调研主要基于 &lt;strong&gt;&lt;a href=&#34;https://github.com/jiangranlv/embodied-ai-start&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PKU EPIC Lab&lt;/a&gt;&lt;/strong&gt; 、&lt;strong&gt;&lt;a href=&#34;https://github.com/TianxingChen/Embodied-AI-Guide&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lumina具身智能社区&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;一、基础概念 (Basic Concepts)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一基础概念-basic-concepts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1、 什么是具身智能&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-什么是具身智能&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。&lt;/p&gt;
&lt;p&gt;典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心特征：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有多模态感知能力（视觉、触觉、语音等）&lt;/li&gt;
&lt;li&gt;能够执行动作并影响环境&lt;/li&gt;
&lt;li&gt;学习可以通过与环境交互而不仅仅是被动监督完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 具身智能与其他AI的区别&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-具身智能与其他ai的区别&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96ai%e7%9a%84%e5%8c%ba%e5%88%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;如何scale up机器人数据？&lt;/strong&gt; 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？&lt;/strong&gt; 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 研究具身智能的核心原则 (Core Principles)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-研究具身智能的核心原则-core-principles&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；&lt;/p&gt;
&lt;p&gt;但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）&lt;/p&gt;
&lt;p&gt;必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？&lt;/p&gt;
&lt;h2&gt;二、AI and Robotics Basis&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二ai-and-robotics-basis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cai-and-robotics-basis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-Embodied-AI&lt;/strong&gt;
王鹤老师《具身智能导论》，找找类似课程替代&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-CV&lt;/strong&gt;
Stanford CS231N&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning (CS285)&lt;/strong&gt;
Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;三、研究平台与工具&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三研究平台与工具&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Simulation Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-simulation-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-simulation-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;2. Robot Platform&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-platform&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-platform&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;3. Daily ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-daily-arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-daily-arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了：
具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jiangranlv/robotics_arXiv_daily&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jiangranlv/robotics_arXiv_daily&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;四、Research Field on Robots&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四research-field-on-robots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bresearch-field-on-robots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Grasping&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-grasping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-grasping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。&lt;/p&gt;
&lt;p&gt;常见任务有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single object grasping（单物体抓取）&lt;/strong&gt;：抓取一个物体，物体通常放在桌子上。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clutter scene grasping（堆叠场景抓取）&lt;/strong&gt;：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional grasping（带语义抓取）&lt;/strong&gt;：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用机械手末端有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suction cup（吸盘）&lt;/strong&gt;：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel gripper（平行夹抓）&lt;/strong&gt;：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-fingered hand（多指手）&lt;/strong&gt;，又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-loop methods（开环执行）&lt;/strong&gt;：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closed-loop methods（闭环执行）&lt;/strong&gt;：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Manipulation&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-manipulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-manipulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。&lt;/p&gt;
&lt;p&gt;**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）
**Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。
**Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。
**Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。
**Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。
**Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同&lt;/p&gt;
&lt;h3&gt;3. Navigation(NOW)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-navigationnow&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-navigationnow&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。&lt;/p&gt;
&lt;p&gt;常见任务包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Point Goal Navigation (PointNav)&lt;/strong&gt;：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Goal Navigation (ObjectNav)&lt;/strong&gt;：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Navigation (VLN)&lt;/strong&gt;：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embodied Question Answering (EQA)&lt;/strong&gt;：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：机器人持续感知并跟随动态目标（如人或移动物体）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map-based Navigation&lt;/strong&gt;：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompting-Large-Model Navigation&lt;/strong&gt;：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video-based VLM Navigation&lt;/strong&gt;：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unified Embodied Navigation&lt;/strong&gt;：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Locomotion&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-locomotion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-locomotion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。&lt;/p&gt;
&lt;p&gt;技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning from manually designed reward&lt;/strong&gt; (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from human data&lt;/strong&gt; (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】
如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -&amp;gt; whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下一个阶段会涉及到一点除了control之外的东西，就是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引入【视觉】实现户外自主化（perceptive locomotion）&lt;/strong&gt;；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入【物体】实现loco-manipulation&lt;/strong&gt;；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对上述两种task的组合&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调【语义的泛化性】&lt;/strong&gt;，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调一些特殊的capability&lt;/strong&gt;（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;五、Learning based Research Field&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五learning-based-research-field&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94learning-based-research-field&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Few-shot Imitation Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-few-shot-imitation-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-few-shot-imitation-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：Behavior Cloning、DAgger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当前主流方法&lt;/strong&gt;：ACT、Diffusion Policy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。&lt;/p&gt;
&lt;h3&gt;2. Robot Foundation Model&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-foundation-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-foundation-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。
目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenVLA&lt;/strong&gt;：第一个开源且易于follow的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi0 / Pi0.5&lt;/strong&gt;：目前公认最work的VLA，10K+ hours teleop data训练的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraspVLA&lt;/strong&gt;：基于纯仿真数据的抓取任务的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有少量工作没有借助VLM&lt;/strong&gt;，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Sim-to-Real Reinforcement Learning (Distillation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-sim-to-real-reinforcement-learning-distillation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-sim-to-real-reinforcement-learning-distillation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。&lt;/p&gt;
&lt;p&gt;目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。&lt;/p&gt;
&lt;p&gt;核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。&lt;/p&gt;
&lt;h3&gt;4. Real-World Reinforcement Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-real-world-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-real-world-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-world RL 指直接在现实环境中进行探索式学习。&lt;/p&gt;
&lt;p&gt;这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**从零开始的真实世界强化学习：**Hil-Serl&lt;/li&gt;
&lt;li&gt;**基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. World Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-world-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-world-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代表性工作包括 Dreamer 系列&lt;/strong&gt;（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。&lt;/p&gt;
&lt;h2&gt;六、相关领域&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六相关领域&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Graphics&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-graphics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-graphics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。&lt;/p&gt;
&lt;p&gt;**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等
**Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。
**系统性学习图形学推荐课程：**Games 101, 103&lt;/p&gt;
&lt;h3&gt;2. Hardware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tele-operation（遥操作）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。
**主从臂系统：**如 Gello，实现高精度的力控遥操作。
**可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。
&lt;strong&gt;Sensors（传感器）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。&lt;/li&gt;
&lt;li&gt;**Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。&lt;/li&gt;
&lt;li&gt;**Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。&lt;/li&gt;
&lt;li&gt;**Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mainstream Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-mainstream-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-mainstream-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Transformer&lt;/li&gt;
&lt;li&gt;Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Foundation Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-foundation-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-foundation-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。&lt;/li&gt;
&lt;li&gt;CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。&lt;/li&gt;
&lt;li&gt;VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. 3D Vision&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-3d-vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-3d-vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维生成与重建&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。&lt;/li&gt;
&lt;li&gt;**单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。&lt;/li&gt;
&lt;li&gt;**单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。&lt;/li&gt;
&lt;li&gt;**位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维表示&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。&lt;/li&gt;
&lt;li&gt;**点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。&lt;/li&gt;
&lt;li&gt;**Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维理解&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>具身职业规划</title>
      <link>http://localhost:1313/blog/2025/2025-11-27-embodied-jobs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-27-embodied-jobs/</guid>
      <description>
        
        
        &lt;p&gt;灵感主要来源于这篇&lt;a href=&#34;http://xhslink.com/o/3r7R0NWWLxE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;26二硕具身秋招小结&lt;/a&gt;的帖子，因为以前我只知道&lt;strong&gt;互联网开发岗&lt;/strong&gt;的实习和工作，对其他领域实在未知且畏惧。这篇帖子最好的就在于博主是&lt;strong&gt;半路出身&lt;/strong&gt;，和我基本一模一样。将来我无论是去&lt;strong&gt;港科广&lt;/strong&gt;还是&lt;strong&gt;港中文&lt;/strong&gt;，提前规划自己的实习或科研都是必要的。将来的工作，如果具身找不到，也可以往&lt;strong&gt;多模态&lt;/strong&gt;转，像是&lt;strong&gt;LLM应用开发&lt;/strong&gt;之类的岗位。&lt;/p&gt;
&lt;p&gt;还有一些更多的内容，基本上就是按照&lt;strong&gt;具身&lt;/strong&gt;、&lt;strong&gt;秋招&lt;/strong&gt;、&lt;strong&gt;实习&lt;/strong&gt;之类的关键词在社交媒体上检索，下面分别是：&lt;/p&gt;
&lt;h3&gt;信息来源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;信息来源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bf%a1%e6%81%af%e6%9d%a5%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;知乎：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1915789829809108777&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【具身智能】招聘帖：校招、实习、社招&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;小红书：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://xhslink.com/o/3r7R0NWWLxE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;26二硕具身秋招小结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://xhslink.com/o/9qTWqsG2aAD&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;具身秋招小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;其中知乎和小红书的垃圾信息最多，容易陷入其中无意义的刷，这里就暂时贴这些&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Github：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/StarCycle/Awesome-Embodied-AI-Job/tree/main&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;具身智能招贤榜&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;CC98：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6342124&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】offer帮选 具身or互联网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6337094&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】机械-&amp;gt;具身智能入坑指南 (附公司红黑榜)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6334298&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】offer犹豫，二选一，具身人形&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6327793&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【实习兼职】具身智能实习生（模仿学习）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6324539&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】具身菜博offer预选（拼尽全力版）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6296440&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】（10.20更新）具身智能方向还能投什么？一图速览&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6271943&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】具身老博offer二选一（大结局）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cc98.org/topic/6250608&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【求职广场】本科毕业可以做具身智能吗&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;当前困境与策略&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;当前困境与策略&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%bd%93%e5%89%8d%e5%9b%b0%e5%a2%83%e4%b8%8e%e7%ad%96%e7%95%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;现在的一个问题是，&lt;strong&gt;本科大四空有时间但是无法投递实习&lt;/strong&gt;——这里选择的领域虽然新兴，但是要求也高，很多都是得在读研究生期间才行。因此最初拟定的策略是大四就忙三件事：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;毕设（论文）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;申请季（找硕士读）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;知识准备（面向JD/PhD）&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但是未来其实也有很多路可以走，这里也可以考虑论文之类的，但是&lt;strong&gt;对口实习至少得1份，最好有2份&lt;/strong&gt;。论文这一块至少现在的不确定性比较大，将来如果进入&lt;strong&gt;港科广获得MPhil的机会&lt;/strong&gt;那就可以往这方面努力，只刷1份实习；如果面试失败的话那就老老实实&lt;strong&gt;港中文港深通勤深圳刷2份实习&lt;/strong&gt;。关键的问题是&lt;strong&gt;第一份实习要怎么才能拿到手&lt;/strong&gt;，这里得面向&lt;strong&gt;JD&lt;/strong&gt;努力，或者找一个初创类似性质的地方去，尽可能不要太底层&lt;strong&gt;Robotics&lt;/strong&gt;（但是基础知识又不能不掌握，主要是单纯Robotics的不太好迁移到&lt;strong&gt;LLM/VLM&lt;/strong&gt;之类的工作上去）&lt;/p&gt;
&lt;h2&gt;具身智能岗位三层架构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;具身智能岗位三层架构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e5%b2%97%e4%bd%8d%e4%b8%89%e5%b1%82%e6%9e%b6%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;总结上述的内容，可以总结目前的&lt;strong&gt;具身智能岗位&lt;/strong&gt;不再是浑浑噩噩的一团，而是清晰地分为了三层：&lt;/p&gt;
&lt;h3&gt;顶层（大脑层 - VLA/多模态）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;顶层大脑层---vla多模态&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%a1%b6%e5%b1%82%e5%a4%a7%e8%84%91%e5%b1%82---vla%e5%a4%9a%e6%a8%a1%e6%80%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;代表机构：&lt;/strong&gt; 上海AI Lab、腾讯Robotics X、华为天才少年、智元/银河通用等明星初创的核心组。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征：&lt;/strong&gt; 极其看重**Paper（CVPR/ICRA/CoRL）**或顶尖竞赛。这是&amp;quot;&lt;strong&gt;神仙打架&lt;/strong&gt;&amp;ldquo;的领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BME的策略：&lt;/strong&gt; &lt;strong&gt;不要硬碰硬&lt;/strong&gt;。除非在读研期间发了顶会，否则很难直接作为第一份实习切入。&lt;/p&gt;
&lt;h3&gt;中层（小脑层 - 运控/RL）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;中层小脑层---运控rl&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%ad%e5%b1%82%e5%b0%8f%e8%84%91%e5%b1%82---%e8%bf%90%e6%8e%a7rl&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;代表机构：&lt;/strong&gt; 宇树（Unitree）、小鹏（Robot Center）、以及大多数人形机器人初创。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征：&lt;/strong&gt; 核心是 &lt;strong&gt;Sim2Real（仿真到真机）&lt;/strong&gt;。&lt;strong&gt;PPO算法&lt;/strong&gt;、&lt;strong&gt;Isaac Gym/Lab&lt;/strong&gt;仿真平台是标配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BME的策略：&lt;/strong&gt; &lt;strong&gt;这是主战场&lt;/strong&gt;。有工程背景，理解物理世界（力、摩擦、惯性），这比纯CS背景的人更有优势。&lt;/p&gt;
&lt;h3&gt;基座层（工程层 - 部署/基础软件）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基座层工程层---部署基础软件&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e5%ba%a7%e5%b1%82%e5%b7%a5%e7%a8%8b%e5%b1%82---%e9%83%a8%e7%bd%b2%e5%9f%ba%e7%a1%80%e8%bd%af%e4%bb%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;代表机构：&lt;/strong&gt; 各大机器人公司的&amp;quot;工程落地组&amp;rdquo;、自动驾驶公司的&amp;quot;工具链组&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征：&lt;/strong&gt; &lt;strong&gt;ROS2&lt;/strong&gt;、&lt;strong&gt;C++&lt;/strong&gt;、&lt;strong&gt;Docker&lt;/strong&gt;、通信协议、传感器驱动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BME的策略：&lt;/strong&gt; &lt;strong&gt;这是保底牌和敲门砖&lt;/strong&gt;。很多算法岗其实都需要强工程能力来落地。&lt;/p&gt;
&lt;h2&gt;两条发展路线&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;两条发展路线&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%a4%e6%9d%a1%e5%8f%91%e5%b1%95%e8%b7%af%e7%ba%bf&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;两周后的面试将决定未来两年的主线任务&lt;/strong&gt;。无论结果如何，利用大四做项目是公约数，但在侧重点上需要完全不同的打法。&lt;/p&gt;
&lt;h3&gt;路线 A：港科广 (HKUST-GZ) / MPhil / 走学术科研流&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;路线-a港科广-hkust-gz--mphil--走学术科研流&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%af%e7%ba%bf-a%e6%b8%af%e7%a7%91%e5%b9%bf-hkust-gz--mphil--%e8%b5%b0%e5%ad%a6%e6%9c%af%e7%a7%91%e7%a0%94%e6%b5%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;假设顺利拿到了&lt;strong&gt;MPhil Offer&lt;/strong&gt;，目标是：&lt;strong&gt;两年后进大厂研究院或读博&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; &lt;strong&gt;Novelty (创新性) &amp;gt; Engineering (工程量)&lt;/strong&gt;。简历上必须要有&lt;strong&gt;Paper&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大四空窗期策略（Unitree项目）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;不要满足于&amp;quot;跑通&amp;quot;，要追求&amp;quot;算法改进&amp;quot;&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;课题方向建议：&lt;/strong&gt; 针对Unitree机器狗在极端非结构化环境（如松软沙地、楼梯废墟）下的&lt;strong&gt;RL适应性研究&lt;/strong&gt;。或者，研究如何用更少的算力实现&lt;strong&gt;VLA模型&lt;/strong&gt;在狗身上的&lt;strong&gt;边缘侧推理&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标产出：&lt;/strong&gt; 一篇由自己一作（或共一）的&lt;strong&gt;Workshop论文&lt;/strong&gt;或会议投稿，哪怕没中，&lt;strong&gt;Draft本身也是申请RA实习的硬通货&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实习投递方向：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;首选：&lt;/strong&gt; 腾讯Robotics X（深圳）、IDEA研究院（深圳）、鹏城实验室。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JD匹配：&lt;/strong&gt; 关注JD里写着&amp;quot;探索&amp;hellip;前沿算法&amp;quot;、&amp;ldquo;发表过&lt;strong&gt;ICRA/IROS&lt;/strong&gt;者优先&amp;quot;的岗位。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;路线 B：港中文 (CUHK) / MSc / 走业界就业流&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;路线-b港中文-cuhk--msc--走业界就业流&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%af%e7%ba%bf-b%e6%b8%af%e4%b8%ad%e6%96%87-cuhk--msc--%e8%b5%b0%e4%b8%9a%e7%95%8c%e5%b0%b1%e4%b8%9a%e6%b5%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;假设去了&lt;strong&gt;港中文或港科广的MSc&lt;/strong&gt;，目标是：&lt;strong&gt;两年后秋招拿高薪Offer&lt;/strong&gt;（华为/小鹏/初创独角兽）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心逻辑：&lt;/strong&gt; &lt;strong&gt;Complexity (系统复杂度) &amp;gt; Novelty&lt;/strong&gt;。面试官不关心算法多新，只关心&lt;strong&gt;Demo多稳、技术栈多全&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大四空窗期策略（Unitree项目）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;必须做一个&amp;quot;全栈闭环&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;课题方向建议：&lt;/strong&gt; 给Unitree Go2上跑通 &lt;strong&gt;&amp;ldquo;语音指令 -&amp;gt; VLM解析 -&amp;gt; 导航规划 -&amp;gt; RL运动控制&amp;rdquo;&lt;/strong&gt; 的全流程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心技能点：&lt;/strong&gt; 必须熟练掌握 &lt;strong&gt;ROS 2 (Humble)&lt;/strong&gt;，这是工业界通用的语言。代码要写得漂亮（&lt;strong&gt;C++为主，Python为辅&lt;/strong&gt;），要会用&lt;strong&gt;Docker&lt;/strong&gt;打包环境。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实习投递方向：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;策略：&lt;/strong&gt; 大四下/研一入学前，先去一家中小型初创（如&lt;strong&gt;帕西尼&lt;/strong&gt;、&lt;strong&gt;众擎&lt;/strong&gt;、或者深圳无数的机器人Startup）刷简历。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;理由：&lt;/strong&gt; 大厂（如华为、大疆）很难进，但&lt;strong&gt;初创公司急缺能干活的人&lt;/strong&gt;。有了第一份实习，研二再冲大厂。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;值得投递梯队榜&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;值得投递梯队榜&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%80%bc%e5%be%97%e6%8a%95%e9%80%92%e6%a2%af%e9%98%9f%e6%a6%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;基于提供的列表，结合地理位置（深圳/香港），整理了一份&amp;quot;梯队榜&amp;quot;：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;筛选标准：&lt;/strong&gt; Base大湾区优先 + 有AI/具身基因 + 薪资尚可 + 对转行友好。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;梯队&lt;/th&gt;
          &lt;th&gt;公司/机构&lt;/th&gt;
          &lt;th&gt;关键词与评价&lt;/th&gt;
          &lt;th&gt;行动建议&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T0 (灯塔级)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;腾讯 Robotics X&lt;/td&gt;
          &lt;td&gt;极难，技术最强，学术圈认可度高。科研路线必投。如果没有顶会，大概率简历挂，但值得一试。&lt;/td&gt;
          &lt;td&gt;科研路线必投&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T0 (灯塔级)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;IDEA 研究院（粤港澳大湾区数字经济研究院）&lt;/td&gt;
          &lt;td&gt;沈向洋带队，VLA很强。极佳的跳板。这里有很多港科/港中文的学生实习，学术资源好。&lt;/td&gt;
          &lt;td&gt;极佳的跳板&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T0 (灯塔级)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;华为 (2012/制造部)&lt;/td&gt;
          &lt;td&gt;薪资天花板，但分工极细。慎重。制造部可能偏传统，天才少年计划难度太高。可关注其具身智能应用组。&lt;/td&gt;
          &lt;td&gt;可关注其具身智能应用组&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T1 (独角兽)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;小鹏 (Robot Center)&lt;/td&gt;
          &lt;td&gt;深圳具身龙头，做人形，薪资55w+ (红榜)。重点目标。他们非常缺懂&amp;quot;运控+AI&amp;quot;的人。BME背景在这里不减分。&lt;/td&gt;
          &lt;td&gt;重点目标&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T1 (独角兽)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;大疆 (DJI)&lt;/td&gt;
          &lt;td&gt;难度极大，但技术栈极佳。可以投，但大疆更偏向极致的工程控制，对纯End-to-End AI持保留态度。&lt;/td&gt;
          &lt;td&gt;可以投，但需注意匹配度&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T1 (独角兽)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Unitree (宇树)&lt;/td&gt;
          &lt;td&gt;使用的设备厂商，虽然总部在杭州，但行业地位高。利用优势。如果能用他们的狗做出比官方Demo还牛的功能，直接发邮件给CTO。&lt;/td&gt;
          &lt;td&gt;利用优势&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T2 (潜力股)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;众擎机器人&lt;/td&gt;
          &lt;td&gt;人形，初创，势头猛。刷实习首选。初创公司流程快，能接触核心代码。&lt;/td&gt;
          &lt;td&gt;刷实习首选&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T2 (潜力股)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;帕西尼&lt;/td&gt;
          &lt;td&gt;感知触觉传感器+灵巧手。刷实习首选。BME背景懂传感器（Sensor），这里可能是降维打击区。&lt;/td&gt;
          &lt;td&gt;BME背景的降维打击区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;T2 (潜力股)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;越疆 / 普渡&lt;/td&gt;
          &lt;td&gt;机械臂 / 移动底盘。保底选择。商业化成熟，适合去学习规范的工程开发流程。&lt;/td&gt;
          &lt;td&gt;保底选择&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;技能黑洞与补课重点&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技能黑洞与补课重点&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e8%83%bd%e9%bb%91%e6%b4%9e%e4%b8%8e%e8%a1%a5%e8%af%be%e9%87%8d%e7%82%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A. 具身算法岗 (The Dream Job)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;a-具身算法岗-the-dream-job&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-%e5%85%b7%e8%ba%ab%e7%ae%97%e6%b3%95%e5%b2%97-the-dream-job&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;JD高频词：&lt;/strong&gt; &lt;strong&gt;Imitation Learning (模仿学习)&lt;/strong&gt;, &lt;strong&gt;PPO/RL&lt;/strong&gt;, &lt;strong&gt;VLA&lt;/strong&gt;, &lt;strong&gt;Diffusion Policy&lt;/strong&gt;, &lt;strong&gt;Sim2Real&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;现状判定：&lt;/strong&gt; 也许懂深度学习基础，但可能没跑过&lt;strong&gt;Diffusion Policy&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;补课重点：&lt;/strong&gt; 去看 &lt;strong&gt;ACT (Action Chunking with Transformers)&lt;/strong&gt; 和 &lt;strong&gt;Diffusion Policy&lt;/strong&gt; 的开源代码。这是目前最火的两个模仿学习基线。&lt;/p&gt;
&lt;h3&gt;B. 机器人软件岗 (The Engineer Job)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;b-机器人软件岗-the-engineer-job&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#b-%e6%9c%ba%e5%99%a8%e4%ba%ba%e8%bd%af%e4%bb%b6%e5%b2%97-the-engineer-job&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;JD高频词：&lt;/strong&gt; &lt;strong&gt;ROS 2&lt;/strong&gt;, &lt;strong&gt;Linux&lt;/strong&gt;, &lt;strong&gt;C++&lt;/strong&gt;, &lt;strong&gt;Docker&lt;/strong&gt;, &lt;strong&gt;Python&lt;/strong&gt;, &lt;strong&gt;SLAM&lt;/strong&gt;, &lt;strong&gt;Planning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;现状判定：&lt;/strong&gt; 本科做过Demo，但代码规范吗？会用&lt;strong&gt;CMake&lt;/strong&gt;吗？懂进程间通信吗？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;补课重点：&lt;/strong&gt; &lt;strong&gt;ROS 2 Humble&lt;/strong&gt;。不要再学ROS 1了。学会如何在一个&lt;strong&gt;Docker容器&lt;/strong&gt;里配置好所有环境，这是多人协作的基础。&lt;/p&gt;
&lt;h3&gt;C. 仿真与合成数据岗 (The Hidden Gem)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;c-仿真与合成数据岗-the-hidden-gem&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#c-%e4%bb%bf%e7%9c%9f%e4%b8%8e%e5%90%88%e6%88%90%e6%95%b0%e6%8d%ae%e5%b2%97-the-hidden-gem&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;JD高频词：&lt;/strong&gt; &lt;strong&gt;Isaac Gym&lt;/strong&gt;, &lt;strong&gt;Isaac Sim&lt;/strong&gt;, &lt;strong&gt;Isaac Lab&lt;/strong&gt;, &lt;strong&gt;MuJoCo&lt;/strong&gt;, &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;现状判定：&lt;/strong&gt; 这是一个非常适合&lt;strong&gt;BME转行&lt;/strong&gt;的切入点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;补课重点：&lt;/strong&gt; &lt;strong&gt;NVIDIA Isaac Lab&lt;/strong&gt; (Isaac Gym的继任者)。现在的具身智能全是靠仿真数据堆出来的。如果能熟练搭建一个&amp;quot;机器狗在火星表面行走&amp;quot;的仿真环境，就是抢手货。&lt;/p&gt;
&lt;h2&gt;下一步&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;下一步&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8b%e4%b8%80%e6%ad%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;目前处于&amp;quot;信息过载但行动迷茫&amp;quot;的状态。&lt;/p&gt;
&lt;p&gt;现在的核心任务不是&lt;strong&gt;海投&lt;/strong&gt;，而是&amp;quot;备弹&amp;quot;。手中的Unitree机器狗就是核武器。大多数CS专业的学生只能在仿真里跑代码，而有&lt;strong&gt;真机&lt;/strong&gt;，这意味着&lt;strong&gt;Sim2Real经验&lt;/strong&gt;将是真实的，这是&lt;strong&gt;最大的竞争力&lt;/strong&gt;。要做的就是毕设所言的面向JD的&lt;strong&gt;Unitree机器狗全栈项目&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;内容精选&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;内容精选&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%86%85%e5%ae%b9%e7%b2%be%e9%80%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;投递公司速览表&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;投递公司速览表&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%95%e9%80%92%e5%85%ac%e5%8f%b8%e9%80%9f%e8%a7%88%e8%a1%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;根据ZJU学长的投递经验整理（&lt;strong&gt;59投，3offer，2入池&lt;/strong&gt;）：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;智能驾驶&lt;/th&gt;
          &lt;th&gt;制造/硬件&lt;/th&gt;
          &lt;th&gt;互联网大厂&lt;/th&gt;
          &lt;th&gt;具身智能（中厂）&lt;/th&gt;
          &lt;th&gt;研究院&lt;/th&gt;
          &lt;th&gt;具身智能（初创）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;理想&lt;/td&gt;
          &lt;td&gt;宇量晟（入池）?&lt;/td&gt;
          &lt;td&gt;字节&lt;/td&gt;
          &lt;td&gt;三一云&lt;/td&gt;
          &lt;td&gt;上海AI Lab（转正实习 oc/一面）?&lt;/td&gt;
          &lt;td&gt;亦方创新（意向）?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;地平线&lt;/td&gt;
          &lt;td&gt;中兴蓝剑（入池）?&lt;/td&gt;
          &lt;td&gt;腾讯&lt;/td&gt;
          &lt;td&gt;旷世&lt;/td&gt;
          &lt;td&gt;鹏城（博后）?&lt;/td&gt;
          &lt;td&gt;极佳?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MMT&lt;/td&gt;
          &lt;td&gt;美的&lt;/td&gt;
          &lt;td&gt;阿里&lt;/td&gt;
          &lt;td&gt;同花顺&lt;/td&gt;
          &lt;td&gt;北京通用人工智能&lt;/td&gt;
          &lt;td&gt;它石智航（意向）?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;卓驭&lt;/td&gt;
          &lt;td&gt;联想&lt;/td&gt;
          &lt;td&gt;快手&lt;/td&gt;
          &lt;td&gt;宇树&lt;/td&gt;
          &lt;td&gt;北京人形&lt;/td&gt;
          &lt;td&gt;普渡&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;小鹏&lt;/td&gt;
          &lt;td&gt;OPPO&lt;/td&gt;
          &lt;td&gt;网易&lt;/td&gt;
          &lt;td&gt;深信服&lt;/td&gt;
          &lt;td&gt;招商局&lt;/td&gt;
          &lt;td&gt;RoboSense&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;元戎&lt;/td&gt;
          &lt;td&gt;vivo&lt;/td&gt;
          &lt;td&gt;蚂蚁&lt;/td&gt;
          &lt;td&gt;优必选（笔试）?&lt;/td&gt;
          &lt;td&gt;智源（一面挂）?&lt;/td&gt;
          &lt;td&gt;傅里叶&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;文远知行&lt;/td&gt;
          &lt;td&gt;大疆&lt;/td&gt;
          &lt;td&gt;京东&lt;/td&gt;
          &lt;td&gt;讯飞飞星（二面）?&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;众擎&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;算能&lt;/td&gt;
          &lt;td&gt;滴滴&lt;/td&gt;
          &lt;td&gt;安克（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;自变量&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;百度（?）&lt;/td&gt;
          &lt;td&gt;TP&lt;/td&gt;
          &lt;td&gt;米哈游&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;银河通用（笔试）?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;蔚来（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Shopee（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;智元&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;千里（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;美团（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;帕西尼&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;哈啰&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;越疆&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;SharpA（?）&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;标注说明：&lt;/strong&gt; ?一面挂、?简历挂、?还有机会&lt;/p&gt;
&lt;h3&gt;薪资行情与offer选择案例&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;薪资行情与offer选择案例&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%96%aa%e8%b5%84%e8%a1%8c%e6%83%85%e4%b8%8eoffer%e9%80%89%e6%8b%a9%e6%a1%88%e4%be%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;2024年薪资行情参考（非浙地区）：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;硕士：&lt;/strong&gt; 有项目或优质实习经历，没顶会的情况下，普遍 &lt;strong&gt;60-70w&lt;/strong&gt;，最高可达 &lt;strong&gt;85w&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;博士：&lt;/strong&gt; 有机会接近 &lt;strong&gt;100w&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;案例：6个offer选择困境&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;讯飞飞星&lt;/strong&gt; - 40-80w（不确定性高），995，二线城市，半国企，VLA算法，浮动大&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;普通初创&lt;/strong&gt; - 70×14，965，一线城市，全栈具身研究员，被裁风险++&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;明星初创&lt;/strong&gt; - 预计可谈到85w左右，9105，一线城市，VLA算法，资金充足&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;央企军工&lt;/strong&gt; - 50w+，965，二线城市，技术管理，AI控制算法，&lt;strong&gt;后续难转具身&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;香港普通初创&lt;/strong&gt; - 90w+（港币），base深圳，HK工签（满7年永居），VLA算法，965+，资金有限&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;等待中&lt;/strong&gt; - AILab、TeleAI、IDEA，进度缓慢&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;选择考虑：&lt;/strong&gt; 对具身短期看空、长期看好，预计3-5年后冷静期，考虑转LLM或进入研究院过渡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;案例：互联网vs机器人方向选择&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;互联网方向：&lt;/strong&gt; 字节多模态算法（搜推组），未开奖&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机器人方向：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;小鹏（已开，总包约55w）&lt;/li&gt;
&lt;li&gt;Sharpa&lt;/li&gt;
&lt;li&gt;中兴（可能给蓝剑，但技术栈可能较落后）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;考虑因素：&lt;/strong&gt; 机器人发展路径不明朗，可能泡沫破裂；互联网存量竞争；薪资差异不大。&lt;/p&gt;
&lt;h3&gt;技术方向建议&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技术方向建议&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e6%9c%af%e6%96%b9%e5%90%91%e5%bb%ba%e8%ae%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;ZJU机械出身学长转具身的推荐路线：&lt;/strong&gt;
以&lt;strong&gt;VLA算法为主&lt;/strong&gt;，主打&lt;strong&gt;综合全栈能力&lt;/strong&gt;，通过错位竞争避免与纯算法同学硬拼。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四个主要技术方向：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLA/WA（决策层）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目前主流demo是&lt;strong&gt;VLA&lt;/strong&gt;，框架分为分层式和一段式&lt;/li&gt;
&lt;li&gt;本质是&lt;strong&gt;VLM的posttrain&lt;/strong&gt;，大规模pretrain学术界资源有限&lt;/li&gt;
&lt;li&gt;掌握&lt;strong&gt;diffusion用于生成轨迹&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;**WA（World Action）**算力需求更高，需生成符合物理规则的动作并对齐视频-action gap，目前较遥远&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RL/IL（运控层）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt;仍是主流，RL发展相对缓慢&lt;/li&gt;
&lt;li&gt;全身运控效果出色，固定任务上成功率有优势（RL100）&lt;/li&gt;
&lt;li&gt;重点掌握：&lt;strong&gt;PPO、SAC、TD3&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLA+RL&lt;/strong&gt;是长远趋势，工厂场景需要few-shot和zero-shot&lt;/li&gt;
&lt;li&gt;纯RL运控竞争激烈，薪资不如以前高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;嵌入式&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;掌握&lt;strong&gt;模型轻量化和部署流程&lt;/strong&gt;（ROS 2节点封装）&lt;/li&gt;
&lt;li&gt;确保算法在边缘设备（如Jetson）实时运行&lt;/li&gt;
&lt;li&gt;掌握CAN和485通信，能写&lt;strong&gt;PID和MPC&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;结构设计&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用SolidWorks，手搓滚珠丝杠、连杆、线驱系统&lt;/li&gt;
&lt;li&gt;液压已过时，现基本为电机驱动&lt;/li&gt;
&lt;li&gt;重视演示能力（&amp;ldquo;No BB, show me your demo&amp;rdquo;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;面试重点：&lt;/strong&gt; 主要考察前两个方向（VLA和RL/IL）。&lt;/p&gt;
&lt;h3&gt;实习机会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;实习机会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ae%9e%e4%b9%a0%e6%9c%ba%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;三一集团耘创新实验室（杭州）- 具身智能算法实习生&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;研究方向：&lt;/strong&gt; 模仿学习、VLA&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;团队背景：&lt;/strong&gt; 顶级会议论文30+篇（NeurIPS、ICML、ICLR、CVPR等）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源：&lt;/strong&gt; 充足尖端显卡&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作内容：&lt;/strong&gt; 智能体策略模型研发、算法研究、系统验证&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;要求：&lt;/strong&gt; 研究生在读，精通Python/C++，熟悉Pytorch/TensorFlow，有模仿学习基础&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;联系方式：&lt;/strong&gt; &lt;a href=&#34;mailto:yunze.pan@irootech.com&#34;&gt;yunze.pan@irootech.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;案例一：转专业二硕求职经验&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;案例一转专业二硕求职经验&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%88%e4%be%8b%e4%b8%80%e8%bd%ac%e4%b8%93%e4%b8%9a%e4%ba%8c%e7%a1%95%e6%b1%82%e8%81%8c%e7%bb%8f%e9%aa%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;背景：&lt;/strong&gt; 9本美硕，本科和一硕均与具身智能无关，24年九月同时修读第二个硕士专业Robotics，一年紧急完成一段对口实习，一篇A会（后面求职发现不太对口），预计12月提前毕业。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;投递策略：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;早投递：&lt;/strong&gt; 因为转专业的弱势背景，选择了在8月中就开始投递，后面证明这其实不一定是一个好的策略&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;只投递初创，不投递大厂：&lt;/strong&gt; 基于对大厂具身部门&amp;quot;规模小，门槛高&amp;quot;的判断。大厂具身部门offer总体上确实呈现一种赢者通吃的状态，冲大厂可能需要非常过硬的Pub/实习&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;除头部初创外只投递上海的公司：&lt;/strong&gt; 个人原因，对象在上海工作&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;投递结果：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;简历挂：&lt;/strong&gt; 智元/银河&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;一面挂：&lt;/strong&gt; Sharpa/光轮/忆生科技&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;二面挂：&lt;/strong&gt; 极佳/维他动力&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offer：&lt;/strong&gt; 穹彻智能/源络科技/小雨智造&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offer后结束推进：&lt;/strong&gt; 矩阵超智/留形科技/帕西尼&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;最终选择：&lt;/strong&gt; 穹彻智能具身算法工程师&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;选择考量：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;穹彻：&lt;/strong&gt; 之前实习过，公司氛围、工作强度、title都很不错，工作内容也很喜欢&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;源络：&lt;/strong&gt; 虽然强度偏大，但给的多，成长快并且leader都比较坦诚开放&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小雨：&lt;/strong&gt; 整体公司非常严谨，从上到下对于公司要干嘛、优先级的划分非常清楚，细分赛道有一套完整的落地预期，非常看好他们未来的商业模式，只可惜因为在北京不得不拒绝&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;个人收获和想法：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;对口实习就是第二学历：&lt;/strong&gt; 对于不追求顶级大包（硕士80+）的同学来说，对口实习非常非常重要，通常比论文在找工作的时候更好用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开奖时机：&lt;/strong&gt; 有时候开奖晚，找得晚一些不是坏事。因为找得比较早所以没有和大部队一起开奖，当时开出来的已经觉得非常高，尤其是源络，但这几天其他初创陆续开奖抢人才发现还是小巫见大巫了&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人才红利期：&lt;/strong&gt; 具身总体处于人才缺口相当大的红利期，今年开始学具身的人在明显变多，但这批人可能还需要一段时间才会进入就业池，过几年人才饱和之后红利期可能不复存在。在红利期结束之前要尽快积累完整项目经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预研vs落地：&lt;/strong&gt; 预研和落地是两条完全不同的路线，目前技术未收敛，企业更愿意为科研支付溢价。技术收敛之后可能会迎来具身初创和大厂的超级混战，落地经验就会更值钱&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;案例二：非科班零论文转行经验&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;案例二非科班零论文转行经验&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%88%e4%be%8b%e4%ba%8c%e9%9d%9e%e7%a7%91%e7%8f%ad%e9%9b%b6%e8%ae%ba%e6%96%87%e8%bd%ac%e8%a1%8c%e7%bb%8f%e9%aa%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;背景：&lt;/strong&gt; 9本德硕非科班，0论文，1个相关实习，2个本科做的水实习，代码能力一般。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;投递情况：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;投递了大概200-300个公司&lt;/li&gt;
&lt;li&gt;初创公司和机器人公司基本都没怎么投递，因为觉得不太稳定&lt;/li&gt;
&lt;li&gt;主要投递的还是制造业&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;关键经验：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对口实习的重要性：&lt;/strong&gt; 秋招期间对口的实习还是非常重要的，在询问的时候基本都只问实习的内容，做的项目也不会问。算法方向还是2段实习最为稳妥，不然背景不够只能海投&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;转行时机：&lt;/strong&gt; 今年3月份的时候有幸找到了一个具身的实习，在面试之前没有任何具身方向的认知，随便问了几个问题就招进去了&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习路径：&lt;/strong&gt; 从0开始入门，完整跑了一遍从运动控制、代码采集系统、数据处理、大模型微调、模型部署的整个pipeline。做完实习之后毕设就开始准备做具身方向，找了个博士生带着做，在秋招期间就读了很多这方面的论文，复现了一下模型，正好就补全了模型框架理论薄弱这方面的问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时间线：&lt;/strong&gt; 从8月底开始投，11月底才开始收到比较好的offer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt; 非常幸运，正好赶在具身这个方向热门之前就入局了。秋招是一个长线的过程，过往的所有经历都会成为你的一部分，不要泄气。&lt;/p&gt;
&lt;h3&gt;上海人工智能实验室招聘&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;上海人工智能实验室招聘&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%8a%e6%b5%b7%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%ae%9e%e9%aa%8c%e5%ae%a4%e6%8b%9b%e8%81%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;机构介绍：&lt;/strong&gt; 上海人工智能实验室具身智能中心，面向国家具身智能领域的重大需求，以构建&amp;quot;一体、可泛化的具身人工智能系统&amp;quot;为目标。科研方向涵盖具身生成与数字化、具身智能大模型、人形机器人与运动智能、世界模型、具身多模态感知、物理仿真等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Base：&lt;/strong&gt; 上海-徐汇滨江。校招、实习、社招都招。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub：&lt;/strong&gt; &lt;a href=&#34;https://github.com/Intern-Robotics&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intern Robotics&lt;/a&gt;，建议准备简历的同时先关注相关仓库，欢迎 star 和提出宝贵意见。&lt;/p&gt;
&lt;h4&gt;主要岗位类型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;主要岗位类型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%bb%e8%a6%81%e5%b2%97%e4%bd%8d%e7%b1%bb%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;校招/社招/实习岗位包括：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-AIGC青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：三维重建与生成、几何处理、三维物体/人体生成、运动捕捉&lt;/li&gt;
&lt;li&gt;要求：硕士+，有三维视觉经验，顶级会议论文优先&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-仿真平台青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：具身控制、大模型驱动的智能体、Sim-Real transfer&lt;/li&gt;
&lt;li&gt;要求：硕士+，熟悉IsaacGym/Sim、Gibson、Habitat、Mujoco等仿真平台&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-人体运动策略青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：数字人/人型机器人/灵巧手运动控制&lt;/li&gt;
&lt;li&gt;要求：博士，熟悉生成模型、动捕动画、物理仿真、大模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-具身智能大模型青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：VLA模型框架设计、模型优化、数据生成、持续学习&lt;/li&gt;
&lt;li&gt;要求：硕士+，1年以上经验，熟悉Omniverse等仿真平台&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-足式机器人青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：双足/四足机器人运控算法&lt;/li&gt;
&lt;li&gt;要求：硕士+，有足式机器人项目经验，熟悉运动学动力学&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身智能-强化学习青年研究员&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究方向：机器人运控与操作、现实世界强化学习&lt;/li&gt;
&lt;li&gt;要求：硕士+，精通PPO、SAC、DDPG等算法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;实习岗位主要类型：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AIGC算法实习生&lt;/strong&gt;：三维重建/生成方向，要求博士/硕士，6个月以上&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机械臂操作算法实习生&lt;/strong&gt;：VLA模型、Sim-to-Real，本硕博均可&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;具身智能大模型算法实习生&lt;/strong&gt;：大模型框架设计、模型优化，本科+&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人体运动策略算法实习生&lt;/strong&gt;：物理仿真、动作生成，博士/硕士，需发表论文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真平台算法实习生&lt;/strong&gt;：本科+，有数据可视化经验优先&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;足式机器人算法实习生&lt;/strong&gt;：双足/四足运控算法，本硕博均可&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习算法实习生&lt;/strong&gt;：RL前沿技术研究，本硕博均可&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真-渲染研发实习生&lt;/strong&gt;：3D重建大模型、物理引擎优化，需并行计算经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真平台系统研发实习生&lt;/strong&gt;：系统架构设计、分布式计算，需开源项目经验&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;社招岗位主要类型：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机器人操作工程师&lt;/strong&gt;：仿真引擎平台搭建，硕士+，有工作经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开放平台产品经理&lt;/strong&gt;：产品设计、需求挖掘，有技术类产品经验优先&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习青年研究员&lt;/strong&gt;：RL前沿研究，硕士+，有论文发表&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习算法工程师&lt;/strong&gt;：大规模并行仿真环境构建，硕士+，有项目经验&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机械臂操作青年研究员&lt;/strong&gt;：VLA模型、端到端操作，博士，1年以上研究经历&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; 所有岗位均支持校招、实习、社招。具体岗位详情建议关注&lt;a href=&#34;https://github.com/Intern-Robotics&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intern Robotics GitHub&lt;/a&gt;&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h2&gt;滚动招聘信息（按领域分类）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;滚动招聘信息按领域分类&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%bb%9a%e5%8a%a8%e6%8b%9b%e8%81%98%e4%bf%a1%e6%81%af%e6%8c%89%e9%a2%86%e5%9f%9f%e5%88%86%e7%b1%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;学术/研究机构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;学术研究机构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%a6%e6%9c%af%e7%a0%94%e7%a9%b6%e6%9c%ba%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;机构&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;UC San Diego Biwei Huang组&lt;/td&gt;
          &lt;td&gt;因果驱动的世界模型&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;浙江大学机器人与物理智能实验室&lt;/td&gt;
          &lt;td&gt;机器人控制&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;中科院自动化所水下机器人团队&lt;/td&gt;
          &lt;td&gt;水下机器人&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;普林斯顿PRISM实验室&lt;/td&gt;
          &lt;td&gt;Robot Foundation Models/Interaction and Autonomous Improvement/Open-World Systems&lt;/td&gt;
          &lt;td&gt;全奖PostDoc/PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.26&lt;/td&gt;
          &lt;td&gt;灵心巧手(北京大钟寺)&lt;/td&gt;
          &lt;td&gt;学术合作&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;具身手术机器人&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;清华深研院江勇课题组&lt;/td&gt;
          &lt;td&gt;研究型实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;上交ScaleLab&lt;/td&gt;
          &lt;td&gt;具身仿真方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;港中文李钟毓组&lt;/td&gt;
          &lt;td&gt;人形/灵巧手/多智能体&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;北京大学先进制造与机器人学院庞智博组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;浙江大学高飞组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;科研实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;华东师范大学计算机科学与技术学院&lt;/td&gt;
          &lt;td&gt;具身智能方向学者&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.29&lt;/td&gt;
          &lt;td&gt;上海交通大学自动化与感知学院&lt;/td&gt;
          &lt;td&gt;机器人研究和应用方向&lt;/td&gt;
          &lt;td&gt;科研助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.25&lt;/td&gt;
          &lt;td&gt;北京人形机器人创新中心&lt;/td&gt;
          &lt;td&gt;具身仿真链路&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.24&lt;/td&gt;
          &lt;td&gt;MBZUAI 机器人系RCL实验室&lt;/td&gt;
          &lt;td&gt;机器人/混合现实/人工智能&lt;/td&gt;
          &lt;td&gt;博士/硕士/访问学生/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.22&lt;/td&gt;
          &lt;td&gt;长三角国创中心智慧农业机器人研究所&lt;/td&gt;
          &lt;td&gt;具身智能机器人系统工程师/数据工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.9&lt;/td&gt;
          &lt;td&gt;MBZUAI 机器人系RCL实验室&lt;/td&gt;
          &lt;td&gt;机器人/混合现实/人工智能&lt;/td&gt;
          &lt;td&gt;博士/硕士/访问学生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.5&lt;/td&gt;
          &lt;td&gt;南京大学机器人与自动化学院空中机器人课题组&lt;/td&gt;
          &lt;td&gt;空中机器人设计与自主导航&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.2&lt;/td&gt;
          &lt;td&gt;上海交通大学ScaleLab&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;科研实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.2&lt;/td&gt;
          &lt;td&gt;中科院自动化所多模态人工智能系统全国重点实验室&lt;/td&gt;
          &lt;td&gt;具身设计/感知智能系统/灵巧操作/多模态大模型&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.1&lt;/td&gt;
          &lt;td&gt;朗毅机器人&lt;/td&gt;
          &lt;td&gt;SLAM算法/AI视觉算法/结构设计&lt;/td&gt;
          &lt;td&gt;全职/实习/2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.29&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;人形机器人全身控制&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.28&lt;/td&gt;
          &lt;td&gt;西湖大学机器智能实验室（MiLAB）&lt;/td&gt;
          &lt;td&gt;机器人具身大模型/深度强化学习（3-5人）&lt;/td&gt;
          &lt;td&gt;科研助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.27&lt;/td&gt;
          &lt;td&gt;灵巧智能&lt;/td&gt;
          &lt;td&gt;灵巧操作基础模型&lt;/td&gt;
          &lt;td&gt;研究实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.24&lt;/td&gt;
          &lt;td&gt;北京理工大学（珠海）跨域智能无人团队&lt;/td&gt;
          &lt;td&gt;AI机器人与具身智能&lt;/td&gt;
          &lt;td&gt;国家高层次人才/准聘教授/博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.18&lt;/td&gt;
          &lt;td&gt;中豫具身智能实验室&lt;/td&gt;
          &lt;td&gt;科研岗位&lt;/td&gt;
          &lt;td&gt;博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.17&lt;/td&gt;
          &lt;td&gt;北航国新院杨顺昆老师&lt;/td&gt;
          &lt;td&gt;软件工程/具身智能方向&lt;/td&gt;
          &lt;td&gt;博士/硕士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.17&lt;/td&gt;
          &lt;td&gt;香港科技大学（广州）钟秉灼老师&lt;/td&gt;
          &lt;td&gt;具身智能安全/信息物理系统控制和形式化方法&lt;/td&gt;
          &lt;td&gt;博士后/全奖博士生/研究助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;北大信研院应急管理具身智能联合实验室&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;香港大学&lt;/td&gt;
          &lt;td&gt;具身智能方向&lt;/td&gt;
          &lt;td&gt;博士生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.25&lt;/td&gt;
          &lt;td&gt;NUS CLeAR Lab&lt;/td&gt;
          &lt;td&gt;机器人操作方向&lt;/td&gt;
          &lt;td&gt;全职/兼职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;NUS CLeAR 实验室&lt;/td&gt;
          &lt;td&gt;机器人操作&lt;/td&gt;
          &lt;td&gt;全职/兼职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.17&lt;/td&gt;
          &lt;td&gt;浙江大学 FAST LAB 实验室&lt;/td&gt;
          &lt;td&gt;「机器人打羽毛球」和「水上漂机器人」项目招聘科研助理&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.16&lt;/td&gt;
          &lt;td&gt;荷兰特文特大学手术机器人实验室&lt;/td&gt;
          &lt;td&gt;手术机器人&lt;/td&gt;
          &lt;td&gt;博士研究生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.16&lt;/td&gt;
          &lt;td&gt;ELIXIR Lab&lt;/td&gt;
          &lt;td&gt;软体机器人的具身协作操控研究&lt;/td&gt;
          &lt;td&gt;博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.11&lt;/td&gt;
          &lt;td&gt;香港科技大学郭嵩教授&lt;/td&gt;
          &lt;td&gt;大模型/多模态等方向&lt;/td&gt;
          &lt;td&gt;博士/RA/博后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.9&lt;/td&gt;
          &lt;td&gt;北京大学计算机学院张史梁老师课题组&lt;/td&gt;
          &lt;td&gt;具身智能/多模态大模型轻量化/AIGC&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;香港理工大学校长青年学者吴郁杰老师&lt;/td&gt;
          &lt;td&gt;脑启发AI/脑认知科学&lt;/td&gt;
          &lt;td&gt;2-5名全奖博士生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;香港科技大学高揚教授&lt;/td&gt;
          &lt;td&gt;空间机器人中的人工智能/用于空间可持续性的机器人系统/主动太空碎片清除技术（ADR）&lt;/td&gt;
          &lt;td&gt;3名博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;同济大学设计创意学院CDI数字创新中心&lt;/td&gt;
          &lt;td&gt;机器人终端用户编程/大模型驱动/具身机器人交互/Unity开发&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;清华大学设计未来课题组&lt;/td&gt;
          &lt;td&gt;具身智能机器人开发与设计&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;加州大学洛杉矶分校Bolei Zhou老师&lt;/td&gt;
          &lt;td&gt;计算机视觉/机器自主系统&lt;/td&gt;
          &lt;td&gt;博士/visiting students&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;香港中文大学李钟毓组&lt;/td&gt;
          &lt;td&gt;VLA/Humanoid/Control Theory/Design&lt;/td&gt;
          &lt;td&gt;PhD/Postdoc/Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;TU Eindhoven (Netherlands)&lt;/td&gt;
          &lt;td&gt;Dynamic Manipulation in Semi-Structured Industrial Settings&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;KTH (Sweden)&lt;/td&gt;
          &lt;td&gt;Industrial automation and intelligent robotics&lt;/td&gt;
          &lt;td&gt;Assistant Professor&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;北京大学董豪组/UC Berkeley&lt;/td&gt;
          &lt;td&gt;计算机视觉、机器人和具身智能&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;University of Luxembourg&lt;/td&gt;
          &lt;td&gt;SLAM &amp;amp; Situational Awareness for Robotics&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Medical Robotics&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;Harvard University (US)&lt;/td&gt;
          &lt;td&gt;Postdoctoral Fellow in Robotics&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology(Sweden)&lt;/td&gt;
          &lt;td&gt;Field Robotics with a focus on Extreme Environments&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology(Sweden)&lt;/td&gt;
          &lt;td&gt;Data-driven and learning based control&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;King&amp;rsquo;s College of London&lt;/td&gt;
          &lt;td&gt;robotics and neuro-technology&lt;/td&gt;
          &lt;td&gt;Research associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Acoustic Soft Robotics&lt;/td&gt;
          &lt;td&gt;Research Fellow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Fluidic Robotics&lt;/td&gt;
          &lt;td&gt;Research Fellow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;北大-灵初联合实验室&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;南洋理工PineLab王子为组&lt;/td&gt;
          &lt;td&gt;具身基础模型学习/具身策略迁移部署/具身模型高效推理&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;清华深圳研究院王智组&lt;/td&gt;
          &lt;td&gt;空间场景数据标注与构建/VLA空间推理部署与验证&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;上海交大黄涛组&lt;/td&gt;
          &lt;td&gt;跨本体VLA/VLA推理加速/具身世界模型/仿真平台和数据生成&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;港科大郑展鹏组&lt;/td&gt;
          &lt;td&gt;多旋翼无人机的开发和应用/基于水下机器人的水下目标识别和三维重建&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;南洋理工大学Chuanxia Zheng组&lt;/td&gt;
          &lt;td&gt;三维重建与数字孪生&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/Intern/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;南洋理工大学王林组&lt;/td&gt;
          &lt;td&gt;多模态人工智能/基于生物激发的多模态传感融/面向操作与感知的机器人学习&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc/访问博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;意大利技术研究院IIT Arash Ajoudani组&lt;/td&gt;
          &lt;td&gt;VLM+机器人&lt;/td&gt;
          &lt;td&gt;全奖PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;香港大学Zhongliang Jiang组&lt;/td&gt;
          &lt;td&gt;Robotic Learning/Autonomous Surgical Robotics&lt;/td&gt;
          &lt;td&gt;博士/研究助理/访问学者&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;上海交通大学汶川组&lt;/td&gt;
          &lt;td&gt;机器人控制策略模型/硬件设计/机器人系统&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;香港科技大学人工智能机器人与空间可持续性研究中心&lt;/td&gt;
          &lt;td&gt;AI for Space Robotics/Robotic Systems for Space Sustainability/Active Debris Removal&lt;/td&gt;
          &lt;td&gt;PostDoc/研究助理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;同济大学齐鹏组&lt;/td&gt;
          &lt;td&gt;具身智能血管介入手术机器人/多模态医学影像人工智能&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;上海交通大学SCALE LAB&lt;/td&gt;
          &lt;td&gt;人形机器人/双臂协作VLA/RoboTwin仿真开发&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Southampton&lt;/td&gt;
          &lt;td&gt;Multimodal Large Language Model in Human-Robot Interaction&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Oxford&lt;/td&gt;
          &lt;td&gt;Multi-collaborative scouting and mapping with a team of highly mobile robots&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Southampton&lt;/td&gt;
          &lt;td&gt;Soft Robots with Integrated Sensing, On-Demand Therapy and AI-assisted Control&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Bristol&lt;/td&gt;
          &lt;td&gt;Dexterous 3D-printed Robot Hands&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Glasgow&lt;/td&gt;
          &lt;td&gt;AI-Powered Resilient Teleoperation for Autonomous Robotics&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Manchester/University of Glasgow/University of Oxford&lt;/td&gt;
          &lt;td&gt;Robotics and AI for Net Zero&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;University of Bath&lt;/td&gt;
          &lt;td&gt;Modelling and analytical framework for developing dexterous soft robotic manipulators&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;MBZUAI(UAE)&lt;/td&gt;
          &lt;td&gt;Robotics and AI&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;The University of Manchester&lt;/td&gt;
          &lt;td&gt;AI Driven Robotics for Intelligent Construction&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;The University of Manchester&lt;/td&gt;
          &lt;td&gt;Cloud robotics/Networked Systems&lt;/td&gt;
          &lt;td&gt;Funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.5&lt;/td&gt;
          &lt;td&gt;Heriot Watt University 华威大学 (UK)&lt;/td&gt;
          &lt;td&gt;Chair in Artificial Intelligence&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.30&lt;/td&gt;
          &lt;td&gt;香港科技大学徐英豪组&lt;/td&gt;
          &lt;td&gt;具身智能/3D重建与生成/世界模型&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;卡迪夫大学玛丽居里学者项目(英国)&lt;/td&gt;
          &lt;td&gt;SLAM/强化学习/可微分物理&lt;/td&gt;
          &lt;td&gt;博后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;港科广褚晓文李昊昂组&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;25Fall or 26 Fall PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;复旦大学智能人机交互实验室&lt;/td&gt;
          &lt;td&gt;可穿戴AGI/开源具身智能&lt;/td&gt;
          &lt;td&gt;2026级硕士博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;上交李永露卢策吾组RHOS&lt;/td&gt;
          &lt;td&gt;具身智能/人机协同学习&lt;/td&gt;
          &lt;td&gt;博士/硕士/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;北京大学智能学院钟亦武组&lt;/td&gt;
          &lt;td&gt;多模态推理/具身智能&lt;/td&gt;
          &lt;td&gt;博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.28&lt;/td&gt;
          &lt;td&gt;南科大机械系STAR课题组&lt;/td&gt;
          &lt;td&gt;空中机器人/具身智能&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.27&lt;/td&gt;
          &lt;td&gt;哈工大深圳研究院杨硕组&lt;/td&gt;
          &lt;td&gt;具身智能/多模态模型&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.23&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;robotics with specialization in visual domain adaptation&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.11&lt;/td&gt;
          &lt;td&gt;香港岭南大学陈曦组&lt;/td&gt;
          &lt;td&gt;软体机器人&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Robotics and geometric machine learning&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;香港科技大学（广州）李昊昂组&lt;/td&gt;
          &lt;td&gt;具身智能操纵&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;北京大学Hao Tang组&lt;/td&gt;
          &lt;td&gt;Embodied AI/AIGC&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/Master/Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Machine Learning and Robotics&lt;/td&gt;
          &lt;td&gt;Postdocs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;BMW Group (Munich)&lt;/td&gt;
          &lt;td&gt;Neuromorphic Multimodal Perception and Learning&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology (Sweden)&lt;/td&gt;
          &lt;td&gt;Robotics and Artificial Intelligence WASP&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;Toyota US&lt;/td&gt;
          &lt;td&gt;Robotics &amp;amp; Machine Learning for Human-Robot Interaction and Intelligent Vehicles&lt;/td&gt;
          &lt;td&gt;Research Engineer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;University of Nottingham&lt;/td&gt;
          &lt;td&gt;Soft Robotics and Wearables&lt;/td&gt;
          &lt;td&gt;PhD (UK students only)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;University of Surrey&lt;/td&gt;
          &lt;td&gt;Neuro-muscular state estimation and control for physical human-robot interaction&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;North Dakota State University&lt;/td&gt;
          &lt;td&gt;Precision Agriculture &amp;amp; Robotics&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;Institut de Robòtica i Informàtica Industrial (Spain)&lt;/td&gt;
          &lt;td&gt;Learning robot behaviors in collaborative manufacturing of large components&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.25&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Robotics with spatial understanding and Modelling&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;Imdea Materials (Spain)&lt;/td&gt;
          &lt;td&gt;Collaborative robots and laboratory automation in materials discovery&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;中南大学&lt;/td&gt;
          &lt;td&gt;医疗多模态大模型和具身智能&lt;/td&gt;
          &lt;td&gt;2名博士补录&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;西交利物浦大学/中科院沈阳自动化所&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;联培PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;University of Surrey&lt;/td&gt;
          &lt;td&gt;Evaluating the next generation of warehouse robotics using generative world models&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Horizon Europe MSCA-DN Project AIGreenBots&lt;/td&gt;
          &lt;td&gt;robotics and AI for agricultural robotics&lt;/td&gt;
          &lt;td&gt;11 PhD Positions&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;哈工大深圳&lt;/td&gt;
          &lt;td&gt;微型机器人&lt;/td&gt;
          &lt;td&gt;博士(3月31日前急招)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;NTU&lt;/td&gt;
          &lt;td&gt;Multi-Sensor Fusion in Mobile Manipulation&lt;/td&gt;
          &lt;td&gt;RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Singapre Institute of Technology&lt;/td&gt;
          &lt;td&gt;Robotics &amp;amp; Automation&lt;/td&gt;
          &lt;td&gt;Professor&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.22&lt;/td&gt;
          &lt;td&gt;MBZUAI (Abu Dhabi, United Arab Emirates)&lt;/td&gt;
          &lt;td&gt;Robotics&lt;/td&gt;
          &lt;td&gt;Assistant, Associate and Full Professor&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.22&lt;/td&gt;
          &lt;td&gt;南京大学智能科学与技术学院&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;副教授/助理教授&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.21&lt;/td&gt;
          &lt;td&gt;KTH Royal Institute of Technology&lt;/td&gt;
          &lt;td&gt;Research engineers within Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;清华大学深研院王智组&lt;/td&gt;
          &lt;td&gt;三维视觉与具身智能&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;清华大学脑与智能实验室&lt;/td&gt;
          &lt;td&gt;机器人硬件/VLA训练和部署/人机交互与心智理论&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;University of Sheffield&lt;/td&gt;
          &lt;td&gt;Deep Reinforcement Learning with Interactive Human Feedback&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;西交利物浦大学(苏州)&lt;/td&gt;
          &lt;td&gt;多模态脑机接口大模型&lt;/td&gt;
          &lt;td&gt;博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;南京大学单彩峰司晨阳组/南洋理工(NTU)刘子纬组&lt;/td&gt;
          &lt;td&gt;具身视觉理解&lt;/td&gt;
          &lt;td&gt;联培博士生/硕士生/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;南京大学龙霄潇组&lt;/td&gt;
          &lt;td&gt;3D生成/自动驾驶世界模型/3D视觉&lt;/td&gt;
          &lt;td&gt;Master/PhD/Visiting students&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;OpenAI (US)&lt;/td&gt;
          &lt;td&gt;Mechanical Architect/Research Engineer/Systems Integration Electrical Engineer, Robotics&lt;/td&gt;
          &lt;td&gt;FullTime&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;中国人民大学信息学院AIM3实验室&lt;/td&gt;
          &lt;td&gt;具身智能/多模态&lt;/td&gt;
          &lt;td&gt;博士/硕士/访问学生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;Georgia Tech Lunar Lab&lt;/td&gt;
          &lt;td&gt;Robot Learning&lt;/td&gt;
          &lt;td&gt;PostDoc/PhD/Master/Undergraduate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;University of Nottingham&lt;/td&gt;
          &lt;td&gt;Soft Robotics and Wearables&lt;/td&gt;
          &lt;td&gt;PhD (UK students only)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;New York University Abu Dhabi (NYUAD)&lt;/td&gt;
          &lt;td&gt;Automation, Robotics, and AI&lt;/td&gt;
          &lt;td&gt;Research Instrumentation Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;西交利物浦大学(苏州)&lt;/td&gt;
          &lt;td&gt;机器人学院&lt;/td&gt;
          &lt;td&gt;R1研究员&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;Cranfield University (UK)&lt;/td&gt;
          &lt;td&gt;Robust Motion Planning for Hopping Robots&lt;/td&gt;
          &lt;td&gt;PhD (self-funded, 3-year)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.16&lt;/td&gt;
          &lt;td&gt;Princeton University&lt;/td&gt;
          &lt;td&gt;Construction Robot&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Cambridge University (Rika Antonova team)&lt;/td&gt;
          &lt;td&gt;robot learning / robot hardware design / reinforcement learning&lt;/td&gt;
          &lt;td&gt;Fully-funded PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Cranfield University (UK)&lt;/td&gt;
          &lt;td&gt;Risk aware planning for multi agent systems&lt;/td&gt;
          &lt;td&gt;PhD (self-funded, 3-year)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;University of Liverpool&lt;/td&gt;
          &lt;td&gt;Adaptive Robotic Chemists for Resilient Pharmaceuticals&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;ETH Artificial Visual Intelligence group (AVI)&lt;/td&gt;
          &lt;td&gt;Computer Vision for Embodied AI&lt;/td&gt;
          &lt;td&gt;PhD（对中国学生有限制）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Chalmers University of Technology (Sweden)&lt;/td&gt;
          &lt;td&gt;Dynamics and Control of Legged Robots&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Technical University of Denmark (DTU)&lt;/td&gt;
          &lt;td&gt;Active Perception and End-to-End AI-driven Intuitive Inspection for Autonomous Aerial Robots&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.12&lt;/td&gt;
          &lt;td&gt;克莱姆森大学(美国)Luyang Zhao组&lt;/td&gt;
          &lt;td&gt;机器人学&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;?rebro University (Sweden)&lt;/td&gt;
          &lt;td&gt;Embodied Learning&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;香港科技大学(广州)许人镜组&lt;/td&gt;
          &lt;td&gt;强化学习/仿生机器人运动控制/具身智能芯片&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;ETH&lt;/td&gt;
          &lt;td&gt;Stretchable Optical Skin&lt;/td&gt;
          &lt;td&gt;Postdoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;Singapore Institute of Technology&lt;/td&gt;
          &lt;td&gt;Assistant Professor / Associate Professor in Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.9&lt;/td&gt;
          &lt;td&gt;NTU Energy Research Institute (南洋理工能源研究所)&lt;/td&gt;
          &lt;td&gt;Computer Science/Robotics/Automation&lt;/td&gt;
          &lt;td&gt;Research Engineer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.9&lt;/td&gt;
          &lt;td&gt;Loughborough University (UK)&lt;/td&gt;
          &lt;td&gt;Agri-Robotics&lt;/td&gt;
          &lt;td&gt;Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.7&lt;/td&gt;
          &lt;td&gt;清华大学协同交互智能研究中心周伯文组&lt;/td&gt;
          &lt;td&gt;人机/多智能体协同&lt;/td&gt;
          &lt;td&gt;PhD / Visiting Student&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.7&lt;/td&gt;
          &lt;td&gt;Arizona State University - Bo Liu&amp;rsquo;s Group&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;Tampere University (Finland)&lt;/td&gt;
          &lt;td&gt;Robotics and AI&lt;/td&gt;
          &lt;td&gt;Postdoctoral Research Fellow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;香港科技大学(广州)李昊昂组&lt;/td&gt;
          &lt;td&gt;具身智能(人形机器人/机械臂)&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;香港大学刘希慧组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PhD/RA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.5&lt;/td&gt;
          &lt;td&gt;MIT&lt;/td&gt;
          &lt;td&gt;Soft and Micro Robotics&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;CMU Robotics Institute - Biorobotics Lab&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;Lule? University of Technology (Sweden)&lt;/td&gt;
          &lt;td&gt;Robotics and AI&lt;/td&gt;
          &lt;td&gt;Senior Lecturer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;NTU南洋理工&lt;/td&gt;
          &lt;td&gt;insect-machine hybrid robot&lt;/td&gt;
          &lt;td&gt;Research Assistant&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;NTU南洋理工&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;Robotics Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.28&lt;/td&gt;
          &lt;td&gt;香港中文大学(深圳)&lt;/td&gt;
          &lt;td&gt;医疗具身智能&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.27&lt;/td&gt;
          &lt;td&gt;Eindhoven University of Technology&lt;/td&gt;
          &lt;td&gt;Mechanical Contact Information Processing of Soft and Large-area Robot Skin&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.26&lt;/td&gt;
          &lt;td&gt;宁波东方理工大学&lt;/td&gt;
          &lt;td&gt;机器人与控制&lt;/td&gt;
          &lt;td&gt;博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.22&lt;/td&gt;
          &lt;td&gt;The University of Western Australia&lt;/td&gt;
          &lt;td&gt;Lecturer in Automation and Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.22&lt;/td&gt;
          &lt;td&gt;University of Luxembourg&lt;/td&gt;
          &lt;td&gt;Research Associate in Space Robotics Manipulation&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;香港大学MMLab罗平组 &amp;amp; 深圳星际光年&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习生/联培PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;Washington State University&lt;/td&gt;
          &lt;td&gt;Robotics Planning&lt;/td&gt;
          &lt;td&gt;PhD/Master&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;港科广&lt;/td&gt;
          &lt;td&gt;三维空间感知/运动估计/导航&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;University of Lincoln&lt;/td&gt;
          &lt;td&gt;Robotic Fleet Coordination&lt;/td&gt;
          &lt;td&gt;PostDoc / Research Associate&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.18&lt;/td&gt;
          &lt;td&gt;Wageningen University (Netherlands)&lt;/td&gt;
          &lt;td&gt;Robotics Sensor Fusion and Robotics Transferable skills&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.18&lt;/td&gt;
          &lt;td&gt;Technical University of Munich&lt;/td&gt;
          &lt;td&gt;Space Robotics or GNC&lt;/td&gt;
          &lt;td&gt;PhD/PostDoc（不推荐中国学生）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.17&lt;/td&gt;
          &lt;td&gt;University of Nottingham&lt;/td&gt;
          &lt;td&gt;Robotics Manipulation Learning&lt;/td&gt;
          &lt;td&gt;PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;Research Engineer in Robotics, Perception and Learning&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;KTH&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PhD Social Robotics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.15&lt;/td&gt;
          &lt;td&gt;ETH Zurich &amp;amp; Tethys Robotics&lt;/td&gt;
          &lt;td&gt;Robotics Hardware Engineer&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.14&lt;/td&gt;
          &lt;td&gt;University of Surrey&lt;/td&gt;
          &lt;td&gt;Robotics Lecturer&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;互联网大厂&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;互联网大厂&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%92%e8%81%94%e7%bd%91%e5%a4%a7%e5%8e%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;公司&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.28&lt;/td&gt;
          &lt;td&gt;京东物流&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;寒假实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;阿里巴巴达摩院&lt;/td&gt;
          &lt;td&gt;具身智能招聘(杭州/北京)&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;高德视觉技术中心&lt;/td&gt;
          &lt;td&gt;世界模型&lt;/td&gt;
          &lt;td&gt;全职（学历不限）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.29&lt;/td&gt;
          &lt;td&gt;TikTok&lt;/td&gt;
          &lt;td&gt;多模态基础模型&amp;amp;多模态推荐大模型/MLLM合成数据研究型/大模型后训练RL&lt;/td&gt;
          &lt;td&gt;社招/校招/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.13&lt;/td&gt;
          &lt;td&gt;蚂蚁天玑实验室&lt;/td&gt;
          &lt;td&gt;机器人算法专家/具身智能遥操作系统开发工程师&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.28&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;语言模型/大模型系统/具身智能/多模态/大模型基础设施/AI安全攻防&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;阿里达摩院具身智能大模型团队&lt;/td&gt;
          &lt;td&gt;大模型的具身智能大脑/世界模型与VLA方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.3&lt;/td&gt;
          &lt;td&gt;阿里达摩院&lt;/td&gt;
          &lt;td&gt;具身智能招聘大模型、多模态方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.19&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;具身智能方向财务专家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.14&lt;/td&gt;
          &lt;td&gt;腾讯RoboticsX实验室&lt;/td&gt;
          &lt;td&gt;具身智能方向&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.21&lt;/td&gt;
          &lt;td&gt;微软&lt;/td&gt;
          &lt;td&gt;图像/视频/3d生成or pretraining/posttraining相关&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.14&lt;/td&gt;
          &lt;td&gt;阿里星顶尖人才计划&lt;/td&gt;
          &lt;td&gt;基础模型/Al Infra/大模型应用/产业AI/计算架构等方向&lt;/td&gt;
          &lt;td&gt;2025/2026届本硕博毕业生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.10&lt;/td&gt;
          &lt;td&gt;腾讯&lt;/td&gt;
          &lt;td&gt;技术研究-机器学习/机器人/多模态方向&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;字节跳动&lt;/td&gt;
          &lt;td&gt;机器人研究员/机器人运动控制算法工程师/机器人多模态大模型算法工程师/NLP算法工程师-应用算法&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;Momenta(北京/上海/苏州)&lt;/td&gt;
          &lt;td&gt;端到端大模型算法工程师&lt;/td&gt;
          &lt;td&gt;面向26届博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;蚂蚁集团(上海)&lt;/td&gt;
          &lt;td&gt;整机硬件产品专家 具身智能方向&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;京东探索研究院(北京亦庄)&lt;/td&gt;
          &lt;td&gt;具身智能VLA/自动驾驶VLA&lt;/td&gt;
          &lt;td&gt;社招|校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;三星电子(北京)&lt;/td&gt;
          &lt;td&gt;VLA/RL/点云处理&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;微软亚洲研究院机器学习组工业创新中心&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;蚂蚁集团(上海)&lt;/td&gt;
          &lt;td&gt;具身感知&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;京东TGT顶尖青年技术天才计划&lt;/td&gt;
          &lt;td&gt;包含具身岗位&lt;/td&gt;
          &lt;td&gt;校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.5&lt;/td&gt;
          &lt;td&gt;Figure AI&lt;/td&gt;
          &lt;td&gt;Training Infra/Large Scale Training/Large Scale Model Evals/Reinforcement Learning&lt;/td&gt;
          &lt;td&gt;Full Time&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.29&lt;/td&gt;
          &lt;td&gt;英伟达(北京)&lt;/td&gt;
          &lt;td&gt;大规模数据集/仿真benchmark/算法部署&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.29&lt;/td&gt;
          &lt;td&gt;百度(北京/上海/深圳)&lt;/td&gt;
          &lt;td&gt;多模态算法/自动驾驶感知算法/深度学习决策规划算法&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.25&lt;/td&gt;
          &lt;td&gt;蚂蚁集团PlanA人才专项&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.12&lt;/td&gt;
          &lt;td&gt;三星中国研究院&lt;/td&gt;
          &lt;td&gt;Robot Leading Scientist&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.8&lt;/td&gt;
          &lt;td&gt;字节跳动实习生招聘——筋斗云人才计划&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.8&lt;/td&gt;
          &lt;td&gt;小米集团2026届转正实习招聘&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.7&lt;/td&gt;
          &lt;td&gt;京东探索研究院具身智能团队&lt;/td&gt;
          &lt;td&gt;VLA算法研发&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;Intel(北京)&lt;/td&gt;
          &lt;td&gt;Robotic System Research&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.2&lt;/td&gt;
          &lt;td&gt;美团大模型北斗实习计划2025&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;截止日期7月31日&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;商汤2025春季校招&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;具身算法/运动控制&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;字节跳动&lt;/td&gt;
          &lt;td&gt;SLAM/运动控制/移动规划&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;Vivo&lt;/td&gt;
          &lt;td&gt;机器人首席科学家&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Waymo&lt;/td&gt;
          &lt;td&gt;Deep Learning &amp;amp; Modeling Research&lt;/td&gt;
          &lt;td&gt;Summer Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Waymo&lt;/td&gt;
          &lt;td&gt;World Modeling&lt;/td&gt;
          &lt;td&gt;Research Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;Foundation Model Training Infrastructure&lt;/td&gt;
          &lt;td&gt;Senior Research Engineer&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Google DeepMind&lt;/td&gt;
          &lt;td&gt;Robotics&lt;/td&gt;
          &lt;td&gt;Research Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;华为中央研究院类脑计算团队(北京/上海/杭州/南京/合肥/深圳)&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;算法实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;Vivo Lab(上海)&lt;/td&gt;
          &lt;td&gt;技术规划工程师&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;Generalist Embodied Agent Research&lt;/td&gt;
          &lt;td&gt;Research Scientist (New College Grad 2025)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;京东探索研究院(北京亦庄京东总部)&lt;/td&gt;
          &lt;td&gt;具身智能/人形机器人&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Amazon Robotics (Germany)&lt;/td&gt;
          &lt;td&gt;Motion Planning &amp;amp; Control&lt;/td&gt;
          &lt;td&gt;Applied Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Huawei (Munich, Germany)&lt;/td&gt;
          &lt;td&gt;Robot Learning&lt;/td&gt;
          &lt;td&gt;Industrial PhD（中国学生可能因签证问题被拒）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.12&lt;/td&gt;
          &lt;td&gt;Qualcomm Netherlands&lt;/td&gt;
          &lt;td&gt;Reinforcement learning/MPC/differentiable world models&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.12&lt;/td&gt;
          &lt;td&gt;Microsoft UK (Cambridge)&lt;/td&gt;
          &lt;td&gt;Robotics&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.8&lt;/td&gt;
          &lt;td&gt;Microsoft (Redmond, Washington, US)&lt;/td&gt;
          &lt;td&gt;Spatial AI&lt;/td&gt;
          &lt;td&gt;Research Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;三星电子中国研究院(北京)&lt;/td&gt;
          &lt;td&gt;机器人/具身智能算法&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.23&lt;/td&gt;
          &lt;td&gt;微软亚洲研究院&lt;/td&gt;
          &lt;td&gt;具身智能算法实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;蚂蚁集团&lt;/td&gt;
          &lt;td&gt;人型机器人工程师 (上海浦东/上海黄埔/杭州西湖)&lt;/td&gt;
          &lt;td&gt;请在猎聘搜索&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;Amazon Robotics Germany&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;PostDoc Scientist&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;Qualcomm&lt;/td&gt;
          &lt;td&gt;Automotive Engineering Internship&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;Toyata US&lt;/td&gt;
          &lt;td&gt;Multimodal Learning&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;Waymo US&lt;/td&gt;
          &lt;td&gt;Planning Selection&lt;/td&gt;
          &lt;td&gt;PhD Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.16&lt;/td&gt;
          &lt;td&gt;Samsung US&lt;/td&gt;
          &lt;td&gt;Embodied Intelligence Intern&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.15&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;PhD Research Intern - Robotics and/or Autonomous Vehicles&lt;/td&gt;
          &lt;td&gt;Remote!&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.15&lt;/td&gt;
          &lt;td&gt;NVIDIA&lt;/td&gt;
          &lt;td&gt;Principal Autonomous Vehicles Engineer - Mapping and Localization&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.13&lt;/td&gt;
          &lt;td&gt;Amazon Robotics&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;Intern/FullTime&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;字节跳动机器人&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;阿里巴巴达摩院视觉技术实验室&lt;/td&gt;
          &lt;td&gt;VLA方向&lt;/td&gt;
          &lt;td&gt;研究实习生&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;机器人/具身智能公司&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;机器人具身智能公司&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e5%85%ac%e5%8f%b8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;公司&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;小鹏机器人中心(北上广深北美)&lt;/td&gt;
          &lt;td&gt;多模态数据/灵巧操作&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;极佳视界(北上广深杭州武汉)&lt;/td&gt;
          &lt;td&gt;具身智能校园招聘&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;优必选(深圳)&lt;/td&gt;
          &lt;td&gt;感知算法/系统开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;晨昏线(深圳)&lt;/td&gt;
          &lt;td&gt;视觉感知/嵌入式/VLA/SLAM/强化学习/机械结构&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;深圳市大寰机器人&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;越疆机器人(深圳)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;普渡机器人(深圳/成都)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;北京术锐机器人&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;埃夫特机器人(吉林)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.11&lt;/td&gt;
          &lt;td&gt;星际光年(深圳)&lt;/td&gt;
          &lt;td&gt;灵巧操作算法&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.27&lt;/td&gt;
          &lt;td&gt;仁新机器人&lt;/td&gt;
          &lt;td&gt;机器人 AI算法工程师(5名)&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.25&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法量化工程师&lt;/td&gt;
          &lt;td&gt;实习/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.23&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;高级机械结构工程师/导航算法工程师/高级硬件工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.21&lt;/td&gt;
          &lt;td&gt;聆动通用&lt;/td&gt;
          &lt;td&gt;机器人运动控制/机器人视觉感知/机械臂运动规划&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.18&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;强化学习算法/量化工程师&lt;/td&gt;
          &lt;td&gt;实习/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.17&lt;/td&gt;
          &lt;td&gt;人形机器人(上海)有限公司&lt;/td&gt;
          &lt;td&gt;算法工程师/软件工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.15&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;具身大模型算法工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.30&lt;/td&gt;
          &lt;td&gt;首形科技&lt;/td&gt;
          &lt;td&gt;机械工程师/嵌入式软件工程师/算法工程师（大模型多模态交互方向）&lt;/td&gt;
          &lt;td&gt;2026届招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.29&lt;/td&gt;
          &lt;td&gt;上海市人形机器人创新孵化器&lt;/td&gt;
          &lt;td&gt;具身智能算法工程师/机器人装配工程师/机械结构设计等&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.26&lt;/td&gt;
          &lt;td&gt;穹彻智能(上海)&lt;/td&gt;
          &lt;td&gt;视觉/三维重建/位姿估计/模仿学习/视频理解&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.26&lt;/td&gt;
          &lt;td&gt;Infermove&lt;/td&gt;
          &lt;td&gt;规划算法负责人/规划算法工程师/资深全栈开发工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.19&lt;/td&gt;
          &lt;td&gt;智平方&lt;/td&gt;
          &lt;td&gt;算法/工程/产品/硬件/设计/生产岗&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.17&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;多模态强化学习算法工程师/具身智能大模型算法工程师/多模态数据算法工程师&lt;/td&gt;
          &lt;td&gt;社招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.16&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;具身智能大模型算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.16&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;多模态强化学习算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.16&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;多模态数据算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.12&lt;/td&gt;
          &lt;td&gt;智拓科技&lt;/td&gt;
          &lt;td&gt;具身智能工程师&lt;/td&gt;
          &lt;td&gt;全职/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.7&lt;/td&gt;
          &lt;td&gt;聆动通用机器人&lt;/td&gt;
          &lt;td&gt;研究算法类/研发类&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.6&lt;/td&gt;
          &lt;td&gt;云鲸智能具身智能团队&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.4&lt;/td&gt;
          &lt;td&gt;零次方机器人&lt;/td&gt;
          &lt;td&gt;机械工程师/硬件工程师/仿真工程师/具身数据工程师/深度强化学习算法工程师等&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.3&lt;/td&gt;
          &lt;td&gt;智身科技&lt;/td&gt;
          &lt;td&gt;解决方案工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.30&lt;/td&gt;
          &lt;td&gt;诺亦腾机器人Noitom Robotics&lt;/td&gt;
          &lt;td&gt;算法研究员/具身智能系统应用开发工程师/结构工程师&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.30&lt;/td&gt;
          &lt;td&gt;微分智飞&lt;/td&gt;
          &lt;td&gt;强化学习算法工程师/机器人大模型算法工程师/机器人系统工程师&lt;/td&gt;
          &lt;td&gt;2026校招/全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.29&lt;/td&gt;
          &lt;td&gt;非夕科技&lt;/td&gt;
          &lt;td&gt;机器人应用&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.27&lt;/td&gt;
          &lt;td&gt;光轮智能&lt;/td&gt;
          &lt;td&gt;机器人仿真框架用户测试&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.23&lt;/td&gt;
          &lt;td&gt;千寻智能 Spirit Al&lt;/td&gt;
          &lt;td&gt;具身智能算法工程师/机器学习系统工程师/机器学习平台后端工程师&lt;/td&gt;
          &lt;td&gt;2026应届校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.22&lt;/td&gt;
          &lt;td&gt;小鹏机器人&lt;/td&gt;
          &lt;td&gt;Research Scientist/Research Intern&lt;/td&gt;
          &lt;td&gt;实习+校/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.21&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;算法、研发、产品、运营、解决方案、职能/支持&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;忆生科技&lt;/td&gt;
          &lt;td&gt;三维重建与生成算法工程师/多模态算法工程师/机器人遥操作系统开发工程师&lt;/td&gt;
          &lt;td&gt;2026届校招/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;大模型类/算法类/软件系统类/测试类/其它&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.20&lt;/td&gt;
          &lt;td&gt;银河通用机器人&lt;/td&gt;
          &lt;td&gt;具身多模态大模型/具身智能操作算法/人形强化学习控制/机器人规划与控制&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.16&lt;/td&gt;
          &lt;td&gt;原力灵机&lt;/td&gt;
          &lt;td&gt;具身智能大模型算法研究员/具身智能强化学习算法研究员/机器人系统算法工程师&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.13&lt;/td&gt;
          &lt;td&gt;Dyna Robotics&lt;/td&gt;
          &lt;td&gt;机械工程师/高级机器人工程师/机器学习工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.13&lt;/td&gt;
          &lt;td&gt;宇树机器人&lt;/td&gt;
          &lt;td&gt;操作员&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.12&lt;/td&gt;
          &lt;td&gt;PaXini&lt;/td&gt;
          &lt;td&gt;具身智能算法/机器人智能系统/运动控制算法&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.12&lt;/td&gt;
          &lt;td&gt;河南具身智能产业发展有限公司&lt;/td&gt;
          &lt;td&gt;招聘10人&lt;/td&gt;
          &lt;td&gt;国企&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.11&lt;/td&gt;
          &lt;td&gt;舞肌科技&lt;/td&gt;
          &lt;td&gt;具身大模型强化学习算法工程师/大模型部署工程师&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.7&lt;/td&gt;
          &lt;td&gt;自变量机器人&lt;/td&gt;
          &lt;td&gt;多模态生成算法工程师/3D生成算法工程师/语音算法工程师/运动控制算法工程师&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.31&lt;/td&gt;
          &lt;td&gt;小鹏机器人中心多模态智能部&lt;/td&gt;
          &lt;td&gt;具身多模态大模型/世界模型方向&lt;/td&gt;
          &lt;td&gt;社招/校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.30&lt;/td&gt;
          &lt;td&gt;原力灵机&lt;/td&gt;
          &lt;td&gt;具身智能大模型算法研究员/机器人系统算法工程师/具身智能传感器工程师&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.29&lt;/td&gt;
          &lt;td&gt;Sharpa&lt;/td&gt;
          &lt;td&gt;机器人机械工程师/机器人电子工程师/机器人系统工程师/触觉应用算法工程师等&lt;/td&gt;
          &lt;td&gt;2026校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.29&lt;/td&gt;
          &lt;td&gt;智元机器人&amp;quot;优才计划&amp;quot;&lt;/td&gt;
          &lt;td&gt;真机强化学习(具身操作)/运动控制算法/感知/规控/力控算法/多模态大模型(VLA/VLM)&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.28&lt;/td&gt;
          &lt;td&gt;硅基方舟(杭州)&lt;/td&gt;
          &lt;td&gt;运动控制算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.28&lt;/td&gt;
          &lt;td&gt;跨维智能(深圳)&lt;/td&gt;
          &lt;td&gt;数据生成研发工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;汉宇晨星&lt;/td&gt;
          &lt;td&gt;机器人研发专家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;帕西尼感知科技（天津）有限公司&lt;/td&gt;
          &lt;td&gt;数据采集员/机器人助理工程师/视觉算法工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.26&lt;/td&gt;
          &lt;td&gt;厨芯科技&lt;/td&gt;
          &lt;td&gt;具身智能算法工程师（数据方向）&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.23&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;仿真开发工程师/硬件工程师/数据分析与挖掘工程师&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.22&lt;/td&gt;
          &lt;td&gt;珞石机器人&lt;/td&gt;
          &lt;td&gt;软件工程师/机械工程师/测试工程师/控制工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.22&lt;/td&gt;
          &lt;td&gt;灵心巧手/赛那德招聘&lt;/td&gt;
          &lt;td&gt;海外销售经理/具身智能大客户经理/机器人KA销售&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;深圳星际光年&lt;/td&gt;
          &lt;td&gt;灵巧手嵌入式软件工程师/灵巧手设计工程师(机械方向)/灵巧操作学习算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;RoboSense 2026&amp;quot;天才罗伯特&amp;quot;人才计划&lt;/td&gt;
          &lt;td&gt;硬件/算法/产品方向&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.17&lt;/td&gt;
          &lt;td&gt;苏州一星机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法岗/软件开发岗/硬件开放岗/GPU计算集群岗/产品岗&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.17&lt;/td&gt;
          &lt;td&gt;深庭纪&lt;/td&gt;
          &lt;td&gt;机器学习工程师/机器人运动控制工程师/高性能计算工程师/强化学习工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.10&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;机器人解决方案实习生/具身感知算法工程师/大语言模型算法实习生&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.9&lt;/td&gt;
          &lt;td&gt;一星机器人(苏州)&lt;/td&gt;
          &lt;td&gt;硬件开发/集群运维工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;大模型类/算法类/软件系统类/测试类/其它&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;Sharpa灵巧手(上海长宁)&lt;/td&gt;
          &lt;td&gt;机器人软件主架构师/强化学习&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;真友科技(武汉)&lt;/td&gt;
          &lt;td&gt;人形强化学习运动控制&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;加速进化(北京)&lt;/td&gt;
          &lt;td&gt;强化学习运动控制算法&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;自变量(深圳优先)&lt;/td&gt;
          &lt;td&gt;slam/导航/点云/传感器标定/VLN&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;RoboSense速腾聚创&lt;/td&gt;
          &lt;td&gt;研发技术类/产品管理类/制造技术类&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;知象光电&lt;/td&gt;
          &lt;td&gt;具身智能算法方向/产品研发方向/市场营销方向&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;深圳特修斯机器人有限公司新余研究所&lt;/td&gt;
          &lt;td&gt;机械/结构设计工程师、工业设计工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;源络科技&lt;/td&gt;
          &lt;td&gt;多模态算法工程师/机器人学习工程师/感知算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;众擎机器人科技&lt;/td&gt;
          &lt;td&gt;机器人本体研发/具身智能/运动控制/生产制造&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.6&lt;/td&gt;
          &lt;td&gt;星猿哲&lt;/td&gt;
          &lt;td&gt;视觉算法/运动算法/机器人仿真/机械设计研发工程师&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.4&lt;/td&gt;
          &lt;td&gt;眸深智能&lt;/td&gt;
          &lt;td&gt;机器人工程师/机器人算法&lt;/td&gt;
          &lt;td&gt;全职/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.3&lt;/td&gt;
          &lt;td&gt;它石智航&lt;/td&gt;
          &lt;td&gt;机器人算法/机器人多模态感知/VLA/机器人SLAM等算法工程师&lt;/td&gt;
          &lt;td&gt;校招/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.3&lt;/td&gt;
          &lt;td&gt;妙动科技&lt;/td&gt;
          &lt;td&gt;硬件/机械/算法&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;银河通用机器人&lt;/td&gt;
          &lt;td&gt;具身大模型/灵巧手/机器人全身控制/足式强化学习/感知/仿真等算法工程师&lt;/td&gt;
          &lt;td&gt;社招/2026届实习生/暑期实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;干寻智能&lt;/td&gt;
          &lt;td&gt;机器人高级机械工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;干寻智能&lt;/td&gt;
          &lt;td&gt;机器人资深硬件工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;一星机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;苏州园区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;一星机器人&lt;/td&gt;
          &lt;td&gt;硬件开放工程师&lt;/td&gt;
          &lt;td&gt;苏州园区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.28&lt;/td&gt;
          &lt;td&gt;卓驭(大疆车载, 深圳)&lt;/td&gt;
          &lt;td&gt;SLAM/决策规划/大模型&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;银河通用&lt;/td&gt;
          &lt;td&gt;灵巧手操作/机器人全身规划/机器人规划/强化学习控制算法工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;宇树科技&lt;/td&gt;
          &lt;td&gt;机器人运动控制/深度强化学习/激光SLAM/AI算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.26&lt;/td&gt;
          &lt;td&gt;跨维（深圳）智能数字科技有限公司&lt;/td&gt;
          &lt;td&gt;机器人算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.26&lt;/td&gt;
          &lt;td&gt;穹彻智能(上海)&lt;/td&gt;
          &lt;td&gt;机械臂运动规划与控制&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;普渡机器人/零次方(深圳)&lt;/td&gt;
          &lt;td&gt;强化学习/运动控制算法/具身通用操作&lt;/td&gt;
          &lt;td&gt;实习/社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;唯实具身智能(北京)&lt;/td&gt;
          &lt;td&gt;步态算法工程师&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;莱福(北京)&lt;/td&gt;
          &lt;td&gt;机器人运动控制&lt;/td&gt;
          &lt;td&gt;实习/社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.25&lt;/td&gt;
          &lt;td&gt;逐际动力(北京/深圳)&lt;/td&gt;
          &lt;td&gt;具身大模型/RLHF/数据/后端/AI软件/Python后端/前端/人形全身控制/产品经理&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;同方鼎欣科技股份有限公司&lt;/td&gt;
          &lt;td&gt;具身机器人系统工程师&lt;/td&gt;
          &lt;td&gt;北京海淀区-全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;自变量机器人(深圳/北京)&lt;/td&gt;
          &lt;td&gt;强化学习算法工程师/多模态理解算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;逐际动力&lt;/td&gt;
          &lt;td&gt;RLHF强化学习/多模态具身大模型/具身仿真Benchmark&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;傅里叶智能&lt;/td&gt;
          &lt;td&gt;视觉感知工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.6&lt;/td&gt;
          &lt;td&gt;无界智慧&lt;/td&gt;
          &lt;td&gt;机器人操作工程师/医疗Agent开发工程师/机器人导航工程师/机器人硬件运动控制工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.5&lt;/td&gt;
          &lt;td&gt;深圳无界智慧&lt;/td&gt;
          &lt;td&gt;机器人导航/医疗Agent开发工程师/具身操作算法&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.4&lt;/td&gt;
          &lt;td&gt;无界智慧&lt;/td&gt;
          &lt;td&gt;操作算法/导航算法/运动控制&lt;/td&gt;
          &lt;td&gt;社招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.4&lt;/td&gt;
          &lt;td&gt;它石智航&lt;/td&gt;
          &lt;td&gt;机器人算法工程师-强化学习/机器人SLAM算法工程师/机器人算法工程师-决策规划&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.2&lt;/td&gt;
          &lt;td&gt;银河通用&lt;/td&gt;
          &lt;td&gt;具身智能课程研究员&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.22&lt;/td&gt;
          &lt;td&gt;安克创新(深圳/北京/上海/杭州)&lt;/td&gt;
          &lt;td&gt;具身智能系列岗位&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;PNDbotics(北京)&lt;/td&gt;
          &lt;td&gt;人形机器人与生成式AI&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;留形科技(深圳/香港)&lt;/td&gt;
          &lt;td&gt;三维重建/SLAM/VLA/定位/结构&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;它石智航(上海)&lt;/td&gt;
          &lt;td&gt;机器人运动控制/计算机视觉/感知算法/端到端/VLA算法/SLAM算法/机器人软件开发/底软开发/自动驾驶开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;智元机器人(北京/上海)&lt;/td&gt;
          &lt;td&gt;人形模仿学习/强化学习&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;小米机器人(北京亦庄)&lt;/td&gt;
          &lt;td&gt;双足人形&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;小雨机器人(北京)&lt;/td&gt;
          &lt;td&gt;RL/运控/VSLAM/软件开发&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;RoboScience(北京)&lt;/td&gt;
          &lt;td&gt;具身智能算法&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;Light Robotics(上海/新加坡)&lt;/td&gt;
          &lt;td&gt;算法/软件/硬件/产品&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.28&lt;/td&gt;
          &lt;td&gt;地瓜机器人(北京)&lt;/td&gt;
          &lt;td&gt;VLA/多模态infra/机器人应用开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.26&lt;/td&gt;
          &lt;td&gt;无限工坊(上海)&lt;/td&gt;
          &lt;td&gt;具身智能算法研发&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.21&lt;/td&gt;
          &lt;td&gt;千觉机器人(上海)&lt;/td&gt;
          &lt;td&gt;触觉传感器/电机/算法/结构/材料/机械/强化学习/仿真/嵌入式软件工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;Industrial Next(苏州)&lt;/td&gt;
          &lt;td&gt;工业机器人多模态感知与自主决策算法/强化学习/模仿学习/VLA/仿真训练/实际落地验证&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;阿加犀智能科技(成都)&lt;/td&gt;
          &lt;td&gt;具身智能&lt;/td&gt;
          &lt;td&gt;暑期实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.5&lt;/td&gt;
          &lt;td&gt;浙江人形机器人中心&lt;/td&gt;
          &lt;td&gt;VLA,灵巧手，操作，仿真急招&lt;/td&gt;
          &lt;td&gt;校招/社招/实习皆可&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.19&lt;/td&gt;
          &lt;td&gt;银河通用&lt;/td&gt;
          &lt;td&gt;三维重建&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;星海图&lt;/td&gt;
          &lt;td&gt;VLA/算法/Infra/三维重建/机械&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;DJI大疆&lt;/td&gt;
          &lt;td&gt;图像算法&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;DJI大疆&lt;/td&gt;
          &lt;td&gt;数据闭环工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;银河通用(北京)&lt;/td&gt;
          &lt;td&gt;足式控制&lt;/td&gt;
          &lt;td&gt;全职急招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;傅利叶智能(上海)&lt;/td&gt;
          &lt;td&gt;具身智能VLA多模态大模型&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;灵初智能&lt;/td&gt;
          &lt;td&gt;2025春季招聘&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.3&lt;/td&gt;
          &lt;td&gt;逐际动力(北京/深圳)&lt;/td&gt;
          &lt;td&gt;具身算法/强化学习工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;众擎机器人&lt;/td&gt;
          &lt;td&gt;运动控制&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;星动纪元&lt;/td&gt;
          &lt;td&gt;大模型算法/电机控制&lt;/td&gt;
          &lt;td&gt;校招/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;石头科技&lt;/td&gt;
          &lt;td&gt;运动控制/导航/视觉&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;云鲸智能&lt;/td&gt;
          &lt;td&gt;运筹优化/机械臂操作/3D感知&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;海康机器人&lt;/td&gt;
          &lt;td&gt;运动控制/计算机视觉&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;它石智航&lt;/td&gt;
          &lt;td&gt;运动控制/SLAM/感知&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.26&lt;/td&gt;
          &lt;td&gt;松灵机器人&lt;/td&gt;
          &lt;td&gt;2025年春季社招&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;荣耀(北京)&lt;/td&gt;
          &lt;td&gt;机器人软件系统开发工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;Neura Robotics (Germany)&lt;/td&gt;
          &lt;td&gt;Embodied AI&lt;/td&gt;
          &lt;td&gt;Intern/Expert&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;小鹏机器人&lt;/td&gt;
          &lt;td&gt;后训练/RL/reasoning/agentic llm/world model/humanoid vla/机器人仿真&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;自变量机器人(深圳/北京)&lt;/td&gt;
          &lt;td&gt;多模态大模型/数据筛选算法/运动控制&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;松应科技(上海/深圳/北京)&lt;/td&gt;
          &lt;td&gt;3D仿真引擎开发/强化学习/数据系统/系统测试和解决方案/产品经理&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;智元机器人&lt;/td&gt;
          &lt;td&gt;多模态大模型/空间智能算法/具身算法/强化学习算法&lt;/td&gt;
          &lt;td&gt;研究员/工程师/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;新松机器人(沈阳)&lt;/td&gt;
          &lt;td&gt;视觉算法工程师/具身智能专家/控制算法工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;国家地方共建人形机器人创新中心/人形机器人(上海)有限公司&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2025校招/社招集中招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;星尘智能&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2025校招/社招/实习集中招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.4&lt;/td&gt;
          &lt;td&gt;星尘智能&lt;/td&gt;
          &lt;td&gt;机器人交互研究实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.2&lt;/td&gt;
          &lt;td&gt;光轮智能&lt;/td&gt;
          &lt;td&gt;模仿学习/强化学习算法实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.19&lt;/td&gt;
          &lt;td&gt;梅卡曼德机器人&lt;/td&gt;
          &lt;td&gt;VLA全职工程师/实习&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.18&lt;/td&gt;
          &lt;td&gt;VLAI未来动力&lt;/td&gt;
          &lt;td&gt;强化学习全职工程师/实习/远程&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.17&lt;/td&gt;
          &lt;td&gt;NOETIX Robotics&lt;/td&gt;
          &lt;td&gt;算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.13&lt;/td&gt;
          &lt;td&gt;千寻智能&lt;/td&gt;
          &lt;td&gt;具身业务算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.13&lt;/td&gt;
          &lt;td&gt;艾欧智能&lt;/td&gt;
          &lt;td&gt;具身智能实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;北京启物科技&lt;/td&gt;
          &lt;td&gt;机器人算法实习/校招-操作/导航/仿真&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;银河通用人型机器人&lt;/td&gt;
          &lt;td&gt;强化学习or运动规划实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.12&lt;/td&gt;
          &lt;td&gt;星尘智能&lt;/td&gt;
          &lt;td&gt;具身智能算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;极佳科技&lt;/td&gt;
          &lt;td&gt;具身智能机器人算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;速腾聚创&lt;/td&gt;
          &lt;td&gt;多模态大模型算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;中科慧灵&lt;/td&gt;
          &lt;td&gt;VLA方向&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;比亚迪&lt;/td&gt;
          &lt;td&gt;人型机器人算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;国家地方共建人形机器人创新中心/人形机器人(上海)有限公司&lt;/td&gt;
          &lt;td&gt;具身大模型实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;清华大学&amp;amp;地瓜机器人&lt;/td&gt;
          &lt;td&gt;具身智能算法实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;逐际动力&lt;/td&gt;
          &lt;td&gt;具身大模型算法+物理仿真+视频生成+世界模型+运动控制实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.11&lt;/td&gt;
          &lt;td&gt;星海图许华哲组&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;研究院/实验室&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究院实验室&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e9%99%a2%e5%ae%9e%e9%aa%8c%e5%ae%a4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;机构&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;光明实验室(深圳)&lt;/td&gt;
          &lt;td&gt;具身智能机器人研究人员/工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.28&lt;/td&gt;
          &lt;td&gt;江淮前沿技术协同创新中心&lt;/td&gt;
          &lt;td&gt;博士后一具身智能方向/智能机器人算法专家/多模态大模型研发工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.8&lt;/td&gt;
          &lt;td&gt;中国空间技术研究院CAST+AI·R人工智能专班&lt;/td&gt;
          &lt;td&gt;大模型算法研发岗/大模型数据治理岗/大模型应用系统开发岗/具身智能算法研发岗&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.18&lt;/td&gt;
          &lt;td&gt;华为制造部&lt;/td&gt;
          &lt;td&gt;视觉算法开发与高级应用工程师/具身智能机器人应用高级工程师&lt;/td&gt;
          &lt;td&gt;2026届博士&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.5&lt;/td&gt;
          &lt;td&gt;智源研究院&lt;/td&gt;
          &lt;td&gt;机器人系统/数据/多模态/具身智能等方向&lt;/td&gt;
          &lt;td&gt;校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.1&lt;/td&gt;
          &lt;td&gt;智源&lt;/td&gt;
          &lt;td&gt;具身大模型研究员&lt;/td&gt;
          &lt;td&gt;社招/校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.31&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室具身智能中心&lt;/td&gt;
          &lt;td&gt;具身智能-AIGC青年研究员/仿真平台青年研究员/人体运动策略青年研究员/具身智能大模型青年研究员/足式机器人青年研究员/强化学习青年研究员/AIGC算法实习生&lt;/td&gt;
          &lt;td&gt;校招/实习/社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;中国科学院空间应用工程与技术中心 空间实验技术研究室&lt;/td&gt;
          &lt;td&gt;空间具身智能系统/空间灵巧机构研发岗/传感器与感知算法开发&lt;/td&gt;
          &lt;td&gt;2025年招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.27&lt;/td&gt;
          &lt;td&gt;嘉陵江实验室&lt;/td&gt;
          &lt;td&gt;大模型工程师(含LLM 与时空模型)/嵌入式与实时系统工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.25&lt;/td&gt;
          &lt;td&gt;深圳人工智能与机器人研究院&lt;/td&gt;
          &lt;td&gt;大模型算法研究员/工程师、具身智能研究员/工程师、机器人导航算法研究员/工程师&lt;/td&gt;
          &lt;td&gt;2026届招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.24&lt;/td&gt;
          &lt;td&gt;中豫具身智能实验室&lt;/td&gt;
          &lt;td&gt;机械工程/自动化/机器人学等专业相关科研岗&lt;/td&gt;
          &lt;td&gt;博士研究生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.18&lt;/td&gt;
          &lt;td&gt;清华+智谱&lt;/td&gt;
          &lt;td&gt;AI基础模型研究/AI机器人技术研究/记忆机理与实时学习算法研究&lt;/td&gt;
          &lt;td&gt;博士后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;北京智源人工智能研究院&lt;/td&gt;
          &lt;td&gt;机器人系统/多模态/数据/信息检索与知识计算/具身智能&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.8&lt;/td&gt;
          &lt;td&gt;上海算法创新研究院/上海交大人工智能学院具身智能团队&lt;/td&gt;
          &lt;td&gt;机器人学习研究工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.4&lt;/td&gt;
          &lt;td&gt;北大银河通用&lt;/td&gt;
          &lt;td&gt;具身智能科研&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.3&lt;/td&gt;
          &lt;td&gt;IDEA机器人感知/VLA 研发招聘&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.30&lt;/td&gt;
          &lt;td&gt;中国信通院人工智能研究所&lt;/td&gt;
          &lt;td&gt;具身智能研究岗/具身智能实验室运营岗&lt;/td&gt;
          &lt;td&gt;全职/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;中国信息通信研究院&lt;/td&gt;
          &lt;td&gt;具身智能研究员&lt;/td&gt;
          &lt;td&gt;北京海淀区&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.4&lt;/td&gt;
          &lt;td&gt;杭州湾具身智能创新中心&lt;/td&gt;
          &lt;td&gt;机器人数采实习生/机器人数据审核实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;三维重建&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室夏季招聘&lt;/td&gt;
          &lt;td&gt;包含具身岗位&lt;/td&gt;
          &lt;td&gt;领军科学家/青年科学家/PostDoc/工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;北京大学信息技术高等研究院MAII Lab&lt;/td&gt;
          &lt;td&gt;机器学习/具身智能&lt;/td&gt;
          &lt;td&gt;PostDoc&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.26&lt;/td&gt;
          &lt;td&gt;北京具身智能机器人创新中心&lt;/td&gt;
          &lt;td&gt;具身智能数据实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.15&lt;/td&gt;
          &lt;td&gt;上海算法创新研究院&amp;amp;上海交大人工智能学院&lt;/td&gt;
          &lt;td&gt;空间智能/具身智能&lt;/td&gt;
          &lt;td&gt;实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.25&lt;/td&gt;
          &lt;td&gt;招商局先进院(深圳)&lt;/td&gt;
          &lt;td&gt;具身数据/多模态/动作捕捉/VSLAM/人体姿态估计/大模型训练/大模型数据工程/机械臂运动控制&lt;/td&gt;
          &lt;td&gt;全职/实习（可给香港身份）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.17&lt;/td&gt;
          &lt;td&gt;OpenDriveLab&lt;/td&gt;
          &lt;td&gt;RAP/PostDoc/机器人硬件工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室&lt;/td&gt;
          &lt;td&gt;2026联培博士招生简章&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.8&lt;/td&gt;
          &lt;td&gt;深圳河套学院&lt;/td&gt;
          &lt;td&gt;24/25/26级各方向PhD&lt;/td&gt;
          &lt;td&gt;4月10日截止&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;深圳科创学院具身智能团队&lt;/td&gt;
          &lt;td&gt;机器人系统工程师/具身智能算法工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;上海期智研究院&lt;/td&gt;
          &lt;td&gt;全职研究员&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;智源研究院&lt;/td&gt;
          &lt;td&gt;端到端VLA&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.17&lt;/td&gt;
          &lt;td&gt;北京通用人工智能研究院(BIGAI)机器人团队&lt;/td&gt;
          &lt;td&gt;跨本体操作策略/异构多机任务规划&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.10&lt;/td&gt;
          &lt;td&gt;西安交大/优艾智合具身智能机器人联合研究院&lt;/td&gt;
          &lt;td&gt;海外优青/青年拔尖人才/博士后&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;复旦大学可信具身智能研究院&lt;/td&gt;
          &lt;td&gt;海外优青&lt;/td&gt;
          &lt;td&gt;博士后/助理教授/副教授/教授&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.6&lt;/td&gt;
          &lt;td&gt;17家国企/央企/研究院&lt;/td&gt;
          &lt;td&gt;具身智能招聘岗位汇总&lt;/td&gt;
          &lt;td&gt;具身智能之心(微信: AIDriver002)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.2.17&lt;/td&gt;
          &lt;td&gt;CVTE中央研究院机器人创新部&lt;/td&gt;
          &lt;td&gt;机器人具身智能算法实习生/全职&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;汽车/自动驾驶&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;汽车自动驾驶&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b1%bd%e8%bd%a6%e8%87%aa%e5%8a%a8%e9%a9%be%e9%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;公司&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;东风汽车研发总院(武汉)&lt;/td&gt;
          &lt;td&gt;模型训练/运控/机械设计&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.6&lt;/td&gt;
          &lt;td&gt;苏州博世XC事业部&lt;/td&gt;
          &lt;td&gt;自动驾驶高精地图算法工程师(两年以上智驾行业经验)&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.25&lt;/td&gt;
          &lt;td&gt;小鹏汽车&lt;/td&gt;
          &lt;td&gt;多模态理解/多模态生成/三维视觉/自动驾驶/大模型/CV/Audio/NLP&lt;/td&gt;
          &lt;td&gt;社招(急)/校招/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.14&lt;/td&gt;
          &lt;td&gt;华为车BU天才少年&lt;/td&gt;
          &lt;td&gt;自动驾驶世界模型/VLA/强化学习/并行训练/仿真/数据/多模态理解生成&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.11&lt;/td&gt;
          &lt;td&gt;小鹏汽车&lt;/td&gt;
          &lt;td&gt;人形机器人运动控制/VLA/VLN/灵巧手/多模态/大模型&lt;/td&gt;
          &lt;td&gt;2026届校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;自动驾驶与具身智能算法研究员 (VLA/具身方向)&lt;/td&gt;
          &lt;td&gt;社招/校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.1&lt;/td&gt;
          &lt;td&gt;华为&lt;/td&gt;
          &lt;td&gt;研发算法岗&lt;/td&gt;
          &lt;td&gt;应届生/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;橙子运力&lt;/td&gt;
          &lt;td&gt;规划算法工程师&lt;/td&gt;
          &lt;td&gt;正式/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;理想汽车『自动驾驶』&lt;/td&gt;
          &lt;td&gt;大模型/端到端/强化学习算法工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.27&lt;/td&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;机器人操作抓取/足式机器人强化学习/大模型强化学习/多模态/机器学习/感知/机器人算法工程师&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.23&lt;/td&gt;
          &lt;td&gt;蔚来&lt;/td&gt;
          &lt;td&gt;春季招聘&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;科大讯飞&lt;/td&gt;
          &lt;td&gt;自动驾驶工程师&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;文远知行&lt;/td&gt;
          &lt;td&gt;春招&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;酷睿程(地平线大众合资)&lt;/td&gt;
          &lt;td&gt;生成式算法/规控/控制/SLAM&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.20&lt;/td&gt;
          &lt;td&gt;小马智行&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2025校招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.20&lt;/td&gt;
          &lt;td&gt;地平线(北京/上海/南京)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;2026春季实习生招聘&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.15&lt;/td&gt;
          &lt;td&gt;Hyundai (US)&lt;/td&gt;
          &lt;td&gt;Autonomous Driving&lt;/td&gt;
          &lt;td&gt;Intern&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.11&lt;/td&gt;
          &lt;td&gt;IAV GmbH (Germany)&lt;/td&gt;
          &lt;td&gt;Autonomous Driving: Path Planning in Unstructured Environments&lt;/td&gt;
          &lt;td&gt;Internship&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;其他（教育/投资/内容/医疗等）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;其他教育投资内容医疗等&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%b6%e4%bb%96%e6%95%99%e8%82%b2%e6%8a%95%e8%b5%84%e5%86%85%e5%ae%b9%e5%8c%bb%e7%96%97%e7%ad%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;日期&lt;/th&gt;
          &lt;th&gt;机构&lt;/th&gt;
          &lt;th&gt;职位方向&lt;/th&gt;
          &lt;th&gt;类型&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;元枢智汇(上海)&lt;/td&gt;
          &lt;td&gt;AI数据开源社区运营&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;上海人工智能实验室具身智能中心&lt;/td&gt;
          &lt;td&gt;具身开源社区运营&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;秦皇岛中秦智能装备有限公司(秦皇岛)&lt;/td&gt;
          &lt;td&gt;机械研发/机器人&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;长智具身智能(海南三亚)&lt;/td&gt;
          &lt;td&gt;销售代表/解决方案工程师/研发工程师/数采员&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;新生纪智能科技有限公司(德国 可远程)&lt;/td&gt;
          &lt;td&gt;FAE工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;西湖机器人(杭州)&lt;/td&gt;
          &lt;td&gt;强化学习/具身智能算法/虚幻引擎/VR/SLAM/机械结构/市场&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.11.24&lt;/td&gt;
          &lt;td&gt;南洋理工大学LinsLab(新加坡)&lt;/td&gt;
          &lt;td&gt;灵巧操作/VLA/世界模型&lt;/td&gt;
          &lt;td&gt;26Fall PhD&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.19&lt;/td&gt;
          &lt;td&gt;深蓝学院&lt;/td&gt;
          &lt;td&gt;机器人算法实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.14&lt;/td&gt;
          &lt;td&gt;深蓝学院&lt;/td&gt;
          &lt;td&gt;人工智能教育产品经理/人工智能教研&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.18&lt;/td&gt;
          &lt;td&gt;Motphys&lt;/td&gt;
          &lt;td&gt;初级引擎开发工程师/具身智能场景美术/具身智能技术应用工程师&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.10.10&lt;/td&gt;
          &lt;td&gt;Motphys(武汉)&lt;/td&gt;
          &lt;td&gt;仿真解决方案/工具链/视觉感知/仿真资产制作/产品经理/仿真训练/物理引擎开发&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.30&lt;/td&gt;
          &lt;td&gt;NVIDIA（北京）&lt;/td&gt;
          &lt;td&gt;三维重建/世界模型/VLM/VLA&lt;/td&gt;
          &lt;td&gt;Solution Architect全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.26&lt;/td&gt;
          &lt;td&gt;中金公司&lt;/td&gt;
          &lt;td&gt;AI&amp;amp;机器人产业研究&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.17&lt;/td&gt;
          &lt;td&gt;辉羲智能&lt;/td&gt;
          &lt;td&gt;AI编译器工程师/专家、NPU算子开发工程师/专家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.17&lt;/td&gt;
          &lt;td&gt;Leading Future&lt;/td&gt;
          &lt;td&gt;多模态大模型科学家&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.11&lt;/td&gt;
          &lt;td&gt;凤麟核集团&lt;/td&gt;
          &lt;td&gt;具身智能机器人项目经理/AI医疗产品经理/AI医学影像软件工程师&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;具身智能内容运营(深圳)&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;机器人数据工程师(深圳)&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;深度学习平台工程师(深圳)&lt;/td&gt;
          &lt;td&gt;实习/全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;腿足/人形全身控制算法专家(深圳)&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;科研采购工程师(深圳)&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.9.10&lt;/td&gt;
          &lt;td&gt;香港大学MMLab&lt;/td&gt;
          &lt;td&gt;深度学习推理工程师(深圳)&lt;/td&gt;
          &lt;td&gt;全职/实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.19&lt;/td&gt;
          &lt;td&gt;自动驾驶之心&lt;/td&gt;
          &lt;td&gt;内容运营(自驾/大模型/具身相关研究方向)&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.16&lt;/td&gt;
          &lt;td&gt;Leading Future&lt;/td&gt;
          &lt;td&gt;多模态大模型科学家（Embodied AI / Robotics Foundation Model）&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.15&lt;/td&gt;
          &lt;td&gt;深蓝学院&lt;/td&gt;
          &lt;td&gt;机器人算法实习生/机械臂研发实习生&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.9&lt;/td&gt;
          &lt;td&gt;智子力控(宁波)&lt;/td&gt;
          &lt;td&gt;机器人电气与控制工程师(ROS和柔顺控制)&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.7&lt;/td&gt;
          &lt;td&gt;稳正资产&lt;/td&gt;
          &lt;td&gt;智能硬件投资总监/具身智能投资总监&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.8.1&lt;/td&gt;
          &lt;td&gt;天奇股份&lt;/td&gt;
          &lt;td&gt;机器人视觉工程师/机器人工程师/机器人销售经理/机器人数据采集工程师&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.28&lt;/td&gt;
          &lt;td&gt;亮源新创(北京/上海/深圳)&lt;/td&gt;
          &lt;td&gt;产品经理&lt;/td&gt;
          &lt;td&gt;全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.26&lt;/td&gt;
          &lt;td&gt;中能坤域科技控股(浙江)有限公司&lt;/td&gt;
          &lt;td&gt;具身智能/产品研发/产品运营等相关岗位&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.16&lt;/td&gt;
          &lt;td&gt;小米(北京)&lt;/td&gt;
          &lt;td&gt;机器人多模态大模型研究专家&lt;/td&gt;
          &lt;td&gt;急招全职&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.7&lt;/td&gt;
          &lt;td&gt;稳正资产&lt;/td&gt;
          &lt;td&gt;智能硬件投资总监/具身智能投资总监&lt;/td&gt;
          &lt;td&gt;社招&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.7.2&lt;/td&gt;
          &lt;td&gt;机器人算法实习生(具身智能方向)&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.24&lt;/td&gt;
          &lt;td&gt;小米(北京)&lt;/td&gt;
          &lt;td&gt;具身智能/世界模型&lt;/td&gt;
          &lt;td&gt;暑期实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.11&lt;/td&gt;
          &lt;td&gt;具身智能之心&lt;/td&gt;
          &lt;td&gt;课程讲师/硬件开发者&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.6.6&lt;/td&gt;
          &lt;td&gt;具身研习社&lt;/td&gt;
          &lt;td&gt;具身智能机器人深度内容作者&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.5.21&lt;/td&gt;
          &lt;td&gt;中金研究院(北京)&lt;/td&gt;
          &lt;td&gt;具身智能产业研究&lt;/td&gt;
          &lt;td&gt;实习&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.20&lt;/td&gt;
          &lt;td&gt;北京中关村学院&lt;/td&gt;
          &lt;td&gt;具身智能方向&lt;/td&gt;
          &lt;td&gt;研究员/工程师/实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;北京中关村学院&lt;/td&gt;
          &lt;td&gt;全球招募副院长/助理院长&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.10&lt;/td&gt;
          &lt;td&gt;中关村人工智能研究院&lt;/td&gt;
          &lt;td&gt;2025年超能实习生&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.4.6&lt;/td&gt;
          &lt;td&gt;音波迭代Embodied Pioneering&lt;/td&gt;
          &lt;td&gt;一级市场具身智能方向&lt;/td&gt;
          &lt;td&gt;投资实习生&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;小米&lt;/td&gt;
          &lt;td&gt;传统运动控制/强化学习运动控制/SLAM&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.28&lt;/td&gt;
          &lt;td&gt;海尔&lt;/td&gt;
          &lt;td&gt;SLAM/运动控制/抓取/视觉&lt;/td&gt;
          &lt;td&gt;算法工程师/算法总监&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.29&lt;/td&gt;
          &lt;td&gt;汇川技术&lt;/td&gt;
          &lt;td&gt;传统运动控制/仿真/电机控制&lt;/td&gt;
          &lt;td&gt;算法工程师&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2025.3.23&lt;/td&gt;
          &lt;td&gt;国金证券具身智能组&lt;/td&gt;
          &lt;td&gt;人形机器人板块&lt;/td&gt;
          &lt;td&gt;实习生(可远程可留用)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;17家国企/央企/研究院岗位包括：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上海电气中央研究院 - 具身智能机器人算法/增材仿真/模拟计算/优化算法 - 校招&lt;/li&gt;
&lt;li&gt;中国电信人工智能研究院(TeleAI) - 具身智能大模型/具身仿真/具身硬件/机器人操作控制/具身感知与规划/灵巧操作/嵌入式开发 - 校招&lt;/li&gt;
&lt;li&gt;中兵智能创新研究院 - 机器人算法研究员&lt;/li&gt;
&lt;li&gt;中兴通讯 - 强化学习/视觉大模型/人形机器人机电 - 校招&lt;/li&gt;
&lt;li&gt;中国移动具身智能产业创新中心&lt;/li&gt;
&lt;li&gt;西安航天自动化 - 机器人开发工程师 - 校招&lt;/li&gt;
&lt;li&gt;国核电站运行服务技术有限公司 - 机器人电气工程师&lt;/li&gt;
&lt;li&gt;中科航天人才服务有限公司 - 人形机器人研发总监&lt;/li&gt;
&lt;li&gt;南方海洋科学与工程广东省实验室 - 水下机器人运动控制&lt;/li&gt;
&lt;li&gt;新兴际华北京智研院 - 智能机器人专业总工/总师&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>具身调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-14/</guid>
      <description>
        
        
        &lt;h1&gt;具身调研&lt;/h1&gt;&lt;p&gt;对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。&lt;/p&gt;
&lt;p&gt;本调研主要基于**&lt;a href=&#34;https://github.com/jiangranlv/embodied-ai-start&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PKU EPIC Lab&lt;/a&gt;&lt;strong&gt;、&lt;/strong&gt;&lt;a href=&#34;https://github.com/TianxingChen/Embodied-AI-Guide&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lumina具身智能社区&lt;/a&gt;**&lt;/p&gt;
&lt;h2&gt;一、基础概念 (Basic Concepts)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一基础概念-basic-concepts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1、 什么是具身智能&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-什么是具身智能&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。&lt;/p&gt;
&lt;p&gt;典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心特征：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有多模态感知能力（视觉、触觉、语音等）&lt;/li&gt;
&lt;li&gt;能够执行动作并影响环境&lt;/li&gt;
&lt;li&gt;学习可以通过与环境交互而不仅仅是被动监督完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 具身智能与其他AI的区别&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-具身智能与其他ai的区别&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96ai%e7%9a%84%e5%8c%ba%e5%88%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;如何scale up机器人数据？&lt;/strong&gt; 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？&lt;/strong&gt; 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 研究具身智能的核心原则 (Core Principles)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-研究具身智能的核心原则-core-principles&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；&lt;/p&gt;
&lt;p&gt;但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）&lt;/p&gt;
&lt;p&gt;必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？&lt;/p&gt;
&lt;h2&gt;二、AI and Robotics Basis&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二ai-and-robotics-basis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cai-and-robotics-basis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-Embodied-AI&lt;/strong&gt;
王鹤老师《具身智能导论》，找找类似课程替代&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-CV&lt;/strong&gt;
Stanford CS231N&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning (CS285)&lt;/strong&gt;
Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;三、研究平台与工具&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三研究平台与工具&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Simulation Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-simulation-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-simulation-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;2. Robot Platform&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-platform&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-platform&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;3. Daily ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-daily-arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-daily-arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了：
具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jiangranlv/robotics_arXiv_daily&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jiangranlv/robotics_arXiv_daily&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;四、Research Field on Robots&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四research-field-on-robots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bresearch-field-on-robots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Grasping&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-grasping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-grasping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。&lt;/p&gt;
&lt;p&gt;常见任务有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single object grasping（单物体抓取）&lt;/strong&gt;：抓取一个物体，物体通常放在桌子上。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clutter scene grasping（堆叠场景抓取）&lt;/strong&gt;：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional grasping（带语义抓取）&lt;/strong&gt;：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用机械手末端有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suction cup（吸盘）&lt;/strong&gt;：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel gripper（平行夹抓）&lt;/strong&gt;：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-fingered hand（多指手）&lt;/strong&gt;，又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-loop methods（开环执行）&lt;/strong&gt;：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closed-loop methods（闭环执行）&lt;/strong&gt;：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Manipulation&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-manipulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-manipulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。&lt;/p&gt;
&lt;p&gt;**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）
**Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。
**Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。
**Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。
**Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。
**Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同&lt;/p&gt;
&lt;h3&gt;3. Navigation(NOW)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-navigationnow&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-navigationnow&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。&lt;/p&gt;
&lt;p&gt;常见任务包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Point Goal Navigation (PointNav)&lt;/strong&gt;：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Goal Navigation (ObjectNav)&lt;/strong&gt;：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Navigation (VLN)&lt;/strong&gt;：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embodied Question Answering (EQA)&lt;/strong&gt;：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：机器人持续感知并跟随动态目标（如人或移动物体）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map-based Navigation&lt;/strong&gt;：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompting-Large-Model Navigation&lt;/strong&gt;：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video-based VLM Navigation&lt;/strong&gt;：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unified Embodied Navigation&lt;/strong&gt;：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Locomotion&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-locomotion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-locomotion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。&lt;/p&gt;
&lt;p&gt;技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning from manually designed reward&lt;/strong&gt; (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from human data&lt;/strong&gt; (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】
如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -&amp;gt; whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下一个阶段会涉及到一点除了control之外的东西，就是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引入【视觉】实现户外自主化（perceptive locomotion）&lt;/strong&gt;；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入【物体】实现loco-manipulation&lt;/strong&gt;；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对上述两种task的组合&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调【语义的泛化性】&lt;/strong&gt;，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调一些特殊的capability&lt;/strong&gt;（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;五、Learning based Research Field&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五learning-based-research-field&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94learning-based-research-field&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Few-shot Imitation Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-few-shot-imitation-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-few-shot-imitation-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：Behavior Cloning、DAgger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当前主流方法&lt;/strong&gt;：ACT、Diffusion Policy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。&lt;/p&gt;
&lt;h3&gt;2. Robot Foundation Model&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-foundation-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-foundation-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。
目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenVLA&lt;/strong&gt;：第一个开源且易于follow的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi0 / Pi0.5&lt;/strong&gt;：目前公认最work的VLA，10K+ hours teleop data训练的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraspVLA&lt;/strong&gt;：基于纯仿真数据的抓取任务的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有少量工作没有借助VLM&lt;/strong&gt;，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Sim-to-Real Reinforcement Learning (Distillation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-sim-to-real-reinforcement-learning-distillation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-sim-to-real-reinforcement-learning-distillation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。&lt;/p&gt;
&lt;p&gt;目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。&lt;/p&gt;
&lt;p&gt;核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。&lt;/p&gt;
&lt;h3&gt;4. Real-World Reinforcement Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-real-world-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-real-world-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-world RL 指直接在现实环境中进行探索式学习。&lt;/p&gt;
&lt;p&gt;这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**从零开始的真实世界强化学习：**Hil-Serl&lt;/li&gt;
&lt;li&gt;**基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. World Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-world-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-world-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代表性工作包括 Dreamer 系列&lt;/strong&gt;（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。&lt;/p&gt;
&lt;h2&gt;六、相关领域&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六相关领域&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Graphics&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-graphics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-graphics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。&lt;/p&gt;
&lt;p&gt;**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等
**Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。
**系统性学习图形学推荐课程：**Games 101, 103&lt;/p&gt;
&lt;h3&gt;2. Hardware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tele-operation（遥操作）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。
**主从臂系统：**如 Gello，实现高精度的力控遥操作。
**可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。
&lt;strong&gt;Sensors（传感器）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。&lt;/li&gt;
&lt;li&gt;**Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。&lt;/li&gt;
&lt;li&gt;**Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。&lt;/li&gt;
&lt;li&gt;**Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mainstream Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-mainstream-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-mainstream-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Transformer&lt;/li&gt;
&lt;li&gt;Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Foundation Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-foundation-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-foundation-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。&lt;/li&gt;
&lt;li&gt;CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。&lt;/li&gt;
&lt;li&gt;VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. 3D Vision&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-3d-vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-3d-vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维生成与重建&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。&lt;/li&gt;
&lt;li&gt;**单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。&lt;/li&gt;
&lt;li&gt;**单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。&lt;/li&gt;
&lt;li&gt;**位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维表示&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。&lt;/li&gt;
&lt;li&gt;**点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。&lt;/li&gt;
&lt;li&gt;**Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维理解&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>港硕申请回顾</title>
      <link>http://localhost:1313/blog/2025/2025-11-28-hk-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-28-hk-master/</guid>
      <description>
        
        
        &lt;h1&gt;港硕申请回顾&lt;/h1&gt;&lt;h2&gt;推免失败&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;推免失败&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a8%e5%85%8d%e5%a4%b1%e8%b4%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本科大三时经历了一段痛苦的PTSD，导致本科规划出现了巨大偏差。理论上应该是以&lt;strong&gt;二作CCF文章&lt;/strong&gt;加上刚刚过推免线的绩点保研，但后来拼尽全力把3年GPA稳到了&lt;strong&gt;88分&lt;/strong&gt;，也算刚刚好拿到推免资格（22/25）。由于对直接读博产生了巨大阴影，且不太喜欢生仪研究院的4个方向，因而选择无论如何都要读一个&lt;strong&gt;CS/AI相关&lt;/strong&gt;的硕士。&lt;/p&gt;
&lt;p&gt;然而硕士申请需要面临committee的拷打，因此推免&lt;strong&gt;浙计&lt;/strong&gt;失败。面试时被问到的5个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;排序算法&lt;/li&gt;
&lt;li&gt;联邦学习&lt;/li&gt;
&lt;li&gt;数组排序&lt;/li&gt;
&lt;li&gt;职业规划&lt;/li&gt;
&lt;li&gt;还有一个忘记了（记忆自动删除了）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;选择浙计确实有点冲动，主要是有老师愿意给我他们组里专硕的名额，但是老师又不能在委员会里直接捞人（这和博士不一样，但出于上述的原因，我又不愿直博）。在&lt;strong&gt;9月22日&lt;/strong&gt;左右结束之后，就立刻开始捡起托福复习英语了。&lt;/p&gt;
&lt;h2&gt;方向确定与选校&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;方向确定与选校&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%96%b9%e5%90%91%e7%a1%ae%e5%ae%9a%e4%b8%8e%e9%80%89%e6%a0%a1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;大三暑假通过对计院/信电老师（比如计院院长那个组，真是年少轻狂）的套磁，排除了AI4Sci等选项后，确定了可能会感兴趣的方向——&lt;strong&gt;空间智能&lt;/strong&gt;这一块，目前国内也比较火的所谓&lt;strong&gt;具身智能&lt;/strong&gt;。但是机器人这一块我确实不太喜欢，本科的时候一直在避免接触Robotics这个概念，当然也和认识一个比较讨厌的控制学院的同学有关系。不过既然决定了方向，下一步就是选校了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI、CS领域&lt;/strong&gt;，想要获得长足的发展，在我看来只有两个国家可以选择：&lt;strong&gt;中与美&lt;/strong&gt;。美国在我大三的时候特朗普刚开始第二任任期，在签证与大学经费（后果是削减招生名额）以及工作机会（美国本地企业实习）都出现了重大利空，因此不做考虑，当然现在可能又回暖了。但是托福是路径依赖的语言，可以和雅思互相替代。因此选择中国这一块。而在本院预推免结束之后，其他（不被视为下保的）学校的也都差不多来不及报名了，唯一一个中科院大学还留有一点点，思来想去还是放掉选择港硕。&lt;/p&gt;
&lt;p&gt;工科信息这一块，&lt;strong&gt;对口实习甚至比论文一作还重要&lt;/strong&gt;，而新加坡限制中国学生数量并且控制工作签证对外国人的发放，因此不考虑。选择离中国大陆更近的香港。至少因为我还有一个&lt;strong&gt;88的GPA&lt;/strong&gt;，看不太上港五那两所，只选&lt;strong&gt;港三&lt;/strong&gt;申请：&lt;/p&gt;
&lt;h3&gt;第一批申请（9月30日）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第一批申请9月30日&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%b8%80%e6%89%b9%e7%94%b3%e8%af%b79%e6%9c%8830%e6%97%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;港科广&lt;/strong&gt; 红鸟&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;港中深&lt;/strong&gt; MAIR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;港中深&lt;/strong&gt; CS（怕不稳补申）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;第二批申请（10月25日之后，考出托福成绩后补申）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;第二批申请10月25日之后考出托福成绩后补申&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%ac%ac%e4%ba%8c%e6%89%b9%e7%94%b3%e8%af%b710%e6%9c%8825%e6%97%a5%e4%b9%8b%e5%90%8e%e8%80%83%e5%87%ba%e6%89%98%e7%a6%8f%e6%88%90%e7%bb%a9%e5%90%8e%e8%a1%a5%e7%94%b3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CUHK&lt;/strong&gt; AI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUHK&lt;/strong&gt; CS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUHK&lt;/strong&gt; IE&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HKUST&lt;/strong&gt; CS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HKUST&lt;/strong&gt; IE&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;没有申请HKU的原因是，HKU CS属于超级大班，感觉含金量基本上靠的是【香港大学】的综排牌子；第二个原因就是这是唯一一个需要笔面的，虽然笔试难度没有咱ZJU的期末考试难度高，但就很恶心。与之形成鲜明对比的就是CUHK，没有笔面，直接花600RMB去抽奖。
BTW，这里面申请页面的前端UI做的，我没有想到会这么拉，看下来港科广做的是最好看的，港中深其次但是它的密码不支持特殊字符（@#￥这些），关键是还不会专门提醒报错是这个原因，我试了好几次换了几个邮箱都输密码错误，最后才被这个问题的原因猜出来气消了。港中港科两所港校本部的申请界面丑的颇有上世纪网站的美感，值得吐槽的是HKUST的Country那一栏不像其他的学校那样是China(Mainland)，它是The Mainland of China，首字母是T还是M来着，导致第一眼根本看不到，只找到一个Chili&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;申请建议不要请中介，如果你是想要找港新的选校的话，欧美啥的另说，但是这几个地区靠ZJU的同学完全可以自己做到。中介的最大作用就是你去加他跟他聊的第一次，它会给你一些关于流程上的信息，比如我就找了一个中介来帮我介绍港科广的申请流程、面试之类的常识信息，后面直接忽略掉他的消息就行。&lt;/p&gt;
&lt;h2&gt;推荐信与申请进度&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;推荐信与申请进度&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a8%e8%8d%90%e4%bf%a1%e4%b8%8e%e7%94%b3%e8%af%b7%e8%bf%9b%e5%ba%a6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;推荐信这一块还是拜本科期间干了点事，拿到了那位计院老师以及本院书记的&lt;strong&gt;两封强推&lt;/strong&gt;（硕士居然要推荐信），所以基本上刚申请就交了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;11月3日&lt;/strong&gt;，CUHK IE发邮件过来催补材料；&lt;strong&gt;11月5日&lt;/strong&gt;回复我说我的申请会被更深入处理。结果最先发offer的居然是&lt;strong&gt;CUHK的AI&lt;/strong&gt;，在&lt;strong&gt;11月20日&lt;/strong&gt;前。&lt;/p&gt;
&lt;p&gt;因为申请比较晚，&lt;strong&gt;港科广红鸟MPhil&lt;/strong&gt;属于第二批，在&lt;strong&gt;11月26日&lt;/strong&gt;才发面邀，时间为线下的&lt;strong&gt;12月10-11日&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;最终选择与规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;最终选择与规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%80%e7%bb%88%e9%80%89%e6%8b%a9%e4%b8%8e%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;比较感兴趣的&lt;strong&gt;CUHKSZ&lt;/strong&gt;居然还不发笔面通知，这样就只能选择&lt;strong&gt;CUHK本部&lt;/strong&gt;了。交了这个留位费之后，剩下像占尽地理劣势的HKUST其实都可以不用考虑了，甚至CUHKSZ都可以不用考虑，专心等港科广的面试结果。如果成功了，每月发的&lt;strong&gt;10000RMB&lt;/strong&gt;凑起来去掉学费完全能把这&lt;strong&gt;11万留位费&lt;/strong&gt;赚回来。&lt;/p&gt;
&lt;p&gt;最坏的情况是港科广面试不过，那样就需要好好进行后面的规划：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;留服认证问题&lt;/strong&gt;：不同于那两所内地校区的港校，CUHK本部算留学要进行留服认证，但是留学生进央国企的话，很多国央企都要求&lt;strong&gt;本硕一致&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学制与实习规划&lt;/strong&gt;：CUHK的CS是&lt;strong&gt;一年制&lt;/strong&gt;，AI是&lt;strong&gt;1.5年制&lt;/strong&gt;。两者都可以延期毕业从而多次参与秋招，从而争取攒出&lt;strong&gt;两段3个月以上的实习&lt;/strong&gt;出来进入这个还未收敛的具身行业。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信息获取渠道&lt;/strong&gt;：像是毕业去向、具体的规划等还是得上&lt;strong&gt;cc98&lt;/strong&gt;问问校友学长之类的，小红书上面中介之类的杂鱼信息太多了。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;最终结果&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;最终结果&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%80%e7%bb%88%e7%bb%93%e6%9e%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;CUHK MscAI&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;cuhk-mscai&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#cuhk-mscai&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;CUHK无需笔面&lt;/strong&gt;，直接根据申请材料下offer。这里聊一聊一般网上不会详细说明的拿到&lt;strong&gt;conditional offer&lt;/strong&gt;之后的流程：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/cuhk-acceptance-of-admission-offer.png&#34; alt=&#34;CUHK录取通知书接受页面&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里分为两步，首先是&lt;strong&gt;交留位费&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/cuhk-deposit-payment.png&#34; alt=&#34;CUHK留位费支付页面&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这样一来这个offer就会为你保留，你不去的话就会亏掉这一笔钱。留位费理论上应该算是学费的一部分（将在第一学期部分学生费用中抵消），而我感觉&lt;strong&gt;CUHK AI是比较新也比较贵的那一批&lt;/strong&gt;。据称其深圳校区的留位费可以退还&lt;strong&gt;90%&lt;/strong&gt;，不过我在写这篇博客的同时还没有给我下面试通知所以无从验证。&lt;/p&gt;
&lt;p&gt;上图所描述的可选支付方式（&lt;strong&gt;选一个即可&lt;/strong&gt;）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;信用卡支付&lt;/strong&gt;（VISA / 万事达 / 银联）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选对应的选项，点 &amp;ldquo;Confirm&amp;rdquo; 按钮，会跳转到在线信用卡支付页面完成付款。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;电子钱包类&lt;/strong&gt;（微信支付 / 支付宝 / AlipayHK/BoC Pay / 银联 App）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后点 &amp;ldquo;Confirm&amp;rdquo;，会跳转到第三方支付平台 &lt;strong&gt;SwiftPass&lt;/strong&gt; 完成付款。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速支付系统（FPS）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后点 &amp;ldquo;Confirm&amp;rdquo;，跳转到 CUHK 的 FPS 支付系统操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;本地支付方式&lt;/strong&gt;（仅香港地区可用：PPS/ATM/ 网上银行 / 现金 / 支票）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后，先下载 &amp;ldquo;Payment Advice&amp;rdquo;，按照页面里 &amp;ldquo;See more&amp;rdquo; 的指引操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;电汇&lt;/strong&gt;（仅非本地支付）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾选后，点 &amp;ldquo;See more&amp;rdquo; 查看 CUHK 的银行账户详情，直接向该账户电汇（注意电汇金额是 &lt;strong&gt;HK$126,916&lt;/strong&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;注意事项&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;付款时要填对参考号：&lt;strong&gt;88103073875&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;非本地支付（如电汇）可能需要 &lt;strong&gt;7 个工作日&lt;/strong&gt;到账，建议提前操作避免逾期&lt;/li&gt;
&lt;li&gt;付款后确认可能需要 &lt;strong&gt;1-3 个工作日&lt;/strong&gt;（本地）或 &lt;strong&gt;7 个工作日&lt;/strong&gt;（非本地）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;小红书上据称是&lt;strong&gt;微信支付的手续费最少&lt;/strong&gt;，不过既然都读港校了也不差这点钱。&lt;/p&gt;
&lt;h4&gt;注册流程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;注册流程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b3%a8%e5%86%8c%e6%b5%81%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;下一步是完成&lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/registration/how-to-register&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;注册&lt;/a&gt;。在接受录取通知书并缴纳留位费后，要按照以下步骤完成注册，以便在开学前完成所有手续。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有在完成注册程序后，才能领取&lt;strong&gt;CU Link（学生证）&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;所有CUHK学生都需要在校园入口出示&lt;strong&gt;CU Link Card&lt;/strong&gt;。有关迎新活动的详情，请访问 &lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/orientation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Orientation Website 2025&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;自2023年8月1日起，CUHK Mobile Pass App 可供新生使用，可生成二维码用于校园通行和学习空间访问。详情请访问 &lt;a href=&#34;https://www.itsc.cuhk.edu.hk/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ITSC网站&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注册步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 1：提交在线注册&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你必须在指定的付款截止日期前缴纳留位费。&lt;/li&gt;
&lt;li&gt;注册链接将在付款完成后生效：本地学生为&lt;strong&gt;第3个工作日&lt;/strong&gt;，非本地学生为&lt;strong&gt;第14个工作日&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;完成在线注册后，你可以&lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/registration/registration-status&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在此处&lt;/a&gt;查看注册状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 2：提交学生签证申请&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建议学生通过快递将原始签证申请表及&lt;strong&gt;所有&lt;/strong&gt;所需支持文件寄送至CUHK，时间安排如下：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;研究型课程学生&lt;/strong&gt;：&lt;strong&gt;4月中旬&lt;/strong&gt;前&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;授课型课程学生&lt;/strong&gt;：&lt;strong&gt;5月中旬&lt;/strong&gt;前（有预科课程的学生应在预科开始日期前至少&lt;strong&gt;8周&lt;/strong&gt;提交签证申请）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 3：提交研究生宿舍申请&lt;/strong&gt;（如适用）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;修读全日制研究型课程或全日制UGC资助授课型课程的学生可以申请宿舍。请从&lt;a href=&#34;https://www.gs.cuhk.edu.hk/admissions/postgraduate-halls&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;研究生宿舍网站&lt;/a&gt;查看申请截止日期，并相应地向研究生宿舍办公室提交申请。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 4：提交文件以完成录取条件&lt;/strong&gt;（如有）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在条件完成截止日期前，通过邮寄或快递将录取通知中指定的所需文件寄送：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;授课型课程学生&lt;/strong&gt;：寄送至课程办公室&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;研究型课程学生&lt;/strong&gt;（MPhil &amp;amp; PhD）：寄送至研究生院&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果你未能在规定截止日期前完成录取条件，你的录取通知书将失效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 5：执行签证文件提交第一步&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 6：激活 MyCUHK 账户&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完成上述步骤后：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地学生&lt;/strong&gt;：将从&lt;strong&gt;7月中旬&lt;/strong&gt;开始通过ITSC的电子邮件和/或短信（仅限香港手机）收到v-code&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非本地学生&lt;/strong&gt;：将从&lt;strong&gt;7月中旬&lt;/strong&gt;开始通过ITSC的电子邮件收到v-code&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用v-code从学生计算账户收集系统检索OnePass密码，以便通过MyCUHK进行选课和其他服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 7：执行签证文件提交第二步&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 8：领取 CU Link（学生证）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地学生&lt;/strong&gt;：可在录取日期或之后领取CU Link。请联系&lt;a href=&#34;https://www.cuhk.edu.hk/cu-link/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CU Link Card Centre&lt;/a&gt;查看CU Link是否已准备好并确认领取安排。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非本地学生&lt;/strong&gt;：在完成上述步骤7后&lt;strong&gt;7-10个工作日&lt;/strong&gt;，请联系CU Link Card Centre查看CU Link是否已准备好并确认领取安排。&lt;/li&gt;
&lt;li&gt;部分课程会为学生统一领取CU Link。如有疑问，请联系课程办公室。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 9：更新 MyCUHK 信息&lt;/strong&gt;（仅限非本地学生）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在MyCUHK更新邮寄地址和联系方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 10：提交研究生奖学金支付指示表&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;填写并返回&amp;quot;研究生奖学金支付指示表&amp;quot;至财务处会计运营与系统组（邵逸夫楼1楼）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤 11：选课&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选课将在&lt;strong&gt;8月初&lt;/strong&gt;进行。请根据你课程的学习计划和研究生院的建议选课。详情请参考&lt;a href=&#34;https://www.gs.cuhk.edu.hk/student/course-selection-and-add-drop&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;研究生院网站&lt;/a&gt;（学生 &amp;ndash;&amp;gt; 选课和加退选 &amp;ndash;&amp;gt; 选择相应的学年）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意事项&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果未完成在线注册，将&lt;strong&gt;无法&lt;/strong&gt;进行选课和领取CU Link。&lt;/li&gt;
&lt;li&gt;请&lt;strong&gt;不要&lt;/strong&gt;通过电子邮件将完成录取条件的证明文件发送至任何研究生院邮箱账户，这可能会延迟处理提交。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;到时候看看要不要&lt;a href=&#34;https://www.pgh.cuhk.edu.hk/sc/hall-residence-application/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;申请宿舍&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;HKUST-GZ MPhil RB&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;hkust-gz-mphil-rb&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hkust-gz-mphil-rb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;面邀到线下面试之间有&lt;strong&gt;2周时间&lt;/strong&gt;左右，足够你去准备&lt;strong&gt;英文PPT与5min讲稿&lt;/strong&gt;、看面经、准备机票与衣物等工作，&lt;strong&gt;别忘了带身份证等&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-invitation.png&#34; alt=&#34;港科广面试邀请函&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通勤方面&lt;/strong&gt;，直接选择&lt;strong&gt;萧山机场→白云机场&lt;/strong&gt;，从ZJU到前者和从HKUST-GZ到后者的时间差不多都是&lt;strong&gt;1h左右&lt;/strong&gt;，飞机飞&lt;strong&gt;2h多一点&lt;/strong&gt;，差不多当天直接注册入住，第二天开面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上午群面，下午个面&lt;/strong&gt;。这里的信息来源来自港科广官方微信公众号以及小红书经验贴，当然你也可以在面邀邮件里找到对应的&lt;a href=&#34;https://cft.hkust-gz.edu.cn/2025/07/04/%e9%a6%99%e6%b8%af%e7%a7%91%e6%8a%80%e5%a4%a7%e5%ad%a6%ef%bc%88%e5%b9%bf%e5%b7%9e%ef%bc%89%e6%9c%aa%e6%9d%a5%e6%8a%80%e6%9c%af%e5%ad%a6%e9%99%a2%e7%ba%a2%e9%b8%9f%e7%a1%95%e5%a3%ab%e9%a1%b9%e7%9b%ae-2/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;流程&lt;/a&gt;：&lt;/p&gt;
&lt;h4&gt;未来城市小组项目活动&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;未来城市小组项目活动&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%aa%e6%9d%a5%e5%9f%8e%e5%b8%82%e5%b0%8f%e7%bb%84%e9%a1%b9%e7%9b%ae%e6%b4%bb%e5%8a%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 头脑风暴阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4-6名申请人&lt;/strong&gt;，抽取一个**A+B（AI随机形容词生成）**的主题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未来健康保健技术&lt;/li&gt;
&lt;li&gt;可持续生活&lt;/li&gt;
&lt;li&gt;智能工业化&lt;/li&gt;
&lt;li&gt;低空经济&lt;/li&gt;
&lt;li&gt;海洋科技与经济&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;示例&lt;/strong&gt;：寒冷的 + 智能工业化&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;30+10分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（创意与沟通类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;创意贡献&lt;/td&gt;
          &lt;td&gt;申请人在头脑风暴中是否提出了至少一个具体的想法？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;积极倾听与反馈&lt;/td&gt;
          &lt;td&gt;申请人是否提出了至少一个澄清性问题或重述了队友的想法？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;创造力与独创性&lt;/td&gt;
          &lt;td&gt;申请人是否提出了至少一个在方法或内容上不同于其他人的想法？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;接受反馈的态度&lt;/td&gt;
          &lt;td&gt;申请人是否口头承认了反馈，或根据反馈提出了调整建议？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;2. 制作阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用基础材料（积木、纸、绳子、竹木、乐高等），在一个&lt;strong&gt;长、宽、高分别为 75.5cm、51.5cm、&amp;gt;43cm&lt;/strong&gt; 的立体空间内创作一个主题关键词的未来城市模型，包含&lt;strong&gt;不少于 5 个模块&lt;/strong&gt;（模块功能或内容由小组自行定义）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例&lt;/strong&gt;：具备高度智能工业化生产能力，且能够在严寒环境中提供可持续的生活条件和高效管理的未来城市，包含了智能工业中心、严寒能源供应与管理系统、智能交通网络、常温智能居住区、抗寒垂直农业与生态等模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2名评委&lt;/strong&gt;全过程观察、记录、评分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;110分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（执行与协作类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;制作贡献&lt;/td&gt;
          &lt;td&gt;申请人是否在项目的某个方面进行了实际操作（例如，放置材料、调整设计等）？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;任务责任感&lt;/td&gt;
          &lt;td&gt;申请人是否负责完成了至少一项分配的任务，并且无需提醒？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;协作能力&lt;/td&gt;
          &lt;td&gt;申请人是否与另一名队员在至少一项具体任务上合作（如规划或构建）？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;主动性与解决问题的能力&lt;/td&gt;
          &lt;td&gt;申请人是否在构建阶段遇到问题时提出了至少一个解决方案？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;领导能力&lt;/td&gt;
          &lt;td&gt;申请人是否至少一次分配或委派任务给队友？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;3. 交易与适应阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随机抽取另一小组作为观察对象进行观察，并结合自己小组的作品情况，有针对性地为被观察的小组提出一个挑战任务（例如：地震、海啸、核污染等人类社会可能面临的重大灾难或者挑战，具体内容由各小组自行定义），同时要求自己小组的作品必须能够满足自己所提给对方的挑战任务要求。此期间允许各小组之间使用&lt;strong&gt;金币&lt;/strong&gt;进行模块的自由交易，但最终需标注出所交换的模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;10分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个小组必须在&lt;strong&gt;20分钟&lt;/strong&gt;内完成挑战任务的应对方案，并在白板上写好小组作品介绍，要求包含应对方案，形式不限。此期间，允许各小组调整各自的作品以及进行组间的模块自由交易。建议各小组妥善使用金币，争取使投资产生最大价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;20分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（交易与适应类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;战略思维&lt;/td&gt;
          &lt;td&gt;申请人是否在交易阶段提出了至少一次交易或模块交换的建议？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;谈判技巧&lt;/td&gt;
          &lt;td&gt;申请人是否至少与其他团队进行了一次谈判，无论是提供还是接受条件？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;设计调整的灵活性&lt;/td&gt;
          &lt;td&gt;申请人是否根据挑战任务提出了至少一个设计调整建议或进行调整？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;压力下的适应性&lt;/td&gt;
          &lt;td&gt;申请人在挑战任务引入时是否保持参与而没有退缩或失去参与感？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;4. 挑战与最终展示&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个小组面向各自评委进行团队陈述，结束后按照现场指引完成组内互评。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间&lt;/strong&gt;：&lt;strong&gt;30分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（展示与团队互动类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;演讲清晰度&lt;/td&gt;
          &lt;td&gt;申请人是否清晰地展示了自己在项目中的角色，无需队友提示？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;挑战任务中的领导力&lt;/td&gt;
          &lt;td&gt;申请人是否提出了至少一项团队应对挑战任务的行动建议？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;回应挑战的创造力&lt;/td&gt;
          &lt;td&gt;申请人是否提出了一个直接应对挑战的解决方案（例如，功能改动，增加新特性等）？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;团队动态&lt;/td&gt;
          &lt;td&gt;申请人是否鼓励了至少一位队友参与或征求了他们的意见？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;与评委的互动&lt;/td&gt;
          &lt;td&gt;申请人在展示过程中是否至少回答了评委的一个问题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;活动地点&lt;/strong&gt;：香港科技大学（广州）校园的 &lt;strong&gt;Highbay&lt;/strong&gt;，全部制作环节必须在指定区域内完成；各小组在活动结束后须快速整理物资，清理现场后，方可离场。&lt;/p&gt;
&lt;h4&gt;个人面试&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;个人面试&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%aa%e4%ba%ba%e9%9d%a2%e8%af%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;总时长&lt;/strong&gt;：&lt;strong&gt;15分钟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 口头演讲&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：申请者的口头演讲使用中文，问答环节必将使用英文；如果口头演讲使用英文，问答环节将可使用中文或英文。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;因此，&lt;strong&gt;这部分必须使用英文&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5分钟内&lt;/strong&gt;阐述你攻读该领域硕士学位的原因，从以下五个主题中选择一个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未来健康保健技术&lt;/li&gt;
&lt;li&gt;可持续生活&lt;/li&gt;
&lt;li&gt;智能工业化&lt;/li&gt;
&lt;li&gt;低空经济&lt;/li&gt;
&lt;li&gt;海洋科技与经济&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;评分指标（演讲与主题契合类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;主题相关性&lt;/td&gt;
          &lt;td&gt;演讲是否聚焦于申请人选择的五个主题之一？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;视角与愿景&lt;/td&gt;
          &lt;td&gt;申请人是否展示了多角度的思考，并解决了现实问题或未来挑战？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;组织与结构&lt;/td&gt;
          &lt;td&gt;演讲是否结构合理，有明确的开头、主体和结论？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;内容熟悉度&lt;/td&gt;
          &lt;td&gt;申请人是否展示了对内容的深入理解，并简化了复杂的主题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;幻灯片设计&lt;/td&gt;
          &lt;td&gt;幻灯片是否无误，与内容一致，并有效支持演讲？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;语言表达&lt;/td&gt;
          &lt;td&gt;申请人表达是否流畅清晰，避免了多余叹词和模糊不清的表达？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;与评委的互动&lt;/td&gt;
          &lt;td&gt;演讲过程中，申请人是否有效地与评委进行眼神接触并互动？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;创造性思维&lt;/td&gt;
          &lt;td&gt;申请人在演讲中是否提供了创造性或独特的观点？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;动机与契合度&lt;/td&gt;
          &lt;td&gt;申请人是否清晰阐述了自己为何适合该项目，将个人目标与项目目标对齐？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;2. 问答环节&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评分指标（问答与综合能力类）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;分数（0-2）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;理解能力&lt;/td&gt;
          &lt;td&gt;申请人是否理解并准确回应了评委的问题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;回答深度&lt;/td&gt;
          &lt;td&gt;回答是否经过深思熟虑，并显示了对主题的深入理解？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;情绪控制&lt;/td&gt;
          &lt;td&gt;面对压力时，申请人是否保持冷静镇定，沉着应对难题？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;危机处理能力&lt;/td&gt;
          &lt;td&gt;申请人是否能够很好地应对挑战性或意外问题，并提供相关的解决方案或观点？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;批判性思维&lt;/td&gt;
          &lt;td&gt;申请人是否展示了逻辑推理能力，分析问题并提供有见地的回答？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;团队合作的关注&lt;/td&gt;
          &lt;td&gt;申请人是否提供了团队合作的具体例子，并表达了对协作的强烈兴趣？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;说服力&lt;/td&gt;
          &lt;td&gt;申请人的观点是否具有说服力，并能够有效支持其立场？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;自我意识与成长&lt;/td&gt;
          &lt;td&gt;申请人是否展示了自我意识，讨论了自身的优势、劣势和需要改进的地方？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;共情与参与&lt;/td&gt;
          &lt;td&gt;申请人是否根据评委的反馈调整了自己的回答，并与他们进行有意义的互动？&lt;/td&gt;
          &lt;td&gt;0-2&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;详细的面试流程可以参考面邀邮件里的文件：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-agenda1.png&#34; alt=&#34;港科广面试流程1&#34;  loading=&#34;lazy&#34; /&gt;
&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-agenda2.png&#34; alt=&#34;港科广面试流程2&#34;  loading=&#34;lazy&#34; /&gt;
&lt;img src=&#34;http://localhost:1313/blog/2025/hkustgz-interview-agenda3.png&#34; alt=&#34;港科广面试流程3&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;注意事项&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;注意事项&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;证件准备&lt;/strong&gt;：请准备申请者有效身份证件原件（身份证、护照或其他有效证件）入校及签到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;住宿安排&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;所有申请者在报到当晚将安排住宿（&lt;strong&gt;8人公寓，135元/晚，自费&lt;/strong&gt;）&lt;/li&gt;
&lt;li&gt;如果申请在面试后延长一晚住宿（续住房型：&lt;strong&gt;3人公寓，214元/晚，自费&lt;/strong&gt;），学校将为你安排三人间住宿&lt;/li&gt;
&lt;li&gt;如因住房紧缺，将由校方另作安排&lt;/li&gt;
&lt;li&gt;未申请续住的申请者应在面试当天&lt;strong&gt;19:00之前&lt;/strong&gt;离开学校&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：因此通勤非常紧凑，当天去注册，第二天晚上就要连夜回来，需要注意买票这一块。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;纪律要求&lt;/strong&gt;：迟到、缺席或不遵守规定，甚至对面试现场造成破坏或不良影响者，将被&lt;strong&gt;直接取消面试资格&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;录音录像&lt;/strong&gt;：主办方将根据实际工作需求，在活动现场安排相关录音、录像或拍摄等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;差旅费用&lt;/strong&gt;：申请者参加线下面试的差旅自理，校方将不提供订票服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;面经分享&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;面经分享&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%9d%a2%e7%bb%8f%e5%88%86%e4%ba%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;因为&lt;strong&gt;红鸟的特殊机制&lt;/strong&gt;，面试的时候尽量伪装自己，或者说释放天性，选一个相对冷门的Hub尽情阐述，毕竟进去之后有&lt;strong&gt;半年才正式选导&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;小红书链接汇总&lt;/strong&gt;：
&lt;strong&gt;1. &lt;a href=&#34;https://www.xiaohongshu.com/explore/6751368b0000000007027b9b?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBK4SfTQTgQ_lcCGKMtNdUBCgtSonOqP_4awBVwt_GPdc=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760345377&amp;amp;share_id=021e983567954ebfb12a0db5555fd31a&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广第二批面经（推研版）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前一天&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下午到达科广签到，&lt;strong&gt;五点老师带队参观校园&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;面试当天&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;离开宿舍时要收房卡退房&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;8:30开始签到&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;团队面试&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;题目：&lt;strong&gt;未来健康保健技术 + 宁静的&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;个人整体感受：team2队员都很友好，过程愉快。评委老师似乎在&lt;strong&gt;最后的陈述部分才动笔记录&lt;/strong&gt;（似乎在打分？所以这个part要重视）。最好开始前上好厕所，带杯水进来场地，全程站着特别累，&lt;strong&gt;12:30才结束&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;个人面试&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中午要抓紧吃饭，时间很紧&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;13:00签到，14:00开始&lt;/strong&gt;，我是最后一个等到&lt;strong&gt;17:00面完&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;项目经历中的合作与困难&lt;/li&gt;
&lt;li&gt;你未来想研究这个方向（老龄化）要达到什么目的&lt;/li&gt;
&lt;li&gt;研究方法？&lt;/li&gt;
&lt;li&gt;假设要研究这个项目，你想要什么队友（两个老师分别问了这个问题，答了两次）&lt;/li&gt;
&lt;li&gt;如果你来到这个学校，我要你换个研究方向你愿意吗？你要换什么方向，为什么&lt;/li&gt;
&lt;li&gt;什么情况下你会换研究方向？（逐渐抽象）&lt;/li&gt;
&lt;li&gt;你对你现在的方向感兴趣，还是你想换的另一个方向？&lt;/li&gt;
&lt;li&gt;假如以后没人生小孩，全是老人，没钱发养老金，你是决策者你要怎么办&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt;：我和评委老师专业不对口，几乎没有专业知识的提问，反而&lt;strong&gt;future plan&lt;/strong&gt;引起了他们的兴趣所以一直在抽象地拷打（maybe我是最后一个，老师们都开始放松了随便乱问）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;2. &lt;a href=&#34;https://www.xiaohongshu.com/explore/67458e09000000000202a288?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBCrT4tjGLAuhlyZIu0hu8LFBIEyOeKqPwRHkO-jf_Q2A=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760345606&amp;amp;share_id=f02f9d8111414513a5b91099332f6ea8&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广第二批MPhil面经&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上午团队面试&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;围绕&lt;strong&gt;未来健康技术、宁静&lt;/strong&gt;的两个关键词进行团队合作搭积木，目标是未来城市的设计&lt;/li&gt;
&lt;li&gt;我负责的是老本行，搭建的&lt;strong&gt;能源电力部分&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;感觉这个团队合作需要&lt;strong&gt;多张嘴多动手，自圆其说即可&lt;/strong&gt;，队友都特别给力，也非常nice&lt;/li&gt;
&lt;li&gt;对于我这个i人其实面完精疲力竭，幸亏抽到了比较e的队友，被带动着也就慢慢热络了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;下午个人面试&lt;/strong&gt;（&lt;strong&gt;10分钟&lt;/strong&gt;的提问）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解释一下CLLLC，通俗一点给我们讲一下&lt;/li&gt;
&lt;li&gt;你为什么不保研&lt;/li&gt;
&lt;li&gt;你还申请了哪些学校（或单位）&lt;/li&gt;
&lt;li&gt;通俗解释一下High power与high power density是什么，怎么实现，作用是什么&lt;/li&gt;
&lt;li&gt;你的变换器怎么实现的low ripple低纹波&lt;/li&gt;
&lt;li&gt;你的参数是在仿真里枚举就行了吗，设计逻辑是什么&lt;/li&gt;
&lt;li&gt;你的意向导师是谁&lt;/li&gt;
&lt;li&gt;（英文提问）你觉得你的变换器还有什么地方可以改进，这个你能靠自己完成吗还是需要teamwork&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：存在英文提问但是可以中文回答，老师们都很好，果然是一所氛围特别好的学校，两天体验下来非常非常好。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;3. &lt;a href=&#34;https://www.xiaohongshu.com/explore/680d20c6000000000f032c93?note_flow_source=wechat&amp;amp;xsec_token=CBIMhMvpQJWY392bghHEfAwharwY8DrwQagLQtKp4Hcdk=&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广红鸟MPhil线下面试复盘&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;申请时间线&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.29春节准备材料 → 1.31完成投递 → 3.31收到面邀 → 4.17面试 → 4.25收到通知&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;线下面试复盘&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;小组群面项目为搭建未来城市，随机抽取的关键词为&lt;strong&gt;奇思妙想的 + 可持续生活&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;我主要关注的点为两个：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 快速满足客观要求&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我把规则分为了客观要求和主观要求，我的第一任务是快速满足最低的客观要求。搭建规则只对模块数量和尺寸做了要求，尺寸中相对最难满足的是最低高度。问题就转化为了如何在材料有限的情况下，用最简单的方法达到指定高度。&lt;/p&gt;
&lt;p&gt;最直接的是用手边的置物箱，但空间太小施展不开手脚。经过测量，椅子的长和宽刚好满足所给的尺寸要求。我的策略是将椅子直接放在桌子上作为城市的地表部分，椅子下面则作为地下部分。这样在满足要求的前提下，空间也更宽敞，搭建的时候也可以施展手脚。在快速把客观要求全部满足的前提后，就可以放开手脚对各模块进行自由发挥了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 主观要求的思考&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我的策略只有三个：&lt;strong&gt;做局部区域最优解，命运共同体，以人为本&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;第一&lt;/strong&gt;，面试整体的玩家数量是固定的，所以在资源和时间有限的情况下，我认为关键不是闭门造车，尽善尽美追求极致，而是只需要在某些方面或某个模块做得比其他组好就可以，做到&lt;strong&gt;局部最优解&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第二&lt;/strong&gt;，小的来说，每个人所在的小组是一个独立的城市，小组之间是竞争关系；但从大局来看，所有的组又都是一个&lt;strong&gt;命运共同体&lt;/strong&gt;，如果把城市与城市连接起来，所有的城市会构成一个更大的城市，连接起来后形成的效益将远远大于自身建设的效益。这一点最难的地方是在于如何把所有的组连通在一起，但最好玩的点也在于当把其中一部分组连接在一起之后，其余没有连接的组反而会失去越来越多的东西，从而迫使其加入连接。其实当把这场面试的设计看作如今世界局势的缩影时，也会很有趣。攒局搭建这个连接平台的团队当然会失去一部分精力和时间来做客户的开拓和运营，在自己的城市搭建上会吃亏，但这是长期可以一本万利的事情，所以值得去做。另外，在连接其他组的过程中，也可以提前了解各个组的进度和搭建模块特色，可以有效的为第一点中的局部最优解策略提供信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第三&lt;/strong&gt;，城市是由人构成的。城市的设计是以用户为中心来展开的，所以搭建策略从一开始就从人的&lt;strong&gt;衣食住行&lt;/strong&gt;四方面来进行规划的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4. &lt;a href=&#34;https://www.xiaohongshu.com/explore/67b8b99f000000002a002475?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBbZWt-5GzBBpEiSiIJjr6x4l6y793ZJCsrwK4xdVma0I=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760345905&amp;amp;share_id=cc0c1848bc4a4aa5a0ebfbd7e41dcea8&amp;amp;wechatWid=a38e28d828ad4076b345e342a9c6fefe&amp;amp;wechatOrigin=menu&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广第三批个人面面经（已推研）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下午个人面试要点&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;首先，能脱稿一定要脱稿脱稿！！！&lt;/strong&gt; 我就抱有侥幸心理没脱稿，老师上来第一个问题就是：你手里拿着的是小抄吗？（真的很压力）还好本人机智，笑着解释了一下。最后答辩完评委老师还特地叮嘱&amp;quot;下次就不要带着稿子了&amp;quot;（万幸是过了，当时面完差点以为自己凉了）。而且听说隔壁组一个没脱稿的uu被狠狠压力了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;不同老师问的问题风格有很大差异&lt;/strong&gt;，有些老师可能会问很专业的问题，有些老师的问题就比较general，遇到什么样的老师是不可控的，所以我们能做的就是准备好每个可能的问题。PPT上所有涉及到自己以前的论文、项目一定要再看看弄清楚每个细节，老师提的问题不管多不可控都是基于PPT的。我也遇到比较厉害的uu，有意在pre的时候挖一些坑，刚好评委老师就问到了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;下午面试包含一定压力面的成分&lt;/strong&gt;，所以面对评委老师的刨根究底一定要思路清晰，保持镇定。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;我被提问的问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：为什么转专业？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：因为巴拉巴拉巴拉巴拉（提前准备了顺利完成）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：你的研究计划好像港科广没有这方面的老师啊？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：详细阐述研究计划，有哪个老师是做这方面的…巴拉巴拉…&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：研究计划里某某某是什么意思呀？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：巴拉巴拉…解释一下&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：联系意向导师了吗？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：还没有（确实是本人拖延症又犯了）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：你手里拿着的是小抄吗？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：是我的讲稿，我想把所有细节都注意到，所以带了讲稿&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;：你能具体讲讲xxx项目的结果吗？PPT上图是什么意思？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;：巴拉巴拉巴拉巴拉（意料之中准备了）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;总之，以不变应万变&lt;/strong&gt;
&lt;strong&gt;5. &lt;a href=&#34;https://www.xiaohongshu.com/explore/6849c12c000000002101ab14?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CB20Qv6po2dCpk24KPlHkXecKzPI-j2H5gSdStHzSKUSE=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760346115&amp;amp;share_id=0c2a5c894c4246869a1bb65ec74f053d&amp;amp;wechatWid=a38e28d828ad4076b345e342a9c6fefe&amp;amp;wechatOrigin=menu&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广红鸟线下面试-面经分享最后一批随缘&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;报道前一天&lt;/strong&gt;：晚八点落地白云机场，坐地铁两个小时到学校附近，找了家宾馆住下开始做PPT，当时PPT完成度只有大概一半（换模版+稿子没写）。&lt;/p&gt;
&lt;p&gt;我PPT内容比较多，精简了好几次之后还是有大概&lt;strong&gt;20页&lt;/strong&gt;。其实也不是东西有多少，主要是之前东西都放到一页，导致图片和文字都很小很密，几次模拟面试反馈都不是很好（加上模版不太学术有点幼稚）。我索性直接重写了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二天&lt;/strong&gt;：报道之后五点下楼有老师领着逛学校，走了一下第二天面试地点路线。晚上吃饭之后就回宿舍做PPT写讲稿，整个Q&amp;amp;A和群面环节几乎没准备（之前忙毕设，就大致扫了几眼面经）。一直做到&lt;strong&gt;凌晨四点&lt;/strong&gt;，上床五点睡到七点半。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上午群面&lt;/strong&gt;：关键词是&lt;strong&gt;未来保健技术和sophisticated&lt;/strong&gt;，我们组氛围还ok，但是大家思路其实比较乱。搭积木还是有点费手的，而且我们的模块很容易倒塌，因为这个原因差一点没搭建完。老师提问的时候一定要听清，当时一个问题：每个人为了应对别的组提出的挑战都做了什么。我看当时没人说话就想打破冷场但是急匆匆的没有回答到点上，感觉是个失分项。群面在这里不详细讲，可以看其他帖子，推荐看我们同组队员的帖子&amp;quot;港科广红鸟硕士面经(被恨版)&amp;ldquo;里的描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下午个面&lt;/strong&gt;：我是最后一个，前边的人反应被拷打了或者提问时间没用完。我问答没有准备特别多，就准备了一些例如为什么选择项目这种比较general的问题，最后都没用到……一直在熟悉稿子，从一开始&lt;strong&gt;800词删到400词&lt;/strong&gt;还是背不下来，索性带平板进去念了，大概念两句背一句看一眼老师。&lt;/p&gt;
&lt;p&gt;主题我选择的是&lt;strong&gt;低空经济&lt;/strong&gt;，项目都是实体机器人和上面的算法，三个小项目一个大项目。中间穿插一些学工和比赛之类的经历。时间分配大概是个人介绍和动机一分钟，项目和经历三分钟，最后原因总结致谢一分钟。没有讲技术细节，图片视频比较多但是每页一两张。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q&amp;amp;A&lt;/strong&gt;：我的感觉是要多说，另外，最好表现的自信一点，本人平时比较e，面试的时候也是语速很快，思路也比较清晰。一开始几个老师是想压力的，但是每个问题问一个方面，我一般会多说好几个点综合性的讲，都超出了他们的预期，后面整个氛围都非常好了。面试结束的时候我说期望再见到你们，几个老师都笑了，有一个老师还让我去官网看一下他的实验室和研究方向（暗示我？）最后关门出去的时候听见几个老师笑着说：年轻人还是热情啊。感觉整体上是认可的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. &lt;a href=&#34;https://www.xiaohongshu.com/explore/6718bf950000000016022927?app_platform=ios&amp;amp;app_version=9.3.2&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBwAOpeg7jwqIUyMksgf-Il0Dw3KznZjyX2AV1cOq66AU=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=ODlHQ0dKOTs2NzUyOTgwNjg5OTc5RzlO&amp;amp;apptime=1760346158&amp;amp;share_id=1e75d75593334459ab3f046cd41bed68&amp;amp;wechatWid=a38e28d828ad4076b345e342a9c6fefe&amp;amp;wechatOrigin=menu&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广RBM第一批面经&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;申请时间线&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;9.1提交网申&lt;/li&gt;
&lt;li&gt;9.13通知补充材料&lt;/li&gt;
&lt;li&gt;10.14面邀&lt;/li&gt;
&lt;li&gt;10.23线下面试&lt;/li&gt;
&lt;li&gt;11.4推研&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;上午小组活动&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;感觉总体很玄学，注意要&lt;strong&gt;全程保持参与&lt;/strong&gt;，然后在&lt;strong&gt;交易阶段做出贡献&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;下午个人面&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;感觉也很玄学，准备了很多专业和研究项目的问题，完全没问到，都是很general的问题。&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;RP的应用场景，为什么工业4.0一直没有得到全面的推广&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;是否准备创业&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;如果让你选择，你是会深入学习本领域的知识，还是会去学习其他领域的知识&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;本科竞赛，自己做了什么&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;本科的TA工作经历&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;RP能给计划的研究领域（智能工业化）带来什么提升&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;RP的具体创新点&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;p&gt;感觉面试老师应该都不是CS/AI/CE相关专业的，没问到专业问题。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;7.&lt;a href=&#34;https://www.xiaohongshu.com/explore/6904cfea000000000301966f?xsec_token=ABWEBsV8CjOxkXWOGeEWymXY6O04guA8mAXsqoSQqwfSo=&amp;amp;xsec_source=pc_search&amp;amp;source=unknown&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;港科广红鸟26fall正式批面经&lt;/a&gt;
📌tl
10.22 报道、刷身份证、签名、付住宿费，有车接送到宿舍楼下，找到宿舍收拾东西/认识新舍友；会领到一张纸，上面有地图以及第二天面试时间
10.22 17:00，步行、宿舍楼下集合逛校园，可以看到第二天上午群面的场地
10.22 ～18:00之后，自行吃饭（可以体验一下学校食堂），回宿舍休息，想晚起的话记得准备明天的早饭
10.23 上午，先退房，然后把行李箱搬到面试的地方，拿身份证签到，现场抽签分组，抽主题，开始面试，有电子钟，注意把握时间
10.24 下午，拿身份证签到，现场抽签面试教室，拷贝ppt，到等候室等待面试，面试完可以直接走人&lt;/p&gt;
&lt;p&gt;📌面经：
1⃣上午小组面：
我们当天抽到的是“可持续生活”+“ 独自漫游的”，算是关联性比较直接的两个主题，当然这会导致大家能想到的东西差不多。总体的流程就是构思讨论、搭乐高、看其他小组的模型并提出挑战、交易以解决自己收到的挑战、向面试老师汇报。
建议：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;务必仔细看官网的评分表&lt;/li&gt;
&lt;li&gt;可以自己找AI模拟一下提示词，想想怎么布局、切合题目&lt;/li&gt;
&lt;li&gt;搭积木以及城市规划不是本质的考核内容，基本切题即可，搭的结构也不需要特别复杂，能说得通就行，抽象表述也ok&lt;/li&gt;
&lt;li&gt;但是，老师也有可能在大家都介绍完之后询问城市的功能区和内容，正常回答即可，没考虑到就及时承认&lt;/li&gt;
&lt;li&gt;感觉自己群面表现一般，在无领导小组中，不是发言很多的或者经常帮助组员的角色（没遇到机会，玄学），但是在交易环节比较活跃，老师应该也关注到了。所以不必太焦虑和纠结，正常表现就可以&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2⃣下午个面：
我选的是英文汇报+中文回答。
PPT的思路是讲故事，把自己的经历融入到故事里面去：为什么未来想做这个主题的工作，我有什么专业基础（这里可以展现标化的绩点、获奖、论文）来支撑未来的研究，然后也可以讲讲科研探索小故事，比如做了第一个项目之后思考到了什么问题，从而想做主题方向的研究，为此又做了哪些努力（论文、项目、实践），最后再往你的主题和构想上面靠一靠，讲讲为什么选择红鸟。
提问环节应该有压力面，正常回答即可。可能会问你的项目需要哪些学域的老师指导、需要哪些专业的同学参与、不同专业的合作者分别负责项目的哪些部分、有无创业经历、（有经历）为什么没有创业成果等等。
建议：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;准备充分，汇报只有五分钟，内容多的话语速要快，但是亲测现场语速更快（甚至会漏了一些点忘记讲），自己准备大概5.30，现场只有4分钟&lt;/li&gt;
&lt;li&gt;一些很难的问题一般都是压力面，所以如实回答，稳住心态，保持微笑 ：）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;8.&lt;a href=&#34;https://www.xiaohongshu.com/explore/69088033000000000701686b?xsec_token=AB0g3k8Fm6ofVBT6UbVgQwO5T9_cBsxgizX-oE_h7ZzWk=&amp;amp;xsec_source=pc_search&amp;amp;source=unknown&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;红鸟计划第一批推研！&lt;/a&gt;
10.1面邀-10.24面试-10.31下推 有始有终的十月！&lt;/p&gt;
&lt;p&gt;群面：智能工业化+Aurora
一开始担心和四个工科生一组会不会一句话说不出来？但很快联想到极光粒子能收集和有色金属后完美的发挥了商科生+e人的诡辩能力，个人觉得群面真的超超常发挥！&lt;/p&gt;
&lt;p&gt;个面：
①陈述：极限准备了一周，但我真的觉得我把个人陈述环节做的非常非常好（需要参考可以私信），紧扣“可持续生活”主题的同时，用了个word play搭了两条很巧妙的主线和暗线联系自身经历贯彻全部，三个导师真的一直在点头！（所以哪怕科研和项目背景较弱也不要怕！！商科生有自己的扬长避短的打法）&lt;/p&gt;
&lt;p&gt;②盘问：详细说了我CV里的一个项目经历和创业经历，前期自认为得心应手，直到有个老师突然灵魂发问，质疑我能否达到红鸟的学术要求？于是乱了阵脚…后面一顿胡说，时间到了之后，这个老师又给了我个追问，潦草收场。（好在结果满意）
总结：
庆幸自己作为极少数商科生脱颖而出，群面五进一（如果组员刷到这里，认出了我，我想说我也真心的感谢你们没有push我，和你们度过了非常快乐两小时）个面也是水来土掩。&lt;/p&gt;
&lt;p&gt;9.&lt;a href=&#34;https://www.xiaohongshu.com/explore/692c68fb000000001e00d2b0?xsec_token=ABilGWB6Une0uN2WfqWRRyo1xgvKUqzq_3lWlzw_Dn-OU=&amp;amp;xsec_source=pc_search&amp;amp;source=unknown&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;拿下港科广红鸟面试，只因我做对了这几件事&lt;/a&gt;
主播是25届mphil的最后一批面试，凭借一些面试的小技巧成功通过！
红鸟的面试分为团面与个面，这篇就先讲讲团面的部分。&lt;/p&gt;
&lt;p&gt;团面主要考察的是团队协作能力与创新思维，内容是用积木搭建未来城市。
主办方会在未来健康保健技术、可持续生活、智能工业化等题材中抽取一个范围，并用AI随机生成一个关键词。
我记得当时我们抽中的是海洋科技与经济，生成的关键词是空灵的。&lt;/p&gt;
&lt;p&gt;在团面环节，我认为有一些会对红鸟面试很加分的点：
第一，做你自己，不必强行角色扮演。
在无领导小组讨论过程中，你只要完成好自己负责的板块，在小组讨论中提出建设性意见，负责好自己里面的任务。
我认为团面的本质是看你是否具有与人合作的潜力，能否利用好团队优势进行分工，不必为了展现自己的领导能力而“过度表演”。&lt;/p&gt;
&lt;p&gt;第二，善用身边工具，带好便签纸。
在团面中，面试官并不纠结你搭建的是否能实现，重视你对想法的表达。这时候运用好便签纸就很重要，可以通过便签纸记下自己搭建部件想要达成的功能是什么，在便签纸中记下一条从What到How的路径，帮助自己与团队更好地记忆与表达设计理念。&lt;/p&gt;
&lt;p&gt;第三，天马行空不一定是贬义词
在积木环节，你可以提一些富有想像力的点，不必太拘泥于技术能否实现，只要能解释通，就可能成为设计的闪光点。
当时我们小组有位同学提出设计“泡泡”作为交通运输工具，灵感源于小时候在公园玩的浮力球。我们并没有详细阐述这个泡泡是怎么驱动，但提出了这一个新奇想法，显得我们有独特的创造力。&lt;/p&gt;
&lt;p&gt;第四，模块化思考是设计的重要能力
模块化建设城市我是很好的思路，我们小组把城市分为了多个不同功能模块，通过模块组合的方式进行城市规划。在团面开始前与小组进行头脑风暴，提出这个想法，可以在早期向面试官展现你们的建设思维，为后续的高效率建设提供方向。&lt;/p&gt;
&lt;p&gt;第五，言简意赅总结自己的贡献
在个人陈述部分，可以把表达重点放在这几个问题，尽可能展现自己的价值：
在头脑风暴环节中，你在团队中扮演的角色是什么？
你在头脑风暴中提出了什么建设性意见？
在制作过程，你和谁进行了合作？合作过程中如何分工的？你们的合作达到了什么效果？
在交易环节，你如何跟其他小组谈判？谈判发挥了什么作用？&lt;/p&gt;
&lt;h4&gt;PI调研&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;pi调研&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#pi%e8%b0%83%e7%a0%94&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;以免个人面试的时候被问到，所以这里得先准备一下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;陈昶昊&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;陈昶昊博士（Dr. Changhao Chen）是香港科技大学（广州）&lt;strong&gt;系统枢纽智能交通学域、信息枢纽人工智能学域联聘助理教授&lt;/strong&gt;，副研究员，博士生导师。获&lt;strong&gt;英国牛津大学计算机科学博士学位&lt;/strong&gt;，并在英国工程和自然科学研究委员会（EPSRC）资助下从事博士后研究。入选&lt;strong&gt;全球前2%顶尖科学家榜单（2024）&lt;/strong&gt;、**中国科协青年人才托举工程（2022）**和国际机器人科学与系统大会（Robotics: Science and Systems）&lt;strong&gt;先锋者（2020）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;组建港科大（广州）&lt;strong&gt;PEAK-Lab (Perception, Embodiment, Autonomy and Kinematics)&lt;strong&gt;课题组，现有博士生及科研助理&lt;/strong&gt;10人&lt;/strong&gt;。课题组聚焦&lt;strong&gt;具身智能和无人系统研究&lt;/strong&gt;，致力于构建在动态开放环境交互的具身智能体，服务&lt;strong&gt;低空经济、智能交通和智慧城市&lt;/strong&gt;等应用领域。主持&lt;strong&gt;国家自然科学基金&lt;/strong&gt;等纵向项目&lt;strong&gt;5项&lt;/strong&gt;，先后三次获/提名权威学术会议优秀论文奖。在人工智能、机器人和智能交通领域已发表高水平论文&lt;strong&gt;50多篇&lt;/strong&gt;，包括TITS、TNNLS、TMC、TIP、TIV等领域权威期刊以及CVPR、ICCV、AAAI、ECCV、ICRA、IROS、WWW、MobiCom、MobiSys、SenSys等国际顶级会议，&lt;strong&gt;谷歌学术引用超过2900次&lt;/strong&gt;。长期担任NeurIPS、ICML、ICLR、CVPR、TRO、IJRR、IJCV、TAC等&lt;strong&gt;30多个&lt;/strong&gt;国际会议、期刊的程序委员会委员/元审稿人/审稿人，中国科协会刊《科技导报》首届青年编委，受邀担任国际机器人与自动化会议（ICRA-2024）以及中俄&amp;quot;导航与运动控制&amp;quot;青年学者论坛的分会场主席。已授权国家发明专利、国际PCT专利、美国和欧洲专利共&lt;strong&gt;14项&lt;/strong&gt;，包含1项在英国成功成果转化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主页&lt;/strong&gt;：https://changhao-chen.github.io/&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;研究方向&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;人工智能在计算机视觉和自然语言处理领域取得显著进展，为**通用人工智能（AGI）**奠定基础。尽管大语言模型在虚拟环境中展现巨大潜力，将人工智能融入现实世界仍面临诸多挑战。未来十年，&lt;strong&gt;具身智能将推动下一次技术革命&lt;/strong&gt;。实现机器智能从虚拟、受限环境延伸到物理、开放世界，需要深入理解三维场景、本体运动、具身智能以及高效计算。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;三维空间感知&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;三维空间感知将多视图几何与深度神经网络结合，提供从低层级特征感知、中间层级几何表达到高层级语义理解。课题组已取得三方面突破：构建自监督学习的空间感知框架，实现二维到三维数据的直接特征提取匹配、基于特征元学习的视觉定位与建图、自监督位姿与深度估计等；构建城市级别的视觉定位与三维重建框架，包括视觉定位Transformer大模型以及多神经渲染网络的合并渲染、匹配与定位；针对烟、雾等视觉受限环境，实现红外相机和毫米波雷达的稀疏数据信号处理，构建超越人类视界的感知系统。未来将进一步面向动态变化的实际场景，开展新场景自适应的空间感知研究，实现感知系统的长效终身学习以及学习模型的可信度分析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;运动状态估计&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;三维空间运动的智能体需进行速度、姿态、位置等系统状态的估计，以支撑规划和决策。课题组开展以本体感知为核心的运动估计研究，在世界范围内首次提出基于深度学习的惯性导航算法并公开相关数据集，得到全球&lt;strong&gt;40多个国家&lt;/strong&gt;的研究人员关注和使用，被后续研究者拓展为数十种算法模型，用于解决四足机器人、无人机、水下机器人、车辆、智能穿戴设备等载体的本体运动状态估计难题。未来将进一步研究结合物理模型和机器学习的动态建模，通过可学习的状态空间模型和神经常微分方程等方法，实现理解物理世界的运动估计、预测以及状态分析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;具身导航决策&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;实现三维空间导航与交互是通往通用智能的重要目标。课题组面向高效可靠自主导航的需求，提出基于特征选择的多传感器融合方法，从数据中学习到传感器间的融合策略，实现强鲁棒多源导航。课题组构建融合深度神经网络的卡尔曼滤波模型，实现具备李雅普诺夫稳定的导航动力学模型自动构建。课题组探索了融合传统控制策略与强化学习的决策控制模型，实现导航策略更高效学习。未来研究将借鉴人类大脑与小脑的功能分工，提出层次化的具身导航与操纵决策框架，通过大语言模型理解语言指令用于全局规划与决策，通过具身基础模型实现局部运动规划和控制，最终实现智能体自主导航、探索、操纵与协作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;高效神经网络计算&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未来社会将依赖数十亿台具身智能系统，传统通用计算架构难以满足低成本、低能耗和低延迟的需求，需探索面向具身智能的专用计算框架。课题组已突破轻量化神经网络知识蒸馏和量化、视觉感知FPGA硬件加速单元、硬件加速的视觉定位与建图系统等关键技术。未来将进一步研究通用计算与神经网络计算融合的新型智能计算框架，同时考虑计算特性和约束条件进行软硬件协同设计，优化系统并行计算需求，加速矩阵乘法等热点函数，高效实现系统整体迭代优化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;汪军&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3DGS World Model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;马骏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LOVON (Legged Open-Vocabulary Object Navigator)&lt;/strong&gt;：足式机器人、开放世界导航、大语言模型任务规划、机器人学习&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;分层任务规划&lt;/strong&gt;：利用大语言模型（LLMs）将复杂的自然语言指令（如&amp;quot;先跑向椅子，再靠近行人&amp;rdquo;）拆解为一系列可执行的子任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开放词汇视觉检测&lt;/strong&gt;：让机器人能够识别超出预定义类别的各种物体，大大增强了在陌生环境中的适应能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语言-运动模型&lt;/strong&gt;：将文字指令和视觉信息直接转化为控制机器人运动的向量，实现精准的导航控制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;抗干扰与鲁棒性&lt;/strong&gt;：项目专门设计了应对视觉画面抖动、目标临时丢失等现实问题的策略，提升了系统在真实场景下的可靠性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;梁俊卫&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维场景理解、具身智能&lt;/strong&gt;；为零样本3D视觉定位方法&lt;strong&gt;SeeGround&lt;/strong&gt;的博士生导师&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SeeGround方法&lt;/strong&gt;，创新性地利用2D视觉语言模型来完成零样本的3D物体定位，无需昂贵的3D标注数据，在复杂场景中表现出色。这项技术对于机器人在未知环境中进行自主操作至关重要。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>社会意识的导航模型</title>
      <link>http://localhost:1313/blog/2025/2025-11-29-social-nav/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-29-social-nav/</guid>
      <description>
        
        
        &lt;p&gt;起因是在小红书上刷到了这一篇2025年11月的新文章&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/social-nav.jpg&#34; alt=&#34;Social Navigation&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;结果却搜到了 &lt;strong&gt;[ICRA 2025] From Cognition to Precognition: A Future-Aware Framework for Social Navigation&lt;/strong&gt;，于是误闯天家到了 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Robot Social Navigation&lt;/a&gt; 的领域。&lt;/p&gt;
&lt;h2&gt;什么是 Social Navigation？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;什么是-social-navigation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bb%80%e4%b9%88%e6%98%af-social-navigation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Social Navigation（社会导航）&lt;/strong&gt; 的核心思想是 &lt;strong&gt;&amp;ldquo;以人为本&amp;rdquo;&lt;/strong&gt;。它要求机器人不仅仅把人类当作需要避开的障碍物，而是能够理解并尊重人类的社会规范与个人空间，最终实现&lt;strong&gt;自然、和谐、无感知压迫&lt;/strong&gt;的共同空间使用。例如，在走廊中与人迎面相遇时，机器人会像人一样靠右行驶；当需要穿过一群人时，它会寻找合适的时机和路径，而不是生硬地&amp;quot;切开&amp;quot;人群。&lt;/p&gt;
&lt;h2&gt;技术对比：Social Navigation vs LOVON vs VLN&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技术对比social-navigation-vs-lovon-vs-vln&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e6%9c%af%e5%af%b9%e6%af%94social-navigation-vs-lovon-vs-vln&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;特性维度&lt;/th&gt;
          &lt;th&gt;Social Navigation (社会导航)&lt;/th&gt;
          &lt;th&gt;LOVON (腿部开放词汇物体导航)&lt;/th&gt;
          &lt;th&gt;VLN (视觉语言导航)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心目标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;安全、舒适、符合社会规范地在人类共享空间中导航&lt;/td&gt;
          &lt;td&gt;在开放世界中，根据物体名称，自主搜索并导航到指定物体&lt;/td&gt;
          &lt;td&gt;根据自然语言指令，在环境中执行导航任务 (如&amp;quot;去厨房拿杯水&amp;quot;)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;环境特点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;动态、拥挤的人类环境，充满不确定性&lt;/td&gt;
          &lt;td&gt;非结构化的开放环境，地形复杂，目标物体可能被遮挡或距离遥远&lt;/td&gt;
          &lt;td&gt;通常基于仿真器（如Habitat, AI2-THOR），环境可以是静态的，也引入动态人类&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;关键输入&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;人类的位置、运动轨迹、群体行为、社会规范&lt;/td&gt;
          &lt;td&gt;目标物体的文本名称 (如 &amp;ldquo;chair&amp;rdquo;)、机器人视觉传感器数据&lt;/td&gt;
          &lt;td&gt;详尽的自然语言指令、机器人视觉传感器数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;技术侧重点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;行人轨迹预测、社交力模型、强化学习策略、舒适度与安全性评估&lt;/td&gt;
          &lt;td&gt;开放词汇目标检测、大语言模型任务分解、腿部机器人运动控制、抗运动模糊&lt;/td&gt;
          &lt;td&gt;视觉-语言对齐、指令理解、跨模态推理、路径规划&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;典型输出/动作&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;避让、保持社交距离、绕行、调整速度、非语言沟通&lt;/td&gt;
          &lt;td&gt;朝向目标物体的运动控制命令 (如速度、方向)，处理复杂地形&lt;/td&gt;
          &lt;td&gt;导航动作 (如&amp;quot;左转&amp;quot;、&amp;ldquo;前进1米&amp;rdquo;、&amp;ldquo;停止&amp;rdquo;)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心挑战&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;对人类意图的预测、复杂社会规则的建模与量化、安全性、舒适感&lt;/td&gt;
          &lt;td&gt;长时序任务规划、动态模糊下的稳定感知、复杂地形下的稳定移动、开放词汇识别泛化能力&lt;/td&gt;
          &lt;td&gt;指令与环境的关联、未知环境泛化、长指令理解、跨模态表示学习&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;学术社区与行业洞察&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;学术社区与行业洞察&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%a6%e6%9c%af%e7%a4%be%e5%8c%ba%e4%b8%8e%e8%a1%8c%e4%b8%9a%e6%b4%9e%e5%af%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;然后去&lt;a href=&#34;http://xhslink.com/o/6M94ZS8vHHm&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;学术社区（迫真）&lt;/a&gt;上搜索了一下，这里 &lt;strong&gt;seven17&lt;/strong&gt; 这位大佬也在2025年11月16-17给出了自己作为人形公司 &lt;strong&gt;SLAM 面试官&lt;/strong&gt;对业界人形机器人在研究的算法的一些经验，非常有参考意义。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;有一说一小红书真的比很多像是CSDN之类的更好的学术交流平台&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;我就很赞同这里在小红书的某个 Ask Me Anything 上看到的&lt;strong&gt;港科广的梁老师&lt;/strong&gt;的话：&lt;/p&gt;
&lt;div style={{display: &#39;flex&#39;, justifyContent: &#39;space-between&#39;, gap: &#39;10px&#39;}}&gt;
  &lt;div style={{flex: 1}}&gt;
    &lt;img src=&#34;http://localhost:1313/blog/2025/gkg-liang1.jpg&#34; alt=&#34;港科广梁老师观点1&#34; style={{width: &#39;100%&#39;}} /&gt;
  &lt;/div&gt;
  &lt;div style={{flex: 1}}&gt;
    &lt;img src=&#34;http://localhost:1313/blog/2025/gkg-liang2.jpg&#34; alt=&#34;港科广梁老师观点2&#34; style={{width: &#39;100%&#39;}} /&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;相关竞赛与研讨会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关竞赛与研讨会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e7%ab%9e%e8%b5%9b%e4%b8%8e%e7%a0%94%e8%ae%a8%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;活动名称&lt;/th&gt;
          &lt;th&gt;主要关联会议&lt;/th&gt;
          &lt;th&gt;活动形式&lt;/th&gt;
          &lt;th&gt;核心侧重点&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RoboSense机器感知挑战赛&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;IROS 2025 (官方认证竞赛)&lt;/td&gt;
          &lt;td&gt;竞赛&lt;/td&gt;
          &lt;td&gt;在动态人群环境中，使机器人的导航行为符合人类的社会规范。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Advances in Social Robot Navigation研讨会&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;ICRA 2025&lt;/td&gt;
          &lt;td&gt;研讨会&lt;/td&gt;
          &lt;td&gt;探讨社交机器人导航在规划、人机交互等领域的最新进展，并包含基准测试挑战。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;RoboSense 挑战赛&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;robosense-挑战赛&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#robosense-%e6%8c%91%e6%88%98%e8%b5%9b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;RoboSense挑战赛&lt;/strong&gt; 是 &lt;strong&gt;IROS 2025&lt;/strong&gt; 的官方认证竞赛，它设置了专门的&lt;strong&gt;社交导航赛道&lt;/strong&gt;，旨在解决机器人在真实动态环境中的导航问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;任务目标：&lt;/strong&gt; 参赛者需要开发一个基于 &lt;strong&gt;RGB-D 输入&lt;/strong&gt;的移动机器人导航模型。该模型的核心任务是让机器人在&lt;strong&gt;不影响周围人类行为&lt;/strong&gt;的前提下完成导航，并使其行为符合人类的社会规范，例如主动避让、保持合适的社交距离等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战与评测：&lt;/strong&gt; 除了衡量导航成功率和路径效率，比赛还特别引入了&lt;strong&gt;个人空间合规性（PSC）&lt;strong&gt;和&lt;/strong&gt;人机碰撞次数（H-Coll）&lt;strong&gt;等指标，专门用于量化机器人行为的&lt;/strong&gt;&amp;ldquo;社交友好度&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;前沿技术：&lt;/strong&gt; 该赛道推荐的基线模型（Baseline）是 &lt;strong&gt;Falcon&lt;/strong&gt;，这是一个由&lt;strong&gt;港科广和港科大联合提出&lt;/strong&gt;的新算法，它通过将&lt;strong&gt;轨迹预测算法融入强化学习框架&lt;/strong&gt;，让机器人能够预测行人未来的移动路径，从而实现&lt;strong&gt;更超前、更安全的规划&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ICRA 2025 研讨会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;icra-2025-研讨会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#icra-2025-%e7%a0%94%e8%ae%a8%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;除了竞争激烈的比赛，&lt;strong&gt;ICRA 的&amp;quot;Advances in Social Robot Navigation&amp;quot;研讨会&lt;/strong&gt;则是深入了解该领域学术研究和前沿发展的绝佳平台。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;活动形式：&lt;/strong&gt; 这是一个学术研讨会，会邀请领域内的专家进行讲座和专题讨论。同时，它也主办 &lt;strong&gt;Arena 4.0 挑战赛&lt;/strong&gt;，旨在为不同的社交导航策略建立基准和评测体系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心议题：&lt;/strong&gt; 研讨会关注如何使机器人的导航行为&lt;strong&gt;更易于理解、更符合社交场景&lt;/strong&gt;。探讨的技术方向包括运动任务规划、&lt;strong&gt;基础模型的应用&lt;/strong&gt;、人机交互策略等。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;我的研究计划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;我的研究计划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%88%91%e7%9a%84%e7%a0%94%e7%a9%b6%e8%ae%a1%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我打算接下来的核心往 &lt;strong&gt;Social Navigation&lt;/strong&gt; 上面靠，这里很符合&lt;strong&gt;以人为本的设计特点&lt;/strong&gt;，而 &lt;strong&gt;LOVON&lt;/strong&gt; 也确实面临这一困境。也如梁老师所言，这是个&lt;strong&gt;容易入门具身的领域&lt;/strong&gt;。可惜这个比赛在这个时候已经结束了，下面计划的第一步是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;研读 Falcon 这个 baseline&lt;/strong&gt;（也就是上面提到的 ICRA 2025 中稿文章）&lt;/li&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://robosense2025.github.io/track2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robosense&lt;/a&gt; 提供的 GitHub 代码和数据集去&lt;strong&gt;复现基线&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;参考排行榜的改进去思考参赛者解决的问题集中在哪里，又是如何进行的&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;相关资源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关资源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%b5%84%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Resource&lt;/th&gt;
          &lt;th&gt;Link&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;GitHub Repository&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/robosense2025/track2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/robosense2025/track2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Baseline code and setup instructions&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Dataset&lt;/td&gt;
          &lt;td&gt;HuggingFace Dataset&lt;/td&gt;
          &lt;td&gt;Dataset with training and test splits&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Baseline Model&lt;/td&gt;
          &lt;td&gt;Pre-Trained Model&lt;/td&gt;
          &lt;td&gt;Weights of the baseline model&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Registration&lt;/td&gt;
          &lt;td&gt;Google Form (Closed on August 15th)&lt;/td&gt;
          &lt;td&gt;Team registration for the challenge&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Evaluation Server&lt;/td&gt;
          &lt;td&gt;EvalAI Platform&lt;/td&gt;
          &lt;td&gt;Online evaluation platform&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;HuggingFace 上的热门研究&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;huggingface-上的热门研究&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#huggingface-%e4%b8%8a%e7%9a%84%e7%83%ad%e9%97%a8%e7%a0%94%e7%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在 huggingface 上按 trending 搜索 social navigation 的&lt;a href=&#34;https://huggingface.co/papers/trending?q=social&amp;#43;navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;结果&lt;/a&gt;如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;论文标题&lt;/th&gt;
          &lt;th&gt;核心工作摘要&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SACSoN&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过最小化机器人对行人行为的**&amp;ldquo;反事实扰动&amp;rdquo;&lt;strong&gt;，学习一种&lt;/strong&gt;不打扰人类的导航策略**。其关键在于使用大量真实人机交互数据进行训练。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Exploiting Proximity-Aware Tasks&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出&lt;strong&gt;邻近感知任务&lt;/strong&gt;，通过让策略理解即时和未来的碰撞危险，为强化学习导航策略注入&lt;strong&gt;常识性社交行为&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SELFI&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出一种&lt;strong&gt;在线自学习方法&lt;/strong&gt;，在预训练策略的基础上，利用在线模型无关的强化学习进行快速微调，使机器人能根据实际经验持续改进社交导航行为。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SocialNav-SUB&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;引入了首个用于评估&lt;strong&gt;视觉语言模型（VLM）&lt;strong&gt;在社交导航场景中理解能力的基准，发现当前 VLM 在&lt;/strong&gt;空间、时空和社交推理&lt;/strong&gt;方面仍有明显不足。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;OLiVia-Nav&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;将&lt;strong&gt;视觉语言模型与在线终身学习框架&lt;/strong&gt;结合，通过独特的蒸馏方法让轻量级 VLM 直接理解社交和环境上下文，并规划符合社交规范的轨迹。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Habitat 3.0&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;推出了一个支持&lt;strong&gt;人、虚拟化身和机器人协同&lt;/strong&gt;的模拟平台，用于研究社交导航等协作任务，并提供了&lt;strong&gt;人类在环的基础设施&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;GOAT&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;通用导航系统&lt;/strong&gt;，能够处理多模态目标，并通过持续构建实例感知的语义记忆，实现&lt;strong&gt;终身学习和跨平台部署&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;GRUtopia&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;构建了一个&lt;strong&gt;大规模的模拟交互式3D社会&lt;/strong&gt;，包含多样化的场景和由 &lt;strong&gt;LLM 驱动的虚拟角色&lt;/strong&gt;，用于支持社交移动导航等具身AI任务的训练与评估。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RoboSense&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;大规模的以自我为中心的多模态数据集&lt;/strong&gt;，专注于拥挤和非结构化环境中的感知与导航，为近场场景理解提供丰富标注。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Social NCE&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过&lt;strong&gt;对比学习&lt;/strong&gt;来提升运动表示的社交感知能力，显式地建模危险负样本，以此降低轨迹预测和行为克隆中的碰撞率。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;DriVLMe&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;探索了基于&lt;strong&gt;视频语言模型的自动驾驶智能体&lt;/strong&gt;，通过模拟环境和真实人类对话进行训练，旨在实现与人类的自然有效沟通。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;EPO&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出&lt;strong&gt;显式策略优化方法&lt;/strong&gt;，利用多轮强化学习和自我博弈来提升大语言模型在社交对话等任务中的战略推理能力。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;EmbodiedEval&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;统一的、交互式的基准&lt;/strong&gt;，用于全面评估多模态大模型在具身任务（如导航、社交交互）中的能力。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SocialEval&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个评估&lt;strong&gt;大语言模型社交智能的双语基准&lt;/strong&gt;，通过叙事脚本从结果和过程两个维度评估模型的人际交往能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;研究趋势分析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究趋势分析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e8%b6%8b%e5%8a%bf%e5%88%86%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;从这些论文可以看出，社交导航领域的研究呈现出一些明显的趋势和重点方向：&lt;/p&gt;
&lt;h3&gt;1. 从&amp;quot;避障&amp;quot;到&amp;quot;避人&amp;quot;，再到&amp;quot;不扰人&amp;quot;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-从避障到避人再到不扰人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%8e%e9%81%bf%e9%9a%9c%e5%88%b0%e9%81%bf%e4%ba%ba%e5%86%8d%e5%88%b0%e4%b8%8d%e6%89%b0%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;像 &lt;strong&gt;SACSoN&lt;/strong&gt; 这样的工作，其目标已经超越了基础的安全避障，而是追求&lt;strong&gt;更高级的社交合规性&lt;/strong&gt;，希望机器人的存在和行为&lt;strong&gt;尽可能不改变人类的自然行为&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;2. 学习与规划的关键：预测与上下文理解&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-学习与规划的关键预测与上下文理解&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%ad%a6%e4%b9%a0%e4%b8%8e%e8%a7%84%e5%88%92%e7%9a%84%e5%85%b3%e9%94%ae%e9%a2%84%e6%b5%8b%e4%b8%8e%e4%b8%8a%e4%b8%8b%e6%96%87%e7%90%86%e8%a7%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;许多研究致力于让机器人更好地&lt;strong&gt;预测未来&lt;/strong&gt;（如行人轨迹）和&lt;strong&gt;理解环境上下文&lt;/strong&gt;（如社交规则）。&lt;strong&gt;Exploiting Proximity-Aware Tasks&lt;/strong&gt; 和 &lt;strong&gt;Social NCE&lt;/strong&gt; 都是通过不同的方式让模型内化对潜在危险和社交规范的理解。&lt;/p&gt;
&lt;h3&gt;3. 基础模型与终身学习成为新风向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-基础模型与终身学习成为新风向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e7%bb%88%e8%ba%ab%e5%ad%a6%e4%b9%a0%e6%88%90%e4%b8%ba%e6%96%b0%e9%a3%8e%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;OLiVia-Nav&lt;/strong&gt; 和 &lt;strong&gt;GOAT&lt;/strong&gt; 等论文清晰地展示了如何利用&lt;strong&gt;视觉语言模型（VLM）的先验知识&lt;/strong&gt;进行社交推理，并强调通过&lt;strong&gt;终身学习&lt;/strong&gt;使机器人能够适应不断变化的环境和新遇到的社交场景。&lt;/p&gt;
&lt;h3&gt;4. 对仿真、数据与评估的持续投入&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-对仿真数据与评估的持续投入&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e5%af%b9%e4%bb%bf%e7%9c%9f%e6%95%b0%e6%8d%ae%e4%b8%8e%e8%af%84%e4%bc%b0%e7%9a%84%e6%8c%81%e7%bb%ad%e6%8a%95%e5%85%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;高质量的仿真平台（&lt;strong&gt;Habitat 3.0&lt;/strong&gt;, &lt;strong&gt;GRUtopia&lt;/strong&gt;）、大规模数据集（&lt;strong&gt;RoboSense&lt;/strong&gt;）和专门的评估基准（&lt;strong&gt;SocialNav-SUB&lt;/strong&gt;, &lt;strong&gt;EmbodiedEval&lt;/strong&gt;, &lt;strong&gt;SocialEval&lt;/strong&gt;）是推动领域发展的&lt;strong&gt;关键基础设施&lt;/strong&gt;，这些工作为训练、测试和公平比较不同算法提供了坚实基础。&lt;/p&gt;
&lt;h2&gt;核心挑战与思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;核心挑战与思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a0%b8%e5%bf%83%e6%8c%91%e6%88%98%e4%b8%8e%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;社会导航的终极目标&lt;/strong&gt;是实现&lt;strong&gt;安全、舒适、符合社会规范的人机共存与协作&lt;/strong&gt;。它关注的是导航行为的**&amp;ldquo;社交智能&amp;quot;和&amp;quot;礼仪&amp;rdquo;&lt;strong&gt;。相比之下，许多&lt;/strong&gt;视觉语言导航（VLN）&lt;strong&gt;或其变体（如 &lt;strong&gt;LOVON&lt;/strong&gt;）更侧重于理解指令、识别物体或地点，并完成具身的导航任务，其核心是&lt;/strong&gt;&amp;ldquo;完成任务&amp;quot;的准确性**。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大的难点在于&lt;/strong&gt;，它需要让机器人理解并量化人类社会中那些&lt;strong&gt;不言自明、动态变化的社交潜规则&lt;/strong&gt;。例如，如何定义并计算**&amp;ldquo;个人空间&amp;rdquo;&lt;strong&gt;？如何判断什么样的路径是&lt;/strong&gt;&amp;ldquo;优雅&amp;quot;而非&amp;quot;冒犯&amp;quot;的**？这与开放词汇任务中要求模型识别未曾见过的物体类别（如 &lt;strong&gt;LOVON&lt;/strong&gt;）相比，是不同类型和层次的挑战。&lt;strong&gt;开放词汇扩展了机器人的&amp;quot;知识面&amp;rdquo;，而社会导航则是在塑造机器人的&amp;quot;情商&amp;quot;和行为方式&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如说 &lt;strong&gt;Track2&lt;/strong&gt; 的工作，核心任务是让机器人学会在充满动态行人的室内环境中（如办公楼、商场），实现&lt;strong&gt;安全、高效且符合社会规范的导航&lt;/strong&gt;。不仅要求机器人成功到达目的地（成功率 &lt;strong&gt;SR&lt;/strong&gt;），还要求其行为**&amp;ldquo;像个有礼貌的人&amp;rdquo;&lt;strong&gt;，比如主动保持舒适的社交距离（个人空间合规性 &lt;strong&gt;PSC&lt;/strong&gt;）、避免碰撞（人类碰撞率 &lt;strong&gt;H-Coll&lt;/strong&gt;），并规划出高效的路径（路径长度加权成功率 &lt;strong&gt;SPL&lt;/strong&gt;）。赛事提供的基线模型是基于 &lt;strong&gt;Falcon 框架&lt;/strong&gt;，它通过融入对&lt;/strong&gt;行人未来轨迹的预测**，来让机器人实现更具前瞻性的导航决策。&lt;/p&gt;
&lt;h2&gt;未来方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;未来方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;而在 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;awesome系列&lt;/a&gt; 里，我们可以看到以下几个重要方向：&lt;/p&gt;
&lt;h3&gt;1. 融合基础模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-融合基础模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e8%9e%8d%e5%90%88%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;这是一个明显的趋势。探索如何利用&lt;strong&gt;大型语言模型（LLM）&lt;strong&gt;和&lt;/strong&gt;视觉语言模型（VLM）&lt;/strong&gt;，让机器人能够理解和遵从复杂、抽象的社会规则（例如，**&amp;ldquo;在拥挤处耐心跟随&amp;rdquo;**而不仅仅是&amp;quot;避开人群&amp;rdquo;），或者更好地解读人类的行为意图。&lt;/p&gt;
&lt;h3&gt;2. 提升仿真环境的真实性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-提升仿真环境的真实性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e6%8f%90%e5%8d%87%e4%bb%bf%e7%9c%9f%e7%8e%af%e5%a2%83%e7%9a%84%e7%9c%9f%e5%ae%9e%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;开发更先进的仿真平台（如持续更新的 &lt;strong&gt;Arena 系列&lt;/strong&gt;），模拟更复杂的人类行为（如&lt;strong&gt;突然驻足、群体交谈、协作避让&lt;/strong&gt;），这对于在低成本前提下验证算法的鲁棒性至关重要。&lt;/p&gt;
&lt;h3&gt;3. 增强算法的可解释性与信任度&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-增强算法的可解释性与信任度&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%a2%9e%e5%bc%ba%e7%ae%97%e6%b3%95%e7%9a%84%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e4%b8%8e%e4%bf%a1%e4%bb%bb%e5%ba%a6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;研究如何让机器人的导航决策过程对人类而言&lt;strong&gt;更透明、更容易理解&lt;/strong&gt;。例如，生成机器人为何选择某条路径的**&amp;ldquo;因果解释&amp;rdquo;**，这能极大地增强人类对机器人的信任，促进人机共处。&lt;/p&gt;
&lt;h3&gt;4. 深化人机交互研究&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-深化人机交互研究&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e6%b7%b1%e5%8c%96%e4%ba%ba%e6%9c%ba%e4%ba%a4%e4%ba%92%e7%a0%94%e7%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;关注机器人的导航行为如何影响人类的感受和效率。通过用户研究，量化什么是让人感到**&amp;ldquo;舒适&amp;rdquo;、&amp;ldquo;自然&amp;rdquo;**的机器人行为，并将这些发现转化为算法设计的指导原则。&lt;/p&gt;
&lt;h3&gt;5. 应对极端与复杂场景&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-应对极端与复杂场景&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e5%ba%94%e5%af%b9%e6%9e%81%e7%ab%af%e4%b8%8e%e5%a4%8d%e6%9d%82%e5%9c%ba%e6%99%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;专注于解决更具挑战性的场景，例如&lt;strong&gt;重度遮挡&lt;/strong&gt;（在人群中&amp;quot;看不见&amp;quot;部分行人）、对**&amp;ldquo;不可预测&amp;quot;行人的识别与避让**，以及在密集人群中如何寻找安全路径。&lt;/p&gt;
&lt;h3&gt;研究方向与未来工作规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究方向与未来工作规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91%e4%b8%8e%e6%9c%aa%e6%9d%a5%e5%b7%a5%e4%bd%9c%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Robot Social Navigation&lt;/a&gt; 的梳理，当前研究主要集中在以下几个方向：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;研究方向&lt;/th&gt;
          &lt;th&gt;具体未来工作规划&lt;/th&gt;
          &lt;th&gt;来源论文&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;模型泛化与适应性&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt;
          &lt;td&gt;开发轻量化VLM便于机器人部署；探索&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;多模态融合（视觉、语言、传感器）&lt;/strong&gt;&lt;/span&gt;；研究&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;在线/终身学习框架以适应新场景&lt;/strong&gt;&lt;/span&gt;；提升对&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;动态场景和长时序任务的理解与规划能力&lt;/strong&gt;&lt;/span&gt;。&lt;/td&gt;
          &lt;td&gt;VLM-Social-Nav, OLiVia-Nav, Following the Human Thread&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;场景理解与交互&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt;
          &lt;td&gt;研究人类轨迹预测与社交动态的&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;实时、精准推断&lt;/strong&gt;&lt;/span&gt;；探索&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;多智能体协同与群体行为建模&lt;/strong&gt;&lt;/span&gt;；开发更强大的&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;场景表征与上下文理解能力&lt;/strong&gt;&lt;/span&gt;，以处理复杂的社会规则。&lt;/td&gt;
          &lt;td&gt;Following the Human Thread, DiPCAN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;评估体系与伦理&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;建立更全面的评估指标（如引入&amp;quot;人类赋权&amp;quot;概念）；设计标准化基准测试与仿真环境；关注算法的公平性、透明度、隐私保护及人类舒适度等社会伦理影响。&lt;/td&gt;
          &lt;td&gt;In Search of a Lost Metric, Frontiers Research Topic&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; benchmark一般貌似都要自己提出一个，这样能增大工作量说是，像TrackVLA就是这样提出了一个EVTbench开源使用&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;基于Awesome系列的具体研究方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基于awesome系列的具体研究方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e4%ba%8eawesome%e7%b3%bb%e5%88%97%e7%9a%84%e5%85%b7%e4%bd%93%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;根据&lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome系列&lt;/a&gt;的详细梳理，以下是从&lt;strong&gt;方法、数据集、评估&lt;/strong&gt;等多个维度总结的具体研究方向：&lt;/p&gt;
&lt;h4&gt;1. 基础模型在社交导航中的应用&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-基础模型在社交导航中的应用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%9c%a8%e7%a4%be%e4%ba%a4%e5%af%bc%e8%88%aa%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; VLM-Social-Nav, OLiVia-Nav, Social-LLaVA, CoNVOI, BehAV&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;轻量化部署：&lt;/strong&gt; 研究如何将大型**视觉语言模型（VLM）&lt;strong&gt;和&lt;/strong&gt;大语言模型（LLM）**蒸馏或微调到适合机器人实时部署的规模&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态融合：&lt;/strong&gt; 探索视觉、语言、传感器数据的深度融合，提升对复杂社交场景的理解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在线终身学习：&lt;/strong&gt; 开发能够持续适应新场景和人类行为的在线学习框架&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. 轨迹预测与场景理解&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-轨迹预测与场景理解&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e8%bd%a8%e8%bf%b9%e9%a2%84%e6%b5%8b%e4%b8%8e%e5%9c%ba%e6%99%af%e7%90%86%e8%a7%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Social LSTM, STGAT, From Cognition to Precognition, Following the Human Thread&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;实时轨迹预测：&lt;/strong&gt; 提升对人类未来移动路径的预测精度和实时性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;群体行为建模：&lt;/strong&gt; 研究多智能体协同、群体动态（如群体分裂与合并）的建模方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上下文理解：&lt;/strong&gt; 开发更强大的场景表征能力，理解复杂的社会规则和社交动态&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. 强化学习与混合方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-强化学习与混合方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%b7%b7%e5%90%88%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; SACSoN, SELFI, DR-MPC, Hybrid Approaches&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;奖励函数设计：&lt;/strong&gt; 探索如何将社交规范、舒适度等抽象概念量化为强化学习的奖励信号&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;混合方法：&lt;/strong&gt; 结合模型预测控制（MPC）、采样规划等传统方法与深度强化学习&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;样本效率：&lt;/strong&gt; 提升强化学习在社交导航任务中的样本效率，减少真实世界训练成本&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4. 可解释性与信任&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-可解释性与信任&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e4%b8%8e%e4%bf%a1%e4%bb%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Generating Causal Explanations, Explainability and Trust&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;因果解释：&lt;/strong&gt; 生成机器人导航决策的因果解释，增强人类对机器人的信任&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;透明度：&lt;/strong&gt; 研究如何让机器人的决策过程对人类更透明、更容易理解&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;5. 数据集与评估基准&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-数据集与评估基准&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e6%95%b0%e6%8d%ae%e9%9b%86%e4%b8%8e%e8%af%84%e4%bc%b0%e5%9f%ba%e5%87%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; SCAND, MuSoHu, SocNavBench, Arena系列, SocialNav-SUB&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;真实世界数据集：&lt;/strong&gt; 构建大规模、多模态的真实人机交互数据集&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真平台：&lt;/strong&gt; 开发更真实的仿真环境（如Arena 4.0, Habitat 3.0），支持复杂人类行为模拟&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标：&lt;/strong&gt; 设计更全面的评估体系，包括&lt;strong&gt;人类赋权&lt;/strong&gt;、个人空间合规性（PSC）、碰撞率（H-Coll）等&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;6. 用户研究与伦理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;6-用户研究与伦理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#6-%e7%94%a8%e6%88%b7%e7%a0%94%e7%a9%b6%e4%b8%8e%e4%bc%a6%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Social Momentum, How Do Robot Experts Measure Success, Overlapping Social Navigation Principles&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;用户研究：&lt;/strong&gt; 通过用户研究量化什么是&amp;quot;舒适&amp;rdquo;、&amp;ldquo;自然&amp;quot;的机器人行为&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;伦理考量：&lt;/strong&gt; 关注算法的公平性、透明度、隐私保护及对人类舒适度的影响&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;研究计划建议&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究计划建议&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e8%ae%a1%e5%88%92%e5%bb%ba%e8%ae%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于以上分析，可以从以下几个方面思考研究计划：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;关注新兴的评估范式：&lt;/strong&gt; 像&lt;a href=&#34;http://export.arxiv.org/abs/2501.01539&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;人类赋权&amp;rdquo;&lt;/a&gt;这类新指标方兴未艾，如何量化、验证并将其有效融入强化学习奖励函数或模型预测控制的代价函数中，是一个很有潜力的方向。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;探索基础模型的高效应用：&lt;/strong&gt; 研究如何蒸馏或微调大型VLM/LMM，在保持其社交推理能力的同时，满足机器人平台对低延迟和低功耗的严苛要求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;致力于弥合仿真与现实差距：&lt;/strong&gt; 开发更好的**领域自适应（Domain Adaptation）&lt;strong&gt;技术或&lt;/strong&gt;元学习（Meta-Learning）**策略，让模型在离开仿真环境后能快速适应真实世界的复杂性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战更复杂的社交场景：&lt;/strong&gt; 可以专注于研究机器人在密集人群、群组交互（如穿越一个正在交谈的群体）或长程、多目标导航任务中的表现，这些场景对现有技术提出了更高要求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;构建自己的评估基准：&lt;/strong&gt; 参考TrackVLA提出EVTbench的做法，开发针对特定场景或问题的标准化评估基准，这不仅能增加研究工作量，还能为领域提供有价值的工具。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
  </channel>
</rss>
