<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Embodied</title>
    <link>http://localhost:1313/tags/embodied/</link>
    <description>Recent content in Embodied on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/embodied/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>LOVON 相关 Baseline 调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-11/</guid>
      <description>
        
        
        &lt;h1&gt;LOVON 相关 Baseline 调研&lt;/h1&gt;&lt;p&gt;本篇内容主要集中在针对 LOVON 论文中所对比的 paper 工作，他的仿真指标，一方面顺着前人的工作一路做下来思路比较直接也比较连贯，另一方面我还是觉得&lt;code&gt;gym-unrealcv&lt;/code&gt;这个模拟仿真的引擎相对&lt;code&gt;MatterPort3d&lt;/code&gt;还是小众了一点，也没有现成的博客文章去汇总有哪些工作用到了这个。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/lovon-baseline.png&#34; alt=&#34;这个仿真得分已经终结这个领域了&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;DIMP: Learning discriminative model prediction for tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;dimp-learning-discriminative-model-prediction-for-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dimp-learning-discriminative-model-prediction-for-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;SARL: End-to-end active object tracking and its real-world deployment via reinforcement learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;sarl-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#sarl-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;AD-VAT: End-to-end active object tracking and its real-world deployment via reinforcement learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ad-vat-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ad-vat-end-to-end-active-object-tracking-and-its-real-world-deployment-via-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;AD-VAT+: An asymmetric dueling mechanism for learning and understanding visual active tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ad-vat-an-asymmetric-dueling-mechanism-for-learning-and-understanding-visual-active-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ad-vat-an-asymmetric-dueling-mechanism-for-learning-and-understanding-visual-active-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;TS: Towards distraction-robust active visual tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;ts-towards-distraction-robust-active-visual-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ts-towards-distraction-robust-active-visual-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;RSPT: reconstruct surroundings and predict trajectory for generalizable active object tracking&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;rspt-reconstruct-surroundings-and-predict-trajectory-for-generalizable-active-object-tracking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#rspt-reconstruct-surroundings-and-predict-trajectory-for-generalizable-active-object-tracking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;EVT: Empowering embodied visual tracking with visual foundation models and offline RL&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;evt-empowering-embodied-visual-tracking-with-visual-foundation-models-and-offline-rl&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#evt-empowering-embodied-visual-tracking-with-visual-foundation-models-and-offline-rl&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;TrakVLA: Embodied visual tracking in the wild&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;trakvla-embodied-visual-tracking-in-the-wild&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#trakvla-embodied-visual-tracking-in-the-wild&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;
  &lt;p&gt;PKU在2025年5月的工作，VLA对训练算力和时间的要求堪称恐怖，所以这里单纯参考一下思想&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;Embodied visual tracking enables an agent to follow a specific target in dynamic environments using &lt;strong&gt;only egocentric vision&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of &lt;strong&gt;severe occlusion&lt;/strong&gt; and &lt;strong&gt;high scene dynamics&lt;/strong&gt;, 也就是遮挡和高动态性。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;研究方向&lt;/th&gt;
          &lt;th&gt;现有方法特点&lt;/th&gt;
          &lt;th&gt;局限性&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;具身视觉跟踪&lt;/td&gt;
          &lt;td&gt;分模型基（IBVS）、RL 基（AD-VAT、EVT [6]）、IL 基（Uni-NaVid）&lt;/td&gt;
          &lt;td&gt;误差累积；Uni-NaVid 依赖离散动作空间&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;具身导航&lt;/td&gt;
          &lt;td&gt;聚焦静态室内环境（如视觉 - 语言导航）&lt;/td&gt;
          &lt;td&gt;忽略真实世界动态性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;VLA 模型&lt;/td&gt;
          &lt;td&gt;用于操纵/导航，基于预训练 VLM 扩展动作生成&lt;/td&gt;
          &lt;td&gt;推理效率低，仅在低动态环境验证&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;而现有的相关工作都是将Recognition和Trajectory Planning给decouple出来的，而these methods are limited to category-level tracking in relatively open areas, 看来大家都意识到了这个问题，而作者指出这是因为上面decoupling的两个模块会产生error accumulation——识别错误可能导致规划失效，反之亦然。&lt;/p&gt;
&lt;p&gt;因此要用unified framework统合起来，共用token encoding（deconding的时候再分为两个头，一个language modeling head解码识别任务的文本响应，一个anchor-based diffusion head生成航点轨迹应用于规划任务）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/trackVLA_overview.png&#34; alt=&#34;overview&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>重新思考三维空间感知与具身导航决策在毕设中的研究点</title>
      <link>http://localhost:1313/blog/2025/2025-11-12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-12/</guid>
      <description>
        
        
        &lt;p&gt;首先，我们的Baseline —— LOVON (Legged Open-Vocabulary Object Navigator, 2025) 是一个在 Gym-Unreal（即 Gym-UnrealCV 风格的仿真 benchmark）上做了大规模仿真实验来验证其开阔词表目标搜索与导航能力；文中强调用虚幻环境来做长航时、动态目标搜索的系统验证（包括视觉抖动、目标短暂消失等问题）并在仿真里验证 Laplacian Variance Filtering、语言→运动模型等模块。也进行了真实腿式机器人（Unitree 系列）上的跨域验证以检验 sim→real。&lt;/p&gt;
&lt;!-- truncate --&gt;
&lt;h2&gt;思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们第一步要做的就是 Define Problem。若能有清晰的问题定位 + 合理指标 +实证结果 +对比分析，就有很大机会产出成果。一个很好的方法就是自问自答：&lt;/p&gt;
&lt;h3&gt;一、现状定位：用 LOVON 的方法在真机上效果很差──最关键的失败点是什么？“机器狗撞门框”？那是什么原因？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一现状定位用-lovon-的方法在真机上效果很差最关键的失败点是什么机器狗撞门框那是什么原因&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e7%8e%b0%e7%8a%b6%e5%ae%9a%e4%bd%8d%e7%94%a8-lovon-%e7%9a%84%e6%96%b9%e6%b3%95%e5%9c%a8%e7%9c%9f%e6%9c%ba%e4%b8%8a%e6%95%88%e6%9e%9c%e5%be%88%e5%b7%ae%e6%9c%80%e5%85%b3%e9%94%ae%e7%9a%84%e5%a4%b1%e8%b4%a5%e7%82%b9%e6%98%af%e4%bb%80%e4%b9%88%e6%9c%ba%e5%99%a8%e7%8b%97%e6%92%9e%e9%97%a8%e6%a1%86%e9%82%a3%e6%98%af%e4%bb%80%e4%b9%88%e5%8e%9f%e5%9b%a0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;是因为纯2D检测＋动作映射，缺乏深度／3D理解？&lt;/li&gt;
&lt;li&gt;是因为没有障碍物避障规划？&lt;/li&gt;
&lt;li&gt;是因为导航规划缺失，仅“向目标走”而不考虑路径？&lt;/li&gt;
&lt;li&gt;还是别的问题（如机器狗控制延迟、检测误差大、目标消失后无追踪策略）？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LOVON的原理，也就是视觉追踪的原理在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;目标提取与筛选（&lt;code&gt;_yolo_image_post_process&lt;/code&gt; 方法）&lt;/p&gt;
&lt;p&gt;先通过 &lt;code&gt;object_extractor&lt;/code&gt; 从任务指令（默认任务是 run to the person at speed of 0.36 m/s，提取目标为 “person”）中提取目标类别。YOLO 模型输出所有检测框后，只保留类别与提取目标一致的框，过滤无关目标。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;滑动窗口历史管理&lt;/p&gt;
&lt;p&gt;初始化 5 个历史缓存列表，分别存储目标类别、置信度、归一化坐标（xyn）、归一化宽高（whn）、像素坐标（xyxy）。每帧仅保留置信度最高的检测框，加入缓存列表；当列表长度超过 &lt;code&gt;lengthen_filter&lt;/code&gt; 时，删除最早的帧，维持窗口大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;追踪结果计算&lt;/p&gt;
&lt;p&gt;对缓存列表中的数据取平均值，得到平滑后的置信度、坐标和宽高。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;object_xyn[0]&lt;/code&gt; 是目标中心的水平归一化坐标（0~1，0 为左边界、0.5 为图像中心、1 为右边界）。&lt;/li&gt;
&lt;li&gt;若目标在图像中心（&lt;code&gt;xyn[0] ≈ 0.5&lt;/code&gt;）：机器狗沿前后方向运动（&lt;code&gt;v_x&lt;/code&gt; 按任务指令速度，如 0.36m/s，&lt;code&gt;v_y = 0&lt;/code&gt;，&lt;code&gt;w_z = 0&lt;/code&gt;），即 “往前走”。&lt;/li&gt;
&lt;li&gt;若目标偏左（&lt;code&gt;xyn[0] &amp;lt; 0.5&lt;/code&gt;）：&lt;code&gt;w_z&lt;/code&gt; 为正（顺时针旋转），同时 &lt;code&gt;v_x&lt;/code&gt; 降低，直到目标回到中心；偏右则相反。&lt;/li&gt;
&lt;li&gt;任务指令中的 “speed” 仅限制 &lt;code&gt;v_x&lt;/code&gt; 的最大值，而非强制固定 &lt;code&gt;v_x&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;统计列表中出现次数最多的目标类别，作为当前追踪目标（避免单帧误检影响）。若目标类别为 “NULL”（无有效检测），则重置追踪结果为默认值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;跟丢的判定标准&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单帧无检测：YOLO 未检测到与 &lt;code&gt;extracted_object&lt;/code&gt; 匹配的框 → 往历史缓存中添加 “NULL” 和 0 置信度。&lt;/li&gt;
&lt;li&gt;连续跟丢：当历史缓存（长度由 &lt;code&gt;lengthen_filter&lt;/code&gt; 控制）中 “NULL” 出现次数最多 → &lt;code&gt;most_common_object&lt;/code&gt; 变为 “NULL”，&lt;code&gt;avg_confidence&lt;/code&gt; 设为 0 → 判定为 “跟丢”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;motion_predictor&lt;/code&gt; 接收 “跟丢状态” 后，生成搜索型 &lt;code&gt;motion_vector&lt;/code&gt;：&lt;/p&gt;
&lt;p&gt;通常是「旋转搜索」：&lt;code&gt;v_x = 0&lt;/code&gt;（不前后动）、&lt;code&gt;v_y = 0&lt;/code&gt;（不左右动）、&lt;code&gt;w_z ≠ 0&lt;/code&gt;（缓慢旋转，扫描周围环境）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;机器狗撞门框的原因在于，这里现实环境的部署代码通过 YOLO 只识别到了目标但是没有理解环境与障碍物，而当人消失在门后时，最后一帧这个目标是在画面中心的，因此机器狗会往前走直到撞到门框，又或者笨笨的在门框那个位置旋转搜索。因为没有开源其仿真智能体的代码所以不知道模拟环境是怎么规避这个问题的&lt;/p&gt;
&lt;h3&gt;二、Gap 与定位：基于你上面的回答，问题在哪儿？用一句话描述这里的 gap（研究空白）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二gap-与定位基于你上面的回答问题在哪儿用一句话描述这里的-gap研究空白&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cgap-%e4%b8%8e%e5%ae%9a%e4%bd%8d%e5%9f%ba%e4%ba%8e%e4%bd%a0%e4%b8%8a%e9%9d%a2%e7%9a%84%e5%9b%9e%e7%ad%94%e9%97%ae%e9%a2%98%e5%9c%a8%e5%93%aa%e5%84%bf%e7%94%a8%e4%b8%80%e5%8f%a5%e8%af%9d%e6%8f%8f%e8%bf%b0%e8%bf%99%e9%87%8c%e7%9a%84-gap%e7%a0%94%e7%a9%b6%e7%a9%ba%e7%99%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;比如：“在足式机器人真实场景下，当前Open-vocab检测＋简单动作生成不能有效处理目标暂时丢失和复杂障碍物，导致跟踪／导航失败”。还是要聚焦 “障碍物避障” 或 “三维深度理解”？&lt;/p&gt;
&lt;p&gt;在足式机器人开放世界目标追踪任务中，现有基于纯 2D 视觉目标检测的追踪 - 运动映射方案，因缺乏环境障碍物感知与三维空间理解，且目标暂时丢失后仅采用无环境适配的旋转搜索策略，导致无法应对 “目标被遮挡 / 消失后因路径误判碰撞障碍物” 等真实场景挑战，难以实现稳健的长时追踪与运动控制（具体有没有3D视觉目标检测的论文工作，现在还没有做过调研）&lt;/p&gt;
&lt;h3&gt;三、重要性在哪里？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三重要性在哪里&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e9%87%8d%e8%a6%81%e6%80%a7%e5%9c%a8%e5%93%aa%e9%87%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对学术来说：为什么“足式机器人 + open-vocab目标导航/跟踪”值得研究？是否当前工作少？&lt;/li&gt;
&lt;li&gt;对应用来说：在真实环境（室内／复杂家具／光照变化）中，解决这个问题会带来什么改进？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于我来说，我还只是一个入门新手，打算通过本科毕设的机会，从3D世界理解和具身导航决策这个小角度切入来入门具身领域，所以我也说不清楚学术和应用上的重要性，只求发ccfb以上的paper证明自己&lt;/p&gt;
&lt;h3&gt;四、创新点初步想法？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四创新点初步想法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9b%e5%88%9b%e6%96%b0%e7%82%b9%e5%88%9d%e6%ad%a5%e6%83%b3%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过和导师学长们讨论列出了很多可能的优化方向（环境理解增强、分层决策升级），从这些中最可能做出论文中&lt;strong&gt;可量化贡献&lt;/strong&gt;的一个或两个是什么？
比如：“用 BEV 俯视地图 +轨迹预测 来增强 open-vocab 目标导航”；或者：“在足式机器人上验证视觉+深度融合检测在目标丢失场景下的跟踪稳定性提升”。哪一个更倾向？为什么？&lt;/p&gt;
&lt;p&gt;我不知道量化贡献的指标可以在哪里进一步优化啊，原因也可能在于我读的文献太少了，LOVON在仿真里所使用的指标为衡量 100 次实验中完成任务的平均步数、衡量 100 次实验中成功完成任务的比例两个，而如何去量化现实任务的指标与sim2real的优化，因为文献读的不多所以暂时我还不能回答这个问题&lt;/p&gt;
&lt;h3&gt;五、可量化指标与对比：要发论文，必须有可测量的结果，可以测量哪些指标？例如：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五可量化指标与对比要发论文必须有可测量的结果可以测量哪些指标例如&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94%e5%8f%af%e9%87%8f%e5%8c%96%e6%8c%87%e6%a0%87%e4%b8%8e%e5%af%b9%e6%af%94%e8%a6%81%e5%8f%91%e8%ae%ba%e6%96%87%e5%bf%85%e9%a1%bb%e6%9c%89%e5%8f%af%e6%b5%8b%e9%87%8f%e7%9a%84%e7%bb%93%e6%9e%9c%e5%8f%af%e4%bb%a5%e6%b5%8b%e9%87%8f%e5%93%aa%e4%ba%9b%e6%8c%87%e6%a0%87%e4%be%8b%e5%a6%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;目标被丢失的次数／恢复次数&lt;/li&gt;
&lt;li&gt;障碍物碰撞次数&lt;/li&gt;
&lt;li&gt;成功到达目标的比例&lt;/li&gt;
&lt;li&gt;路径长度／时间／效率&lt;/li&gt;
&lt;li&gt;跟踪保持时间／跟丢时间
− 真机 vs 仿真的差距（sim2real gap）
能够在实机上测这些指标吗？哪些可能无法测？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;六、实验平台／可行性：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六实验平台可行性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e5%ae%9e%e9%aa%8c%e5%b9%b3%e5%8f%b0%e5%8f%af%e8%a1%8c%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;已有的硬件是 Unitree Go2 足式机器人，这很好。你能控制机器人做什么动作（向前、转、停止、避障）？你能获取哪些传感器数据（RGB、深度、IMU、里程计）？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否有仿真实验环境（如 Gym-UnrealCV 场景）可以先做仿真再到实机？仿真与实机之间能记录相同指标吗？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;时间上本科毕设资源有限，预计能做多少场景／多少实验次数？这个对决定指标和可行性很重要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于硬件设备，要关注[官方SDK文档](&lt;a href=&#34;https://support.unitree.com/home/en/developer&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://support.unitree.com/home/en/developer&lt;/a&gt;）：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;一、动作控制能力&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基础运动控制
&lt;ul&gt;
&lt;li&gt;前进 / 后退 / 转向：通过Move(vx, vy, vyaw)函数直接设置线速度（vx/vy）和角速度（vyaw），支持相对于世界坐标系的运动控制。例如，Move(0.5, 0, 0)使机器人以 0.5m/s 速度向前移动。&lt;/li&gt;
&lt;li&gt;停止：调用StopMove()立即终止所有运动，进入静止状态。&lt;/li&gt;
&lt;li&gt;步态切换：通过SwitchGait(int d)选择不同步态（如小跑、踱步），或使用ContinuousGait(bool flag)启用连续步态模式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;高级动作与姿态调整
&lt;ul&gt;
&lt;li&gt;站立 / 坐下：StandUp()和Sit()实现起立和坐下动作，RecoveryStand()用于从侧翻状态恢复。&lt;/li&gt;
&lt;li&gt;身体姿态控制：Euler(roll, pitch, yaw)可调整机身倾角，BodyHeight(float height)动态改变离地高度。&lt;/li&gt;
&lt;li&gt;特技动作：支持FrontFlip()前空翻、FrontJump()跳跃等复杂动作（需硬件支持）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;避障功能
&lt;ul&gt;
&lt;li&gt;自主避障：通过ObstacleAvoidClient类启用避障模块，机器人可实时检测障碍物并调整路径。需调用EnableObstacleAvoidance()激活，并在移动时保持避障服务运行。&lt;/li&gt;
&lt;li&gt;传感器融合：避障依赖激光雷达（PRO/EDU 版）或深度相机（AIR 版）与 IMU 数据融合，实现动态环境下的安全导航。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;二、传感器数据获取&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉传感器
&lt;ul&gt;
&lt;li&gt;RGB 图像：通过 ROS2 话题/camera/image_raw获取 720P/1080P 实时视频流，支持 WebRTC 低延迟传输。&lt;/li&gt;
&lt;li&gt;深度数据：PRO/EDU 版搭载 4D 激光雷达（L1），可输出 360°×90° 点云数据（/go2/camera/depth）；AIR 版通过 Intel RealSense D435i 深度相机提供毫米级深度信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;惯性测量单元（IMU）
&lt;ul&gt;
&lt;li&gt;原始数据：通过 ROS2 话题/imu/data获取加速度（a_x, a_y, a_z）、角速度（ω_x, ω_y, ω_z）和四元数姿态（q_w, q_x, q_y, q_z）。&lt;/li&gt;
&lt;li&gt;坐标系转换：SDK 提供工具函数处理不同框架下的四元数顺序（如 Isaac Gym 与 Isaac Sim 的差异）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;里程计与定位
&lt;ul&gt;
&lt;li&gt;状态估计：通过激光雷达 + IMU 融合（如 LIO-SAM 算法）或腿部运动学模型（关节编码器数据）实现里程计输出。ROS2 话题/odom提供机器人位姿（x, y, θ）和速度信息。&lt;/li&gt;
&lt;li&gt;精度优化：紧耦合 LiDAR-IMU - 腿部里程计系统可在无特征环境下实现亚米级定位精度，在线学习机制适应负载和地形变化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他传感器
&lt;ul&gt;
&lt;li&gt;关节状态：实时获取 12 个关节的角度、角速度和扭矩（/joint_states），支持电机健康监测。&lt;/li&gt;
&lt;li&gt;足端力反馈：PRO/EDU 版配备足端力传感器（F_z），用于复杂地形下的步态调整。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于仿真环境，我有本地的Gym-Unrealcv仿真场景，但是苦恼于LOVON没有开源其仿真代码所以搁置着，不清楚下一步是根据部署代码反推仿真代码还是换一个仿真环境如MatterPort3D
时间本身还是比较充裕的，到开题答辩之前至少有1个月时间&lt;/p&gt;
&lt;h2&gt;研究现状&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究现状&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e7%8e%b0%e7%8a%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;维度&lt;/th&gt;
          &lt;th&gt;内容总结&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;基线模型&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON（LoVi: Open-vocabulary Visual Navigation and Tracking）在仿真中近乎完美（≈100% success rate），但在真实环境严重失效。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心问题&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON 只用 YOLO 的 2D 框坐标来做“视觉 → 动作”映射，没有任何 3D 环境建模或避障机制。目标消失（如进门）时，机器人仍执行“往前走”动作 → 撞门框。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;可用硬件&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Unitree Go2（有RGB、深度、IMU、里程计、足端力传感器）。具备基本避障API、Move(vx,vy,vyaw)控制接口。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;仿真环境&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;有Gym-UnrealCV，但缺少LOVON仿真智能体代码。可能考虑复刻或转向MatterPort3D。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;研究目标雏形&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;希望提升LOVON从2D视觉到更稳健3D环境理解（environment understanding + navigation fusion）的能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;问题定义&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题定义&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e5%ae%9a%e4%b9%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;当前的 open-vocabulary 视觉追踪方法（如 LOVON）在仿真中表现优异，但在真实足式机器人环境中严重退化，其原因在于缺乏对三维环境几何与障碍物的建模能力。
其技术设计恰好规避了仿真环境的局限性，同时最大化了自身优势，具体体现在 3 个 “无冲突”：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “真实场景的 3D 感知需求”，纯 2D 视觉足够&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真环境中，目标的 “2D 图像坐标” 与 “实际空间位置” 完全对齐（如虚拟场景中 xyn=0.5 即代表物理上的正前方，无门框等 3D 遮挡物），无需深度信息即可判断路径是否可行。而 LOVON 的核心是 “2D 视觉 + 运动向量映射”，恰好适配这种需求，无需额外的 3D 深度理解模块。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “不可控干扰”，搜索策略高效&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真中的 “目标丢失” 仅为 “目标移出 90 度扇形视野”（可通过旋转搜索快速重新捕获），无真实场景的 “目标被门框完全遮挡”“机器人被碰撞” 等不可控干扰。LOVON 的旋转搜索策略（vx=0、w_z≠0）在仿真中能高效覆盖视野，而不会像真实场景那样因 “旋转时忽略障碍物” 导致碰撞。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真数据与模型训练 “高度同源”&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真使用的目标类别（背包、椅子、行人）、运动速度（0.3~0.7m/s）、场景光照均与 LOVON 的训练数据集（100 万样本，摘要 1）高度匹配：IOE 对 “椅子”“行人” 的类别映射无误差，L2MM 的运动预测参数（如 β=10）也针对仿真场景校准（摘要 3），避免了真实场景中 “未见过的目标形态”“突发速度变化” 导致的误差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当目标被暂时遮挡（如进入门后）或在复杂结构环境中移动时，机器人仅凭2D像素坐标进行动作决策，无法有效区分“自由空间”与“障碍区域”，导致运动策略失效（如撞门、原地旋转）。
因此，本研究旨在探索一种融合3D环境理解的目标跟踪与导航方法，在保持LOVON开放词汇指令能力的前提下，提高其在真实环境中的鲁棒性与安全性。&lt;/p&gt;
&lt;h2&gt;研究方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;方向&lt;/th&gt;
          &lt;th&gt;名称&lt;/th&gt;
          &lt;th&gt;思路简述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;A. 环境理解增强（BEV / Depth / 3D Occupancy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;给LOVON加“视觉深度感知”，即在YOLO检测的基础上，通过深度图重投影到3D坐标系或BEV平面，建立占用图。再利用该图进行避障或规划。&lt;/td&gt;
          &lt;td&gt;你能做仿真+实机对比，提出一种“轻量级3D-aware追踪方法”。 → 投稿到 &lt;strong&gt;IROS/ICRA workshop 或 CCF-C AI Robotics会议&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;B. 跟踪 + 导航分层融合（Hierarchical Policy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;把“跟踪”和“导航”分成两个层次：高层目标预测、低层路径规划。你可以用简单预测（如卡尔曼滤波预测目标短期轨迹）+ BEV局部避障（A*或DWA）。&lt;/td&gt;
          &lt;td&gt;可以与LOVON对比“复杂场景成功率”→ 写出完整paper。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;选择 A + B 结合的小主题：“基于3D视觉感知与分层导航策略的开放词汇足式机器人目标追踪”(但这个一听就感觉不少人做过类似的课题非常卷)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;含义&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;任务层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Success Rate (SR)&lt;/td&gt;
          &lt;td&gt;机器人在有限步数内到达目标的比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Average Steps (AS)&lt;/td&gt;
          &lt;td&gt;成功任务平均步数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Collision Rate (CR)&lt;/td&gt;
          &lt;td&gt;发生障碍碰撞的任务比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;视觉层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Target Loss Time (TLT)&lt;/td&gt;
          &lt;td&gt;目标丢失后重新识别的平均时间&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Tracking Stability (TS)&lt;/td&gt;
          &lt;td&gt;目标检测框抖动方差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Sim2Real 层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;ΔSR (Sim→Real)&lt;/td&gt;
          &lt;td&gt;仿真与实机成功率差距&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;效率指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;FPS / Latency&lt;/td&gt;
          &lt;td&gt;模型推理帧率与系统延迟&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;安全指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Distance Margin&lt;/td&gt;
          &lt;td&gt;与障碍最近距离的平均值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;在仿真中先实现自动收集 SR、AS、CR。&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;实机可手动统计 SR 和 CR，或用里程计测轨迹。&lt;/p&gt;
&lt;p&gt;可定义 3 个场景（开阔场 / 门框 / 桌椅环境）各跑10次。&lt;/p&gt;
&lt;h2&gt;创新点（暂定）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;创新点暂定&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%88%9b%e6%96%b0%e7%82%b9%e6%9a%82%e5%ae%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们提出一种融合深度视觉感知与分层控制的开放词汇目标追踪框架。
相较于LOVON仅依赖2D目标检测进行运动控制，我们的方法通过深度投影构建局部BEV占用图，并引入预测-驱动的路径规划层，从而显著减少在真实环境中因遮挡或障碍导致的失败&lt;/p&gt;
&lt;h2&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;时间&lt;/th&gt;
          &lt;th&gt;任务&lt;/th&gt;
          &lt;th&gt;目标&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;第1阶段&lt;/td&gt;
          &lt;td&gt;阅读文献：LOVON、LOVi、BEVFusion、LIO-SAM、SceneGPT&lt;/td&gt;
          &lt;td&gt;明确3D环境理解技术路线&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第2阶段&lt;/td&gt;
          &lt;td&gt;在Gym-UnrealCV中复现或简化LOVON策略（YOLO+Motion mapping）&lt;/td&gt;
          &lt;td&gt;建立baseline可控环境&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第3阶段&lt;/td&gt;
          &lt;td&gt;集成深度图或BEV投影模块，实现障碍建模与避障决策&lt;/td&gt;
          &lt;td&gt;形成改进方法&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第4阶段&lt;/td&gt;
          &lt;td&gt;实机测试 + 指标对比 + 论文撰写&lt;/td&gt;
          &lt;td&gt;形成可投稿版本&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;第一阶段的文献调研一方面要包括LOVON引用的和引用LOVON的文献（但是因为VPN节点问题我的Scholar Google给我挂掉了，说我是机器人不让我访问），另一方面是尽可能的调研3D-aware Tracking/Navigation&lt;/p&gt;
&lt;h2&gt;医疗交叉（答辩）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;医疗交叉答辩&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8c%bb%e7%96%97%e4%ba%a4%e5%8f%89%e7%ad%94%e8%be%a9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里值得注意的是，论文里面要写的内容是一个宏大的改进，但是本院答辩时要突出和BME相关、医疗交叉的内容，HexGuide可以作为一个很大的参考&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;《HexGuide: A Hexapod Robot for Autonomous Blind Guidance in Challenging Environments》，一篇期刊论文&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;层级&lt;/th&gt;
          &lt;th&gt;内容&lt;/th&gt;
          &lt;th&gt;对应写作作用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;① 背景&lt;/td&gt;
          &lt;td&gt;世界上有数亿视障人群，对自主出行有刚性需求&lt;/td&gt;
          &lt;td&gt;让读者意识到社会价值和痛点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;② 矛盾&lt;/td&gt;
          &lt;td&gt;现有导盲设备（如导盲犬或轮式机器人）有明显局限，不能稳定地应对复杂地形&lt;/td&gt;
          &lt;td&gt;设置“冲突”——为什么我们必须做新系统&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;③ 概念&lt;/td&gt;
          &lt;td&gt;上交高峰团队设计了一个六足机器人 &lt;strong&gt;HexGuide&lt;/strong&gt;，模仿昆虫式稳定步态，在复杂环境中实现安全引导&lt;/td&gt;
          &lt;td&gt;提出核心创新点和愿景&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;④ 方法&lt;/td&gt;
          &lt;td&gt;通过算法与机械协同，实现&lt;strong&gt;路径规划 + 稳定行走 + 动态避障 + 交通识别 + 人机交互&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;展示技术路线是如何支撑“稳定、安全”这两个关键词的&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;⑤ 验证&lt;/td&gt;
          &lt;td&gt;在机场、十字路口等复杂场景下实测验证，引导成功率高，路径平滑且避障成功&lt;/td&gt;
          &lt;td&gt;用结果“闭环”故事——愿景得以实现&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;论文的立意不是“做一个六足机器人”，而是要证明“六足+智能控制” = 可靠的盲人引导方式。这篇论文的核心任务不是“跟踪一个已知目标”或“视觉跟随”，而是“带领盲人从一个地点到另一个地点”，比如：“from the arrival gate to the baggage claim area in Shanghai Hongqiao Airport.”&lt;/p&gt;
&lt;p&gt;核心流程是盲人用户通过语音指令（如“去出口”）输入目标；在地图上自动规划从当前位置到目标的安全路径；机器人沿着规划路径行走；实时感知环境并修正轨迹。&lt;/p&gt;
&lt;p&gt;目标不是视觉追踪的对象，而是一个空间位置目标，因此这种导航是Goal-based而非Object-based tracking，而且泛化性有限：“The system can autonomously navigate in challenging environments once a map is available.”&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模块&lt;/th&gt;
          &lt;th&gt;故事逻辑&lt;/th&gt;
          &lt;th&gt;手法&lt;/th&gt;
          &lt;th&gt;指标体现&lt;/th&gt;
          &lt;th&gt;补充说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;机械稳定性：六足结构的天然稳态&lt;/td&gt;
          &lt;td&gt;盲人行走必须安全 → 足式比轮式更抗地形 → 六足比四足更稳&lt;/td&gt;
          &lt;td&gt;· “三足支撑步态（Tripod gait）”确保任意时刻三条腿接地&lt;br /&gt;· 单腿轨迹采用三次样条插值，区分支撑相与摆动相以减冲击&lt;br /&gt;· 控制顶点高度以跨越障碍、维持步态连续&lt;/td&gt;
          &lt;td&gt;· 平均支撑腿数 ≥ 3&lt;br /&gt;· 步态周期内质心（CoM）位移波动 &amp;lt; 5 mm&lt;br /&gt;· 10° 坡面及不平地面仍能维持姿态&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;规划稳定性：安全路径生成&lt;/td&gt;
          &lt;td&gt;“安全通行”要求路径不过度摆动、不贴近障碍&lt;/td&gt;
          &lt;td&gt;· 基于 A* 进行全局规划&lt;br /&gt;· 融合人工势场（APF）调整代价，使路径自动远离障碍&lt;br /&gt;· Bézier 曲线平滑路径&lt;br /&gt;· 拐点以贪心方式优化，减少急转角&lt;/td&gt;
          &lt;td&gt;· 路径平滑度提升（转向角波动减少约 40%）&lt;br /&gt;· 路径与障碍最小距离 ≥ 0.3 m&lt;br /&gt;· 平均路径长度仅比最短路径长 ≤ 5%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;运动控制稳定性：MPC 路径跟踪控制&lt;/td&gt;
          &lt;td&gt;六足控制复杂，需让行走对路径偏差“有反馈、能预测”&lt;/td&gt;
          &lt;td&gt;· 使用模型预测控制（MPC）&lt;br /&gt;· 目标函数最小化未来时域的位姿偏差&lt;br /&gt;· 实时约束关节速度与姿态角&lt;br /&gt;· 借助力矩传感器反馈修正步态&lt;/td&gt;
          &lt;td&gt;· 路径跟踪误差 &amp;lt; 3 cm&lt;br /&gt;· 姿态偏角误差 &amp;lt; 2°&lt;br /&gt;· 延迟控制补偿 ≤ 100 ms&lt;/td&gt;
          &lt;td&gt;核心体现“动态预测 + 约束最优控制”，区别于传统 PID&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;环境与交互稳定性：避免危险与错误指令&lt;/td&gt;
          &lt;td&gt;盲人处于动态环境，需识别行人、车辆、信号灯并安全互动&lt;/td&gt;
          &lt;td&gt;· LiDAR + IMU + RGB 摄像头多传感融合&lt;br /&gt;· 基于 LiDAR 点云的区域划分与加权速度修正，实现动态避障&lt;br /&gt;· YOLOv5 交通灯识别结合模板匹配&lt;br /&gt;· 语音识别与反馈交互（“请跟我走”“前方有障碍”）&lt;/td&gt;
          &lt;td&gt;· 动态障碍避让成功率 95%&lt;br /&gt;· 信号灯识别准确率 97.8%&lt;br /&gt;· 平均避障响应时间 &amp;lt; 0.3 s&lt;br /&gt;· 机场/路口场景连续引导成功率 100%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所以这里的Navigation就比较难讲故事了。我们的亮点在于LLM对于自然语言指令能够分解成子任务，但足式机器人比较尴尬的一点是没有手，导致在医疗领域能实现的指令就局限了，比如说有一个RoboNurse-VLA enables the robot to recognize, grasp, and handover surgical instrument.是灵巧手的，但是足式机器人就只是狗了&lt;/p&gt;
&lt;p&gt;我也调研了其他的可能可以相关的领域，秉承**“助残/助盲/助老”**的理念：
第一个是家庭服务，这个还挺好说的，比如越疆 Rover X1/Unitree GO2可在光滑地板、草地、小坡坎等多场景行走，负载能力达日常物品级别，但最大的问题就是没有手，导致比如“帮老人取床头老花镜”“客厅物品递送”这种实现不了 —— 没有机械臂的足式机器人，到底“服务”什么？如果不能取物、开门，它的价值在哪里？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以胡诌做成可语音召唤的移动置物台 ，为上肢失能者提供 室内 5 m 范围内的即时物品可达性 ，用 3D 感知+分层导航解决 家用杂乱环境 下的 安全-连续 难题，从而 以移动代偿 而非 抓取代偿 的方式，提升上肢失能人群的 居家独立指数？
第二个是康复检测，问题是回答不了为什么需要一个狗跟着，而不是穿戴传感器设备/用固定的摄像头进行openpose骨骼分析，就算用了狗也不过是一个移动摄像头，那为什么不让残疾人动或者医生手动挪动摄像头？
第三个就是继续去纯助盲，我想通了，它不是Tracking，而是Object-based Tracking，只是默认命令是跟着person而已，没有说一定要跟在人后面，给他下一个其他目标的指令不就行了？但问题就出在了这里，传统SLAM的方法比结合AI的方法又快又好，你在AI基础上绑一个什么激光雷达/SLAM的话就有点尾大不掉很难绷。&lt;/li&gt;
&lt;li&gt;也有&lt;a href=&#34;https://github.com/Li-Ruiqi777/BlindGuideDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;导盲盲道的参考&lt;/a&gt;，不过是基于A1的本科毕设&lt;/li&gt;
&lt;li&gt;往&lt;a href=&#34;https://zhuanlan.zhihu.com/p/684356655&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RoboGuide这个方向&lt;/a&gt;去做的话也可以，只不过更多是放在VLM而非Tracking/Navigation本身了&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/BestAnHongjun/InternDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InternDog&lt;/a&gt;这篇西工大的工作不知道是怎么做的，看起来很牛，还上了&lt;a href=&#34;https://www.bilibili.com/video/BV1kK421a7sP/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;央视&lt;/a&gt;，据说是我国首个应用在导盲任务/场景下的四足机器人？&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bestxiangest/Intelligent-Guide-Cane&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intelligent-Guide-Cane&lt;/a&gt;或者回归ESP32的导盲？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感觉越调研越有信心了，那就继续往导盲这个领域讲故事应该没有问题！&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
