<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Gap-Analysis</title>
    <link>http://localhost:1313/tags/gap-analysis/</link>
    <description>Recent content in Gap-Analysis on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/gap-analysis/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>VLN 正交分析法寻找创新点</title>
      <link>http://localhost:1313/blog/2025/2025-11-23-vln-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-23-vln-matrix/</guid>
      <description>
        
        
        &lt;h1&gt;VLN 正交分析法寻找创新点&lt;/h1&gt;&lt;h2&gt;框架一：【表征-推理】矩阵 (Representation-Reasoning Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架一表征-推理矩阵-representation-reasoning-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%b8%80%e8%a1%a8%e5%be%81-%e6%8e%a8%e7%90%86%e7%9f%a9%e9%98%b5-representation-reasoning-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：解决&amp;quot;机器人怎么看世界&amp;quot;和&amp;quot;机器人怎么做决策&amp;quot;的匹配问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：推理范式 (Reasoning) \ &lt;strong&gt;横轴&lt;/strong&gt;：环境表征 (Representation)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;推理范式 \ 环境表征&lt;/th&gt;
          &lt;th&gt;A. 纯视觉流&lt;/th&gt;
          &lt;th&gt;B. 2D 语义地图&lt;/th&gt;
          &lt;th&gt;C. 3D 场景图&lt;/th&gt;
          &lt;th&gt;D. 拓扑/文本图&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. End-to-End RL / IL&lt;/td&gt;
          &lt;td&gt;已拥挤&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;较少&lt;/td&gt;
          &lt;td&gt;较少&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Modular + LLM Prompting&lt;/td&gt;
          &lt;td&gt;难点&lt;/td&gt;
          &lt;td&gt;拥挤&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. System 1 + System 2&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. World Model / Generative&lt;/td&gt;
          &lt;td&gt;前沿&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 1 (A-3)&lt;/strong&gt;: 目前 End-to-End 模型（如 NaVid）反应快但缺乏长程逻辑，而 LLM 反应慢。能否设计一个机制，平时用小模型看视频流走路（System 1），遇到&amp;quot;迷路&amp;quot;或&amp;quot;歧义&amp;quot;时，动态唤醒 LLM 分析当前视频帧（System 2）？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 2 (C-4)&lt;/strong&gt;: 现在的 Scene Graph 都是用来做当前状态的 Prompt。能否基于 Scene Graph 做&amp;quot;世界模型&amp;quot;？ 即：让 LLM 预测&amp;quot;如果我向左走，场景图会变成什么样？&amp;quot;，从而在图空间里做 Model-Based Planning，而不是由 LLM 直接瞎猜。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;框架二：【反馈-修正】矩阵 (Feedback-Correction Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架二反馈-修正矩阵-feedback-correction-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%ba%8c%e5%8f%8d%e9%a6%88-%e4%bf%ae%e6%ad%a3%e7%9f%a9%e9%98%b5-feedback-correction-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：针对 2024 年后的趋势——从&amp;quot;如何走对&amp;quot;转向&amp;quot;走错了如何修正&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：修正机制 (Correction) \ &lt;strong&gt;横轴&lt;/strong&gt;：错误源 (Source of Error)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;修正机制 \ 错误源&lt;/th&gt;
          &lt;th&gt;A. 感知幻觉&lt;/th&gt;
          &lt;th&gt;B. 空间迷失&lt;/th&gt;
          &lt;th&gt;C. 指令歧义&lt;/th&gt;
          &lt;th&gt;D. 动态障碍/变化&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. Passive (被动重规划)&lt;/td&gt;
          &lt;td&gt;传统方法&lt;/td&gt;
          &lt;td&gt;传统方法&lt;/td&gt;
          &lt;td&gt;无解&lt;/td&gt;
          &lt;td&gt;传统 DWA/TEB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Active Perception (主动探索)&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. Dialogue / Interaction&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
          &lt;td&gt;已拥挤&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. Self-Reflexion (自省)&lt;/td&gt;
          &lt;td&gt;热门&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 3 (B-4)&lt;/strong&gt;: 现在的 Self-Reflexion 大多是在想&amp;quot;我是不是理解错指令了&amp;quot;。很少有工作做&amp;quot;空间自省&amp;quot;——即 LLM 结合历史轨迹图，反思&amp;quot;我现在的视觉观测和我记忆中的地图不一致，我是不是已经走到错误的房间了？&amp;quot;（Spatial Consistency Check via LLM）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gap 4 (A-3)&lt;/strong&gt;: 当 VLM 觉得前面是&amp;quot;椅子&amp;quot;但又不确定时（Confidence score 低），目前的做法是硬着头皮走。创新点可以是：主动发起一轮对话确认，或者主动移动相机去验证（Active Perception for VLM uncertainty）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;框架三：【多模态融合-时空】矩阵 (Fusion-Spatiotemporal Matrix)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;框架三多模态融合-时空矩阵-fusion-spatiotemporal-matrix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a1%86%e6%9e%b6%e4%b8%89%e5%a4%9a%e6%a8%a1%e6%80%81%e8%9e%8d%e5%90%88-%e6%97%b6%e7%a9%ba%e7%9f%a9%e9%98%b5-fusion-spatiotemporal-matrix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;：针对 CoRL/ICRA 等机器人会议，关注&amp;quot;具体怎么融合特征&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纵轴&lt;/strong&gt;：融合阶段 (Fusion Stage) \ &lt;strong&gt;横轴&lt;/strong&gt;：时间维处理 (Temporal)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;融合阶段 \ 时间维处理&lt;/th&gt;
          &lt;th&gt;A. Frame-wise (单帧)&lt;/th&gt;
          &lt;th&gt;B. Feature Buffer&lt;/th&gt;
          &lt;th&gt;C. Explicit Map&lt;/th&gt;
          &lt;th&gt;D. Neural Memory&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1. Early Fusion&lt;/td&gt;
          &lt;td&gt;基础&lt;/td&gt;
          &lt;td&gt;计算量大&lt;/td&gt;
          &lt;td&gt;VLMaps&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2. Late Fusion&lt;/td&gt;
          &lt;td&gt;CLIP-Nav&lt;/td&gt;
          &lt;td&gt;NaVid&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;IVLN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3. LLM-in-the-loop&lt;/td&gt;
          &lt;td&gt;GPT-4V Nav&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;UniGoal&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4. Cross-Attention (Query-based)&lt;/td&gt;
          &lt;td&gt;传统 Transformer&lt;/td&gt;
          &lt;td&gt;常见&lt;/td&gt;
          &lt;td&gt;少见&lt;/td&gt;
          &lt;td&gt;空白/机会&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;潜在创新点挖掘&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gap 5 (D-3)&lt;/strong&gt;: 目前的 LLM 导航要么看单张图，要么看 3D 场景图。很少有结合 Mamba 或 SSM (State Space Models) 的工作。 创新点：利用 Mamba 这种长序列处理能力极强的架构，作为 VLN 的&amp;quot;隐式记忆体&amp;quot;，替代显式的地图构建，实现无图但有长记忆的导航（Mamba-VLN）。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
