<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Human-Model</title>
    <link>http://localhost:1313/tags/human-model/</link>
    <description>Recent content in Human-Model on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/human-model/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>VLN 综述以及后续文献</title>
      <link>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</guid>
      <description>
        
        
        &lt;h1&gt;VLN 系列&lt;/h1&gt;&lt;p&gt;从 Poing Navigation 到 Object Navigation，这也太难了，找Idea真的太难了。&lt;/p&gt;
&lt;p&gt;然后秋冬学期的一半，也就是大四上的一半已经过去了，马上就要寒假了，寒假做什么，实习还是论文？真能憋出论文吗？&lt;/p&gt;
&lt;h2&gt;Survey&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;survey&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#survey&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;研发能够与人类及其周边环境进行交互的&lt;strong&gt;具身智能体&lt;/strong&gt;（embodied agents），是&lt;strong&gt;人工智能&lt;/strong&gt;（AI）领域长期以来的核心目标之一。这类 AI 系统在现实世界中具有巨大的应用潜力，可作为日常生活中的多功能助手，例如&lt;strong&gt;家用机器人&lt;/strong&gt;、&lt;strong&gt;自动驾驶汽车&lt;/strong&gt;以及&lt;strong&gt;个人助手&lt;/strong&gt;。推动这一研究方向的一个正式问题设定是&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（Vision-and-Language Navigation, &lt;strong&gt;VLN&lt;/strong&gt;）—— 这是一项多模态协作任务，要求智能体遵循人类指令、探索&lt;strong&gt;三维&lt;/strong&gt;（3D）环境，并在存在各类歧义的场景下开展情境化通信。多年来，研究者已在&lt;strong&gt;照片级真实感模拟器&lt;/strong&gt;和&lt;strong&gt;真实环境&lt;/strong&gt;中对 VLN 展开探索，由此形成了一系列&lt;strong&gt;基准数据集&lt;/strong&gt;，每个数据集的问题表述略有不同。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/law-Challenge.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类&lt;/strong&gt;（Human）：给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;rdquo;；在智能体询问 &amp;ldquo;左边的房间还是前面的房间？&amp;rdquo; 时回复 &amp;ldquo;左边&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;物理环境&lt;/strong&gt;（Physical Environment）：智能体感知的视觉场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体&lt;/strong&gt;（VLN Agent）：接收指令后进行 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;（Grounding &amp;amp; Reasoning）、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;（Planning）、&lt;strong&gt;&amp;ldquo;对话&amp;rdquo;&lt;/strong&gt;（Dialogue），执行 &lt;strong&gt;&amp;ldquo;导航动作&amp;rdquo;&lt;/strong&gt;（Navigation Execution），并生成&lt;strong&gt;语言响应&lt;/strong&gt;（Language Response）；过程中可能产生疑问，如 &amp;ldquo;…… 进入房间。左边？右边？&amp;ldquo;&amp;ldquo;左边的房间还是前面的房间？&amp;quot;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心模块&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;（World Model）、&lt;strong&gt;人类模型&lt;/strong&gt;（Human Model），分别支撑智能体的环境理解与人类意图解读。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其&lt;a href=&#34;https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;仓库&lt;/a&gt;提到了一些工作内容，但是不全。&lt;/p&gt;
&lt;h3&gt;背景与任务基础&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;背景与任务基础&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%83%8c%e6%99%af%e4%b8%8e%e4%bb%bb%e5%8a%a1%e5%9f%ba%e7%a1%80&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;人类及其他具备导航能力的动物，很早就展现出对环境导航的理解与策略。例如，加利斯特尔（Gallistel）提出了两种基础机制：其一为&lt;strong&gt;引导法&lt;/strong&gt;（piloting），即利用环境地标计算距离与角度；其二为&lt;strong&gt;路径积分&lt;/strong&gt;（path integration），即通过自运动感知计算位移与方向变化。理解空间导航的核心是&lt;strong&gt;认知地图假说&lt;/strong&gt;（cognitive map hypothesis）—— 该假说认为，大脑会形成统一的空间表征，以支持记忆存储并指导导航行为。例如，托尔曼（Tolman）观察到，当熟悉的路径被阻断且地标消失时，大鼠仍能选择正确的新路径。神经科学家还发现了&lt;strong&gt;海马体位置细胞&lt;/strong&gt;（hippocampal place cells），这表明存在一种以&lt;strong&gt;异中心视角&lt;/strong&gt;（allocentrically）编码地标与目标的空间坐标系。&lt;/p&gt;
&lt;p&gt;传统上，&lt;strong&gt;&amp;ldquo;遵循自然语言导航指令&amp;rdquo;&lt;strong&gt;的任务多采用地图等&lt;/strong&gt;符号化世界表征&lt;/strong&gt;（symbolic world representations）进行建模。然而，本综述聚焦于采用视觉环境的模型，重点探讨&lt;strong&gt;多模态理解与接地&lt;/strong&gt;（grounding）的相关挑战。与此相对，关于&lt;strong&gt;视觉导航&lt;/strong&gt;和&lt;strong&gt;移动机器人导航&lt;/strong&gt;的综述文献已十分丰富，这类综述主要关注视觉感知与物理具身性，但若涉及 &lt;strong&gt;&amp;ldquo;语言在导航任务中的作用&amp;rdquo;&lt;/strong&gt;，则讨论较为简略，建议读者参考这些文献以获取相关背景。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;接地（Grounding）指将抽象的语言符号与具体的物理世界或感知数据建立对应关系的过程。在 VLN 中，接地是将自然语言指令映射到视觉场景中的具体位置、物体或动作。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;尽管在讨论 VLN 时，我们难免会将范围拓展到导航之外的领域（如移动操作、对话），但本综述的核心焦点仍是&lt;strong&gt;导航任务&lt;/strong&gt;，并将针对该任务提供详细的文献梳理。此外，以往的 VLN 综述多采用 &lt;strong&gt;&amp;ldquo;自下而上&amp;rdquo;&lt;/strong&gt; 的总结方式，聚焦于基准数据集与建模创新；而本综述则采用 &lt;strong&gt;&amp;ldquo;自上而下&amp;rdquo;&lt;/strong&gt; 的视角，并以&lt;strong&gt;基础模型&lt;/strong&gt;的角色为核心，将现有研究成果从 &lt;strong&gt;&amp;ldquo;世界模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;VLN 智能体&amp;rdquo;&lt;/strong&gt; 三个维度，归类为&lt;strong&gt;三大核心挑战&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;典型的 &lt;strong&gt;VLN 智能体&lt;/strong&gt;会在指定位置接收人类指令者给出的（一系列）&lt;strong&gt;语言指令&lt;/strong&gt;。该智能体以&lt;strong&gt;自我为中心的视觉视角&lt;/strong&gt;（egocentric visual perspective）在环境中导航，其核心任务是遵循指令生成&lt;strong&gt;轨迹&lt;/strong&gt; —— 轨迹可基于一系列离散视角，也可基于低层级动作与控制指令（例如 &amp;ldquo;前进 0.25 米&amp;rdquo;），最终抵达&lt;strong&gt;目标终点&lt;/strong&gt;。若智能体最终位置与目标终点的距离在指定范围内（例如 3 米），则判定为&lt;strong&gt;导航成功&lt;/strong&gt;。此外，智能体在导航过程中可与指令者交互：既可以请求帮助，也可进行自由形式的语言沟通。近年来，研究者对 VLN 智能体的期望进一步提升，要求其在导航的同时整合附加任务，例如&lt;strong&gt;操作任务&lt;/strong&gt;与&lt;strong&gt;目标检测任务&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-benchmark-2024.png&#34; alt=&#34;vln-benchmark-2024&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如上表，现有（2024）VLN &lt;strong&gt;基准数据集&lt;/strong&gt;可分为以下四类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航发生的 &amp;ldquo;世界&amp;rdquo;&lt;/strong&gt;：包括领域（室内或室外）与具体环境（如模拟器或真实场景）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类交互类型&lt;/strong&gt;：包括交互轮次（单轮或多轮）、通信格式（自由对话、受限对话或多指令）、语言粒度（动作导向或目标导向）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体属性&lt;/strong&gt;：包括智能体类型（如家用机器人、自动驾驶车辆、自主无人机）、动作空间（图基、离散或连续）、附加任务（操作与目标检测）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集收集方式&lt;/strong&gt;：包括文本收集（人类生成或模板生成）与路线演示（人类执行或规划器生成）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;研究者主要采用三类指标评估 VLN 智能体的&lt;strong&gt;导航寻路性能&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航误差&lt;/strong&gt;（Navigation Error, &lt;strong&gt;NE&lt;/strong&gt;）：智能体最终位置与目标终点之间最短路径距离的平均值&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成功率&lt;/strong&gt;（Success Rate, &lt;strong&gt;SR&lt;/strong&gt;）：最终位置足够接近目标终点的任务占比&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;路径长度加权成功率&lt;/strong&gt;（Success Rate Weighted Path Length, &lt;strong&gt;SPL&lt;/strong&gt;）：通过轨迹长度对成功率进行归一化，平衡 &lt;strong&gt;&amp;ldquo;抵达正确终点的成功率&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;路径效率&amp;rdquo;&lt;/strong&gt; 两大指标&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，还有一类指标用于衡量 &lt;strong&gt;&amp;ldquo;指令遵循的忠实度&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;预测轨迹和真实轨迹的一致性&amp;rdquo;&lt;/strong&gt;，例如：
4. &lt;strong&gt;长度加权覆盖得分&lt;/strong&gt;（Coverage Weighted by Length Score, &lt;strong&gt;CLS&lt;/strong&gt;）：衡量智能体轨迹与参考路径的贴合程度，通过 &lt;strong&gt;&amp;ldquo;参考路径覆盖范围&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;轨迹长度效率&amp;rdquo;&lt;/strong&gt; 两个维度平衡智能体性能
5. &lt;strong&gt;归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping, &lt;strong&gt;nDTW&lt;/strong&gt;）：对偏离真实轨迹的行为进行惩罚
6. &lt;strong&gt;成功率加权归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping Weighted by Success Rate, &lt;strong&gt;sDTW&lt;/strong&gt;）：在惩罚轨迹偏离的同时，还会结合导航成功率综合评估&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-challenges-and-soluions.png&#34; alt=&#34;vln-challenges-and-soluions&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;该图反映的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心模块关联&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;历史与记忆&amp;rdquo;&lt;/strong&gt;，&lt;strong&gt;人类模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;模糊指令&amp;rdquo;&lt;/strong&gt;，两者均涉及 &lt;strong&gt;&amp;ldquo;泛化能力&amp;rdquo;&lt;/strong&gt;；&lt;strong&gt;VLN 智能体&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;基础模型适配为智能体&amp;rdquo;&lt;/strong&gt; 三大方法&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基础模型角色&lt;/strong&gt;：根据基础模型承担的功能，将方法分为四类 —— &lt;strong&gt;数据与知识处理&lt;/strong&gt;（预处理 / 增强 / 合成数据、利用预训练常识）、&lt;strong&gt;表征学习&lt;/strong&gt;（通用文本 / 视觉表征、历史记忆处理）、&lt;strong&gt;决策制定&lt;/strong&gt;（导航规划器、信息寻求对话管理器、通用决策智能体）、&lt;strong&gt;任务学习&lt;/strong&gt;（具身推理、语言接地、少样本 / 上下文 / 微调学习具身任务）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;交互示例&lt;/strong&gt;：人类给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;quot;&amp;ldquo;去卫生间&amp;rdquo;；智能体通过提问（&amp;ldquo;走廊在哪里？&amp;ldquo;&amp;ldquo;哪个房间？&amp;quot;）寻求信息，人类回复（&amp;ldquo;左边的房间还是前面的房间？&amp;ldquo;&amp;ldquo;左边&amp;rdquo;）后，智能体执行动作（&amp;ldquo;前进&amp;quot;&amp;ldquo;左转&amp;rdquo;）并生成轨迹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战与未来方向&lt;/strong&gt;：&lt;strong&gt;基准数据集&lt;/strong&gt;（数据与任务局限）、&lt;strong&gt;世界模型&lt;/strong&gt;（从 2D 世界到 3D 世界）、&lt;strong&gt;人类模型&lt;/strong&gt;（从指令到对话）、&lt;strong&gt;智能体模型&lt;/strong&gt;（LLM 与 VLM 适配）、&lt;strong&gt;部署&lt;/strong&gt;（从仿真到真实机器人）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;三大解决方案&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三大解决方案&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e5%a4%a7%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;World Model: Learning and Representing the Visual Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-learning-and-representing-the-visual-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-learning-and-representing-the-visual-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;能够帮助 VLN 智能体理解周边环境、预测自身动作对世界状态的改变，并使自身感知与动作与语言指令对齐。现有研究中，学习世界模型主要面临两大挑战：一是将当前任务段内的&lt;strong&gt;视觉观测历史&lt;/strong&gt;编码为&lt;strong&gt;记忆&lt;/strong&gt;，二是实现对未见过环境的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;History and Memory&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;history-and-memory&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#history-and-memory&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;与&lt;strong&gt;视觉问答&lt;/strong&gt;（Visual Question Answering, &lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;视觉蕴含&lt;/strong&gt;（Visual Entailment）等其他视觉-语言任务不同，VLN 智能体需将过去动作与观测的&lt;strong&gt;历史信息&lt;/strong&gt;融入当前步骤的输入中以决策动作，而非仅依赖单一步骤的图像与文本。在 VLN 中应用基础模型之前，研究者通常采用 &lt;strong&gt;LSTM 隐藏状态&lt;/strong&gt;作为支持智能体导航决策的隐式记忆，并进一步设计不同的&lt;strong&gt;注意力机制&lt;/strong&gt;或&lt;strong&gt;辅助任务&lt;/strong&gt;，以提升编码历史与指令的对齐程度。&lt;/p&gt;
&lt;p&gt;目前已有多种基于基础模型的&lt;strong&gt;导航历史编码技术&lt;/strong&gt;，核心可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）基于令牌更新或序列建模的编码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer 初始化&lt;/strong&gt;：以基于域内指令-轨迹数据预训练的模型（如 &lt;strong&gt;Prevalent&lt;/strong&gt;）为基础，构建&lt;strong&gt;多模态 Transformer&lt;/strong&gt;，将编码后的指令与导航历史作为输入以实现决策。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;循环状态令牌编码&lt;/strong&gt;：部分方法通过循环更新的&lt;strong&gt;状态令牌&lt;/strong&gt;编码导航历史。例如，利用上一步的单个 &lt;strong&gt;[CLS] 令牌&lt;/strong&gt;编码历史信息；或设计&lt;strong&gt;变长记忆框架&lt;/strong&gt;，将过去步骤的多个动作激活值存储在记忆库中，作为历史编码。但这类方法需逐步骤更新令牌，难以高效检索导航轨迹中任意步骤的历史编码，限制了预训练的可扩展性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;全景与历史分层编码&lt;/strong&gt;：另一类方法直接通过多模态 Transformer 将导航历史编码为序列。例如，对轨迹中每一步的单视角图像进行编码；或进一步提出 &lt;strong&gt;&amp;ldquo;全景编码器 + 历史编码器&amp;rdquo;&lt;/strong&gt; 的分层设计 —— &lt;strong&gt;全景编码器&lt;/strong&gt;处理每一时间步的全景视觉观测，&lt;strong&gt;历史编码器&lt;/strong&gt;则编码所有过往观测。这种设计可分离全景视图中的空间关系与导航历史中跨全景的时间动态性，且无需依赖循环更新的状态令牌，便于基于指令-路径对进行高效、大规模的预训练。后续研究分别用 &lt;strong&gt;&amp;ldquo;图像均值池化&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;前视图像编码&amp;rdquo;&lt;/strong&gt; 替代全景编码器，均保持了良好的导航性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）基于 LLM 的文本化历史编码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随着基于 &lt;strong&gt;LLM&lt;/strong&gt; 的导航智能体兴起，&lt;strong&gt;&amp;ldquo;将视觉环境转换为文本描述&amp;rdquo;&lt;/strong&gt; 成为主流趋势。此时导航历史被编码为 &lt;strong&gt;&amp;ldquo;图像描述序列 + 相对空间信息&amp;rdquo;&lt;/strong&gt;（如朝向、高度、距离）的组合。例如，&lt;strong&gt;HELPER&lt;/strong&gt; 设计了 &lt;strong&gt;&amp;ldquo;语言-程序对&amp;rdquo;&lt;/strong&gt; 的外部记忆，通过检索增强的 LLM 提示，将人类与机器人的自由形式对话解析为动作程序。&lt;/p&gt;
&lt;p&gt;另一类研究通过融入&lt;strong&gt;图信息&lt;/strong&gt;增强导航历史建模，核心思路是利用&lt;strong&gt;结构化图表征&lt;/strong&gt;环境几何与空间关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拓扑图与结构化编码&lt;/strong&gt;：部分方法采用&lt;strong&gt;结构化 Transformer 编码器&lt;/strong&gt;捕捉环境中的几何线索。除编码中使用的&lt;strong&gt;拓扑图&lt;/strong&gt;外，许多研究还将&lt;strong&gt;俯视图信息&lt;/strong&gt;（如&lt;strong&gt;网格图&lt;/strong&gt;、&lt;strong&gt;语义图&lt;/strong&gt;、&lt;strong&gt;局部度量图&lt;/strong&gt;）与&lt;strong&gt;局部邻域图&lt;/strong&gt;纳入导航过程中的观测历史建模。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM 与图的结合&lt;/strong&gt;：近期基于 LLM 的导航智能体在记忆构建中引入了创新性的图应用。例如，提出一种基于&lt;strong&gt;地图引导的 GPT 智能体&lt;/strong&gt;，利用语言化形式的地图存储和管理拓扑图信息；&lt;strong&gt;MC-GPT&lt;/strong&gt; 则将拓扑图作为记忆结构，记录视角、物体及其空间关系的信息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;跨环境泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;跨环境泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%a8%e7%8e%af%e5%a2%83%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;VLN 的核心挑战之一是：如何从有限的可用环境中学习，并泛化到新的、未见过的环境中。现有研究表明，以下方法可提升智能体对未见过环境的泛化性能：&lt;strong&gt;学习语义分割特征&lt;/strong&gt;、&lt;strong&gt;利用训练过程中环境的 dropout 信息&lt;/strong&gt;、&lt;strong&gt;最大化不同环境中语义对齐图像对的相似度&lt;/strong&gt;。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;方法&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.1 预训练视觉表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;传统视觉编码器&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究采用在 ImageNet 上预训练的 &lt;strong&gt;ResNet&lt;/strong&gt; 提取视觉表征&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基于 VL 基础模型的表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;用 &lt;strong&gt;CLIP 视觉编码器&lt;/strong&gt;替代 ResNet——CLIP 通过图文对的对比损失预训练，可自然实现图像与指令的更好对齐，显著提升 VLN 性能&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;视频预训练表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;探索将从视频数据中学习的视觉表征迁移到 VLN 任务中，证实视频中的&lt;strong&gt;时间信息&lt;/strong&gt;对导航至关重要&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.2 环境增强&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;静态环境修改&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;EnvEdit&lt;/strong&gt;、&lt;strong&gt;EnvMix&lt;/strong&gt;、&lt;strong&gt;KED&lt;/strong&gt; 与 &lt;strong&gt;FDA&lt;/strong&gt; 通过修改 Matterport3D 中的现有环境生成合成数据，具体手段包括混合不同环境的房间、改变环境外观与风格、对环境高频特征进行插值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;动态环境合成&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;Pathdreamer&lt;/strong&gt; 与 &lt;strong&gt;SE3DS&lt;/strong&gt; 进一步实现 &lt;strong&gt;&amp;ldquo;基于当前观测合成未来步骤环境&amp;rdquo;&lt;/strong&gt;，并探索将合成视图作为 VLN 训练的增强数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.3 学习范式的转变&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;前基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究直接用自动收集的新环境增强训练环境，并微调基于 LSTM 的 VLN 智能体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;预训练&lt;/strong&gt;被证实对基础模型至关重要，因此 &lt;strong&gt;&amp;ldquo;在预训练阶段从收集的环境中学习&amp;rdquo;&lt;/strong&gt; 成为 VLN 的标准做法。基于增强域内数据的&lt;strong&gt;大规模预训练&lt;/strong&gt;，已成为缩小智能体与人类性能差距的关键；且域内预训练的多模态 Transformer，被证实比从 VLMs（如 &lt;strong&gt;Oscar&lt;/strong&gt;、&lt;strong&gt;LXMERT&lt;/strong&gt;）初始化的多模态 Transformer 更有效&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Human Model: Interpreting and Communicating with Humans&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;human-model-interpreting-and-communicating-with-humans&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#human-model-interpreting-and-communicating-with-humans&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;除学习和建模世界外，VLN 智能体还需一个 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; —— 该模型能根据具体场景理解人类提供的自然语言指令，从而完成导航任务。这一过程主要面临两大挑战：一是解决&lt;strong&gt;指令的模糊性&lt;/strong&gt;，二是实现 &lt;strong&gt;&amp;ldquo;接地指令&amp;rdquo;&lt;/strong&gt; 在不同视觉环境中的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;模糊指令&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;模糊指令&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a8%a1%e7%b3%8a%e6%8c%87%e4%bb%a4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;模糊指令&lt;/strong&gt;主要出现在&lt;strong&gt;单轮导航场景&lt;/strong&gt;中：智能体仅遵循初始指令执行任务，无法通过进一步人类交互获取澄清。这类指令缺乏灵活性，难以训练智能体根据动态环境调整自身的&lt;strong&gt;语言理解&lt;/strong&gt;与&lt;strong&gt;视觉感知能力&lt;/strong&gt;。例如，指令中可能包含 &lt;strong&gt;&amp;ldquo;当前视角不可见的地标&amp;rdquo;&lt;/strong&gt;，或 &lt;strong&gt;&amp;ldquo;从多个视角观察均难以区分的地标&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在基础模型应用于 VLN 之前，模糊指令问题几乎未得到有效解决。尽管 &lt;strong&gt;LEO 模型&lt;/strong&gt;尝试通过整合 &lt;strong&gt;&amp;ldquo;从不同视角描述同一轨迹的多条指令&amp;rdquo;&lt;/strong&gt; 来缓解该问题，但仍依赖人工标注的指令。而基础模型所具备的 &lt;strong&gt;&amp;ldquo;全面感知上下文&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;常识知识&amp;rdquo;&lt;/strong&gt;，使智能体既能利用外部知识解读模糊指令，也能向其他 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; 寻求协助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CLIP&lt;/strong&gt; 等大规模跨模态预训练模型具备&lt;strong&gt;视觉语义与文本的匹配能力&lt;/strong&gt;，这使得 VLN 智能体可利用 &lt;strong&gt;&amp;ldquo;当前感知到的视觉物体及其状态&amp;rdquo;&lt;/strong&gt; 来解决指令模糊性问题，在单轮导航场景中尤为有效。具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Trans 模型&lt;/strong&gt;：通过 CLIP 提取 &lt;strong&gt;&amp;ldquo;可见且具有辨识度的物体&amp;rdquo;&lt;/strong&gt;，构建易于遵循的子指令；并预训练一个 &lt;strong&gt;&amp;ldquo;转换器&amp;rdquo;&lt;/strong&gt;（Translator），将原始模糊指令转换为易于理解的子指令表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LANA+ 模型&lt;/strong&gt;：利用 CLIP，以视觉全景观测为输入，查询 &lt;strong&gt;&amp;ldquo;地标语义标签文本列表&amp;rdquo;&lt;/strong&gt;，并选取排名靠前的检索文本线索作为 &lt;strong&gt;&amp;ldquo;待跟随显著地标的表征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;KERM 模型&lt;/strong&gt;：提出一种 &lt;strong&gt;&amp;ldquo;知识增强推理模型&amp;rdquo;&lt;/strong&gt;，可检索 &lt;strong&gt;&amp;ldquo;以语言描述形式存储的导航视角相关知识事实&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavHint 方法&lt;/strong&gt;：构建一个提示数据集，提供详细的视觉描述，帮助 VLN 智能体全面理解视觉环境，而非仅聚焦于指令中提及的物体。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另一方面，&lt;strong&gt;LLM&lt;/strong&gt; 的&lt;strong&gt;常识推理能力&lt;/strong&gt;可用于 &lt;strong&gt;&amp;ldquo;澄清或修正指令中的模糊地标&amp;rdquo;&lt;/strong&gt;，并将指令拆解为可执行步骤。例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 LLM 提供 &lt;strong&gt;&amp;ldquo;开放世界中地标共现的常识&amp;rdquo;&lt;/strong&gt;，并结合 CLIP 实现地标探测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayCan 方法&lt;/strong&gt;：将指令拆解为 &lt;strong&gt;&amp;ldquo;预定义可执行动作的排序列表&amp;rdquo;&lt;/strong&gt;，并结合一个 &lt;strong&gt;&amp;ldquo;效用函数&amp;rdquo;&lt;/strong&gt; —— 该函数对当前场景中出现的物体赋予更高权重。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管可通过&lt;strong&gt;视觉感知&lt;/strong&gt;与&lt;strong&gt;场景上下文&lt;/strong&gt;解决模糊指令问题，但更直接的方法是向 &lt;strong&gt;&amp;ldquo;通信伙伴&amp;rdquo;&lt;/strong&gt;（即生成指令的人类）寻求帮助。这类研究主要面临三大核心挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断 &lt;strong&gt;&amp;ldquo;何时请求帮助&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;生成 &lt;strong&gt;&amp;ldquo;信息寻求问题&amp;rdquo;&lt;/strong&gt;（如询问下一步动作、物体位置、方向等）&lt;/li&gt;
&lt;li&gt;设计 &lt;strong&gt;&amp;ldquo;信息提供方&amp;rdquo;&lt;/strong&gt;（oracle）—— 可为真实人类、规则与模板或神经模型&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 与 &lt;strong&gt;VLM&lt;/strong&gt; 在该框架中可承担两种角色：一是 &lt;strong&gt;&amp;ldquo;信息寻求模型&amp;rdquo;&lt;/strong&gt;，二是 &lt;strong&gt;&amp;ldquo;人类助手的代理&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;信息提供模型&amp;rdquo;&lt;/strong&gt;。已有初步研究探索将 LLM 用作信息寻求模型，解决 &lt;strong&gt;&amp;ldquo;何时问&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;问什么&amp;rdquo;&lt;/strong&gt; 的问题 —— 这需借助 &lt;strong&gt;&amp;ldquo;保形预测&amp;rdquo;&lt;/strong&gt;（conformal prediction, &lt;strong&gt;CP&lt;/strong&gt;）或 &lt;strong&gt;&amp;ldquo;上下文学习&amp;rdquo;&lt;/strong&gt;（in-context learning, &lt;strong&gt;ICL&lt;/strong&gt;）等技术实现。&lt;/p&gt;
&lt;p&gt;对于 &lt;strong&gt;&amp;ldquo;信息提供&amp;rdquo;&lt;/strong&gt; 角色，基础模型需扮演 &lt;strong&gt;&amp;ldquo;掌握信息提供方专属信息的助手&amp;rdquo;&lt;/strong&gt; —— 例如知晓目标位置、环境地图等任务执行者无法获取的信息。近期相关研究包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Copilot 方法&lt;/strong&gt;：使智能体在遇到困惑时主动寻求协助，其中 LLM 扮演 &lt;strong&gt;&amp;ldquo;副驾驶&amp;rdquo;&lt;/strong&gt; 角色，为导航提供支持。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;GPT-3&lt;/strong&gt; 可逐步拆解训练数据中的真实响应，这有助于利用预训练的 &lt;strong&gt;SwinBert&lt;/strong&gt; 视频-语言模型训练信息提供方模型；同时，&lt;strong&gt;mPLUG-Owl&lt;/strong&gt; 等大型视觉-语言模型可作为 &lt;strong&gt;&amp;ldquo;现成的强零样本信息提供方&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自驱动通信智能体&lt;/strong&gt;：通过学习 &lt;strong&gt;&amp;ldquo;信息提供方给出肯定答案的置信度&amp;rdquo;&lt;/strong&gt; 实现，可采用 &lt;strong&gt;&amp;ldquo;自我问答&amp;rdquo;&lt;/strong&gt; 模式，在推理阶段无需依赖信息提供方。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;接地指令的泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地指令的泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e6%8c%87%e4%bb%a4%e7%9a%84%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;导航数据在规模与多样性上的局限，是影响 VLN 智能体 &lt;strong&gt;&amp;ldquo;理解多样语言表达、有效遵循指令&amp;rdquo;&lt;/strong&gt; 的另一重要问题 —— 在未见过的导航环境中该问题尤为突出。尽管&lt;strong&gt;语言风格&lt;/strong&gt;本身在 &lt;strong&gt;&amp;ldquo;见过与未见过的环境&amp;rdquo;&lt;/strong&gt; 中具备良好泛化能力，但受限于训练指令的规模，&lt;strong&gt;&amp;ldquo;如何将指令与未见过的环境进行接地&amp;rdquo;&lt;/strong&gt; 仍是一项难题。基础模型通过 &lt;strong&gt;&amp;ldquo;预训练表征&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;指令生成数据增强&amp;rdquo;&lt;/strong&gt; 两种方式，为解决这些问题提供了支持。&lt;/p&gt;
&lt;p&gt;在基础模型出现前，多数研究依赖 &lt;strong&gt;LSTM&lt;/strong&gt; 等文本编码器表征文本指令。而基础模型通过&lt;strong&gt;预训练表征&lt;/strong&gt;，显著提升了 VLN 智能体的&lt;strong&gt;语言泛化能力&lt;/strong&gt;，具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRESS 方法&lt;/strong&gt;：对预训练语言模型 &lt;strong&gt;BERT&lt;/strong&gt; 进行微调，获得对 &lt;strong&gt;&amp;ldquo;未见过指令&amp;rdquo;&lt;/strong&gt; 泛化性更强的文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer&lt;/strong&gt;：为 &lt;strong&gt;VLN-BERT&lt;/strong&gt;、&lt;strong&gt;PREVALENT&lt;/strong&gt; 等方法提供支撑 —— 这些方法通过在 &lt;strong&gt;&amp;ldquo;从网络收集的大规模图文对&amp;rdquo;&lt;/strong&gt; 上预训练，获得更通用的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Airbert 模型&lt;/strong&gt;：训练一个类 &lt;strong&gt;ViLBERT&lt;/strong&gt; 架构，从 &lt;strong&gt;&amp;ldquo;互联网收集的图像-标题对&amp;rdquo;&lt;/strong&gt; 中学习文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLEAR 方法&lt;/strong&gt;：学习 &lt;strong&gt;&amp;ldquo;跨语言语言表征&amp;rdquo;&lt;/strong&gt;，捕捉指令背后的&lt;strong&gt;视觉概念&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProbES 方法&lt;/strong&gt;：通过采样轨迹实现&lt;strong&gt;环境自探索&lt;/strong&gt;，并利用 CLIP 检测到的 &lt;strong&gt;&amp;ldquo;动作与物体短语&amp;rdquo;&lt;/strong&gt; 填充指令模板，自动构建对应指令；同时借助 &lt;strong&gt;&amp;ldquo;基于提示的学习&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;语言嵌入&lt;/strong&gt;的快速适配。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT-2 模型&lt;/strong&gt;：探索利用 &lt;strong&gt;&amp;ldquo;预训练 VLMs&amp;rdquo;&lt;/strong&gt;（如结合 &lt;strong&gt;Flan-T5&lt;/strong&gt; 或 &lt;strong&gt;Vicuna&lt;/strong&gt; 的 &lt;strong&gt;InstructBLIP&lt;/strong&gt;）的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;，提升&lt;strong&gt;导航策略学习&lt;/strong&gt;与&lt;strong&gt;导航推理能力&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;提升智能体泛化能力的另一方法是 &lt;strong&gt;&amp;ldquo;合成更多指令&amp;rdquo;&lt;/strong&gt;。相关研究可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）离线指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;早期研究采用 &lt;strong&gt;&amp;ldquo;说话者-跟随者（Speaker-Follower）框架&amp;rdquo;&lt;/strong&gt;：利用人工标注的 &lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 训练一个 &lt;strong&gt;&amp;ldquo;离线说话者（指令生成器）&amp;rdquo;&lt;/strong&gt;，再让其基于 &lt;strong&gt;&amp;ldquo;给定轨迹上的全景序列&amp;rdquo;&lt;/strong&gt; 生成新指令。但发现这类方法生成的指令质量较低，在人类寻路评估中表现不佳。&lt;/p&gt;
&lt;p&gt;后续改进方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Marky 模型&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;多语言 T5 模型的多模态扩展版本&amp;rdquo;&lt;/strong&gt;，结合 &lt;strong&gt;&amp;ldquo;文本对齐的视觉地标对应关系&amp;rdquo;&lt;/strong&gt;，在未见过环境的 R2R 风格路径上生成 &lt;strong&gt;&amp;ldquo;接近人类质量&amp;rdquo;&lt;/strong&gt; 的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PASTS 模型&lt;/strong&gt;：引入 &lt;strong&gt;&amp;ldquo;进度感知的时空 Transformer 说话者&amp;rdquo;&lt;/strong&gt;，更好地利用 &lt;strong&gt;&amp;ldquo;有序的多视觉与动作特征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SAS 方法&lt;/strong&gt;：利用环境的 &lt;strong&gt;&amp;ldquo;语义与结构线索&amp;rdquo;&lt;/strong&gt;，生成包含丰富&lt;strong&gt;空间信息&lt;/strong&gt;的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SRDF 方法&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;迭代自训练&amp;rdquo;&lt;/strong&gt; 构建一个性能强劲的指令生成器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）导航中实时指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;部分近期研究不再训练离线指令生成器，而是在&lt;strong&gt;导航过程中实时生成指令&lt;/strong&gt;。例如，&lt;strong&gt;LANA 模型&lt;/strong&gt;提出一种 &lt;strong&gt;&amp;ldquo;具备语言能力的导航智能体&amp;rdquo;&lt;/strong&gt; —— 该智能体不仅能执行导航指令，还可生成路线描述。&lt;/p&gt;
&lt;h4&gt;VLN Agent: Learning an Embodied Agent for Reasoning and Planning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管&lt;strong&gt;世界模型&lt;/strong&gt;与&lt;strong&gt;人类模型&lt;/strong&gt;为智能体赋予了&lt;strong&gt;视觉与语言理解能力&lt;/strong&gt;，但 VLN 智能体仍需培养&lt;strong&gt;具身推理&lt;/strong&gt;（embodied reasoning）与&lt;strong&gt;规划能力&lt;/strong&gt;，以支撑自身决策。从这一角度出发，我们将探讨两大挑战：&lt;strong&gt;接地与推理&lt;/strong&gt;、&lt;strong&gt;规划&lt;/strong&gt;；同时还将研究 &lt;strong&gt;&amp;ldquo;直接以基础模型作为 VLN 智能体核心骨干&amp;rdquo;&lt;/strong&gt; 的方法。&lt;/p&gt;
&lt;h5&gt;接地与推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地与推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e4%b8%8e%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;视觉-语言领域的其他任务（如&lt;strong&gt;视觉问答&lt;/strong&gt;（&lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;图像描述生成&lt;/strong&gt;）主要聚焦于 &lt;strong&gt;&amp;ldquo;图像与对应文本描述之间的静态对齐&amp;rdquo;&lt;/strong&gt;，而 VLN 智能体则需基于自身动作，对 &lt;strong&gt;&amp;ldquo;指令与环境中的时空动态信息&amp;rdquo;&lt;/strong&gt; 进行推理。具体而言，智能体需考虑&lt;strong&gt;过往动作&lt;/strong&gt;、识别&lt;strong&gt;待执行的子指令片段&lt;/strong&gt;，并将文本与视觉环境进行&lt;strong&gt;接地&lt;/strong&gt;（grounding），从而执行相应动作。&lt;/p&gt;
&lt;p&gt;传统方法主要通过 &lt;strong&gt;&amp;ldquo;显式语义建模&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;辅助任务设计&amp;rdquo;&lt;/strong&gt; 获取上述能力；但随着基础模型的兴起，&lt;strong&gt;&amp;ldquo;基于特定设计任务的预训练&amp;rdquo;&lt;/strong&gt; 已成为主流方案。&lt;/p&gt;
&lt;p&gt;传统研究通过 &lt;strong&gt;&amp;ldquo;视觉与语言模态的显式语义建模&amp;rdquo;&lt;/strong&gt; 提升智能体的&lt;strong&gt;显式接地能力&lt;/strong&gt;，具体方向包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;建模动作与地标&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;利用指令中的句法信息&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;建模空间关系&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前，&lt;strong&gt;&amp;ldquo;基于基础模型实现 VLN 智能体显式接地&amp;rdquo;&lt;/strong&gt; 的研究仍较少。例如，提出 &lt;strong&gt;&amp;ldquo;动作原子概念学习&amp;rdquo;&lt;/strong&gt;，并将视觉观测映射为&lt;strong&gt;多模态对齐特征&lt;/strong&gt;，以辅助接地。&lt;/p&gt;
&lt;p&gt;除显式语义建模外，传统研究还通过 &lt;strong&gt;&amp;ldquo;辅助推理任务&amp;rdquo;&lt;/strong&gt; 提升智能体的接地能力。但在基于基础模型的 VLN 智能体中，这类方法较少被探索 —— 因为基础模型的预训练过程已使其在导航前就具备了对 &lt;strong&gt;&amp;ldquo;时空语义&amp;rdquo;&lt;/strong&gt; 的通用理解。&lt;/p&gt;
&lt;p&gt;现有研究通过设计&lt;strong&gt;特定预训练任务&lt;/strong&gt;，进一步提升智能体的接地能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;设计专门针对 &lt;strong&gt;&amp;ldquo;场景与物体接地&amp;rdquo;&lt;/strong&gt; 的预训练任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOViS&lt;/strong&gt;：提出两项专项预训练任务，分别增强智能体的 &lt;strong&gt;&amp;ldquo;方向感知&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;视觉信息理解&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HOP&lt;/strong&gt;：提出 &lt;strong&gt;&amp;ldquo;历史与顺序感知预训练范式&amp;rdquo;&lt;/strong&gt;，重点强调&lt;strong&gt;历史信息&lt;/strong&gt;与&lt;strong&gt;轨迹顺序&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;&amp;ldquo;增强智能体的未来视角语义预测能力&amp;rdquo;&lt;/strong&gt; 有助于提升其在&lt;strong&gt;长路径导航&lt;/strong&gt;中的性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设计 &lt;strong&gt;&amp;ldquo;掩码路径建模目标&amp;rdquo;&lt;/strong&gt; —— 给定随机掩码的子路径，重建原始完整路径&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;提出 &lt;strong&gt;&amp;ldquo;实体感知预训练&amp;rdquo;&lt;/strong&gt;，通过预测&lt;strong&gt;接地实体&lt;/strong&gt;并将其与文本对齐实现接地能力提升&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;动态规划&lt;/strong&gt;能让 VLN 智能体实时适应环境变化、优化导航策略。目前，规划方法主要分为两类：一类是 &lt;strong&gt;&amp;ldquo;利用全局图信息增强局部动作空间&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;图基规划器&lt;/strong&gt;；另一类是随基础模型（尤其是 &lt;strong&gt;LLM&lt;/strong&gt;）兴起的 &lt;strong&gt;LLM 基规划器&lt;/strong&gt; —— 这类规划器借助 LLM 的&lt;strong&gt;海量常识&lt;/strong&gt;与&lt;strong&gt;先进推理能力&lt;/strong&gt;，生成动态规划方案，提升决策效果。&lt;/p&gt;
&lt;p&gt;近期 VLN 研究的核心方向之一，是通过 &lt;strong&gt;&amp;ldquo;全局图信息&amp;rdquo;&lt;/strong&gt; 增强导航智能体的规划能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;已访问节点的图边界&amp;rdquo;&lt;/strong&gt; 中的&lt;strong&gt;全局动作步骤&lt;/strong&gt;，增强&lt;strong&gt;局部导航动作空间&lt;/strong&gt;，以实现更优&lt;strong&gt;全局规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过 &lt;strong&gt;&amp;ldquo;高层规划（区域选择）+ 低层规划（节点选择）&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;分层策略&lt;/strong&gt;，进一步优化导航决策&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;&amp;ldquo;基于图边界的全局与局部动作空间&amp;rdquo;&lt;/strong&gt; 中融入 &lt;strong&gt;&amp;ldquo;网格级动作&amp;rdquo;&lt;/strong&gt;，提升&lt;strong&gt;动作预测精度&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在&lt;strong&gt;连续环境&lt;/strong&gt;中，规划方法进一步演进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;采用&lt;strong&gt;分层规划思路&lt;/strong&gt; —— 通过 &lt;strong&gt;&amp;ldquo;从预测的局部可导航性图中选择局部航点&amp;rdquo;&lt;/strong&gt;，用&lt;strong&gt;高层动作空间&lt;/strong&gt;替代&lt;strong&gt;低层动作空间&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CM2&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;在局部地图中实现指令接地&amp;rdquo;&lt;/strong&gt;，辅助&lt;strong&gt;轨迹规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拓展上述策略，构建&lt;strong&gt;全局拓扑图&lt;/strong&gt;或&lt;strong&gt;网格图&lt;/strong&gt;，支持 &lt;strong&gt;&amp;ldquo;基于地图的全局规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;视频预测模型&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;神经辐射表征模型&amp;rdquo;&lt;/strong&gt; 预测多个&lt;strong&gt;未来航点&lt;/strong&gt;，并基于 &lt;strong&gt;&amp;ldquo;预测候选航点的长期影响&amp;rdquo;&lt;/strong&gt; 规划最优动作&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与此同时，部分研究借助 LLM 的&lt;strong&gt;常识知识&lt;/strong&gt;生成 &lt;strong&gt;&amp;ldquo;基于文本的规划方案&amp;rdquo;&lt;/strong&gt;，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-Planner&lt;/strong&gt;：生成由 &lt;strong&gt;&amp;ldquo;子目标&amp;rdquo;&lt;/strong&gt; 构成的详细规划，并根据&lt;strong&gt;预定义程序模式&lt;/strong&gt;整合检测到的物体，实时动态调整规划&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mic&lt;/strong&gt; 与 &lt;strong&gt;A²Nav&lt;/strong&gt;：专注于将导航任务拆解为详细文本指令 —— Mic 从&lt;strong&gt;静态与动态双视角&lt;/strong&gt;生成分步规划，A²Nav 则利用 &lt;strong&gt;GPT-3&lt;/strong&gt; 将指令解析为可执行子任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ThinkBot&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;思维链推理&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought Reasoning），生成 &lt;strong&gt;&amp;ldquo;与交互物体相关的缺失动作&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VL-Map&lt;/strong&gt;：基于 &lt;strong&gt;&amp;ldquo;代码化 LLM&amp;rdquo;&lt;/strong&gt;（遵循 &lt;strong&gt;Code-as-Policy&lt;/strong&gt; 框架），将导航指令拆解为 &lt;strong&gt;&amp;ldquo;代码格式的时序化目标相关函数&amp;rdquo;&lt;/strong&gt;，并利用 &lt;strong&gt;&amp;ldquo;动态构建的可查询地图&amp;rdquo;&lt;/strong&gt; 指导目标执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayNav&lt;/strong&gt;：构建 &lt;strong&gt;&amp;ldquo;已探索环境的 3D 场景图&amp;rdquo;&lt;/strong&gt;，将其作为 LLM 输入，为导航器生成 &lt;strong&gt;&amp;ldquo;可行且符合上下文的高层规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;作为 VLN 智能体的基础模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;作为-vln-智能体的基础模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bd%9c%e4%b8%ba-vln-%e6%99%ba%e8%83%bd%e4%bd%93%e7%9a%84%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;主流方案以 &lt;strong&gt;&amp;ldquo;单流 VL 模型&amp;rdquo;&lt;/strong&gt; 作为 VLN 智能体的核心结构：这类模型在每个时间步同时处理 &lt;strong&gt;&amp;ldquo;语言、视觉、历史令牌（token）&amp;rdquo;&lt;/strong&gt; 输入，通过对&lt;strong&gt;跨模态令牌&lt;/strong&gt;的自注意力运算捕捉 &lt;strong&gt;&amp;ldquo;文本-视觉对应关系&amp;rdquo;&lt;/strong&gt;，进而推断动作概率。&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;零样本 VLN&lt;/strong&gt; 场景中，&lt;strong&gt;CLIP-NAV&lt;/strong&gt; 利用 CLIP 获取 &lt;strong&gt;&amp;ldquo;描述目标物体的自然语言指称表达式&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;序贯导航决策&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;此外，&lt;strong&gt;VLN-CE&lt;/strong&gt;（连续环境 VLN）智能体与 &lt;strong&gt;VLN-DE&lt;/strong&gt;（离散环境 VLN）智能体的核心差异在于&lt;strong&gt;动作空间&lt;/strong&gt;：前者在连续环境中执行&lt;strong&gt;低层控制&lt;/strong&gt;，而非后者 &lt;strong&gt;&amp;ldquo;基于图的高层视角选择动作&amp;rdquo;&lt;/strong&gt;。尽管早期研究采用 &lt;strong&gt;LSTM&lt;/strong&gt; 推断低层动作，但 &lt;strong&gt;&amp;ldquo;航点预测器&amp;rdquo;&lt;/strong&gt;（waypoint predictor）的引入实现了 &lt;strong&gt;&amp;ldquo;从 DE 到 CE 的方法迁移&amp;rdquo;&lt;/strong&gt; —— 所有这些方法均通过航点预测器获取 &lt;strong&gt;&amp;ldquo;局部可导航性图&amp;rdquo;&lt;/strong&gt;，使 DE 场景中的基础模型能适配连续环境。具体而言，航点检测过程主要通过 &lt;strong&gt;&amp;ldquo;视觉观测&amp;rdquo;&lt;/strong&gt;（如全景 RGBD 图像），从智能体当前位置预测 &lt;strong&gt;&amp;ldquo;可导航的相邻候选航点&amp;rdquo;&lt;/strong&gt; 作为潜在目标，再由智能体选择其一作为当前目的地。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 具备强大的&lt;strong&gt;推理能力&lt;/strong&gt;与&lt;strong&gt;世界语义抽象能力&lt;/strong&gt;，且在 &lt;strong&gt;&amp;ldquo;未知大规模环境&amp;rdquo;&lt;/strong&gt; 中表现出优异的&lt;strong&gt;泛化性&lt;/strong&gt; —— 因此，近期 VLN 研究开始直接将 LLM 作为智能体执行导航任务。其核心流程为：将&lt;strong&gt;视觉观测&lt;/strong&gt;转换为&lt;strong&gt;文本描述&lt;/strong&gt;，与指令一同输入 LLM，由 LLM 完成&lt;strong&gt;动作预测&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;代表性创新方案包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT&lt;/strong&gt; 与 &lt;strong&gt;MapGPT&lt;/strong&gt;：验证了&lt;strong&gt;零样本导航&lt;/strong&gt;的可行性 —— NavGPT 利用 &lt;strong&gt;GPT-4&lt;/strong&gt; 自主生成动作，MapGPT 将&lt;strong&gt;拓扑图&lt;/strong&gt;转换为&lt;strong&gt;全局探索提示&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiscussNav&lt;/strong&gt;：拓展上述思路，部署 &lt;strong&gt;&amp;ldquo;多领域专用 VLN 专家&amp;rdquo;&lt;/strong&gt;（包括&lt;strong&gt;指令分析专家&lt;/strong&gt;、&lt;strong&gt;视觉感知专家&lt;/strong&gt;、&lt;strong&gt;完成度估计专家&lt;/strong&gt;、&lt;strong&gt;决策测试专家&lt;/strong&gt;），减少导航任务中的人工参与：通过将任务分配给专用智能体，减轻单一模型负担，实现 &lt;strong&gt;&amp;ldquo;任务专属优化处理&amp;rdquo;&lt;/strong&gt;，并借助&lt;strong&gt;多大型模型的协同优势&lt;/strong&gt;提升&lt;strong&gt;鲁棒性&lt;/strong&gt;、&lt;strong&gt;透明度&lt;/strong&gt;与&lt;strong&gt;整体性能&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MC-GPT&lt;/strong&gt;：利用 &lt;strong&gt;&amp;ldquo;记忆拓扑图&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;人类导航示例&amp;rdquo;&lt;/strong&gt; 丰富&lt;strong&gt;策略多样性&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;InstructNav&lt;/strong&gt;：将导航拆解为&lt;strong&gt;子任务&lt;/strong&gt;，并结合 &lt;strong&gt;&amp;ldquo;多源价值图&amp;rdquo;&lt;/strong&gt; 实现高效执行&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与 &lt;strong&gt;&amp;ldquo;零样本使用&amp;rdquo;&lt;/strong&gt; 不同，部分研究通过 &lt;strong&gt;&amp;ldquo;微调 LLM&amp;rdquo;&lt;/strong&gt;，使其能更有效地处理&lt;strong&gt;具身导航任务&lt;/strong&gt;。另有研究融入 &lt;strong&gt;&amp;ldquo;思维链&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought, &lt;strong&gt;CoT&lt;/strong&gt;）推理机制提升推理过程，例如 &lt;strong&gt;Nav-CoT&lt;/strong&gt; 将 LLM 转化为 &lt;strong&gt;&amp;ldquo;世界模型与导航推理智能体&amp;rdquo;&lt;/strong&gt;，通过模拟未来环境简化决策 —— 这一方案证实了 &lt;strong&gt;&amp;ldquo;微调语言模型&amp;rdquo;&lt;/strong&gt; 在仿真与真实场景中的&lt;strong&gt;灵活性&lt;/strong&gt;与&lt;strong&gt;实用潜力&lt;/strong&gt;，较传统应用实现了显著突破。&lt;/p&gt;
&lt;h3&gt;挑战与未来方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;挑战与未来方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8c%91%e6%88%98%e4%b8%8e%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;尽管基础模型为&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（&lt;strong&gt;VLN&lt;/strong&gt;）提供了创新性解决方案，但仍有若干局限尚未得到充分探索，同时新的挑战也随之出现。在本节中，我们将从&lt;strong&gt;基准数据集&lt;/strong&gt;、&lt;strong&gt;世界模型&lt;/strong&gt;、&lt;strong&gt;人类模型&lt;/strong&gt;、&lt;strong&gt;智能体模型&lt;/strong&gt;及&lt;strong&gt;真实机器人部署&lt;/strong&gt;五个维度，梳理 VLN 领域的挑战与未来研究方向。&lt;/p&gt;
&lt;h4&gt;基准数据集：数据与任务的局限&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基准数据集数据与任务的局限&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e5%87%86%e6%95%b0%e6%8d%ae%e9%9b%86%e6%95%b0%e6%8d%ae%e4%b8%8e%e4%bb%bb%e5%8a%a1%e7%9a%84%e5%b1%80%e9%99%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;当前 &lt;strong&gt;VLN 数据集&lt;/strong&gt;在&lt;strong&gt;质量&lt;/strong&gt;、&lt;strong&gt;多样性&lt;/strong&gt;、&lt;strong&gt;偏差&lt;/strong&gt;及&lt;strong&gt;可扩展性&lt;/strong&gt;方面存在明显局限。例如，在 &lt;strong&gt;R2R&lt;/strong&gt; 数据集中，&lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 偏向于&lt;strong&gt;最短路径&lt;/strong&gt;，无法准确反映现实世界的导航场景。下文将探讨 VLN 基准数据集的改进趋势与建议方向：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;统一且贴近现实的任务与平台&lt;/strong&gt;：构建可靠的基准数据集并确保结果可复现，是评估真实场景下 VLN 性能的关键。现实世界的复杂性要求基准数据集需全面覆盖各类导航挑战，因此需要一个通用的 &lt;strong&gt;&amp;ldquo;仿真到现实&amp;rdquo;&lt;/strong&gt; 评估平台（如 &lt;strong&gt;OVMM&lt;/strong&gt;），以实现仿真与真实场景下的标准化测试。此外，任务与活动设计需贴近现实且源于人类需求，例如 &lt;strong&gt;BEHAVIOR-1K&lt;/strong&gt; 基准数据集，在虚拟、交互式且具生态性的环境中构建&lt;strong&gt;日常家庭活动场景&lt;/strong&gt;，以满足对 &lt;strong&gt;&amp;ldquo;多样性&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;真实性&amp;rdquo;&lt;/strong&gt; 的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;动态环境&lt;/strong&gt;：现实世界环境本质上具有&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;动态性&lt;/strong&gt; —— &lt;strong&gt;移动物体&lt;/strong&gt;、&lt;strong&gt;行人&lt;/strong&gt;，以及&lt;strong&gt;光照&lt;/strong&gt;、&lt;strong&gt;天气&lt;/strong&gt;等环境变化，均可能引发&lt;strong&gt;突发情况&lt;/strong&gt;。这些因素会干扰导航系统的&lt;strong&gt;视觉感知&lt;/strong&gt;，使其难以维持稳定性能。近期部分研究（如 &lt;strong&gt;HAZARD&lt;/strong&gt;、&lt;strong&gt;Habitat 3.0&lt;/strong&gt;、&lt;strong&gt;HA-VLN&lt;/strong&gt;）已开始关注&lt;strong&gt;动态环境&lt;/strong&gt;，为后续研究提供了良好起点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从室内到室外&lt;/strong&gt;：适用于室外环境的 VLN 智能体（如&lt;strong&gt;自动驾驶车辆&lt;/strong&gt;、&lt;strong&gt;无人机&lt;/strong&gt;）正逐渐获得更多关注，相关&lt;strong&gt;语言引导数据集&lt;/strong&gt;也已陆续开发。早期研究尝试将 LLM 融入&lt;strong&gt;室外 VLN 任务&lt;/strong&gt;，具体方式包括&lt;strong&gt;提示工程&lt;/strong&gt;，或通过&lt;strong&gt;微调 LLM&lt;/strong&gt; 实现 &lt;strong&gt;&amp;ldquo;预测下一步动作&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;规划未来轨迹&amp;rdquo;&lt;/strong&gt;。为使现成的 VLN 模型适配&lt;strong&gt;室外导航场景&lt;/strong&gt;，研究者利用&lt;strong&gt;真实驾驶视频&lt;/strong&gt;、&lt;strong&gt;仿真驾驶数据&lt;/strong&gt;或两者结合进行&lt;strong&gt;指令微调&lt;/strong&gt;，使基础模型能够学习预测未来的&lt;strong&gt;油门与转向角度&lt;/strong&gt;。此外，研究者还在基于基础模型的驾驶智能体中集成了额外的&lt;strong&gt;推理与规划模块&lt;/strong&gt;。关于室外 VLN 的详细综述，建议读者参考相关综述文献与立场论文。&lt;/p&gt;
&lt;h4&gt;世界模型：从二维（2D）到三维（3D）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;世界模型从二维2d到三维3d&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%96%e7%95%8c%e6%a8%a1%e5%9e%8b%e4%bb%8e%e4%ba%8c%e7%bb%b42d%e5%88%b0%e4%b8%89%e7%bb%b43d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;构建有效的&lt;strong&gt;世界表征&lt;/strong&gt;是&lt;strong&gt;具身感知&lt;/strong&gt;、&lt;strong&gt;推理&lt;/strong&gt;与&lt;strong&gt;规划&lt;/strong&gt;领域的核心研究主题。VLN 本质上是一项 &lt;strong&gt;3D 任务&lt;/strong&gt; —— 智能体需以 3D 形式感知真实世界环境。尽管当前研究已能通过强大的通用 &lt;strong&gt;2D 表征&lt;/strong&gt;描述世界，但这类表征无法充分支持 3D 场景下的&lt;strong&gt;空间语言理解&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;以往研究已提出多种显式 &lt;strong&gt;3D 表征方式&lt;/strong&gt;，包括各类&lt;strong&gt;语义同步定位与地图构建&lt;/strong&gt;（semantic &lt;strong&gt;SLAM&lt;/strong&gt;）、&lt;strong&gt;体素表征&lt;/strong&gt;、&lt;strong&gt;深度信息&lt;/strong&gt;、&lt;strong&gt;鸟瞰图&lt;/strong&gt;（Bird&amp;rsquo;s-Eye-View）表征（如&lt;strong&gt;网格图&lt;/strong&gt;）及&lt;strong&gt;局部度量图&lt;/strong&gt;。但这些表征存在局限：它们将物体集合限定为 &lt;strong&gt;&amp;ldquo;封闭集合&amp;rdquo;&lt;/strong&gt;，无法适配自然语言对应的 &lt;strong&gt;&amp;ldquo;开放词汇场景&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;部分研究尝试构建 &lt;strong&gt;&amp;ldquo;可查询的地图/场景表征&amp;rdquo;&lt;/strong&gt;，例如将 CLIP 提取的&lt;strong&gt;多视角图像特征&lt;/strong&gt;整合到 &lt;strong&gt;3D 体素网格&lt;/strong&gt;或&lt;strong&gt;俯视特征图&lt;/strong&gt;中，或利用&lt;strong&gt;场景图&lt;/strong&gt;表征&lt;strong&gt;空间关系&lt;/strong&gt;。然而，&lt;strong&gt;&amp;ldquo;如何将大规模数据中学习到的 3D 表征适配于 VLN 智能体，以提升其 3D 环境感知能力&amp;rdquo;&lt;/strong&gt; 仍是待探索的问题。近期兴起的 &lt;strong&gt;3D 基础模型&lt;/strong&gt; —— 包括 &lt;strong&gt;3D 重建模型&lt;/strong&gt; 与 &lt;strong&gt;3D 多模态表征模型&lt;/strong&gt; —— 有望为 VLN 领域提供关键支撑。&lt;/p&gt;
&lt;h4&gt;人类模型：从指令到对话&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;人类模型从指令到对话&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%ba%e7%b1%bb%e6%a8%a1%e5%9e%8b%e4%bb%8e%e6%8c%87%e4%bb%a4%e5%88%b0%e5%af%b9%e8%af%9d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;以往研究多采用 &lt;strong&gt;&amp;ldquo;说话者-倾听者范式&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;受限问答对话&amp;rdquo;&lt;/strong&gt; —— 这类方式仅允许智能体主动请求帮助。近年来，涌现出一批以 &lt;strong&gt;&amp;ldquo;开放式对话指令&amp;rdquo;&lt;/strong&gt; 为核心的新基准数据集，支持智能体在模糊或困惑场景下进行完全&lt;strong&gt;自由形式的通信&lt;/strong&gt;，包括&lt;strong&gt;提问&lt;/strong&gt;、&lt;strong&gt;提议&lt;/strong&gt;、&lt;strong&gt;解释&lt;/strong&gt;、&lt;strong&gt;建议&lt;/strong&gt;、&lt;strong&gt;澄清&lt;/strong&gt;与&lt;strong&gt;协商&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;然而，当前方法仍依赖 &lt;strong&gt;&amp;ldquo;基于规则的对话模板&amp;rdquo;&lt;/strong&gt; 应对上述复杂场景，即便部分方法包含基础模型组件，也未充分发挥其能力。通过 &lt;strong&gt;&amp;ldquo;人类对话数据 + 仿真导航视频&amp;rdquo;&lt;/strong&gt; 对&lt;strong&gt;视频-语言模型&lt;/strong&gt;进行&lt;strong&gt;对话调优&lt;/strong&gt;，使模型在导航过程中具备更强的&lt;strong&gt;对话生成能力&lt;/strong&gt;。未来研究需重点关注两方面：一是将基础模型融入 &lt;strong&gt;&amp;ldquo;情境化任务导向对话管理&amp;rdquo;&lt;/strong&gt;；二是探索现有基础模型在 &lt;strong&gt;&amp;ldquo;任务导向对话&amp;rdquo;&lt;/strong&gt; 中的应用潜力。&lt;/p&gt;
&lt;h4&gt;智能体模型：基础模型在 VLN 中的适配&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;智能体模型基础模型在-vln-中的适配&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%99%ba%e8%83%bd%e4%bd%93%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%9c%a8-vln-%e4%b8%ad%e7%9a%84%e9%80%82%e9%85%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管基础模型具有强大的&lt;strong&gt;泛化能力&lt;/strong&gt;，但将其融入导航任务仍面临挑战：&lt;strong&gt;LLM&lt;/strong&gt; 本质上缺乏对真实环境的&lt;strong&gt;视觉感知能力&lt;/strong&gt;，且易产生 &lt;strong&gt;&amp;ldquo;幻觉&amp;rdquo;&lt;/strong&gt;；下文还将探讨 LLM 在规划与推理方面的能力局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺乏具身经验&lt;/strong&gt;：这一局限可能导致 LLM 在任务规划与推理中仅依赖&lt;strong&gt;预设常识&lt;/strong&gt;，无法满足真实场景的特定需求。部分研究通过 &lt;strong&gt;&amp;ldquo;将视觉观测转换为文本描述，作为 LLM 的提示&amp;rdquo;&lt;/strong&gt; 解决该问题，但这种方式可能丢失关键&lt;strong&gt;视觉语义&lt;/strong&gt;。与 LLM 相比，&lt;strong&gt;VLM&lt;/strong&gt;（视觉-语言模型）智能体虽展现出 &lt;strong&gt;&amp;ldquo;感知视觉世界与规划&amp;rdquo;&lt;/strong&gt; 的潜力，但其训练数据主要源于互联网，缺乏&lt;strong&gt;具身经验&lt;/strong&gt;，需通过&lt;strong&gt;微调&lt;/strong&gt;实现稳健的智能体决策。未来需进一步研究 &lt;strong&gt;&amp;ldquo;如何将基础模型智能体中的常识知识迁移到具身场景中&amp;rdquo;&lt;/strong&gt;。近期提出的 &lt;strong&gt;&amp;ldquo;具身基础模型&amp;rdquo;&lt;/strong&gt;（如 &lt;strong&gt;EmbodieGPT&lt;/strong&gt;、&lt;strong&gt;PaLM-E&lt;/strong&gt;、&lt;strong&gt;Octopus&lt;/strong&gt;）为解决该问题提供了可行方向：这些模型通过在多类具身任务上微调基础模型，缩小智能体在 &lt;strong&gt;&amp;ldquo;视觉-语言-具身动作&amp;rdquo;&lt;/strong&gt; 理解上的差距，提升其基于多模态输入的理解与执行能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;幻觉问题&lt;/strong&gt;：LLM 与 VLM 可能生成 &lt;strong&gt;&amp;ldquo;不存在的物体&amp;rdquo;&lt;/strong&gt;，导致&lt;strong&gt;信息失真&lt;/strong&gt;。例如，LLM 在任务规划时可能生成 &lt;strong&gt;&amp;ldquo;向前走并在沙发处左转&amp;rdquo;&lt;/strong&gt; 的指令，即便房间内并无沙发。这种偏差可能导致智能体执行错误或无法完成的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM 在规划与推理中的能力局限&lt;/strong&gt;：已有文献针对 LLM 的&lt;strong&gt;零样本推理与规划能力&lt;/strong&gt;展开评估（尤其是结合 &lt;strong&gt;PlanBench&lt;/strong&gt; 与 &lt;strong&gt;CogEval&lt;/strong&gt;），结果表明 LLM 在&lt;strong&gt;复杂规划任务&lt;/strong&gt;中存在明显局限。这些研究在 &lt;strong&gt;&amp;ldquo;规划生成、最优性、稳健性、推理&amp;rdquo;&lt;/strong&gt; 等挑战性场景下评估 LLM，发现其不仅易产生幻觉，还可能无法理解复杂规划问题背后的&lt;strong&gt;关系结构&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在 VLN 场景中，由于室内环境固定且导航动作集合有限，&lt;strong&gt;动作空间&lt;/strong&gt;与&lt;strong&gt;规划需求&lt;/strong&gt;相对受限。这种 &lt;strong&gt;&amp;ldquo;有界场景&amp;rdquo;&lt;/strong&gt; 使 LLM 能够生成 &lt;strong&gt;&amp;ldquo;粗粒度方向的分步指令&amp;rdquo;&lt;/strong&gt; —— 已有研究证实该方式的有效性。需强调的是，在 VLN 任务中，LLM 并非主导整个规划过程，而是通过 &lt;strong&gt;&amp;ldquo;结构化拆解指令&amp;rdquo;&lt;/strong&gt; 提供辅助；智能体的实际决策仍主要依赖&lt;strong&gt;感知&lt;/strong&gt;、&lt;strong&gt;运动控制&lt;/strong&gt;等其他组件。因此，LLM 的规划功能更多是 &lt;strong&gt;&amp;ldquo;补充性指导&amp;rdquo;&lt;/strong&gt;，而非唯一决策依据。&lt;/p&gt;
&lt;h4&gt;部署：从仿真到真实机器人&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;部署从仿真到真实机器人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%83%a8%e7%bd%b2%e4%bb%8e%e4%bb%bf%e7%9c%9f%e5%88%b0%e7%9c%9f%e5%ae%9e%e6%9c%ba%e5%99%a8%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;仿真环境往往缺乏真实世界的&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;多样性&lt;/strong&gt;，且低质量渲染图像会进一步加剧这一问题。具体而言，当前部署面临三大瓶颈：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知差距&lt;/strong&gt;：仿真与真实场景的&lt;strong&gt;视觉差异&lt;/strong&gt;导致智能体性能与精度下降，因此需构建更稳健的感知系统。例如，尝试利用&lt;strong&gt;语义地图&lt;/strong&gt;与 &lt;strong&gt;3D 特征场&lt;/strong&gt;为单目机器人提供&lt;strong&gt;全景感知&lt;/strong&gt;，显著提升了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具身差距与数据稀缺&lt;/strong&gt;：仿真环境的&lt;strong&gt;物理规则&lt;/strong&gt;与真实机器人的&lt;strong&gt;具身特性&lt;/strong&gt;不匹配，且真实场景下 VLN 数据收集成本高、规模有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据规模化解决方案&lt;/strong&gt;：&lt;strong&gt;机器人远程操控&lt;/strong&gt;的兴起为解决数据稀缺提供了新思路 —— 通过人类远程控制机器人，可在真实人机交互场景中规模化收集 VLN 数据，为基础模型训练提供支撑。&lt;/p&gt;
&lt;h3&gt;仓库论文链接&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;仓库论文链接&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bb%93%e5%ba%93%e8%ae%ba%e6%96%87%e9%93%be%e6%8e%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：本表格按分类和子分类组织所有 VLN 相关论文，便于浏览和筛选。分类包括：Survey（综述）、World Model（世界模型）、Human Model（人类模型）、VLN Agent（VLN 智能体）、Behavior Analysis（行为分析）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;分类&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;子分类&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Survey&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.12667&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/eric-ai-lab/awesome-vision-language-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s10462-022-10174-9&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual language navigation: A survey and open challenges&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11544&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation: A Survey and Taxonomy&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;World Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.13451&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.03561&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.14158&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Volumetric Environment Representation for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/DefaultRui/VLN-VER&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2023/0204.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision Language Navigation with Knowledge-driven Environmental Dreamer&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;IJCAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/hekj/FDA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.19195&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.02960&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple and Effective Synthesis of Indoor 3D Scenes&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research/se3ds&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Navigational Visual Representations with Semantic Map Supervision&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.11984&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning vision-and-language navigation from youtube videos&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.12907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GridMM: Grid Memory Map for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/GridMM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.04385&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BEVBert: Multimodal Map Pre-training for Language-guided Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/VLN-BEVBert&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15644&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scaling Data Generation in Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/ScaleVLN/tree/main?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.03112&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clin1223/MTVM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.15685&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EnvEdit: Environment Editing for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960375.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.06383&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Much Can CLIP Benefit Vision-and-Language Tasks?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clip-vil/CLIP-ViL&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.11742&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/cshizhe/VLN-DUET&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.13309&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;History Aware Multimodal Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://cshizhe.github.io/projects/vln_hamt.html&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.08756&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pathdreamer: A World Model for Indoor Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Pashevich_Episodic_Transformer_for_Vision-and-Language_Navigation_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Episodic Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.09105&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbert: In-domain Pretraining for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://airbert-vln.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.07876&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation with Random Environmental Mixup&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LCFractal/VLNREM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Human Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene Map-based Prompt Tuning for Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.11142&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/NavRAG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.08467&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/VLN-SRDF&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.15087&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigation Instruction Generation with BEV Perception and Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/FanScy/BEVInstructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.07433&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Controllable Navigation Instruction Generation with Chain of Thought Prompting&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/refkxh/C-Instructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.acl-long.734.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/gmuraleekrishna/SAS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.18721&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Correctable Landmark Discovery via Large Models for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/CONSOLE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.02559&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavHint: Vision and Language Navigation Agent with a Hint Generator&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/NavHint&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10359152&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Follow and Generate Instructions for Language-Capable Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.08409&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lana: A Language-Capable Navigator for Instruction Following and Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wxh1996/LANA-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/xiangyangli-cn/KERM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.11918&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.00852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.09230&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Trans: Translator for the Vision and Language Navigation Agent&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/VLN-trans&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.04006&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/liangcici/Probes-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.14973&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Less is More: Generating Grounded Navigation Instructions from Landmarks&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research-datasets/RxR/tree/main/marky-mT5&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.10504&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Evaluation of Vision-and-Language Navigation Instructions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://say-can.github.io/assets/palm_saycan.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do As I Can, Not As I Say:Grounding Language in Robotic Affordances&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://say-can.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN Agent&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.05552&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/SAME&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.18800&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniVLN: Efficient Vision-and-Language Navigation byProgressive Knowledge Distillation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.06072&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.12587&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CSir1996/VLN-GELA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/chengaopro/AZHP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.04758&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bird&amp;rsquo;s-Eye-View Scene Graph for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14268&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Masked Path Modeling for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.04907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10006384&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2207.11201&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Target-Driven Structured Transformer Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YushengZhao/TD-STP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9880046&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/HOP-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.505.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;COLING&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/LOViS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.12944&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene-Intuitive Agent for Remote Embodied Visual Grounding&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.14143&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YuankaiQi/ORIST&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN BERT: A Recurrent Vision-and-Language BERT for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Recurrent-VLN-BERT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.10638&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/weituo12321/PREVALENT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;VLN-CE&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.02549&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.22548&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://miv-xjtu.github.io/JanusVLN.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23468&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Feliciaxyao/NavMorph&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.05890&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.01943&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/HNR-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03047v2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;PAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/ETPNav?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2408.10388&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Narrowing the Gap between Vision and Action in Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02764&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Discrete-Continuous-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.02857&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jacobkrantz/VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Zero-shot)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10611565&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/DiscussNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.07314&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://chen-judge.github.io/MapGPT/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.10620&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04882&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/InstructNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.16986&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March in Chat: Interactive Prompting for Remote Embodied Referring Expression&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/MiC&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.10822&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision and Language Navigation in the Real World via Online Visual Language Mapping&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://peihaochen.github.io/files/publications/A2Nav.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS Workshop&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.16649&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Fine-tuning)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.01551&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/EvolveNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.findings-naacl.60.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LangNav: Language as a Perceptual Representation for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/pbw-Berwin/LangNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07376&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/NavCoT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.02010&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generalist Model for Embodied Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LaVi-Lab/NaviLLM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2407.12366&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.15852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;RSS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Behavior Analysis&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.16394&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Visual Imaginations Improve Vision-and-Language Navigation Agents?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.17313&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/zehao-wang/navnuances&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Behavioral Analysis of Vision-and-Language Navigation Agents&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Yoark/vln-behave&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.438.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diagnosing Vision-and-Language Navigation: What Really Matters&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/VegB/Diagnose_VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;后续工作&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;后续工作&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%90%8e%e7%bb%ad%e5%b7%a5%e4%bd%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里夸奖一下Gemini3和qwen的deep Research，真的救我狗命。&lt;/p&gt;
&lt;p&gt;重点精读部分就看下面Gemini3提供的一份经过深度调研、严格筛选的 &lt;strong&gt;2023–2025&lt;/strong&gt; 年间顶会（CVPR, ICCV, ECCV, ICLR, NeurIPS, CoRL, RSS, ICRA, IROS）&lt;strong&gt;已接收 (Accepted)&lt;/strong&gt; 且 &lt;strong&gt;已公开代码&lt;/strong&gt; 的 VLN / ObjectNav / Zero-Shot / LLM-assisted Navigation 相关论文列表。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;简介&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;关键词&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CVPR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;UniGoal: Towards Universal Zero-shot Goal-oriented Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了基于场景图（Scene Graph）和 LLM 的通用导航框架，统一了 Object, Image, Text 三种目标导航任务，解决 Zero-Shot 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Zero-Shot, Scene Graph, LLM, Universal Goal&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;专注于提升 VLM 的空间理解能力，通过构建空间感知的指令微调数据集，大幅提升了机器人在 3D 环境中的导航和操作能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/RoboSpatial/RoboSpatial&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Spatial Reasoning, VLM, Robotics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-and-Language Navigation via Causal Learning (VLN-GOAT)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入因果推断（Causal Inference）消除数据偏差，提升 VLN 模型的泛化性&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CrystalSixone/VLN-GOAT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Causal Learning, Deconfounding&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;首个基于视频的大模型（Video-based VLM）端到端导航器，无需构建显式地图，直接从视频流规划动作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jzhzhang/NaVid-VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Video VLM, Mapless, End-to-End&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AIGeN: An Adversarial Approach for Instruction Generation in VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用对抗生成网络生成高质量的导航指令，用于数据增强&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/jialuli-luka/AIGeN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Iterative Vision-and-Language Navigation (IVLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了&amp;quot;迭代式导航&amp;quot;新基准，要求机器人在同一环境中持续执行多条指令，考察记忆能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/JacobKrantz/IVLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Continuous Navigation, Memory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics (VLN-SIG)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过生成未来视角的语义图像来辅助当前决策&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://jialuli-luka.github.io/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;模拟人类认知过程（感知-推理-决策），利用 LLM 进行常识推理和空间推理，解决 ObjectNav 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://yhancao.github.io/CogNav/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page &amp;amp; Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Cognitive Modeling, LLM, ObjectNav&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Learning Vision-and-Language Navigation from YouTube Videos (YouTube-VLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用大规模 YouTube 房屋导览视频进行预训练，学习真实世界先验&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入&amp;quot;心理规划&amp;quot;机制，在执行前在潜在空间预演路径&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/HanqingWangAI/DreamWalker&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ECCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN-Copilot: LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出&amp;quot;副驾驶&amp;quot;概念，当导航智能体困惑时，LLM 提供详细的指导和推理辅助&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zun-Wang/VLN-Copilot&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;LLM Agent, Coarse-grained VLN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过微调适配，激发通用多模态大模型（VLM）的导航推理能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/WZMIAOMIAO/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-Language Navigation with Energy-Based Policy (ENP)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出基于能量的模型（Energy-Based Model）来建模导航策略，更好地模拟专家轨迹分布&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://neurips.cc/virtual/2025/poster/93232&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS Page/GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;构建在线 3D 场景图作为 Prompt，实现无需训练的 Zero-Shot 导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/SG-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;InstructNav: Zero-shot Vision-and-Language Navigation with Instruction Tuning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;这是一个通用的导航大模型框架，统一了 VLN 和 ObjectNav&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;(查看作者 Hao Dong 的 GitHub 或 Project Page)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;PanoGen: Text-Conditioned Panoramic Environment Generation for VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;使用生成式模型根据文本生成全景环境，用于 VLN 的数据增强和训练&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CoRL&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;GC-VLN: Graph-Constrained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;UniGoal&lt;/em&gt; 团队新作，将导航建模为图约束优化问题，无需训练即可在连续环境中导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Training-free, Graph Constraints&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LeLaN: Learning a Language-Conditioned Navigation Policy from In-the-Wild Video&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;直接从野外（In-the-Wild）视频数据中学习语言条件的导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://lelan-video.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OpenVLA: An Open-Source Vision-Language-Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;虽然主要针对操作（Manipulation），但其架构和预训练模型被大量用于导航任务的底座&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/openvla/openvla&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 VLM 进行零样本 3D 视觉定位，是导航的关键前置任务&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/desdemonawang/VLM-Grounder&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OVSG: Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;基于开放词汇 3D 场景图的实体定位与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/ovsg-code/ovsg&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICRA&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation with Open-Source LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;探索使用 Llama 等开源模型替代 GPT-4 进行 Zero-Shot 导航，提出时空思维链&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/Open-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;MonoTransmotion&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;涉及单目视觉下的运动规划与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vita-epfl/MonoTransmotion&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 CLIP 和前沿点（Frontier）地图，指导机器人探索语义目标&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bdaiinstitute/vlfm&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLMaps: Visual Language Maps for Robot Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将 VLM 特征融合进 3D 地图，允许使用自然语言索引地图位置&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vlmaps/vlmaps&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;IROS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LLM3: Large Language Model-based Task and Motion Planning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 LLM 进行任务和运动规划，虽然偏 TAMP，但也包含导航组件&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zju-Robotics-Lab/LLM3&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RSS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Unified Video Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;统一的视频动作模型，涵盖导航和操作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page/Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;雖然偏向操作，但其 Policy 蒸馏方法正被用于加速导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/DLR-RM/Consistency-Policy&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICLR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 Web 教程合成智能体轨迹，辅助导航和任务执行&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/xduan7/AgentTrek&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
  </channel>
</rss>
