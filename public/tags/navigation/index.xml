<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Navigation</title>
    <link>http://localhost:1313/tags/navigation/</link>
    <description>Recent content in Navigation on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/navigation/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Social Navigation Idea</title>
      <link>http://localhost:1313/blog/2025/2025-12-15-social-navigation-idea/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-15-social-navigation-idea/</guid>
      <description>
        
        
        &lt;h2&gt;自问自答&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;自问自答&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%87%aa%e9%97%ae%e8%87%aa%e7%ad%94&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在接触 LOVON 这类开放词汇物体导航工作时，发现它们对语义的理解停留在 YOLO 层面，追目标时避障效果很差。当时考虑过用边走边建图的 3D 重建方式，但看到 social navigation 后就搁置了。这里有两个问题值得深入思考。&lt;/p&gt;
&lt;h3&gt;问题一：Social Navigation 需要 3D 重建吗？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题一social-navigation-需要-3d-重建吗&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e4%b8%80social-navigation-%e9%9c%80%e8%a6%81-3d-%e9%87%8d%e5%bb%ba%e5%90%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在 Social Navigation 领域，做显式 3D 重建是一条歧路。这个结论需要从 ObjectNav 和 SocialNav 的本质区别说起。&lt;/p&gt;
&lt;p&gt;ObjectNav 的核心难点是记忆。机器人需要记住&amp;quot;我去过厨房没？那个杯子在哪？&amp;ldquo;这类问题。这里用 3D 建图（如语义地图）是有用的，因为它解决的是静态环境的探索与回溯。LOVON 避障差，通常是因为它是模块化的（YOLO 指方向，然后 Planner 走），尽管利用了 LLM 进行 NER 分解，这也算是一种传统 Planner，对近距离动态避障很弱。&lt;/p&gt;
&lt;p&gt;但 SocialNav 的核心难点是动态性。人是会动的。如果做 3D 重建（TSDF Fusion、NeRF-SLAM 等），会面临严重的鬼影问题。一个人从左走到右，3D 地图上会留下一串残影，不仅不能辅助导航，反而会变成一堆不存在的障碍物墙，把路堵死。&lt;/p&gt;
&lt;p&gt;Falcon、Rank 1、Rank 2 的成功证明了：处理动态环境，需要的是第一视角感知（Egocentric Perception）加上时序记忆（RNN/Transformer），而不是全局静态地图。&lt;/p&gt;
&lt;p&gt;关于&amp;quot;避障不行&amp;quot;的问题，之前遇到的 YOLO 检测到了但还是撞上去的情况，正是端到端强化学习（SocialNav）试图解决的。关键是把&amp;quot;识别&amp;quot;和&amp;quot;运动控制&amp;quot;分开。SocialNav 的方法（如 Falcon）是把深度/RGB 直接映射到动作。如果智能体撞了，它会直接受到惩罚。这比&amp;quot;YOLO 告诉 Planner 有人，Planner 计算路径&amp;quot;的链路反应更快，且更能处理复杂交互。&lt;/p&gt;
&lt;h3&gt;问题二：Depth Anything V3 是合适的&amp;quot;新锤子&amp;quot;吗？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题二depth-anything-v3-是合适的新锤子吗&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e4%ba%8cdepth-anything-v3-%e6%98%af%e5%90%88%e9%80%82%e7%9a%84%e6%96%b0%e9%94%a4%e5%ad%90%e5%90%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;用目标驱动研究的标准来评估这个&amp;quot;锤子&amp;rdquo;，会发现两种截然不同的使用方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;工程思维（不推荐）&lt;/strong&gt;：把 Falcon 输入端的深度传感器换成 Depth Anything V3 生成的深度。问题在于，在仿真器（如 Habitat）里，已经有了完美深度（Ground Truth Depth）。Depth Anything V3 生成的深度再好，也不可能比仿真器自带的完美深度更好。结果可能是性能下降（因为引入了推理延迟和误差），且没有任何创新性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;科研思维（推荐）&lt;/strong&gt;：Depth Anything V3 的真正价值在于&amp;quot;仿真到现实的鸿沟&amp;quot;或&amp;quot;语义感知&amp;quot;。&lt;/p&gt;
&lt;p&gt;第一个方向是鲁棒性/仿真到现实。SocialNav 在仿真里跑得好，是因为仿真里的深度是完美的。真实世界的深度传感器（Realsense/LiDAR）有噪声、有盲区（玻璃、强光）。Depth Anything V3 是一个鲁棒感知探针，它是从大量真实图片训练出来的，对真实世界的噪声有极强的鲁棒性。可以提出一个框架，证明在仿真里使用 Depth Anything V3 提取的特征（而非原始深度），能够让智能体在零样本迁移到真实世界时表现得更好，因为它学到的是通用的深度特征，而不是仿真特定的几何特征。&lt;/p&gt;
&lt;p&gt;第二个方向是隐式语义引导（更高级的）。Depth Anything 不仅仅是深度，它其实是基础模型。为了估计深度，它必须&amp;quot;理解&amp;quot;物体是什么（比如理解这块平滑的像素是墙而不是空洞）。利用 Depth Anything V3 的编码器特征作为 SocialNav 的额外输入，它的特征里不仅包含距离，还隐式包含了物体语义。这可能解决 Falcon&amp;quot;把人当圆柱体&amp;quot;的问题，让智能体能区分&amp;quot;人&amp;quot;和&amp;quot;人形雕塑&amp;quot;，或者区分&amp;quot;柔软的窗帘&amp;quot;和&amp;quot;坚硬的墙&amp;quot;。&lt;/p&gt;
&lt;p&gt;Falcon（基线）和 Rank 2 都依赖几何信息（位置、速度、距离）来判断风险。但有些情况仅靠深度和坐标无法区分：一个人站着不动是在玩手机（不会突然动），还是在等人（可能会突然拥抱）？一个人跑过来是冲着我来的（攻击性），还是只是路过（中性）？&lt;/p&gt;
&lt;p&gt;科学的做法是：不要用 Depth Anything V3 做深度，用它（或者 VLM 如 CLIP/SigLIP）做视觉语义特征提取。参考 Rank 2 的架构，加一个辅助模块，但不是预测&amp;quot;风险分数&amp;quot;，而是预测&amp;quot;社会意图&amp;quot;或&amp;quot;语义状态&amp;quot;。&lt;/p&gt;
&lt;p&gt;为什么这样做有效？Rank 1 证明了数据重要，可以利用基础模型里的海量数据。Rank 2 证明了显式预测隐变量重要，可以预测比风险更高级的意图。&lt;/p&gt;
&lt;h2&gt;Literature Review&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;literature-review&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#literature-review&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在 Google Scholar 里检索引用了 Falcon 的工作，整理如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;序号&lt;/th&gt;
          &lt;th&gt;论文标题&lt;/th&gt;
          &lt;th&gt;中稿状态&lt;/th&gt;
          &lt;th&gt;主要功能&lt;/th&gt;
          &lt;th&gt;核心内容&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding&lt;/td&gt;
          &lt;td&gt;已中稿 CVPR 2025&lt;/td&gt;
          &lt;td&gt;解决零样本开放词汇的 3D 视觉定位问题，即根据自然语言描述在 3D 场景中找到特定物体&lt;/td&gt;
          &lt;td&gt;提出 SeeGround 方法，无需针对 3D 数据进行专门训练。核心思想是将 3D 场景转化为 2D VLM 可理解的格式，通过渲染 3D 场景图像并使用视觉提示技术建立 2D 图像与 3D 空间信息的对应关系，利用预训练的 2D VLM 理解场景并定位物体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 技术报告，Top-2&lt;/td&gt;
          &lt;td&gt;解决跨模态无人机导航中的图像检索问题，即根据自然语言描述从大规模数据库中检索对应的无人机视角图像&lt;/td&gt;
          &lt;td&gt;提出 CGRS 两阶段检索增强框架：第一阶段使用基线模型进行粗略排序；第二阶段利用 VLM 为候选图像生成详细描述，计算查询文本与生成描述的相似度进行精细重排序，显著提高检索精度&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;Zero-Shot 3D Visual Grounding from Vision-Language Models&lt;/td&gt;
          &lt;td&gt;arXiv Preprint (arXiv:2505.22429)&lt;/td&gt;
          &lt;td&gt;零样本 3D 视觉定位&lt;/td&gt;
          &lt;td&gt;从作者和题目看，极大概率是 Seeground (CVPR 2025) 的预印本或其前身，内容与 Seeground 一致，探讨如何利用 VLM 实现零样本 3D 视觉定位&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;Learning to Navigate Socially Through Proactive Risk Perception&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告，第 2 名&lt;/td&gt;
          &lt;td&gt;解决社会导航问题，让机器人在人群密集的动态环境中安全、合乎社会规范地导航&lt;/td&gt;
          &lt;td&gt;基于 Falcon 模型改进，增加主动风险感知模块，能够预测周围行人的基于距离的碰撞风险分数，让机器人具备更强的空间感知能力，主动采取避障行为并保持合适的社交距离&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;解决多楼层环境下的零样本物体目标导航&lt;/td&gt;
          &lt;td&gt;提出 ASCENT 框架，结合多楼层空间抽象和基于 LLM 的由粗到细的边界探索，利用 LLM 的常识推理能力（如&amp;quot;瑜伽垫可能在健身房，而健身房在楼下&amp;quot;）指导机器人跨楼层搜索&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;零样本社会导航&lt;/td&gt;
          &lt;td&gt;提出 SocialNav-Map 框架，结合动态人类轨迹预测和占据栅格地图。不需要针对特定环境训练，使用两种互补方法预测人类轨迹（基于历史路径和基于朝向），将预测结果作为动态障碍物整合进地图，使机器人能预见人的移动并提前规划路径&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;7&lt;/td&gt;
          &lt;td&gt;View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;零样本 3D 视觉定位&lt;/td&gt;
          &lt;td&gt;提出 VoG 方法，不同于直接把图像喂给模型，该方法将 3D 场景构建为多模态、多层级的场景图。VLM 被设计为主动代理，在图上进行遍历和推理，逐步搜索并定位目标物体，这种结构化方式降低了推理难度，提高了可解释性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;社会导航，强调符合人类社会规范的舒适度&lt;/td&gt;
          &lt;td&gt;提出 RLSLM 混合强化学习框架，将心理学实验推导出的规则基社会运动模型整合到 RL 的奖励函数中，让 RL 智能体在学习导航策略时天生倾向于遵守人类的社交舒适区，实现规则可解释性与 RL 适应性的结合&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;Comfort-Aware Trajectory Optimization for Immersive Human-Robot Interaction&lt;/td&gt;
          &lt;td&gt;已发表于 IEEE Open Journal on Immersive Displays (2025)&lt;/td&gt;
          &lt;td&gt;针对沉浸式环境（如 VR）中的人机交互，优化机器人运动轨迹&lt;/td&gt;
          &lt;td&gt;提出轨迹预测与优化框架，专门针对舒适度和路径合理性进行优化。在 VR 环境中进行用户研究，证明该方法生成的轨迹比传统方法更自然、更让用户感到舒适&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;Where to Fuse in the VLM Era: A Survey on Integrating Knowledge into Object Goal Navigation&lt;/td&gt;
          &lt;td&gt;Workshop Paper，发表于 HEAI Workshop&lt;/td&gt;
          &lt;td&gt;综述&lt;/td&gt;
          &lt;td&gt;探讨在物体目标导航任务中应该在&amp;quot;哪里&amp;quot;融合 VLM/LLM 的知识。借鉴自动驾驶的感知-预测-规划范式，将现有工作分类为在感知层融合、在预测层融合或在规划层融合，并分析各类优缺点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;Layout-Robust LiDAR 3D Object Detection via Multi-Representation Fusion&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;解决跨不同车辆平台的 LiDAR 3D 目标检测问题&lt;/td&gt;
          &lt;td&gt;针对不同车辆上 LiDAR 传感器布局（位置、数量、角度）不同导致模型泛化能力差的问题，提出统一表示框架，包含多视图融合模块（通过点-体素注意力机制学习统一视图不变表示）和运动引导的时空融合模块&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;Enhancing Multi-View Driving VLMs via Pseudo-Label Pretraining and Long-Tail Balancing&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;提升视觉语言模型在自动驾驶场景中的理解能力（感知、预测、规划）&lt;/td&gt;
          &lt;td&gt;基于 InternVL3-8B 模型提出两阶段优化框架：第一阶段利用伪标签预训练，结合思维链推理，将多视角图像按固定序列拼接；第二阶段针对长尾数据进行平衡处理，结合官方数据与合成数据进行混合微调，通过模型集成提升鲁棒性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;13&lt;/td&gt;
          &lt;td&gt;Robust 3D Object Detection under Sensor Placement Variability&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;增强 3D 目标检测模型对传感器安装位置变化的鲁棒性&lt;/td&gt;
          &lt;td&gt;针对不同车型 LiDAR 安装位置差异大导致模型失效的问题，提出三种策略集成：时序增强（聚合连续 LiDAR 扫描帧以丰富几何信息）、混合位置训练（在训练中模拟多种传感器配置）、推理时增强。在 Track 5 基准测试中表现出色&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;14&lt;/td&gt;
          &lt;td&gt;Enhancing VLMs for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;解决 VLM 在自动驾驶中空间推理弱和多任务干扰的问题&lt;/td&gt;
          &lt;td&gt;提出系统解决方案，核心是任务特定的提示。Prompt Routing：根据问题类型（感知、预测、规划等）将问题路由到专门的 expert prompt。空间推理增强：显式定义多视图坐标系和领域约束（如&amp;quot;后视摄像头的物体一定在车后&amp;quot;），帮助 VLM 理解空间关系。在 Track 1 的 Phase-1 和 Phase-2 中均取得很高准确率（70%+）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;15&lt;/td&gt;
          &lt;td&gt;Towards Socially Compliant Navigation: Hybrid Parameter Optimization for Falcon in Dynamic Environments&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;优化社会导航模型 Falcon 的参数，使其更符合社会规范&lt;/td&gt;
          &lt;td&gt;针对 Falcon 模型在平衡&amp;quot;任务效率&amp;quot;和&amp;quot;遵守社会规范&amp;quot;之间的矛盾，提出混合参数优化策略，结合比例约束的参数耦合和网格搜索，解决奖励函数参数过多导致的维度爆炸问题，更有效地找到让机器人既跑得快又懂礼貌的参数组合&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;16&lt;/td&gt;
          &lt;td&gt;HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;解决无人机跨模态导航中的定位匹配问题&lt;/td&gt;
          &lt;td&gt;针对无人机视角（俯视、广角）与文本描述之间的巨大差异，提出 HCCM 框架，核心在于分层和跨粒度。不仅做整体图像匹配，还将图像和文本分解为不同层级（如全局场景 vs. 局部地标），在不同粒度上进行对比学习和匹配，提高在大范围航拍图像中定位具体目标的准确性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;17&lt;/td&gt;
          &lt;td&gt;Unsupervised Domain Adaptation for 3D Object Detection via Adversarial Learning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;解决跨平台 3D 目标检测的域适应问题&lt;/td&gt;
          &lt;td&gt;针对源域和目标域 LiDAR 配置不同导致的数据分布差异，采用无监督域适应方法，核心引入对抗学习。通过训练域判别器区分特征来自哪个平台，同时强制特征提取器欺骗判别器，提取平台无关特征，使模型能泛化到新车型上&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;18&lt;/td&gt;
          &lt;td&gt;Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 获奖方案&lt;/td&gt;
          &lt;td&gt;高效的跨平台 3D 检测&lt;/td&gt;
          &lt;td&gt;基于强力的 PVRCNN++ 基线模型，使用两项关键技术弥补域差异：强数据增强（在数据层面模拟不同传感器噪声和几何变换）和伪标签（使用模型在未标注目标域数据上生成的置信度高的预测结果作为伪标签进行自我训练，逐步适应新环境）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;19&lt;/td&gt;
          &lt;td&gt;Task Aware Prompt Routing and CoT Augmented Fine Tuning for Driving VQA&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;提升自动驾驶 VLM 处理复杂问答（感知、预测、规划）的能力&lt;/td&gt;
          &lt;td&gt;提出任务感知提示路由：不使用通用提示，先判断问题属于哪类任务（如&amp;quot;前方有车吗？&amp;ldquo;属感知，&amp;ldquo;它会左转吗？&amp;ldquo;属预测），然后路由到专门优化的 Prompt 模板。思维链增强微调：在微调过程中加入推理步骤，强迫模型在给出结论前先生成推理过程，显著提升复杂逻辑问题的回答准确率&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;Driving Robustly through Corruptions: Multi-Source LoRA Fine-Tuning of Driving VLMs for Multi-View Reasoning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;增强 VLM 对图像腐蚀/干扰的鲁棒性&lt;/td&gt;
          &lt;td&gt;针对雨雪雾、传感器噪声等恶劣视觉条件，采用 Multi-Source LoRA 微调策略。在训练时故意引入多种类型的图像腐蚀数据作为多源输入，通过轻量级 LoRA 模块让大模型快速适应这些低质量输入，保证在视觉条件退化时仍能安全推理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;21&lt;/td&gt;
          &lt;td&gt;SegSy3D: Segmentation-Guided Self-Training and Model Synergy for Cross-Platform 3D Detection&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;利用语义信息辅助跨平台 3D 检测&lt;/td&gt;
          &lt;td&gt;分割引导：认为仅仅做检测不够，利用点云的语义分割任务作为辅助，帮助模型更好地理解物体形状和背景，从而提升检测器的特征质量。模型协同：涉及多个模型（如分割模型和检测模型）之间的互助学习或集成，以克服单一模型的偏差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;22&lt;/td&gt;
          &lt;td&gt;Towards Generalizable 3D Object Detection Across Sensor Placements&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;解决 LiDAR 安装位置变化带来的检测失效问题&lt;/td&gt;
          &lt;td&gt;重点研究当 LiDAR 安装高度、俯仰角发生变化时点云分布的改变，提出通用检测框架，可能包含几何校正模块，或在特征空间进行视角对齐，确保无论雷达装在车顶还是车头，提取出的车辆特征是一致的&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;23&lt;/td&gt;
          &lt;td&gt;PlaceRecover: A Transformer-based Point Cloud Recovery Network with Implicit Neural Representations for Robust LiDAR Placement Adaptation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;通过重建/恢复点云来解决传感器位置差异问题&lt;/td&gt;
          &lt;td&gt;独特思路：不同于调整检测器，试图直接调整数据。PlaceRecover 是基于 Transformer 的网络，结合隐式神经表示，目标是将不同位置采集的畸变点云恢复/重构为标准视角下的点云，这样后续检测模型不需要修改，直接在恢复后的标准点云上运行即可&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;24&lt;/td&gt;
          &lt;td&gt;A Parameter-Efficient MoE Framework for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 冠军方案&lt;/td&gt;
          &lt;td&gt;高效、高精度的无人机跨模态检索与导航&lt;/td&gt;
          &lt;td&gt;引入混合专家模型架构，MoE 允许模型拥有巨大参数量但推理计算量很小（每次只激活部分专家）。在无人机导航任务中，不同专家可能分别负责处理文本理解、视觉特征提取或地理空间推理。参数高效通常意味着使用 Adapter 或 LoRA 等技术，使模型在有限算力下快速适应新任务&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;25&lt;/td&gt;
          &lt;td&gt;Robust 3D Object Detection via Physical-Aware Augmentation and Class-Specific Model Ensembling&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;通过物理感知增强和集成学习提升检测鲁棒性&lt;/td&gt;
          &lt;td&gt;物理感知增强：传统复制粘贴增强可能把车放在天上或穿墙，该方法设计符合物理规律的增强策略（如贴地、防碰撞），生成更逼真的训练样本。类别特定模型集成：针对不同类别（如车、人、骑行者）训练专门检测器，最后进行集成，利用不同模型在不同类别上的优势，最大化整体分数&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;除了上述文献，RoboSense 2025 Track 2 的两篇冠亚军方案也值得关注。它们的改进思路截然不同：一个改了模型架构，另一个改了训练策略。这种对比很有意思，展现了同一个问题可以从不同角度切入。&lt;/p&gt;
&lt;p&gt;亚军方案来自小米的工作，核心思路是**&amp;ldquo;预知不够，还需要风险评估&amp;rdquo;**。这个洞察很有意思。&lt;/p&gt;
&lt;p&gt;Falcon 虽然能预测人类未来的轨迹，但它对危险的感知是滞后的。Falcon 主要靠碰撞后的惩罚来学习，这导致智能体知道人要去哪，但不知道**&amp;ldquo;离得近有多危险&amp;rdquo;**。就像一个人能预测另一个人会走到哪里，但不知道保持多远的距离才安全。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，研究者在 Falcon 的架构上增加了一个&lt;strong&gt;主动风险感知模块&lt;/strong&gt;。这是一个轻量级的神经网络，利用共享的隐层状态，显式地预测周围每个人类的碰撞风险分数。它把风险分成了三个等级：&lt;strong&gt;Safe、Warning、Danger&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;这个模块的贡献在于将&lt;strong&gt;隐式的避障逻辑变成了显式的风险监督信号&lt;/strong&gt;。智能体在还没撞上之前，就学会了&amp;quot;这种距离是不舒服的&amp;rdquo;，从而更早地进行微调。这种从隐式到显式的转变，让模型的行为更加可解释，也更容易调试。&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;这个模块的贡献在于将隐式的避障逻辑变成了显式的风险监督信号智能体在还没撞上之前就学会了这种距离是不舒服的从而更早地进行微调这种从隐式到显式的转变让模型的行为更加可解释也更容易调试&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%bf%99%e4%b8%aa%e6%a8%a1%e5%9d%97%e7%9a%84%e8%b4%a1%e7%8c%ae%e5%9c%a8%e4%ba%8e%e5%b0%86%e9%9a%90%e5%bc%8f%e7%9a%84%e9%81%bf%e9%9a%9c%e9%80%bb%e8%be%91%e5%8f%98%e6%88%90%e4%ba%86%e6%98%be%e5%bc%8f%e7%9a%84%e9%a3%8e%e9%99%a9%e7%9b%91%e7%9d%a3%e4%bf%a1%e5%8f%b7%e6%99%ba%e8%83%bd%e4%bd%93%e5%9c%a8%e8%bf%98%e6%b2%a1%e6%92%9e%e4%b8%8a%e4%b9%8b%e5%89%8d%e5%b0%b1%e5%ad%a6%e4%bc%9a%e4%ba%86%e8%bf%99%e7%a7%8d%e8%b7%9d%e7%a6%bb%e6%98%af%e4%b8%8d%e8%88%92%e6%9c%8d%e7%9a%84%e4%bb%8e%e8%80%8c%e6%9b%b4%e6%97%a9%e5%9c%b0%e8%bf%9b%e8%a1%8c%e5%be%ae%e8%b0%83%e8%bf%99%e7%a7%8d%e4%bb%8e%e9%9a%90%e5%bc%8f%e5%88%b0%e6%98%be%e5%bc%8f%e7%9a%84%e8%bd%ac%e5%8f%98%e8%ae%a9%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%a1%8c%e4%b8%ba%e6%9b%b4%e5%8a%a0%e5%8f%af%e8%a7%a3%e9%87%8a%e4%b9%9f%e6%9b%b4%e5%ae%b9%e6%98%93%e8%b0%83%e8%af%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;冠军方案来自 Zhang 等人的 PER-Falcon，核心思路是**&amp;ldquo;模型不需要动，是数据利用效率太低&amp;rdquo;**。这个角度很独特，大多数工作都在改模型，但这个方法选择改训练策略。&lt;/p&gt;
&lt;p&gt;在强化学习训练社会导航时，大部分回合都是失败的，要么撞人，要么超时。成功的回合非常稀缺且珍贵，包含了完美的绕行和避让操作。Falcon 在训练时对所有数据一视同仁，导致智能体学了一堆&amp;quot;怎么死&amp;rdquo;，却没学够&amp;quot;怎么活&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;PER-Falcon 引入了&lt;strong&gt;正样本回放机制&lt;/strong&gt;。这是一种数据为中心的方法：把那些回报大于 10 的回合（即成功到达且避障良好）存到一个专门的缓冲区里。每隔一段时间，把这些&amp;quot;满分作业&amp;quot;拿出来让智能体再复习一遍，通过辅助的 PPO 更新来强化这些好的行为。&lt;/p&gt;
&lt;p&gt;这个方法的贡献在于证明了在社会导航中，&lt;strong&gt;强化好的行为比单纯修补坏的行为更有效&lt;/strong&gt;。这其实是一个训练技巧，但效果极好，提升了 7 个百分点。有时候，简单的方法反而最有效，关键是要找到问题的本质。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;特征&lt;/th&gt;
          &lt;th&gt;Rank 2 (Risk Perception)&lt;/th&gt;
          &lt;th&gt;Rank 1 (PER-Falcon)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;改进维度&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Perception / Loss Design (感知/损失函数)&lt;/td&gt;
          &lt;td&gt;Data Efficiency / Optimization (数据效率/优化)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;把&amp;quot;危险&amp;quot;显式量化，作为辅助监督信号&lt;/td&gt;
          &lt;td&gt;把&amp;quot;成功经验&amp;quot;加权，避免被噪音数据淹没&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;新增参数量&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;极小 (两层 MLP)&lt;/td&gt;
          &lt;td&gt;0 (仅改变训练流程)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;性能&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;SR 0.656, H-Coll 0.33&lt;/td&gt;
          &lt;td&gt;SR 0.660, H-Coll 0.32&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;借鉴点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;引入 Dense Signal 辅助 RL 训练&lt;/td&gt;
          &lt;td&gt;在 World Model 训练中进行数据筛选 (Data Curation)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Paper List&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;paper-list&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#paper-list&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于上述分析，可以梳理出两类值得深入阅读的论文。第一类是直接处理社会导航核心问题的论文，第二类是可以作为方法论迁移的&amp;quot;新锤子&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;直接相关的必读论文&lt;/strong&gt;主要解决如何在人群中导航的核心问题。&lt;strong&gt;SocialNav-Map&lt;/strong&gt; 是一个很好的基线或对比对象，它尝试把&amp;quot;预测&amp;quot;显式地画在地图上（Occupancy Map），而 Falcon 是隐式编码在特征里。对比这两者的优劣是很好的讨论点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RLSLM&lt;/strong&gt; 试图将基于规则的社会力模型的可解释性融合进强化学习。这直接关联到 Falcon 缺乏显式社会规范的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Towards Socially Compliant Navigation&lt;/strong&gt; 是针对 Falcon 的超参数调优。虽然技术含量可能不高，但它揭示了奖励函数设计的敏感性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comfort-Aware Trajectory Optimization&lt;/strong&gt; 关注&amp;quot;舒适度&amp;quot;指标。如果目标是 Level 5（社会智能导航），这篇论文定义的指标可能比单纯的成功率更有用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论迁移的选读论文&lt;/strong&gt;虽然不在 SocialNav 领域，但其中的技术（VLM、Visual Grounding）正是需要的&amp;quot;新锤子&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Seeground / Zero-Shot 3D Visual Grounding&lt;/strong&gt; 是极其重要的&amp;quot;新锤子&amp;quot;来源。如果想做语义社会导航，需要这篇论文的方法：如何把 3D 场景转化为 2D VLM 能理解的 Prompt。可以把它的&amp;quot;物体定位&amp;quot;任务替换为&amp;quot;社会规范定位&amp;quot;任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where to Fuse in the VLM Era&lt;/strong&gt; 是一本&amp;quot;操作手册&amp;quot;。当决定引入 VLM 时，这篇综述告诉应该把它放在感知层（用来理解人）、预测层（用来预测意图）还是规划层（用来写代码）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enhancing VLMs for Autonomous Driving&lt;/strong&gt; 虽然是自动驾驶（室外），但它处理思维链（Chain of Thought）和空间推理的 Prompt Engineering 技巧，完全可以迁移到室内 SocialNav。例如：&amp;ldquo;那个人在看手机 -&amp;gt; 所以他不会让路 -&amp;gt; 我应该从左边绕&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;Top 2 的方法依然在&lt;strong&gt;几何空间（距离、坐标）&lt;strong&gt;和&lt;/strong&gt;RL 优化（Loss、Replay）&lt;strong&gt;里打转。它们都没有解决&lt;/strong&gt;语义理解（Semantic Understanding）&lt;strong&gt;和&lt;/strong&gt;显式博弈（Explicit Negotiation）&lt;/strong&gt;。这正是未来研究的机会所在。&lt;/p&gt;
&lt;h3&gt;After&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;after&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#after&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;读完这 7 篇论文后，可以清晰地看到当前社会导航领域的图景。目前正在见证一场**&amp;ldquo;分裂&amp;rdquo;**：一边在优化几何强化学习范式（从深度传感器和 PPO 中榨取性能），另一边则倡导结构化/混合方法（地图、规则和 VLM）来绕过纯强化学习的低效。&lt;/p&gt;
&lt;p&gt;这个文献图谱的中心节点是 &lt;strong&gt;Falcon&lt;/strong&gt;，它通过将范式从反应式避障转向预测式轨迹规划，建立了一个强基线。&lt;/p&gt;
&lt;p&gt;**Falcon（基础）**提出了&amp;quot;未来感知&amp;quot;框架。它不再仅仅对当前人类位置做出反应，而是使用辅助任务显式预测人类轨迹（未来 $H$ 步），并惩罚机器人阻塞这些未来路径。关键指标是在 Social-HM3D 上达到了 55% 的成功率。但它的弱点是依赖&amp;quot;盲目&amp;quot;的端到端强化学习，需要大量训练（约 2400 GPU 小时），并且将人类视为简单的移动障碍物，缺乏语义上下文。&lt;/p&gt;
&lt;p&gt;有三篇论文接受 Falcon 的架构，但认为其训练方法有缺陷。它们旨在解决样本效率和奖励稀疏性问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据效率（&amp;ldquo;好学生&amp;quot;方法）&lt;/strong&gt;：PER-Falcon（Rank 1）发现 Falcon 通过平等对待所有训练回合而浪费数据。它引入了正样本回放（PER）机制，缓存&amp;quot;高价值&amp;quot;回合（成功导航）并定期回放给策略网络。结果是在成功率上比基线提升了约 12%，证明了课程/数据质量比模型大小更重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;信号密度（&amp;ldquo;直觉&amp;quot;方法）&lt;/strong&gt;：主动风险感知（Rank 2）认为 Falcon 的碰撞惩罚太&amp;quot;稀疏&amp;rdquo;（只有在撞到人时才受到惩罚）。它添加了一个模块来基于距离预测连续的风险分数，让机器人在碰撞发生前很久就能&amp;quot;感知危险&amp;rdquo;。结果获得了第 2 名，证明了密集监督有助于强化学习收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;超参数调优（&amp;ldquo;暴力&amp;quot;方法）&lt;/strong&gt;：混合参数优化认为 Falcon 的奖励权重（平衡效率与社会合规性）不是最优的。它使用网格搜索和耦合调优来找到参数的&amp;quot;帕累托前沿&amp;rdquo;。结果仅通过调参就实现了 15% 的成功率提升，凸显了强化学习基线的脆弱性。&lt;/p&gt;
&lt;p&gt;有两篇论文挑战端到端强化学习的主导地位，认为结构和心理学是比试错更好的老师。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;零样本&amp;quot;反叛&lt;/strong&gt;：SocialNav-Map 是一个关键的挑战者。它认为强化学习&amp;quot;不透明&amp;quot;且&amp;quot;难以泛化&amp;rdquo;。它实时构建动态占据地图，预测人类轨迹（使用历史+朝向）并将其&amp;quot;绘制&amp;quot;到地图上作为临时障碍物，然后使用经典路径规划器（快速行进方法）。洞察是它在没有训练的情况下击败了基于强化学习的 Falcon（后者需要 2396 GPU 小时训练），这表明显式世界建模可能优于隐式强化学习记忆。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;以人为中心&amp;quot;的混合&lt;/strong&gt;：RLSLM 认为机器人不应该只是&amp;quot;猜测&amp;quot;社会规范。它将从心理学实验推导出的社会运动模型（SLM）直接整合到奖励函数中，创建了一个&amp;quot;非对称舒适场&amp;rdquo;（人类讨厌从前方接近，而不是从后方）。验证是通过 VR 让人类评价机器人，证明它比标准基于规则的方法&amp;quot;更有礼貌&amp;quot;。&lt;/p&gt;
&lt;p&gt;最后两篇论文虽然不严格属于&amp;quot;社会导航&amp;quot;论文，但它们提供了解决社会导航中&amp;quot;语义鸿沟&amp;quot;的&amp;quot;新锤子&amp;quot;（技术）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SeeGround&lt;/strong&gt; 解决了 3D 视觉定位（通过文本找到物体）而无需 3D 训练数据。技术是使用&amp;quot;视角适应模块&amp;quot;（模拟相机看向物体）从 3D 场景渲染 2D 图像，并将其输入到 2D VLM。相关性在于它证明了如果正确格式化数据（渲染图像+空间提示），可以使用冻结的 2D VLM 来理解 3D 空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where to Fuse&lt;/strong&gt; 是一篇综述，分类了如何将 VLM/LLM 知识注入导航。框架将融合分为感知（识别物体）、预测（猜测关系）和规划（边界选择）。相关性在于它明确将&amp;quot;社会交互导航&amp;quot;称为未来，指出当前方法&amp;quot;将人类简化为移动障碍物&amp;quot;。&lt;/p&gt;
&lt;p&gt;基于这些论文，可以绘制出研究路线图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Novelty Tree（技术演进）&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Level 1：几何反应（过去）&lt;/strong&gt; → ORCA、Social Force&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 2：几何预测（Falcon 时代）&lt;/strong&gt; → Falcon、PER-Falcon（预测轨迹）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 3：结构化混合（当前 SOTA）&lt;/strong&gt; → SocialNav-Map（显式映射动态风险）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 4：语义社会智能（空白）&lt;/strong&gt; → 理解上下文（例如：&amp;ldquo;那两个人正在交谈，不要从中间走过&amp;rdquo;，或&amp;quot;那个人在赶路，让路&amp;quot;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge-Insight Tree（工具箱）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;社会导航中的挑战&lt;/th&gt;
          &lt;th&gt;当前解决方案（&amp;ldquo;旧&amp;quot;方法）&lt;/th&gt;
          &lt;th&gt;提出的洞察（使用文献）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;数据效率&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;训练 1000 万步（Falcon）&lt;/td&gt;
          &lt;td&gt;回放缓冲区：优先&amp;quot;正样本回合&amp;rdquo;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;奖励稀疏性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;稀疏碰撞惩罚&lt;/td&gt;
          &lt;td&gt;密集风险：预测连续&amp;quot;风险分数&amp;quot;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;泛化性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;在新地图上微调&lt;/td&gt;
          &lt;td&gt;零样本映射：使用动态占据地图&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;社会规范&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;希望强化学习&amp;quot;学会&amp;quot;它们&lt;/td&gt;
          &lt;td&gt;显式建模：注入心理学规则（SLM）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;语义盲区&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;仅深度输入（看不到&amp;quot;活动&amp;quot;）&lt;/td&gt;
          &lt;td&gt;VLM 注入：使用 SeeGround 的&amp;quot;视角渲染&amp;quot;来分类社会情境&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;文献清楚地表明，&lt;strong&gt;几何问题已经解决&lt;/strong&gt;（SocialNav-Map 证明了可以零样本完成）。下一个前沿是&lt;strong&gt;语义&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;机会：&amp;ldquo;语义社会地图&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;空白：Falcon 和 SocialNav-Map 将人类视为动态圆柱体。RLSLM 将它们视为磁场。它们都不知道人类在做什么。&lt;/p&gt;
&lt;p&gt;新锤子：SeeGround 证明了可以使用 VLM 来&amp;quot;看&amp;quot;特定的 3D 坐标并理解它。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;提出的想法：&amp;ldquo;VLM 驱动的社会可供性地图&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;感知&lt;/strong&gt;：使用 VLM（如 SeeGround）分析来自 RGB 相机的人类裁剪图像。分类它们的状态：&amp;ldquo;交互中&amp;rdquo;、&amp;ldquo;等待中&amp;rdquo;、&amp;ldquo;看手机&amp;rdquo;、&amp;ldquo;匆忙&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;映射&lt;/strong&gt;：不仅仅是&amp;quot;占据地图&amp;quot;（如 SocialNav-Map），而是构建&amp;quot;社会规范地图&amp;quot;。
&lt;ul&gt;
&lt;li&gt;示例：如果两个人正在&amp;quot;交互&amp;quot;，在它们之间创建&amp;quot;禁止通行区&amp;quot;。&lt;/li&gt;
&lt;li&gt;示例：如果一个人正在&amp;quot;看手机&amp;quot;，扩大其风险半径（他们分心了）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规划&lt;/strong&gt;：在这个新的语义地图上使用 SocialNav-Map 规划器（FMM）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这结合了 SocialNav-Map 的结构和 SeeGround 的语义能力，解决了 Where to Fuse 中识别的局限性。&lt;/p&gt;
&lt;p&gt;Google Search 验证建议：你可以搜一下 &amp;ldquo;Foundation model for social navigation&amp;rdquo; 或 &amp;ldquo;Language-guided social navigation&amp;rdquo;，看看Level 4/5目前是否已经有人在用VLM/DepthAnything的Feature做SocialNav了。如果没有，这就是Blue Ocean。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>VLN 综述以及后续文献</title>
      <link>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-22-vln-survey/</guid>
      <description>
        
        
        &lt;h1&gt;VLN 系列&lt;/h1&gt;&lt;p&gt;从 Poing Navigation 到 Object Navigation，这也太难了，找Idea真的太难了。&lt;/p&gt;
&lt;p&gt;然后秋冬学期的一半，也就是大四上的一半已经过去了，马上就要寒假了，寒假做什么，实习还是论文？真能憋出论文吗？&lt;/p&gt;
&lt;h2&gt;Survey&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;survey&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#survey&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;研发能够与人类及其周边环境进行交互的&lt;strong&gt;具身智能体&lt;/strong&gt;（embodied agents），是&lt;strong&gt;人工智能&lt;/strong&gt;（AI）领域长期以来的核心目标之一。这类 AI 系统在现实世界中具有巨大的应用潜力，可作为日常生活中的多功能助手，例如&lt;strong&gt;家用机器人&lt;/strong&gt;、&lt;strong&gt;自动驾驶汽车&lt;/strong&gt;以及&lt;strong&gt;个人助手&lt;/strong&gt;。推动这一研究方向的一个正式问题设定是&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（Vision-and-Language Navigation, &lt;strong&gt;VLN&lt;/strong&gt;）—— 这是一项多模态协作任务，要求智能体遵循人类指令、探索&lt;strong&gt;三维&lt;/strong&gt;（3D）环境，并在存在各类歧义的场景下开展情境化通信。多年来，研究者已在&lt;strong&gt;照片级真实感模拟器&lt;/strong&gt;和&lt;strong&gt;真实环境&lt;/strong&gt;中对 VLN 展开探索，由此形成了一系列&lt;strong&gt;基准数据集&lt;/strong&gt;，每个数据集的问题表述略有不同。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/law-Challenge.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类&lt;/strong&gt;（Human）：给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;rdquo;；在智能体询问 &amp;ldquo;左边的房间还是前面的房间？&amp;rdquo; 时回复 &amp;ldquo;左边&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;物理环境&lt;/strong&gt;（Physical Environment）：智能体感知的视觉场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体&lt;/strong&gt;（VLN Agent）：接收指令后进行 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;（Grounding &amp;amp; Reasoning）、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;（Planning）、&lt;strong&gt;&amp;ldquo;对话&amp;rdquo;&lt;/strong&gt;（Dialogue），执行 &lt;strong&gt;&amp;ldquo;导航动作&amp;rdquo;&lt;/strong&gt;（Navigation Execution），并生成&lt;strong&gt;语言响应&lt;/strong&gt;（Language Response）；过程中可能产生疑问，如 &amp;ldquo;…… 进入房间。左边？右边？&amp;ldquo;&amp;ldquo;左边的房间还是前面的房间？&amp;quot;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心模块&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;（World Model）、&lt;strong&gt;人类模型&lt;/strong&gt;（Human Model），分别支撑智能体的环境理解与人类意图解读。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其&lt;a href=&#34;https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;仓库&lt;/a&gt;提到了一些工作内容，但是不全。&lt;/p&gt;
&lt;h3&gt;背景与任务基础&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;背景与任务基础&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%83%8c%e6%99%af%e4%b8%8e%e4%bb%bb%e5%8a%a1%e5%9f%ba%e7%a1%80&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;人类及其他具备导航能力的动物，很早就展现出对环境导航的理解与策略。例如，加利斯特尔（Gallistel）提出了两种基础机制：其一为&lt;strong&gt;引导法&lt;/strong&gt;（piloting），即利用环境地标计算距离与角度；其二为&lt;strong&gt;路径积分&lt;/strong&gt;（path integration），即通过自运动感知计算位移与方向变化。理解空间导航的核心是&lt;strong&gt;认知地图假说&lt;/strong&gt;（cognitive map hypothesis）—— 该假说认为，大脑会形成统一的空间表征，以支持记忆存储并指导导航行为。例如，托尔曼（Tolman）观察到，当熟悉的路径被阻断且地标消失时，大鼠仍能选择正确的新路径。神经科学家还发现了&lt;strong&gt;海马体位置细胞&lt;/strong&gt;（hippocampal place cells），这表明存在一种以&lt;strong&gt;异中心视角&lt;/strong&gt;（allocentrically）编码地标与目标的空间坐标系。&lt;/p&gt;
&lt;p&gt;传统上，&lt;strong&gt;&amp;ldquo;遵循自然语言导航指令&amp;rdquo;&lt;strong&gt;的任务多采用地图等&lt;/strong&gt;符号化世界表征&lt;/strong&gt;（symbolic world representations）进行建模。然而，本综述聚焦于采用视觉环境的模型，重点探讨&lt;strong&gt;多模态理解与接地&lt;/strong&gt;（grounding）的相关挑战。与此相对，关于&lt;strong&gt;视觉导航&lt;/strong&gt;和&lt;strong&gt;移动机器人导航&lt;/strong&gt;的综述文献已十分丰富，这类综述主要关注视觉感知与物理具身性，但若涉及 &lt;strong&gt;&amp;ldquo;语言在导航任务中的作用&amp;rdquo;&lt;/strong&gt;，则讨论较为简略，建议读者参考这些文献以获取相关背景。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;接地（Grounding）指将抽象的语言符号与具体的物理世界或感知数据建立对应关系的过程。在 VLN 中，接地是将自然语言指令映射到视觉场景中的具体位置、物体或动作。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;尽管在讨论 VLN 时，我们难免会将范围拓展到导航之外的领域（如移动操作、对话），但本综述的核心焦点仍是&lt;strong&gt;导航任务&lt;/strong&gt;，并将针对该任务提供详细的文献梳理。此外，以往的 VLN 综述多采用 &lt;strong&gt;&amp;ldquo;自下而上&amp;rdquo;&lt;/strong&gt; 的总结方式，聚焦于基准数据集与建模创新；而本综述则采用 &lt;strong&gt;&amp;ldquo;自上而下&amp;rdquo;&lt;/strong&gt; 的视角，并以&lt;strong&gt;基础模型&lt;/strong&gt;的角色为核心，将现有研究成果从 &lt;strong&gt;&amp;ldquo;世界模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;VLN 智能体&amp;rdquo;&lt;/strong&gt; 三个维度，归类为&lt;strong&gt;三大核心挑战&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;典型的 &lt;strong&gt;VLN 智能体&lt;/strong&gt;会在指定位置接收人类指令者给出的（一系列）&lt;strong&gt;语言指令&lt;/strong&gt;。该智能体以&lt;strong&gt;自我为中心的视觉视角&lt;/strong&gt;（egocentric visual perspective）在环境中导航，其核心任务是遵循指令生成&lt;strong&gt;轨迹&lt;/strong&gt; —— 轨迹可基于一系列离散视角，也可基于低层级动作与控制指令（例如 &amp;ldquo;前进 0.25 米&amp;rdquo;），最终抵达&lt;strong&gt;目标终点&lt;/strong&gt;。若智能体最终位置与目标终点的距离在指定范围内（例如 3 米），则判定为&lt;strong&gt;导航成功&lt;/strong&gt;。此外，智能体在导航过程中可与指令者交互：既可以请求帮助，也可进行自由形式的语言沟通。近年来，研究者对 VLN 智能体的期望进一步提升，要求其在导航的同时整合附加任务，例如&lt;strong&gt;操作任务&lt;/strong&gt;与&lt;strong&gt;目标检测任务&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-benchmark-2024.png&#34; alt=&#34;vln-benchmark-2024&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如上表，现有（2024）VLN &lt;strong&gt;基准数据集&lt;/strong&gt;可分为以下四类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航发生的 &amp;ldquo;世界&amp;rdquo;&lt;/strong&gt;：包括领域（室内或室外）与具体环境（如模拟器或真实场景）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类交互类型&lt;/strong&gt;：包括交互轮次（单轮或多轮）、通信格式（自由对话、受限对话或多指令）、语言粒度（动作导向或目标导向）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VLN 智能体属性&lt;/strong&gt;：包括智能体类型（如家用机器人、自动驾驶车辆、自主无人机）、动作空间（图基、离散或连续）、附加任务（操作与目标检测）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集收集方式&lt;/strong&gt;：包括文本收集（人类生成或模板生成）与路线演示（人类执行或规划器生成）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;研究者主要采用三类指标评估 VLN 智能体的&lt;strong&gt;导航寻路性能&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;导航误差&lt;/strong&gt;（Navigation Error, &lt;strong&gt;NE&lt;/strong&gt;）：智能体最终位置与目标终点之间最短路径距离的平均值&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成功率&lt;/strong&gt;（Success Rate, &lt;strong&gt;SR&lt;/strong&gt;）：最终位置足够接近目标终点的任务占比&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;路径长度加权成功率&lt;/strong&gt;（Success Rate Weighted Path Length, &lt;strong&gt;SPL&lt;/strong&gt;）：通过轨迹长度对成功率进行归一化，平衡 &lt;strong&gt;&amp;ldquo;抵达正确终点的成功率&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;路径效率&amp;rdquo;&lt;/strong&gt; 两大指标&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，还有一类指标用于衡量 &lt;strong&gt;&amp;ldquo;指令遵循的忠实度&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;预测轨迹和真实轨迹的一致性&amp;rdquo;&lt;/strong&gt;，例如：
4. &lt;strong&gt;长度加权覆盖得分&lt;/strong&gt;（Coverage Weighted by Length Score, &lt;strong&gt;CLS&lt;/strong&gt;）：衡量智能体轨迹与参考路径的贴合程度，通过 &lt;strong&gt;&amp;ldquo;参考路径覆盖范围&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;轨迹长度效率&amp;rdquo;&lt;/strong&gt; 两个维度平衡智能体性能
5. &lt;strong&gt;归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping, &lt;strong&gt;nDTW&lt;/strong&gt;）：对偏离真实轨迹的行为进行惩罚
6. &lt;strong&gt;成功率加权归一化动态时间规整&lt;/strong&gt;（Normalized Dynamic Time Warping Weighted by Success Rate, &lt;strong&gt;sDTW&lt;/strong&gt;）：在惩罚轨迹偏离的同时，还会结合导航成功率综合评估&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/vln-challenges-and-soluions.png&#34; alt=&#34;vln-challenges-and-soluions&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;该图反映的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心模块关联&lt;/strong&gt;：&lt;strong&gt;世界模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;历史与记忆&amp;rdquo;&lt;/strong&gt;，&lt;strong&gt;人类模型&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;模糊指令&amp;rdquo;&lt;/strong&gt;，两者均涉及 &lt;strong&gt;&amp;ldquo;泛化能力&amp;rdquo;&lt;/strong&gt;；&lt;strong&gt;VLN 智能体&lt;/strong&gt;中讨论 &lt;strong&gt;&amp;ldquo;接地与推理&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;规划&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;基础模型适配为智能体&amp;rdquo;&lt;/strong&gt; 三大方法&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基础模型角色&lt;/strong&gt;：根据基础模型承担的功能，将方法分为四类 —— &lt;strong&gt;数据与知识处理&lt;/strong&gt;（预处理 / 增强 / 合成数据、利用预训练常识）、&lt;strong&gt;表征学习&lt;/strong&gt;（通用文本 / 视觉表征、历史记忆处理）、&lt;strong&gt;决策制定&lt;/strong&gt;（导航规划器、信息寻求对话管理器、通用决策智能体）、&lt;strong&gt;任务学习&lt;/strong&gt;（具身推理、语言接地、少样本 / 上下文 / 微调学习具身任务）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;交互示例&lt;/strong&gt;：人类给出指令 &amp;ldquo;穿过客厅区域进入走廊。右转，然后再右转并进入房间&amp;quot;&amp;ldquo;去卫生间&amp;rdquo;；智能体通过提问（&amp;ldquo;走廊在哪里？&amp;ldquo;&amp;ldquo;哪个房间？&amp;quot;）寻求信息，人类回复（&amp;ldquo;左边的房间还是前面的房间？&amp;ldquo;&amp;ldquo;左边&amp;rdquo;）后，智能体执行动作（&amp;ldquo;前进&amp;quot;&amp;ldquo;左转&amp;rdquo;）并生成轨迹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战与未来方向&lt;/strong&gt;：&lt;strong&gt;基准数据集&lt;/strong&gt;（数据与任务局限）、&lt;strong&gt;世界模型&lt;/strong&gt;（从 2D 世界到 3D 世界）、&lt;strong&gt;人类模型&lt;/strong&gt;（从指令到对话）、&lt;strong&gt;智能体模型&lt;/strong&gt;（LLM 与 VLM 适配）、&lt;strong&gt;部署&lt;/strong&gt;（从仿真到真实机器人）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;三大解决方案&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三大解决方案&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e5%a4%a7%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;World Model: Learning and Representing the Visual Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-learning-and-representing-the-visual-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-learning-and-representing-the-visual-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;能够帮助 VLN 智能体理解周边环境、预测自身动作对世界状态的改变，并使自身感知与动作与语言指令对齐。现有研究中，学习世界模型主要面临两大挑战：一是将当前任务段内的&lt;strong&gt;视觉观测历史&lt;/strong&gt;编码为&lt;strong&gt;记忆&lt;/strong&gt;，二是实现对未见过环境的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;History and Memory&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;history-and-memory&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#history-and-memory&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;与&lt;strong&gt;视觉问答&lt;/strong&gt;（Visual Question Answering, &lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;视觉蕴含&lt;/strong&gt;（Visual Entailment）等其他视觉-语言任务不同，VLN 智能体需将过去动作与观测的&lt;strong&gt;历史信息&lt;/strong&gt;融入当前步骤的输入中以决策动作，而非仅依赖单一步骤的图像与文本。在 VLN 中应用基础模型之前，研究者通常采用 &lt;strong&gt;LSTM 隐藏状态&lt;/strong&gt;作为支持智能体导航决策的隐式记忆，并进一步设计不同的&lt;strong&gt;注意力机制&lt;/strong&gt;或&lt;strong&gt;辅助任务&lt;/strong&gt;，以提升编码历史与指令的对齐程度。&lt;/p&gt;
&lt;p&gt;目前已有多种基于基础模型的&lt;strong&gt;导航历史编码技术&lt;/strong&gt;，核心可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）基于令牌更新或序列建模的编码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer 初始化&lt;/strong&gt;：以基于域内指令-轨迹数据预训练的模型（如 &lt;strong&gt;Prevalent&lt;/strong&gt;）为基础，构建&lt;strong&gt;多模态 Transformer&lt;/strong&gt;，将编码后的指令与导航历史作为输入以实现决策。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;循环状态令牌编码&lt;/strong&gt;：部分方法通过循环更新的&lt;strong&gt;状态令牌&lt;/strong&gt;编码导航历史。例如，利用上一步的单个 &lt;strong&gt;[CLS] 令牌&lt;/strong&gt;编码历史信息；或设计&lt;strong&gt;变长记忆框架&lt;/strong&gt;，将过去步骤的多个动作激活值存储在记忆库中，作为历史编码。但这类方法需逐步骤更新令牌，难以高效检索导航轨迹中任意步骤的历史编码，限制了预训练的可扩展性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;全景与历史分层编码&lt;/strong&gt;：另一类方法直接通过多模态 Transformer 将导航历史编码为序列。例如，对轨迹中每一步的单视角图像进行编码；或进一步提出 &lt;strong&gt;&amp;ldquo;全景编码器 + 历史编码器&amp;rdquo;&lt;/strong&gt; 的分层设计 —— &lt;strong&gt;全景编码器&lt;/strong&gt;处理每一时间步的全景视觉观测，&lt;strong&gt;历史编码器&lt;/strong&gt;则编码所有过往观测。这种设计可分离全景视图中的空间关系与导航历史中跨全景的时间动态性，且无需依赖循环更新的状态令牌，便于基于指令-路径对进行高效、大规模的预训练。后续研究分别用 &lt;strong&gt;&amp;ldquo;图像均值池化&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;前视图像编码&amp;rdquo;&lt;/strong&gt; 替代全景编码器，均保持了良好的导航性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）基于 LLM 的文本化历史编码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随着基于 &lt;strong&gt;LLM&lt;/strong&gt; 的导航智能体兴起，&lt;strong&gt;&amp;ldquo;将视觉环境转换为文本描述&amp;rdquo;&lt;/strong&gt; 成为主流趋势。此时导航历史被编码为 &lt;strong&gt;&amp;ldquo;图像描述序列 + 相对空间信息&amp;rdquo;&lt;/strong&gt;（如朝向、高度、距离）的组合。例如，&lt;strong&gt;HELPER&lt;/strong&gt; 设计了 &lt;strong&gt;&amp;ldquo;语言-程序对&amp;rdquo;&lt;/strong&gt; 的外部记忆，通过检索增强的 LLM 提示，将人类与机器人的自由形式对话解析为动作程序。&lt;/p&gt;
&lt;p&gt;另一类研究通过融入&lt;strong&gt;图信息&lt;/strong&gt;增强导航历史建模，核心思路是利用&lt;strong&gt;结构化图表征&lt;/strong&gt;环境几何与空间关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拓扑图与结构化编码&lt;/strong&gt;：部分方法采用&lt;strong&gt;结构化 Transformer 编码器&lt;/strong&gt;捕捉环境中的几何线索。除编码中使用的&lt;strong&gt;拓扑图&lt;/strong&gt;外，许多研究还将&lt;strong&gt;俯视图信息&lt;/strong&gt;（如&lt;strong&gt;网格图&lt;/strong&gt;、&lt;strong&gt;语义图&lt;/strong&gt;、&lt;strong&gt;局部度量图&lt;/strong&gt;）与&lt;strong&gt;局部邻域图&lt;/strong&gt;纳入导航过程中的观测历史建模。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM 与图的结合&lt;/strong&gt;：近期基于 LLM 的导航智能体在记忆构建中引入了创新性的图应用。例如，提出一种基于&lt;strong&gt;地图引导的 GPT 智能体&lt;/strong&gt;，利用语言化形式的地图存储和管理拓扑图信息；&lt;strong&gt;MC-GPT&lt;/strong&gt; 则将拓扑图作为记忆结构，记录视角、物体及其空间关系的信息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;跨环境泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;跨环境泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b7%a8%e7%8e%af%e5%a2%83%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;VLN 的核心挑战之一是：如何从有限的可用环境中学习，并泛化到新的、未见过的环境中。现有研究表明，以下方法可提升智能体对未见过环境的泛化性能：&lt;strong&gt;学习语义分割特征&lt;/strong&gt;、&lt;strong&gt;利用训练过程中环境的 dropout 信息&lt;/strong&gt;、&lt;strong&gt;最大化不同环境中语义对齐图像对的相似度&lt;/strong&gt;。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;方法&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.1 预训练视觉表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;传统视觉编码器&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究采用在 ImageNet 上预训练的 &lt;strong&gt;ResNet&lt;/strong&gt; 提取视觉表征&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基于 VL 基础模型的表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;用 &lt;strong&gt;CLIP 视觉编码器&lt;/strong&gt;替代 ResNet——CLIP 通过图文对的对比损失预训练，可自然实现图像与指令的更好对齐，显著提升 VLN 性能&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;视频预训练表征&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;探索将从视频数据中学习的视觉表征迁移到 VLN 任务中，证实视频中的&lt;strong&gt;时间信息&lt;/strong&gt;对导航至关重要&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.2 环境增强&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;静态环境修改&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;EnvEdit&lt;/strong&gt;、&lt;strong&gt;EnvMix&lt;/strong&gt;、&lt;strong&gt;KED&lt;/strong&gt; 与 &lt;strong&gt;FDA&lt;/strong&gt; 通过修改 Matterport3D 中的现有环境生成合成数据，具体手段包括混合不同环境的房间、改变环境外观与风格、对环境高频特征进行插值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;动态环境合成&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;Pathdreamer&lt;/strong&gt; 与 &lt;strong&gt;SE3DS&lt;/strong&gt; 进一步实现 &lt;strong&gt;&amp;ldquo;基于当前观测合成未来步骤环境&amp;rdquo;&lt;/strong&gt;，并探索将合成视图作为 VLN 训练的增强数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3.2.3 学习范式的转变&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;前基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多数研究直接用自动收集的新环境增强训练环境，并微调基于 LSTM 的 VLN 智能体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;基础模型时代&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;预训练&lt;/strong&gt;被证实对基础模型至关重要，因此 &lt;strong&gt;&amp;ldquo;在预训练阶段从收集的环境中学习&amp;rdquo;&lt;/strong&gt; 成为 VLN 的标准做法。基于增强域内数据的&lt;strong&gt;大规模预训练&lt;/strong&gt;，已成为缩小智能体与人类性能差距的关键；且域内预训练的多模态 Transformer，被证实比从 VLMs（如 &lt;strong&gt;Oscar&lt;/strong&gt;、&lt;strong&gt;LXMERT&lt;/strong&gt;）初始化的多模态 Transformer 更有效&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Human Model: Interpreting and Communicating with Humans&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;human-model-interpreting-and-communicating-with-humans&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#human-model-interpreting-and-communicating-with-humans&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;除学习和建模世界外，VLN 智能体还需一个 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; —— 该模型能根据具体场景理解人类提供的自然语言指令，从而完成导航任务。这一过程主要面临两大挑战：一是解决&lt;strong&gt;指令的模糊性&lt;/strong&gt;，二是实现 &lt;strong&gt;&amp;ldquo;接地指令&amp;rdquo;&lt;/strong&gt; 在不同视觉环境中的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
&lt;h5&gt;模糊指令&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;模糊指令&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a8%a1%e7%b3%8a%e6%8c%87%e4%bb%a4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;模糊指令&lt;/strong&gt;主要出现在&lt;strong&gt;单轮导航场景&lt;/strong&gt;中：智能体仅遵循初始指令执行任务，无法通过进一步人类交互获取澄清。这类指令缺乏灵活性，难以训练智能体根据动态环境调整自身的&lt;strong&gt;语言理解&lt;/strong&gt;与&lt;strong&gt;视觉感知能力&lt;/strong&gt;。例如，指令中可能包含 &lt;strong&gt;&amp;ldquo;当前视角不可见的地标&amp;rdquo;&lt;/strong&gt;，或 &lt;strong&gt;&amp;ldquo;从多个视角观察均难以区分的地标&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在基础模型应用于 VLN 之前，模糊指令问题几乎未得到有效解决。尽管 &lt;strong&gt;LEO 模型&lt;/strong&gt;尝试通过整合 &lt;strong&gt;&amp;ldquo;从不同视角描述同一轨迹的多条指令&amp;rdquo;&lt;/strong&gt; 来缓解该问题，但仍依赖人工标注的指令。而基础模型所具备的 &lt;strong&gt;&amp;ldquo;全面感知上下文&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;常识知识&amp;rdquo;&lt;/strong&gt;，使智能体既能利用外部知识解读模糊指令，也能向其他 &lt;strong&gt;&amp;ldquo;人类模型&amp;rdquo;&lt;/strong&gt; 寻求协助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CLIP&lt;/strong&gt; 等大规模跨模态预训练模型具备&lt;strong&gt;视觉语义与文本的匹配能力&lt;/strong&gt;，这使得 VLN 智能体可利用 &lt;strong&gt;&amp;ldquo;当前感知到的视觉物体及其状态&amp;rdquo;&lt;/strong&gt; 来解决指令模糊性问题，在单轮导航场景中尤为有效。具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Trans 模型&lt;/strong&gt;：通过 CLIP 提取 &lt;strong&gt;&amp;ldquo;可见且具有辨识度的物体&amp;rdquo;&lt;/strong&gt;，构建易于遵循的子指令；并预训练一个 &lt;strong&gt;&amp;ldquo;转换器&amp;rdquo;&lt;/strong&gt;（Translator），将原始模糊指令转换为易于理解的子指令表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LANA+ 模型&lt;/strong&gt;：利用 CLIP，以视觉全景观测为输入，查询 &lt;strong&gt;&amp;ldquo;地标语义标签文本列表&amp;rdquo;&lt;/strong&gt;，并选取排名靠前的检索文本线索作为 &lt;strong&gt;&amp;ldquo;待跟随显著地标的表征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;KERM 模型&lt;/strong&gt;：提出一种 &lt;strong&gt;&amp;ldquo;知识增强推理模型&amp;rdquo;&lt;/strong&gt;，可检索 &lt;strong&gt;&amp;ldquo;以语言描述形式存储的导航视角相关知识事实&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavHint 方法&lt;/strong&gt;：构建一个提示数据集，提供详细的视觉描述，帮助 VLN 智能体全面理解视觉环境，而非仅聚焦于指令中提及的物体。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另一方面，&lt;strong&gt;LLM&lt;/strong&gt; 的&lt;strong&gt;常识推理能力&lt;/strong&gt;可用于 &lt;strong&gt;&amp;ldquo;澄清或修正指令中的模糊地标&amp;rdquo;&lt;/strong&gt;，并将指令拆解为可执行步骤。例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 LLM 提供 &lt;strong&gt;&amp;ldquo;开放世界中地标共现的常识&amp;rdquo;&lt;/strong&gt;，并结合 CLIP 实现地标探测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayCan 方法&lt;/strong&gt;：将指令拆解为 &lt;strong&gt;&amp;ldquo;预定义可执行动作的排序列表&amp;rdquo;&lt;/strong&gt;，并结合一个 &lt;strong&gt;&amp;ldquo;效用函数&amp;rdquo;&lt;/strong&gt; —— 该函数对当前场景中出现的物体赋予更高权重。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管可通过&lt;strong&gt;视觉感知&lt;/strong&gt;与&lt;strong&gt;场景上下文&lt;/strong&gt;解决模糊指令问题，但更直接的方法是向 &lt;strong&gt;&amp;ldquo;通信伙伴&amp;rdquo;&lt;/strong&gt;（即生成指令的人类）寻求帮助。这类研究主要面临三大核心挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断 &lt;strong&gt;&amp;ldquo;何时请求帮助&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;生成 &lt;strong&gt;&amp;ldquo;信息寻求问题&amp;rdquo;&lt;/strong&gt;（如询问下一步动作、物体位置、方向等）&lt;/li&gt;
&lt;li&gt;设计 &lt;strong&gt;&amp;ldquo;信息提供方&amp;rdquo;&lt;/strong&gt;（oracle）—— 可为真实人类、规则与模板或神经模型&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 与 &lt;strong&gt;VLM&lt;/strong&gt; 在该框架中可承担两种角色：一是 &lt;strong&gt;&amp;ldquo;信息寻求模型&amp;rdquo;&lt;/strong&gt;，二是 &lt;strong&gt;&amp;ldquo;人类助手的代理&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;信息提供模型&amp;rdquo;&lt;/strong&gt;。已有初步研究探索将 LLM 用作信息寻求模型，解决 &lt;strong&gt;&amp;ldquo;何时问&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;问什么&amp;rdquo;&lt;/strong&gt; 的问题 —— 这需借助 &lt;strong&gt;&amp;ldquo;保形预测&amp;rdquo;&lt;/strong&gt;（conformal prediction, &lt;strong&gt;CP&lt;/strong&gt;）或 &lt;strong&gt;&amp;ldquo;上下文学习&amp;rdquo;&lt;/strong&gt;（in-context learning, &lt;strong&gt;ICL&lt;/strong&gt;）等技术实现。&lt;/p&gt;
&lt;p&gt;对于 &lt;strong&gt;&amp;ldquo;信息提供&amp;rdquo;&lt;/strong&gt; 角色，基础模型需扮演 &lt;strong&gt;&amp;ldquo;掌握信息提供方专属信息的助手&amp;rdquo;&lt;/strong&gt; —— 例如知晓目标位置、环境地图等任务执行者无法获取的信息。近期相关研究包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-Copilot 方法&lt;/strong&gt;：使智能体在遇到困惑时主动寻求协助，其中 LLM 扮演 &lt;strong&gt;&amp;ldquo;副驾驶&amp;rdquo;&lt;/strong&gt; 角色，为导航提供支持。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;GPT-3&lt;/strong&gt; 可逐步拆解训练数据中的真实响应，这有助于利用预训练的 &lt;strong&gt;SwinBert&lt;/strong&gt; 视频-语言模型训练信息提供方模型；同时，&lt;strong&gt;mPLUG-Owl&lt;/strong&gt; 等大型视觉-语言模型可作为 &lt;strong&gt;&amp;ldquo;现成的强零样本信息提供方&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自驱动通信智能体&lt;/strong&gt;：通过学习 &lt;strong&gt;&amp;ldquo;信息提供方给出肯定答案的置信度&amp;rdquo;&lt;/strong&gt; 实现，可采用 &lt;strong&gt;&amp;ldquo;自我问答&amp;rdquo;&lt;/strong&gt; 模式，在推理阶段无需依赖信息提供方。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;接地指令的泛化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地指令的泛化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e6%8c%87%e4%bb%a4%e7%9a%84%e6%b3%9b%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;导航数据在规模与多样性上的局限，是影响 VLN 智能体 &lt;strong&gt;&amp;ldquo;理解多样语言表达、有效遵循指令&amp;rdquo;&lt;/strong&gt; 的另一重要问题 —— 在未见过的导航环境中该问题尤为突出。尽管&lt;strong&gt;语言风格&lt;/strong&gt;本身在 &lt;strong&gt;&amp;ldquo;见过与未见过的环境&amp;rdquo;&lt;/strong&gt; 中具备良好泛化能力，但受限于训练指令的规模，&lt;strong&gt;&amp;ldquo;如何将指令与未见过的环境进行接地&amp;rdquo;&lt;/strong&gt; 仍是一项难题。基础模型通过 &lt;strong&gt;&amp;ldquo;预训练表征&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;指令生成数据增强&amp;rdquo;&lt;/strong&gt; 两种方式，为解决这些问题提供了支持。&lt;/p&gt;
&lt;p&gt;在基础模型出现前，多数研究依赖 &lt;strong&gt;LSTM&lt;/strong&gt; 等文本编码器表征文本指令。而基础模型通过&lt;strong&gt;预训练表征&lt;/strong&gt;，显著提升了 VLN 智能体的&lt;strong&gt;语言泛化能力&lt;/strong&gt;，具体案例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRESS 方法&lt;/strong&gt;：对预训练语言模型 &lt;strong&gt;BERT&lt;/strong&gt; 进行微调，获得对 &lt;strong&gt;&amp;ldquo;未见过指令&amp;rdquo;&lt;/strong&gt; 泛化性更强的文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多模态 Transformer&lt;/strong&gt;：为 &lt;strong&gt;VLN-BERT&lt;/strong&gt;、&lt;strong&gt;PREVALENT&lt;/strong&gt; 等方法提供支撑 —— 这些方法通过在 &lt;strong&gt;&amp;ldquo;从网络收集的大规模图文对&amp;rdquo;&lt;/strong&gt; 上预训练，获得更通用的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Airbert 模型&lt;/strong&gt;：训练一个类 &lt;strong&gt;ViLBERT&lt;/strong&gt; 架构，从 &lt;strong&gt;&amp;ldquo;互联网收集的图像-标题对&amp;rdquo;&lt;/strong&gt; 中学习文本表征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLEAR 方法&lt;/strong&gt;：学习 &lt;strong&gt;&amp;ldquo;跨语言语言表征&amp;rdquo;&lt;/strong&gt;，捕捉指令背后的&lt;strong&gt;视觉概念&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProbES 方法&lt;/strong&gt;：通过采样轨迹实现&lt;strong&gt;环境自探索&lt;/strong&gt;，并利用 CLIP 检测到的 &lt;strong&gt;&amp;ldquo;动作与物体短语&amp;rdquo;&lt;/strong&gt; 填充指令模板，自动构建对应指令；同时借助 &lt;strong&gt;&amp;ldquo;基于提示的学习&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;语言嵌入&lt;/strong&gt;的快速适配。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT-2 模型&lt;/strong&gt;：探索利用 &lt;strong&gt;&amp;ldquo;预训练 VLMs&amp;rdquo;&lt;/strong&gt;（如结合 &lt;strong&gt;Flan-T5&lt;/strong&gt; 或 &lt;strong&gt;Vicuna&lt;/strong&gt; 的 &lt;strong&gt;InstructBLIP&lt;/strong&gt;）的&lt;strong&gt;视觉-语言表征&lt;/strong&gt;，提升&lt;strong&gt;导航策略学习&lt;/strong&gt;与&lt;strong&gt;导航推理能力&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;提升智能体泛化能力的另一方法是 &lt;strong&gt;&amp;ldquo;合成更多指令&amp;rdquo;&lt;/strong&gt;。相关研究可分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）离线指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;早期研究采用 &lt;strong&gt;&amp;ldquo;说话者-跟随者（Speaker-Follower）框架&amp;rdquo;&lt;/strong&gt;：利用人工标注的 &lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 训练一个 &lt;strong&gt;&amp;ldquo;离线说话者（指令生成器）&amp;rdquo;&lt;/strong&gt;，再让其基于 &lt;strong&gt;&amp;ldquo;给定轨迹上的全景序列&amp;rdquo;&lt;/strong&gt; 生成新指令。但发现这类方法生成的指令质量较低，在人类寻路评估中表现不佳。&lt;/p&gt;
&lt;p&gt;后续改进方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Marky 模型&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;多语言 T5 模型的多模态扩展版本&amp;rdquo;&lt;/strong&gt;，结合 &lt;strong&gt;&amp;ldquo;文本对齐的视觉地标对应关系&amp;rdquo;&lt;/strong&gt;，在未见过环境的 R2R 风格路径上生成 &lt;strong&gt;&amp;ldquo;接近人类质量&amp;rdquo;&lt;/strong&gt; 的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PASTS 模型&lt;/strong&gt;：引入 &lt;strong&gt;&amp;ldquo;进度感知的时空 Transformer 说话者&amp;rdquo;&lt;/strong&gt;，更好地利用 &lt;strong&gt;&amp;ldquo;有序的多视觉与动作特征&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SAS 方法&lt;/strong&gt;：利用环境的 &lt;strong&gt;&amp;ldquo;语义与结构线索&amp;rdquo;&lt;/strong&gt;，生成包含丰富&lt;strong&gt;空间信息&lt;/strong&gt;的指令。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SRDF 方法&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;迭代自训练&amp;rdquo;&lt;/strong&gt; 构建一个性能强劲的指令生成器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（2）导航中实时指令生成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;部分近期研究不再训练离线指令生成器，而是在&lt;strong&gt;导航过程中实时生成指令&lt;/strong&gt;。例如，&lt;strong&gt;LANA 模型&lt;/strong&gt;提出一种 &lt;strong&gt;&amp;ldquo;具备语言能力的导航智能体&amp;rdquo;&lt;/strong&gt; —— 该智能体不仅能执行导航指令，还可生成路线描述。&lt;/p&gt;
&lt;h4&gt;VLN Agent: Learning an Embodied Agent for Reasoning and Planning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vln-agent-learning-an-embodied-agent-for-reasoning-and-planning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管&lt;strong&gt;世界模型&lt;/strong&gt;与&lt;strong&gt;人类模型&lt;/strong&gt;为智能体赋予了&lt;strong&gt;视觉与语言理解能力&lt;/strong&gt;，但 VLN 智能体仍需培养&lt;strong&gt;具身推理&lt;/strong&gt;（embodied reasoning）与&lt;strong&gt;规划能力&lt;/strong&gt;，以支撑自身决策。从这一角度出发，我们将探讨两大挑战：&lt;strong&gt;接地与推理&lt;/strong&gt;、&lt;strong&gt;规划&lt;/strong&gt;；同时还将研究 &lt;strong&gt;&amp;ldquo;直接以基础模型作为 VLN 智能体核心骨干&amp;rdquo;&lt;/strong&gt; 的方法。&lt;/p&gt;
&lt;h5&gt;接地与推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;接地与推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8e%a5%e5%9c%b0%e4%b8%8e%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;视觉-语言领域的其他任务（如&lt;strong&gt;视觉问答&lt;/strong&gt;（&lt;strong&gt;VQA&lt;/strong&gt;）、&lt;strong&gt;图像描述生成&lt;/strong&gt;）主要聚焦于 &lt;strong&gt;&amp;ldquo;图像与对应文本描述之间的静态对齐&amp;rdquo;&lt;/strong&gt;，而 VLN 智能体则需基于自身动作，对 &lt;strong&gt;&amp;ldquo;指令与环境中的时空动态信息&amp;rdquo;&lt;/strong&gt; 进行推理。具体而言，智能体需考虑&lt;strong&gt;过往动作&lt;/strong&gt;、识别&lt;strong&gt;待执行的子指令片段&lt;/strong&gt;，并将文本与视觉环境进行&lt;strong&gt;接地&lt;/strong&gt;（grounding），从而执行相应动作。&lt;/p&gt;
&lt;p&gt;传统方法主要通过 &lt;strong&gt;&amp;ldquo;显式语义建模&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;辅助任务设计&amp;rdquo;&lt;/strong&gt; 获取上述能力；但随着基础模型的兴起，&lt;strong&gt;&amp;ldquo;基于特定设计任务的预训练&amp;rdquo;&lt;/strong&gt; 已成为主流方案。&lt;/p&gt;
&lt;p&gt;传统研究通过 &lt;strong&gt;&amp;ldquo;视觉与语言模态的显式语义建模&amp;rdquo;&lt;/strong&gt; 提升智能体的&lt;strong&gt;显式接地能力&lt;/strong&gt;，具体方向包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;建模动作与地标&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;利用指令中的句法信息&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;建模空间关系&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前，&lt;strong&gt;&amp;ldquo;基于基础模型实现 VLN 智能体显式接地&amp;rdquo;&lt;/strong&gt; 的研究仍较少。例如，提出 &lt;strong&gt;&amp;ldquo;动作原子概念学习&amp;rdquo;&lt;/strong&gt;，并将视觉观测映射为&lt;strong&gt;多模态对齐特征&lt;/strong&gt;，以辅助接地。&lt;/p&gt;
&lt;p&gt;除显式语义建模外，传统研究还通过 &lt;strong&gt;&amp;ldquo;辅助推理任务&amp;rdquo;&lt;/strong&gt; 提升智能体的接地能力。但在基于基础模型的 VLN 智能体中，这类方法较少被探索 —— 因为基础模型的预训练过程已使其在导航前就具备了对 &lt;strong&gt;&amp;ldquo;时空语义&amp;rdquo;&lt;/strong&gt; 的通用理解。&lt;/p&gt;
&lt;p&gt;现有研究通过设计&lt;strong&gt;特定预训练任务&lt;/strong&gt;，进一步提升智能体的接地能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;设计专门针对 &lt;strong&gt;&amp;ldquo;场景与物体接地&amp;rdquo;&lt;/strong&gt; 的预训练任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOViS&lt;/strong&gt;：提出两项专项预训练任务，分别增强智能体的 &lt;strong&gt;&amp;ldquo;方向感知&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;视觉信息理解&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HOP&lt;/strong&gt;：提出 &lt;strong&gt;&amp;ldquo;历史与顺序感知预训练范式&amp;rdquo;&lt;/strong&gt;，重点强调&lt;strong&gt;历史信息&lt;/strong&gt;与&lt;strong&gt;轨迹顺序&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;证实 &lt;strong&gt;&amp;ldquo;增强智能体的未来视角语义预测能力&amp;rdquo;&lt;/strong&gt; 有助于提升其在&lt;strong&gt;长路径导航&lt;/strong&gt;中的性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设计 &lt;strong&gt;&amp;ldquo;掩码路径建模目标&amp;rdquo;&lt;/strong&gt; —— 给定随机掩码的子路径，重建原始完整路径&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;提出 &lt;strong&gt;&amp;ldquo;实体感知预训练&amp;rdquo;&lt;/strong&gt;，通过预测&lt;strong&gt;接地实体&lt;/strong&gt;并将其与文本对齐实现接地能力提升&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;动态规划&lt;/strong&gt;能让 VLN 智能体实时适应环境变化、优化导航策略。目前，规划方法主要分为两类：一类是 &lt;strong&gt;&amp;ldquo;利用全局图信息增强局部动作空间&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;图基规划器&lt;/strong&gt;；另一类是随基础模型（尤其是 &lt;strong&gt;LLM&lt;/strong&gt;）兴起的 &lt;strong&gt;LLM 基规划器&lt;/strong&gt; —— 这类规划器借助 LLM 的&lt;strong&gt;海量常识&lt;/strong&gt;与&lt;strong&gt;先进推理能力&lt;/strong&gt;，生成动态规划方案，提升决策效果。&lt;/p&gt;
&lt;p&gt;近期 VLN 研究的核心方向之一，是通过 &lt;strong&gt;&amp;ldquo;全局图信息&amp;rdquo;&lt;/strong&gt; 增强导航智能体的规划能力，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;已访问节点的图边界&amp;rdquo;&lt;/strong&gt; 中的&lt;strong&gt;全局动作步骤&lt;/strong&gt;，增强&lt;strong&gt;局部导航动作空间&lt;/strong&gt;，以实现更优&lt;strong&gt;全局规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过 &lt;strong&gt;&amp;ldquo;高层规划（区域选择）+ 低层规划（节点选择）&amp;rdquo;&lt;/strong&gt; 的&lt;strong&gt;分层策略&lt;/strong&gt;，进一步优化导航决策&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;&amp;ldquo;基于图边界的全局与局部动作空间&amp;rdquo;&lt;/strong&gt; 中融入 &lt;strong&gt;&amp;ldquo;网格级动作&amp;rdquo;&lt;/strong&gt;，提升&lt;strong&gt;动作预测精度&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在&lt;strong&gt;连续环境&lt;/strong&gt;中，规划方法进一步演进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;采用&lt;strong&gt;分层规划思路&lt;/strong&gt; —— 通过 &lt;strong&gt;&amp;ldquo;从预测的局部可导航性图中选择局部航点&amp;rdquo;&lt;/strong&gt;，用&lt;strong&gt;高层动作空间&lt;/strong&gt;替代&lt;strong&gt;低层动作空间&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CM2&lt;/strong&gt;：通过 &lt;strong&gt;&amp;ldquo;在局部地图中实现指令接地&amp;rdquo;&lt;/strong&gt;，辅助&lt;strong&gt;轨迹规划&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拓展上述策略，构建&lt;strong&gt;全局拓扑图&lt;/strong&gt;或&lt;strong&gt;网格图&lt;/strong&gt;，支持 &lt;strong&gt;&amp;ldquo;基于地图的全局规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用 &lt;strong&gt;&amp;ldquo;视频预测模型&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;神经辐射表征模型&amp;rdquo;&lt;/strong&gt; 预测多个&lt;strong&gt;未来航点&lt;/strong&gt;，并基于 &lt;strong&gt;&amp;ldquo;预测候选航点的长期影响&amp;rdquo;&lt;/strong&gt; 规划最优动作&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与此同时，部分研究借助 LLM 的&lt;strong&gt;常识知识&lt;/strong&gt;生成 &lt;strong&gt;&amp;ldquo;基于文本的规划方案&amp;rdquo;&lt;/strong&gt;，代表性工作包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-Planner&lt;/strong&gt;：生成由 &lt;strong&gt;&amp;ldquo;子目标&amp;rdquo;&lt;/strong&gt; 构成的详细规划，并根据&lt;strong&gt;预定义程序模式&lt;/strong&gt;整合检测到的物体，实时动态调整规划&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mic&lt;/strong&gt; 与 &lt;strong&gt;A²Nav&lt;/strong&gt;：专注于将导航任务拆解为详细文本指令 —— Mic 从&lt;strong&gt;静态与动态双视角&lt;/strong&gt;生成分步规划，A²Nav 则利用 &lt;strong&gt;GPT-3&lt;/strong&gt; 将指令解析为可执行子任务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ThinkBot&lt;/strong&gt;：采用 &lt;strong&gt;&amp;ldquo;思维链推理&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought Reasoning），生成 &lt;strong&gt;&amp;ldquo;与交互物体相关的缺失动作&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VL-Map&lt;/strong&gt;：基于 &lt;strong&gt;&amp;ldquo;代码化 LLM&amp;rdquo;&lt;/strong&gt;（遵循 &lt;strong&gt;Code-as-Policy&lt;/strong&gt; 框架），将导航指令拆解为 &lt;strong&gt;&amp;ldquo;代码格式的时序化目标相关函数&amp;rdquo;&lt;/strong&gt;，并利用 &lt;strong&gt;&amp;ldquo;动态构建的可查询地图&amp;rdquo;&lt;/strong&gt; 指导目标执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SayNav&lt;/strong&gt;：构建 &lt;strong&gt;&amp;ldquo;已探索环境的 3D 场景图&amp;rdquo;&lt;/strong&gt;，将其作为 LLM 输入，为导航器生成 &lt;strong&gt;&amp;ldquo;可行且符合上下文的高层规划&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;作为 VLN 智能体的基础模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;作为-vln-智能体的基础模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bd%9c%e4%b8%ba-vln-%e6%99%ba%e8%83%bd%e4%bd%93%e7%9a%84%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;主流方案以 &lt;strong&gt;&amp;ldquo;单流 VL 模型&amp;rdquo;&lt;/strong&gt; 作为 VLN 智能体的核心结构：这类模型在每个时间步同时处理 &lt;strong&gt;&amp;ldquo;语言、视觉、历史令牌（token）&amp;rdquo;&lt;/strong&gt; 输入，通过对&lt;strong&gt;跨模态令牌&lt;/strong&gt;的自注意力运算捕捉 &lt;strong&gt;&amp;ldquo;文本-视觉对应关系&amp;rdquo;&lt;/strong&gt;，进而推断动作概率。&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;零样本 VLN&lt;/strong&gt; 场景中，&lt;strong&gt;CLIP-NAV&lt;/strong&gt; 利用 CLIP 获取 &lt;strong&gt;&amp;ldquo;描述目标物体的自然语言指称表达式&amp;rdquo;&lt;/strong&gt;，实现&lt;strong&gt;序贯导航决策&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;此外，&lt;strong&gt;VLN-CE&lt;/strong&gt;（连续环境 VLN）智能体与 &lt;strong&gt;VLN-DE&lt;/strong&gt;（离散环境 VLN）智能体的核心差异在于&lt;strong&gt;动作空间&lt;/strong&gt;：前者在连续环境中执行&lt;strong&gt;低层控制&lt;/strong&gt;，而非后者 &lt;strong&gt;&amp;ldquo;基于图的高层视角选择动作&amp;rdquo;&lt;/strong&gt;。尽管早期研究采用 &lt;strong&gt;LSTM&lt;/strong&gt; 推断低层动作，但 &lt;strong&gt;&amp;ldquo;航点预测器&amp;rdquo;&lt;/strong&gt;（waypoint predictor）的引入实现了 &lt;strong&gt;&amp;ldquo;从 DE 到 CE 的方法迁移&amp;rdquo;&lt;/strong&gt; —— 所有这些方法均通过航点预测器获取 &lt;strong&gt;&amp;ldquo;局部可导航性图&amp;rdquo;&lt;/strong&gt;，使 DE 场景中的基础模型能适配连续环境。具体而言，航点检测过程主要通过 &lt;strong&gt;&amp;ldquo;视觉观测&amp;rdquo;&lt;/strong&gt;（如全景 RGBD 图像），从智能体当前位置预测 &lt;strong&gt;&amp;ldquo;可导航的相邻候选航点&amp;rdquo;&lt;/strong&gt; 作为潜在目标，再由智能体选择其一作为当前目的地。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM&lt;/strong&gt; 具备强大的&lt;strong&gt;推理能力&lt;/strong&gt;与&lt;strong&gt;世界语义抽象能力&lt;/strong&gt;，且在 &lt;strong&gt;&amp;ldquo;未知大规模环境&amp;rdquo;&lt;/strong&gt; 中表现出优异的&lt;strong&gt;泛化性&lt;/strong&gt; —— 因此，近期 VLN 研究开始直接将 LLM 作为智能体执行导航任务。其核心流程为：将&lt;strong&gt;视觉观测&lt;/strong&gt;转换为&lt;strong&gt;文本描述&lt;/strong&gt;，与指令一同输入 LLM，由 LLM 完成&lt;strong&gt;动作预测&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;代表性创新方案包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NavGPT&lt;/strong&gt; 与 &lt;strong&gt;MapGPT&lt;/strong&gt;：验证了&lt;strong&gt;零样本导航&lt;/strong&gt;的可行性 —— NavGPT 利用 &lt;strong&gt;GPT-4&lt;/strong&gt; 自主生成动作，MapGPT 将&lt;strong&gt;拓扑图&lt;/strong&gt;转换为&lt;strong&gt;全局探索提示&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiscussNav&lt;/strong&gt;：拓展上述思路，部署 &lt;strong&gt;&amp;ldquo;多领域专用 VLN 专家&amp;rdquo;&lt;/strong&gt;（包括&lt;strong&gt;指令分析专家&lt;/strong&gt;、&lt;strong&gt;视觉感知专家&lt;/strong&gt;、&lt;strong&gt;完成度估计专家&lt;/strong&gt;、&lt;strong&gt;决策测试专家&lt;/strong&gt;），减少导航任务中的人工参与：通过将任务分配给专用智能体，减轻单一模型负担，实现 &lt;strong&gt;&amp;ldquo;任务专属优化处理&amp;rdquo;&lt;/strong&gt;，并借助&lt;strong&gt;多大型模型的协同优势&lt;/strong&gt;提升&lt;strong&gt;鲁棒性&lt;/strong&gt;、&lt;strong&gt;透明度&lt;/strong&gt;与&lt;strong&gt;整体性能&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MC-GPT&lt;/strong&gt;：利用 &lt;strong&gt;&amp;ldquo;记忆拓扑图&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;人类导航示例&amp;rdquo;&lt;/strong&gt; 丰富&lt;strong&gt;策略多样性&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;InstructNav&lt;/strong&gt;：将导航拆解为&lt;strong&gt;子任务&lt;/strong&gt;，并结合 &lt;strong&gt;&amp;ldquo;多源价值图&amp;rdquo;&lt;/strong&gt; 实现高效执行&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与 &lt;strong&gt;&amp;ldquo;零样本使用&amp;rdquo;&lt;/strong&gt; 不同，部分研究通过 &lt;strong&gt;&amp;ldquo;微调 LLM&amp;rdquo;&lt;/strong&gt;，使其能更有效地处理&lt;strong&gt;具身导航任务&lt;/strong&gt;。另有研究融入 &lt;strong&gt;&amp;ldquo;思维链&amp;rdquo;&lt;/strong&gt;（Chain-of-Thought, &lt;strong&gt;CoT&lt;/strong&gt;）推理机制提升推理过程，例如 &lt;strong&gt;Nav-CoT&lt;/strong&gt; 将 LLM 转化为 &lt;strong&gt;&amp;ldquo;世界模型与导航推理智能体&amp;rdquo;&lt;/strong&gt;，通过模拟未来环境简化决策 —— 这一方案证实了 &lt;strong&gt;&amp;ldquo;微调语言模型&amp;rdquo;&lt;/strong&gt; 在仿真与真实场景中的&lt;strong&gt;灵活性&lt;/strong&gt;与&lt;strong&gt;实用潜力&lt;/strong&gt;，较传统应用实现了显著突破。&lt;/p&gt;
&lt;h3&gt;挑战与未来方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;挑战与未来方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8c%91%e6%88%98%e4%b8%8e%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;尽管基础模型为&lt;strong&gt;视觉-语言导航&lt;/strong&gt;（&lt;strong&gt;VLN&lt;/strong&gt;）提供了创新性解决方案，但仍有若干局限尚未得到充分探索，同时新的挑战也随之出现。在本节中，我们将从&lt;strong&gt;基准数据集&lt;/strong&gt;、&lt;strong&gt;世界模型&lt;/strong&gt;、&lt;strong&gt;人类模型&lt;/strong&gt;、&lt;strong&gt;智能体模型&lt;/strong&gt;及&lt;strong&gt;真实机器人部署&lt;/strong&gt;五个维度，梳理 VLN 领域的挑战与未来研究方向。&lt;/p&gt;
&lt;h4&gt;基准数据集：数据与任务的局限&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基准数据集数据与任务的局限&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e5%87%86%e6%95%b0%e6%8d%ae%e9%9b%86%e6%95%b0%e6%8d%ae%e4%b8%8e%e4%bb%bb%e5%8a%a1%e7%9a%84%e5%b1%80%e9%99%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;当前 &lt;strong&gt;VLN 数据集&lt;/strong&gt;在&lt;strong&gt;质量&lt;/strong&gt;、&lt;strong&gt;多样性&lt;/strong&gt;、&lt;strong&gt;偏差&lt;/strong&gt;及&lt;strong&gt;可扩展性&lt;/strong&gt;方面存在明显局限。例如，在 &lt;strong&gt;R2R&lt;/strong&gt; 数据集中，&lt;strong&gt;&amp;ldquo;指令-轨迹对&amp;rdquo;&lt;/strong&gt; 偏向于&lt;strong&gt;最短路径&lt;/strong&gt;，无法准确反映现实世界的导航场景。下文将探讨 VLN 基准数据集的改进趋势与建议方向：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;统一且贴近现实的任务与平台&lt;/strong&gt;：构建可靠的基准数据集并确保结果可复现，是评估真实场景下 VLN 性能的关键。现实世界的复杂性要求基准数据集需全面覆盖各类导航挑战，因此需要一个通用的 &lt;strong&gt;&amp;ldquo;仿真到现实&amp;rdquo;&lt;/strong&gt; 评估平台（如 &lt;strong&gt;OVMM&lt;/strong&gt;），以实现仿真与真实场景下的标准化测试。此外，任务与活动设计需贴近现实且源于人类需求，例如 &lt;strong&gt;BEHAVIOR-1K&lt;/strong&gt; 基准数据集，在虚拟、交互式且具生态性的环境中构建&lt;strong&gt;日常家庭活动场景&lt;/strong&gt;，以满足对 &lt;strong&gt;&amp;ldquo;多样性&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;真实性&amp;rdquo;&lt;/strong&gt; 的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;动态环境&lt;/strong&gt;：现实世界环境本质上具有&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;动态性&lt;/strong&gt; —— &lt;strong&gt;移动物体&lt;/strong&gt;、&lt;strong&gt;行人&lt;/strong&gt;，以及&lt;strong&gt;光照&lt;/strong&gt;、&lt;strong&gt;天气&lt;/strong&gt;等环境变化，均可能引发&lt;strong&gt;突发情况&lt;/strong&gt;。这些因素会干扰导航系统的&lt;strong&gt;视觉感知&lt;/strong&gt;，使其难以维持稳定性能。近期部分研究（如 &lt;strong&gt;HAZARD&lt;/strong&gt;、&lt;strong&gt;Habitat 3.0&lt;/strong&gt;、&lt;strong&gt;HA-VLN&lt;/strong&gt;）已开始关注&lt;strong&gt;动态环境&lt;/strong&gt;，为后续研究提供了良好起点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从室内到室外&lt;/strong&gt;：适用于室外环境的 VLN 智能体（如&lt;strong&gt;自动驾驶车辆&lt;/strong&gt;、&lt;strong&gt;无人机&lt;/strong&gt;）正逐渐获得更多关注，相关&lt;strong&gt;语言引导数据集&lt;/strong&gt;也已陆续开发。早期研究尝试将 LLM 融入&lt;strong&gt;室外 VLN 任务&lt;/strong&gt;，具体方式包括&lt;strong&gt;提示工程&lt;/strong&gt;，或通过&lt;strong&gt;微调 LLM&lt;/strong&gt; 实现 &lt;strong&gt;&amp;ldquo;预测下一步动作&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;规划未来轨迹&amp;rdquo;&lt;/strong&gt;。为使现成的 VLN 模型适配&lt;strong&gt;室外导航场景&lt;/strong&gt;，研究者利用&lt;strong&gt;真实驾驶视频&lt;/strong&gt;、&lt;strong&gt;仿真驾驶数据&lt;/strong&gt;或两者结合进行&lt;strong&gt;指令微调&lt;/strong&gt;，使基础模型能够学习预测未来的&lt;strong&gt;油门与转向角度&lt;/strong&gt;。此外，研究者还在基于基础模型的驾驶智能体中集成了额外的&lt;strong&gt;推理与规划模块&lt;/strong&gt;。关于室外 VLN 的详细综述，建议读者参考相关综述文献与立场论文。&lt;/p&gt;
&lt;h4&gt;世界模型：从二维（2D）到三维（3D）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;世界模型从二维2d到三维3d&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%96%e7%95%8c%e6%a8%a1%e5%9e%8b%e4%bb%8e%e4%ba%8c%e7%bb%b42d%e5%88%b0%e4%b8%89%e7%bb%b43d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;构建有效的&lt;strong&gt;世界表征&lt;/strong&gt;是&lt;strong&gt;具身感知&lt;/strong&gt;、&lt;strong&gt;推理&lt;/strong&gt;与&lt;strong&gt;规划&lt;/strong&gt;领域的核心研究主题。VLN 本质上是一项 &lt;strong&gt;3D 任务&lt;/strong&gt; —— 智能体需以 3D 形式感知真实世界环境。尽管当前研究已能通过强大的通用 &lt;strong&gt;2D 表征&lt;/strong&gt;描述世界，但这类表征无法充分支持 3D 场景下的&lt;strong&gt;空间语言理解&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;以往研究已提出多种显式 &lt;strong&gt;3D 表征方式&lt;/strong&gt;，包括各类&lt;strong&gt;语义同步定位与地图构建&lt;/strong&gt;（semantic &lt;strong&gt;SLAM&lt;/strong&gt;）、&lt;strong&gt;体素表征&lt;/strong&gt;、&lt;strong&gt;深度信息&lt;/strong&gt;、&lt;strong&gt;鸟瞰图&lt;/strong&gt;（Bird&amp;rsquo;s-Eye-View）表征（如&lt;strong&gt;网格图&lt;/strong&gt;）及&lt;strong&gt;局部度量图&lt;/strong&gt;。但这些表征存在局限：它们将物体集合限定为 &lt;strong&gt;&amp;ldquo;封闭集合&amp;rdquo;&lt;/strong&gt;，无法适配自然语言对应的 &lt;strong&gt;&amp;ldquo;开放词汇场景&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;部分研究尝试构建 &lt;strong&gt;&amp;ldquo;可查询的地图/场景表征&amp;rdquo;&lt;/strong&gt;，例如将 CLIP 提取的&lt;strong&gt;多视角图像特征&lt;/strong&gt;整合到 &lt;strong&gt;3D 体素网格&lt;/strong&gt;或&lt;strong&gt;俯视特征图&lt;/strong&gt;中，或利用&lt;strong&gt;场景图&lt;/strong&gt;表征&lt;strong&gt;空间关系&lt;/strong&gt;。然而，&lt;strong&gt;&amp;ldquo;如何将大规模数据中学习到的 3D 表征适配于 VLN 智能体，以提升其 3D 环境感知能力&amp;rdquo;&lt;/strong&gt; 仍是待探索的问题。近期兴起的 &lt;strong&gt;3D 基础模型&lt;/strong&gt; —— 包括 &lt;strong&gt;3D 重建模型&lt;/strong&gt; 与 &lt;strong&gt;3D 多模态表征模型&lt;/strong&gt; —— 有望为 VLN 领域提供关键支撑。&lt;/p&gt;
&lt;h4&gt;人类模型：从指令到对话&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;人类模型从指令到对话&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%ba%e7%b1%bb%e6%a8%a1%e5%9e%8b%e4%bb%8e%e6%8c%87%e4%bb%a4%e5%88%b0%e5%af%b9%e8%af%9d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;以往研究多采用 &lt;strong&gt;&amp;ldquo;说话者-倾听者范式&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;受限问答对话&amp;rdquo;&lt;/strong&gt; —— 这类方式仅允许智能体主动请求帮助。近年来，涌现出一批以 &lt;strong&gt;&amp;ldquo;开放式对话指令&amp;rdquo;&lt;/strong&gt; 为核心的新基准数据集，支持智能体在模糊或困惑场景下进行完全&lt;strong&gt;自由形式的通信&lt;/strong&gt;，包括&lt;strong&gt;提问&lt;/strong&gt;、&lt;strong&gt;提议&lt;/strong&gt;、&lt;strong&gt;解释&lt;/strong&gt;、&lt;strong&gt;建议&lt;/strong&gt;、&lt;strong&gt;澄清&lt;/strong&gt;与&lt;strong&gt;协商&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;然而，当前方法仍依赖 &lt;strong&gt;&amp;ldquo;基于规则的对话模板&amp;rdquo;&lt;/strong&gt; 应对上述复杂场景，即便部分方法包含基础模型组件，也未充分发挥其能力。通过 &lt;strong&gt;&amp;ldquo;人类对话数据 + 仿真导航视频&amp;rdquo;&lt;/strong&gt; 对&lt;strong&gt;视频-语言模型&lt;/strong&gt;进行&lt;strong&gt;对话调优&lt;/strong&gt;，使模型在导航过程中具备更强的&lt;strong&gt;对话生成能力&lt;/strong&gt;。未来研究需重点关注两方面：一是将基础模型融入 &lt;strong&gt;&amp;ldquo;情境化任务导向对话管理&amp;rdquo;&lt;/strong&gt;；二是探索现有基础模型在 &lt;strong&gt;&amp;ldquo;任务导向对话&amp;rdquo;&lt;/strong&gt; 中的应用潜力。&lt;/p&gt;
&lt;h4&gt;智能体模型：基础模型在 VLN 中的适配&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;智能体模型基础模型在-vln-中的适配&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%99%ba%e8%83%bd%e4%bd%93%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%9c%a8-vln-%e4%b8%ad%e7%9a%84%e9%80%82%e9%85%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;尽管基础模型具有强大的&lt;strong&gt;泛化能力&lt;/strong&gt;，但将其融入导航任务仍面临挑战：&lt;strong&gt;LLM&lt;/strong&gt; 本质上缺乏对真实环境的&lt;strong&gt;视觉感知能力&lt;/strong&gt;，且易产生 &lt;strong&gt;&amp;ldquo;幻觉&amp;rdquo;&lt;/strong&gt;；下文还将探讨 LLM 在规划与推理方面的能力局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺乏具身经验&lt;/strong&gt;：这一局限可能导致 LLM 在任务规划与推理中仅依赖&lt;strong&gt;预设常识&lt;/strong&gt;，无法满足真实场景的特定需求。部分研究通过 &lt;strong&gt;&amp;ldquo;将视觉观测转换为文本描述，作为 LLM 的提示&amp;rdquo;&lt;/strong&gt; 解决该问题，但这种方式可能丢失关键&lt;strong&gt;视觉语义&lt;/strong&gt;。与 LLM 相比，&lt;strong&gt;VLM&lt;/strong&gt;（视觉-语言模型）智能体虽展现出 &lt;strong&gt;&amp;ldquo;感知视觉世界与规划&amp;rdquo;&lt;/strong&gt; 的潜力，但其训练数据主要源于互联网，缺乏&lt;strong&gt;具身经验&lt;/strong&gt;，需通过&lt;strong&gt;微调&lt;/strong&gt;实现稳健的智能体决策。未来需进一步研究 &lt;strong&gt;&amp;ldquo;如何将基础模型智能体中的常识知识迁移到具身场景中&amp;rdquo;&lt;/strong&gt;。近期提出的 &lt;strong&gt;&amp;ldquo;具身基础模型&amp;rdquo;&lt;/strong&gt;（如 &lt;strong&gt;EmbodieGPT&lt;/strong&gt;、&lt;strong&gt;PaLM-E&lt;/strong&gt;、&lt;strong&gt;Octopus&lt;/strong&gt;）为解决该问题提供了可行方向：这些模型通过在多类具身任务上微调基础模型，缩小智能体在 &lt;strong&gt;&amp;ldquo;视觉-语言-具身动作&amp;rdquo;&lt;/strong&gt; 理解上的差距，提升其基于多模态输入的理解与执行能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;幻觉问题&lt;/strong&gt;：LLM 与 VLM 可能生成 &lt;strong&gt;&amp;ldquo;不存在的物体&amp;rdquo;&lt;/strong&gt;，导致&lt;strong&gt;信息失真&lt;/strong&gt;。例如，LLM 在任务规划时可能生成 &lt;strong&gt;&amp;ldquo;向前走并在沙发处左转&amp;rdquo;&lt;/strong&gt; 的指令，即便房间内并无沙发。这种偏差可能导致智能体执行错误或无法完成的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM 在规划与推理中的能力局限&lt;/strong&gt;：已有文献针对 LLM 的&lt;strong&gt;零样本推理与规划能力&lt;/strong&gt;展开评估（尤其是结合 &lt;strong&gt;PlanBench&lt;/strong&gt; 与 &lt;strong&gt;CogEval&lt;/strong&gt;），结果表明 LLM 在&lt;strong&gt;复杂规划任务&lt;/strong&gt;中存在明显局限。这些研究在 &lt;strong&gt;&amp;ldquo;规划生成、最优性、稳健性、推理&amp;rdquo;&lt;/strong&gt; 等挑战性场景下评估 LLM，发现其不仅易产生幻觉，还可能无法理解复杂规划问题背后的&lt;strong&gt;关系结构&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在 VLN 场景中，由于室内环境固定且导航动作集合有限，&lt;strong&gt;动作空间&lt;/strong&gt;与&lt;strong&gt;规划需求&lt;/strong&gt;相对受限。这种 &lt;strong&gt;&amp;ldquo;有界场景&amp;rdquo;&lt;/strong&gt; 使 LLM 能够生成 &lt;strong&gt;&amp;ldquo;粗粒度方向的分步指令&amp;rdquo;&lt;/strong&gt; —— 已有研究证实该方式的有效性。需强调的是，在 VLN 任务中，LLM 并非主导整个规划过程，而是通过 &lt;strong&gt;&amp;ldquo;结构化拆解指令&amp;rdquo;&lt;/strong&gt; 提供辅助；智能体的实际决策仍主要依赖&lt;strong&gt;感知&lt;/strong&gt;、&lt;strong&gt;运动控制&lt;/strong&gt;等其他组件。因此，LLM 的规划功能更多是 &lt;strong&gt;&amp;ldquo;补充性指导&amp;rdquo;&lt;/strong&gt;，而非唯一决策依据。&lt;/p&gt;
&lt;h4&gt;部署：从仿真到真实机器人&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;部署从仿真到真实机器人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%83%a8%e7%bd%b2%e4%bb%8e%e4%bb%bf%e7%9c%9f%e5%88%b0%e7%9c%9f%e5%ae%9e%e6%9c%ba%e5%99%a8%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;仿真环境往往缺乏真实世界的&lt;strong&gt;复杂性&lt;/strong&gt;与&lt;strong&gt;多样性&lt;/strong&gt;，且低质量渲染图像会进一步加剧这一问题。具体而言，当前部署面临三大瓶颈：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知差距&lt;/strong&gt;：仿真与真实场景的&lt;strong&gt;视觉差异&lt;/strong&gt;导致智能体性能与精度下降，因此需构建更稳健的感知系统。例如，尝试利用&lt;strong&gt;语义地图&lt;/strong&gt;与 &lt;strong&gt;3D 特征场&lt;/strong&gt;为单目机器人提供&lt;strong&gt;全景感知&lt;/strong&gt;，显著提升了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具身差距与数据稀缺&lt;/strong&gt;：仿真环境的&lt;strong&gt;物理规则&lt;/strong&gt;与真实机器人的&lt;strong&gt;具身特性&lt;/strong&gt;不匹配，且真实场景下 VLN 数据收集成本高、规模有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据规模化解决方案&lt;/strong&gt;：&lt;strong&gt;机器人远程操控&lt;/strong&gt;的兴起为解决数据稀缺提供了新思路 —— 通过人类远程控制机器人，可在真实人机交互场景中规模化收集 VLN 数据，为基础模型训练提供支撑。&lt;/p&gt;
&lt;h3&gt;仓库论文链接&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;仓库论文链接&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bb%93%e5%ba%93%e8%ae%ba%e6%96%87%e9%93%be%e6%8e%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：本表格按分类和子分类组织所有 VLN 相关论文，便于浏览和筛选。分类包括：Survey（综述）、World Model（世界模型）、Human Model（人类模型）、VLN Agent（VLN 智能体）、Behavior Analysis（行为分析）。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;分类&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;子分类&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Survey&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.12667&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/eric-ai-lab/awesome-vision-language-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s10462-022-10174-9&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual language navigation: A survey and open challenges&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11544&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation: A Survey and Taxonomy&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;World Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.13451&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.03561&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.14158&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Volumetric Environment Representation for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/DefaultRui/VLN-VER&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2023/0204.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision Language Navigation with Knowledge-driven Environmental Dreamer&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;IJCAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/hekj/FDA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.19195&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.02960&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple and Effective Synthesis of Indoor 3D Scenes&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research/se3ds&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Navigational Visual Representations with Semantic Map Supervision&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.11984&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning vision-and-language navigation from youtube videos&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.12907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GridMM: Grid Memory Map for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/GridMM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.04385&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BEVBert: Multimodal Map Pre-training for Language-guided Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/VLN-BEVBert&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15644&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scaling Data Generation in Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/ScaleVLN/tree/main?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.03112&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clin1223/MTVM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.15685&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EnvEdit: Environment Editing for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960375.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.06383&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Much Can CLIP Benefit Vision-and-Language Tasks?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/clip-vil/CLIP-ViL&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.11742&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/cshizhe/VLN-DUET&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.13309&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;History Aware Multimodal Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://cshizhe.github.io/projects/vln_hamt.html&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.08756&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pathdreamer: A World Model for Indoor Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Pashevich_Episodic_Transformer_for_Vision-and-Language_Navigation_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Episodic Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.09105&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbert: In-domain Pretraining for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://airbert-vln.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.07876&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision-Language Navigation with Random Environmental Mixup&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LCFractal/VLNREM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Human Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene Map-based Prompt Tuning for Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.11142&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/NavRAG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.08467&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICLR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wz0919/VLN-SRDF&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.15087&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigation Instruction Generation with BEV Perception and Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/FanScy/BEVInstructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.07433&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Controllable Navigation Instruction Generation with Chain of Thought Prompting&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/refkxh/C-Instructor&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.acl-long.734.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/gmuraleekrishna/SAS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.18721&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Correctable Landmark Discovery via Large Models for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/CONSOLE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.02559&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavHint: Vision and Language Navigation Agent with a Hint Generator&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/NavHint&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10359152&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Follow and Generate Instructions for Language-Capable Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.08409&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lana: A Language-Capable Navigator for Instruction Following and Generation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/wxh1996/LANA-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/xiangyangli-cn/KERM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.11918&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.00852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.09230&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN-Trans: Translator for the Vision and Language Navigation Agent&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/VLN-trans&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.04006&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/liangcici/Probes-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.14973&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Less is More: Generating Grounded Navigation Instructions from Landmarks&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/google-research-datasets/RxR/tree/main/marky-mT5&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.10504&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Evaluation of Vision-and-Language Navigation Instructions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://say-can.github.io/assets/palm_saycan.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do As I Can, Not As I Say:Grounding Language in Robotic Affordances&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://say-can.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN Agent&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.05552&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/SAME&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.18800&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniVLN: Efficient Vision-and-Language Navigation byProgressive Knowledge Distillation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.06072&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.12587&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CSir1996/VLN-GELA&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/chengaopro/AZHP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.04758&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bird&amp;rsquo;s-Eye-View Scene Graph for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14268&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Masked Path Modeling for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.04907&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10006384&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;TPAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2207.11201&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Target-Driven Structured Transformer Planner for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YushengZhao/TD-STP&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9880046&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/HOP-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.505.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;COLING&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/HLR/LOViS&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.12944&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene-Intuitive Agent for Remote Embodied Visual Grounding&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.14143&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YuankaiQi/ORIST&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLN BERT: A Recurrent Vision-and-Language BERT for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2021&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Recurrent-VLN-BERT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.10638&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/weituo12321/PREVALENT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;VLN-CE&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.02549&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.22548&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://miv-xjtu.github.io/JanusVLN.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23468&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Feliciaxyao/NavMorph&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.05890&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.01943&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MrZihan/HNR-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03047v2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;PAMI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/MarSaKi/ETPNav?tab=readme-ov-file&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2408.10388&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Narrowing the Gap between Vision and Action in Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;MM&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02764&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YicongHong/Discrete-Continuous-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.02857&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2020&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jacobkrantz/VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Zero-shot)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10611565&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICRA&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/DiscussNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.07314&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ACL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://chen-judge.github.io/MapGPT/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.10620&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04882&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LYX0501/InstructNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.16986&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;AAAI&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March in Chat: Interactive Prompting for Remote Embodied Referring Expression&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ICCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/MiC&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.10822&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision and Language Navigation in the Real World via Online Visual Language Mapping&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://peihaochen.github.io/files/publications/A2Nav.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NeurIPS Workshop&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.16649&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;LLM/VLM (Fine-tuning)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.01551&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Arxiv&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/EvolveNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2024.findings-naacl.60.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LangNav: Language as a Perceptual Representation for Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/pbw-Berwin/LangNav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07376&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/expectorlin/NavCoT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.02010&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Learning a Generalist Model for Embodied Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/LaVi-Lab/NaviLLM&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2407.12366&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ECCV&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.15852&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;RSS&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/GengzeZhou/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Behavior Analysis&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.16394&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Visual Imaginations Improve Vision-and-Language Navigation Agents?&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.17313&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EMNLP Findings&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/zehao-wang/navnuances&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Behavioral Analysis of Vision-and-Language Navigation Agents&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;CVPR&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/Yoark/vln-behave&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.438.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diagnosing Vision-and-Language Navigation: What Really Matters&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;NACCL&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2022&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/VegB/Diagnose_VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;后续工作&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;后续工作&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%90%8e%e7%bb%ad%e5%b7%a5%e4%bd%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里夸奖一下Gemini3和qwen的deep Research，真的救我狗命。&lt;/p&gt;
&lt;p&gt;重点精读部分就看下面Gemini3提供的一份经过深度调研、严格筛选的 &lt;strong&gt;2023–2025&lt;/strong&gt; 年间顶会（CVPR, ICCV, ECCV, ICLR, NeurIPS, CoRL, RSS, ICRA, IROS）&lt;strong&gt;已接收 (Accepted)&lt;/strong&gt; 且 &lt;strong&gt;已公开代码&lt;/strong&gt; 的 VLN / ObjectNav / Zero-Shot / LLM-assisted Navigation 相关论文列表。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;会议&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;年份&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;标题&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;简介&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;代码&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;关键词&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CVPR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;UniGoal: Towards Universal Zero-shot Goal-oriented Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了基于场景图（Scene Graph）和 LLM 的通用导航框架，统一了 Object, Image, Text 三种目标导航任务，解决 Zero-Shot 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Zero-Shot, Scene Graph, LLM, Universal Goal&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;专注于提升 VLM 的空间理解能力，通过构建空间感知的指令微调数据集，大幅提升了机器人在 3D 环境中的导航和操作能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/RoboSpatial/RoboSpatial&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Spatial Reasoning, VLM, Robotics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-and-Language Navigation via Causal Learning (VLN-GOAT)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入因果推断（Causal Inference）消除数据偏差，提升 VLN 模型的泛化性&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/CrystalSixone/VLN-GOAT&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Causal Learning, Deconfounding&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;首个基于视频的大模型（Video-based VLM）端到端导航器，无需构建显式地图，直接从视频流规划动作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jzhzhang/NaVid-VLN-CE&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Video VLM, Mapless, End-to-End&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AIGeN: An Adversarial Approach for Instruction Generation in VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用对抗生成网络生成高质量的导航指令，用于数据增强&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/jialuli-luka/AIGeN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Iterative Vision-and-Language Navigation (IVLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出了&amp;quot;迭代式导航&amp;quot;新基准，要求机器人在同一环境中持续执行多条指令，考察记忆能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/JacobKrantz/IVLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Continuous Navigation, Memory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Improving Vision-and-Language Navigation by Generating Future-View Image Semantics (VLN-SIG)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过生成未来视角的语义图像来辅助当前决策&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://jialuli-luka.github.io/VLN-SIG&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;模拟人类认知过程（感知-推理-决策），利用 LLM 进行常识推理和空间推理，解决 ObjectNav 问题&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://yhancao.github.io/CogNav/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page &amp;amp; Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Cognitive Modeling, LLM, ObjectNav&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Learning Vision-and-Language Navigation from YouTube Videos (YouTube-VLN)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用大规模 YouTube 房屋导览视频进行预训练，学习真实世界先验&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/JeremyLinky/YouTube-VLN&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;引入&amp;quot;心理规划&amp;quot;机制，在执行前在潜在空间预演路径&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/HanqingWangAI/DreamWalker&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ECCV&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLN-Copilot: LLM as Copilot for Coarse-grained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出&amp;quot;副驾驶&amp;quot;概念，当导航智能体困惑时，LLM 提供详细的指导和推理辅助&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zun-Wang/VLN-Copilot&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;LLM Agent, Coarse-grained VLN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;通过微调适配，激发通用多模态大模型（VLM）的导航推理能力&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/WZMIAOMIAO/NavGPT-2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Vision-Language Navigation with Energy-Based Policy (ENP)&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出基于能量的模型（Energy-Based Model）来建模导航策略，更好地模拟专家轨迹分布&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://neurips.cc/virtual/2025/poster/93232&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS Page/GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;构建在线 3D 场景图作为 Prompt，实现无需训练的 Zero-Shot 导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/SG-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;InstructNav: Zero-shot Vision-and-Language Navigation with Instruction Tuning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;这是一个通用的导航大模型框架，统一了 VLN 和 ObjectNav&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;(查看作者 Hao Dong 的 GitHub 或 Project Page)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;PanoGen: Text-Conditioned Panoramic Environment Generation for VLN&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;使用生成式模型根据文本生成全景环境，用于 VLN 的数据增强和训练&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CoRL&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;GC-VLN: Graph-Constrained Vision-and-Language Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;UniGoal&lt;/em&gt; 团队新作，将导航建模为图约束优化问题，无需训练即可在连续环境中导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bagh2178/UniGoal&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Training-free, Graph Constraints&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LeLaN: Learning a Language-Conditioned Navigation Policy from In-the-Wild Video&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;直接从野外（In-the-Wild）视频数据中学习语言条件的导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://lelan-video.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OpenVLA: An Open-Source Vision-Language-Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;虽然主要针对操作（Manipulation），但其架构和预训练模型被大量用于导航任务的底座&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/openvla/openvla&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 VLM 进行零样本 3D 视觉定位，是导航的关键前置任务&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/desdemonawang/VLM-Grounder&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;OVSG: Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;基于开放词汇 3D 场景图的实体定位与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/ovsg-code/ovsg&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICRA&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation with Open-Source LLMs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;探索使用 Llama 等开源模型替代 GPT-4 进行 Zero-Shot 导航，提出时空思维链&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/YanyuanQiao/Open-Nav&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;MonoTransmotion&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;涉及单目视觉下的运动规划与导航&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vita-epfl/MonoTransmotion&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 CLIP 和前沿点（Frontier）地图，指导机器人探索语义目标&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/bdaiinstitute/vlfm&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2023&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;VLMaps: Visual Language Maps for Robot Navigation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将 VLM 特征融合进 3D 地图，允许使用自然语言索引地图位置&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/vlmaps/vlmaps&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;IROS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;LLM3: Large Language Model-based Task and Motion Planning&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;结合 LLM 进行任务和运动规划，虽然偏 TAMP，但也包含导航组件&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/Zju-Robotics-Lab/LLM3&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RSS&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2025&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Unified Video Action Model&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;统一的视频动作模型，涵盖导航和操作&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Page/Code&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;雖然偏向操作，但其 Policy 蒸馏方法正被用于加速导航策略&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/DLR-RM/Consistency-Policy&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ICLR&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2024&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用 Web 教程合成智能体轨迹，辅助导航和任务执行&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;a href=&#34;https://www.google.com/search?q=https://github.com/xduan7/AgentTrek&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>具身导航调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-14-navigation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-14-navigation/</guid>
      <description>
        
        
        &lt;h1&gt;具身导航调研&lt;/h1&gt;&lt;p&gt;对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。&lt;/p&gt;
&lt;p&gt;本调研主要基于 &lt;strong&gt;&lt;a href=&#34;https://github.com/jiangranlv/embodied-ai-start&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PKU EPIC Lab&lt;/a&gt;&lt;/strong&gt; 、&lt;strong&gt;&lt;a href=&#34;https://github.com/TianxingChen/Embodied-AI-Guide&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lumina具身智能社区&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;一、基础概念 (Basic Concepts)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一基础概念-basic-concepts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1、 什么是具身智能&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-什么是具身智能&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。&lt;/p&gt;
&lt;p&gt;典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心特征：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有多模态感知能力（视觉、触觉、语音等）&lt;/li&gt;
&lt;li&gt;能够执行动作并影响环境&lt;/li&gt;
&lt;li&gt;学习可以通过与环境交互而不仅仅是被动监督完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 具身智能与其他AI的区别&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-具身智能与其他ai的区别&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96ai%e7%9a%84%e5%8c%ba%e5%88%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;如何scale up机器人数据？&lt;/strong&gt; 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？&lt;/strong&gt; 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 研究具身智能的核心原则 (Core Principles)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-研究具身智能的核心原则-core-principles&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；&lt;/p&gt;
&lt;p&gt;但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）&lt;/p&gt;
&lt;p&gt;必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？&lt;/p&gt;
&lt;h2&gt;二、AI and Robotics Basis&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二ai-and-robotics-basis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cai-and-robotics-basis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-Embodied-AI&lt;/strong&gt;
王鹤老师《具身智能导论》，找找类似课程替代&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-CV&lt;/strong&gt;
Stanford CS231N&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning (CS285)&lt;/strong&gt;
Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;三、研究平台与工具&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三研究平台与工具&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Simulation Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-simulation-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-simulation-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;2. Robot Platform&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-platform&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-platform&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;3. Daily ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-daily-arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-daily-arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了：
具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jiangranlv/robotics_arXiv_daily&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jiangranlv/robotics_arXiv_daily&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;四、Research Field on Robots&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四research-field-on-robots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bresearch-field-on-robots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Grasping&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-grasping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-grasping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。&lt;/p&gt;
&lt;p&gt;常见任务有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single object grasping（单物体抓取）&lt;/strong&gt;：抓取一个物体，物体通常放在桌子上。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clutter scene grasping（堆叠场景抓取）&lt;/strong&gt;：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional grasping（带语义抓取）&lt;/strong&gt;：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用机械手末端有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suction cup（吸盘）&lt;/strong&gt;：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel gripper（平行夹抓）&lt;/strong&gt;：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-fingered hand（多指手）&lt;/strong&gt;，又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-loop methods（开环执行）&lt;/strong&gt;：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closed-loop methods（闭环执行）&lt;/strong&gt;：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Manipulation&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-manipulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-manipulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。&lt;/p&gt;
&lt;p&gt;**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）
**Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。
**Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。
**Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。
**Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。
**Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同&lt;/p&gt;
&lt;h3&gt;3. Navigation(NOW)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-navigationnow&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-navigationnow&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。&lt;/p&gt;
&lt;p&gt;常见任务包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Point Goal Navigation (PointNav)&lt;/strong&gt;：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Goal Navigation (ObjectNav)&lt;/strong&gt;：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Navigation (VLN)&lt;/strong&gt;：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embodied Question Answering (EQA)&lt;/strong&gt;：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：机器人持续感知并跟随动态目标（如人或移动物体）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map-based Navigation&lt;/strong&gt;：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompting-Large-Model Navigation&lt;/strong&gt;：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video-based VLM Navigation&lt;/strong&gt;：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unified Embodied Navigation&lt;/strong&gt;：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Locomotion&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-locomotion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-locomotion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。&lt;/p&gt;
&lt;p&gt;技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning from manually designed reward&lt;/strong&gt; (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from human data&lt;/strong&gt; (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】
如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -&amp;gt; whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下一个阶段会涉及到一点除了control之外的东西，就是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引入【视觉】实现户外自主化（perceptive locomotion）&lt;/strong&gt;；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入【物体】实现loco-manipulation&lt;/strong&gt;；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对上述两种task的组合&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调【语义的泛化性】&lt;/strong&gt;，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调一些特殊的capability&lt;/strong&gt;（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;五、Learning based Research Field&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五learning-based-research-field&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94learning-based-research-field&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Few-shot Imitation Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-few-shot-imitation-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-few-shot-imitation-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：Behavior Cloning、DAgger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当前主流方法&lt;/strong&gt;：ACT、Diffusion Policy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。&lt;/p&gt;
&lt;h3&gt;2. Robot Foundation Model&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-foundation-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-foundation-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。
目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenVLA&lt;/strong&gt;：第一个开源且易于follow的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi0 / Pi0.5&lt;/strong&gt;：目前公认最work的VLA，10K+ hours teleop data训练的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraspVLA&lt;/strong&gt;：基于纯仿真数据的抓取任务的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有少量工作没有借助VLM&lt;/strong&gt;，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Sim-to-Real Reinforcement Learning (Distillation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-sim-to-real-reinforcement-learning-distillation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-sim-to-real-reinforcement-learning-distillation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。&lt;/p&gt;
&lt;p&gt;目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。&lt;/p&gt;
&lt;p&gt;核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。&lt;/p&gt;
&lt;h3&gt;4. Real-World Reinforcement Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-real-world-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-real-world-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-world RL 指直接在现实环境中进行探索式学习。&lt;/p&gt;
&lt;p&gt;这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**从零开始的真实世界强化学习：**Hil-Serl&lt;/li&gt;
&lt;li&gt;**基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. World Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-world-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-world-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代表性工作包括 Dreamer 系列&lt;/strong&gt;（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。&lt;/p&gt;
&lt;h2&gt;六、相关领域&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六相关领域&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Graphics&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-graphics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-graphics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。&lt;/p&gt;
&lt;p&gt;**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等
**Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。
**系统性学习图形学推荐课程：**Games 101, 103&lt;/p&gt;
&lt;h3&gt;2. Hardware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tele-operation（遥操作）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。
**主从臂系统：**如 Gello，实现高精度的力控遥操作。
**可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。
&lt;strong&gt;Sensors（传感器）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。&lt;/li&gt;
&lt;li&gt;**Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。&lt;/li&gt;
&lt;li&gt;**Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。&lt;/li&gt;
&lt;li&gt;**Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mainstream Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-mainstream-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-mainstream-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Transformer&lt;/li&gt;
&lt;li&gt;Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Foundation Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-foundation-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-foundation-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。&lt;/li&gt;
&lt;li&gt;CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。&lt;/li&gt;
&lt;li&gt;VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. 3D Vision&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-3d-vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-3d-vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维生成与重建&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。&lt;/li&gt;
&lt;li&gt;**单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。&lt;/li&gt;
&lt;li&gt;**单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。&lt;/li&gt;
&lt;li&gt;**位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维表示&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。&lt;/li&gt;
&lt;li&gt;**点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。&lt;/li&gt;
&lt;li&gt;**Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维理解&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>具身调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-14/</guid>
      <description>
        
        
        &lt;h1&gt;具身调研&lt;/h1&gt;&lt;p&gt;对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。&lt;/p&gt;
&lt;p&gt;本调研主要基于**&lt;a href=&#34;https://github.com/jiangranlv/embodied-ai-start&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PKU EPIC Lab&lt;/a&gt;&lt;strong&gt;、&lt;/strong&gt;&lt;a href=&#34;https://github.com/TianxingChen/Embodied-AI-Guide&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lumina具身智能社区&lt;/a&gt;**&lt;/p&gt;
&lt;h2&gt;一、基础概念 (Basic Concepts)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一基础概念-basic-concepts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1、 什么是具身智能&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-什么是具身智能&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。&lt;/p&gt;
&lt;p&gt;典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心特征：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有多模态感知能力（视觉、触觉、语音等）&lt;/li&gt;
&lt;li&gt;能够执行动作并影响环境&lt;/li&gt;
&lt;li&gt;学习可以通过与环境交互而不仅仅是被动监督完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 具身智能与其他AI的区别&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-具身智能与其他ai的区别&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96ai%e7%9a%84%e5%8c%ba%e5%88%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;如何scale up机器人数据？&lt;/strong&gt; 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？&lt;/strong&gt; 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 研究具身智能的核心原则 (Core Principles)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-研究具身智能的核心原则-core-principles&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；&lt;/p&gt;
&lt;p&gt;但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）&lt;/p&gt;
&lt;p&gt;必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？&lt;/p&gt;
&lt;h2&gt;二、AI and Robotics Basis&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二ai-and-robotics-basis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cai-and-robotics-basis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-Embodied-AI&lt;/strong&gt;
王鹤老师《具身智能导论》，找找类似课程替代&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-CV&lt;/strong&gt;
Stanford CS231N&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning (CS285)&lt;/strong&gt;
Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;三、研究平台与工具&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三研究平台与工具&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Simulation Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-simulation-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-simulation-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;2. Robot Platform&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-platform&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-platform&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;3. Daily ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-daily-arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-daily-arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了：
具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jiangranlv/robotics_arXiv_daily&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jiangranlv/robotics_arXiv_daily&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;四、Research Field on Robots&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四research-field-on-robots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bresearch-field-on-robots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Grasping&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-grasping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-grasping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。&lt;/p&gt;
&lt;p&gt;常见任务有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single object grasping（单物体抓取）&lt;/strong&gt;：抓取一个物体，物体通常放在桌子上。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clutter scene grasping（堆叠场景抓取）&lt;/strong&gt;：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional grasping（带语义抓取）&lt;/strong&gt;：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用机械手末端有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suction cup（吸盘）&lt;/strong&gt;：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel gripper（平行夹抓）&lt;/strong&gt;：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-fingered hand（多指手）&lt;/strong&gt;，又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-loop methods（开环执行）&lt;/strong&gt;：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closed-loop methods（闭环执行）&lt;/strong&gt;：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Manipulation&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-manipulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-manipulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。&lt;/p&gt;
&lt;p&gt;**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）
**Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。
**Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。
**Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。
**Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。
**Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同&lt;/p&gt;
&lt;h3&gt;3. Navigation(NOW)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-navigationnow&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-navigationnow&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。&lt;/p&gt;
&lt;p&gt;常见任务包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Point Goal Navigation (PointNav)&lt;/strong&gt;：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Goal Navigation (ObjectNav)&lt;/strong&gt;：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Navigation (VLN)&lt;/strong&gt;：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embodied Question Answering (EQA)&lt;/strong&gt;：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：机器人持续感知并跟随动态目标（如人或移动物体）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map-based Navigation&lt;/strong&gt;：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompting-Large-Model Navigation&lt;/strong&gt;：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video-based VLM Navigation&lt;/strong&gt;：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unified Embodied Navigation&lt;/strong&gt;：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Locomotion&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-locomotion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-locomotion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。&lt;/p&gt;
&lt;p&gt;技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning from manually designed reward&lt;/strong&gt; (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from human data&lt;/strong&gt; (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】
如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -&amp;gt; whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下一个阶段会涉及到一点除了control之外的东西，就是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引入【视觉】实现户外自主化（perceptive locomotion）&lt;/strong&gt;；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入【物体】实现loco-manipulation&lt;/strong&gt;；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对上述两种task的组合&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调【语义的泛化性】&lt;/strong&gt;，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调一些特殊的capability&lt;/strong&gt;（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;五、Learning based Research Field&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五learning-based-research-field&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94learning-based-research-field&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Few-shot Imitation Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-few-shot-imitation-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-few-shot-imitation-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：Behavior Cloning、DAgger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当前主流方法&lt;/strong&gt;：ACT、Diffusion Policy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。&lt;/p&gt;
&lt;h3&gt;2. Robot Foundation Model&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-foundation-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-foundation-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。
目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenVLA&lt;/strong&gt;：第一个开源且易于follow的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi0 / Pi0.5&lt;/strong&gt;：目前公认最work的VLA，10K+ hours teleop data训练的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraspVLA&lt;/strong&gt;：基于纯仿真数据的抓取任务的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有少量工作没有借助VLM&lt;/strong&gt;，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Sim-to-Real Reinforcement Learning (Distillation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-sim-to-real-reinforcement-learning-distillation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-sim-to-real-reinforcement-learning-distillation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。&lt;/p&gt;
&lt;p&gt;目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。&lt;/p&gt;
&lt;p&gt;核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。&lt;/p&gt;
&lt;h3&gt;4. Real-World Reinforcement Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-real-world-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-real-world-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-world RL 指直接在现实环境中进行探索式学习。&lt;/p&gt;
&lt;p&gt;这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**从零开始的真实世界强化学习：**Hil-Serl&lt;/li&gt;
&lt;li&gt;**基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. World Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-world-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-world-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代表性工作包括 Dreamer 系列&lt;/strong&gt;（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。&lt;/p&gt;
&lt;h2&gt;六、相关领域&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六相关领域&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Graphics&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-graphics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-graphics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。&lt;/p&gt;
&lt;p&gt;**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等
**Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。
**系统性学习图形学推荐课程：**Games 101, 103&lt;/p&gt;
&lt;h3&gt;2. Hardware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tele-operation（遥操作）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。
**主从臂系统：**如 Gello，实现高精度的力控遥操作。
**可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。
&lt;strong&gt;Sensors（传感器）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。&lt;/li&gt;
&lt;li&gt;**Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。&lt;/li&gt;
&lt;li&gt;**Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。&lt;/li&gt;
&lt;li&gt;**Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mainstream Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-mainstream-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-mainstream-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Transformer&lt;/li&gt;
&lt;li&gt;Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Foundation Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-foundation-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-foundation-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。&lt;/li&gt;
&lt;li&gt;CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。&lt;/li&gt;
&lt;li&gt;VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. 3D Vision&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-3d-vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-3d-vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维生成与重建&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。&lt;/li&gt;
&lt;li&gt;**单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。&lt;/li&gt;
&lt;li&gt;**单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。&lt;/li&gt;
&lt;li&gt;**位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维表示&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。&lt;/li&gt;
&lt;li&gt;**点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。&lt;/li&gt;
&lt;li&gt;**Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维理解&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>社会意识的导航模型</title>
      <link>http://localhost:1313/blog/2025/2025-11-29-social-nav/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-29-social-nav/</guid>
      <description>
        
        
        &lt;p&gt;起因是在小红书上刷到了这一篇2025年11月的新文章&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/social-nav.jpg&#34; alt=&#34;Social Navigation&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;结果却搜到了 &lt;strong&gt;[ICRA 2025] From Cognition to Precognition: A Future-Aware Framework for Social Navigation&lt;/strong&gt;，于是误闯天家到了 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Robot Social Navigation&lt;/a&gt; 的领域。&lt;/p&gt;
&lt;h2&gt;什么是 Social Navigation？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;什么是-social-navigation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%bb%80%e4%b9%88%e6%98%af-social-navigation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Social Navigation（社会导航）&lt;/strong&gt; 的核心思想是 &lt;strong&gt;&amp;ldquo;以人为本&amp;rdquo;&lt;/strong&gt;。它要求机器人不仅仅把人类当作需要避开的障碍物，而是能够理解并尊重人类的社会规范与个人空间，最终实现&lt;strong&gt;自然、和谐、无感知压迫&lt;/strong&gt;的共同空间使用。例如，在走廊中与人迎面相遇时，机器人会像人一样靠右行驶；当需要穿过一群人时，它会寻找合适的时机和路径，而不是生硬地&amp;quot;切开&amp;quot;人群。&lt;/p&gt;
&lt;h2&gt;技术对比：Social Navigation vs LOVON vs VLN&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;技术对比social-navigation-vs-lovon-vs-vln&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%8a%80%e6%9c%af%e5%af%b9%e6%af%94social-navigation-vs-lovon-vs-vln&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;特性维度&lt;/th&gt;
          &lt;th&gt;Social Navigation (社会导航)&lt;/th&gt;
          &lt;th&gt;LOVON (腿部开放词汇物体导航)&lt;/th&gt;
          &lt;th&gt;VLN (视觉语言导航)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心目标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;安全、舒适、符合社会规范地在人类共享空间中导航&lt;/td&gt;
          &lt;td&gt;在开放世界中，根据物体名称，自主搜索并导航到指定物体&lt;/td&gt;
          &lt;td&gt;根据自然语言指令，在环境中执行导航任务 (如&amp;quot;去厨房拿杯水&amp;quot;)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;环境特点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;动态、拥挤的人类环境，充满不确定性&lt;/td&gt;
          &lt;td&gt;非结构化的开放环境，地形复杂，目标物体可能被遮挡或距离遥远&lt;/td&gt;
          &lt;td&gt;通常基于仿真器（如Habitat, AI2-THOR），环境可以是静态的，也引入动态人类&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;关键输入&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;人类的位置、运动轨迹、群体行为、社会规范&lt;/td&gt;
          &lt;td&gt;目标物体的文本名称 (如 &amp;ldquo;chair&amp;rdquo;)、机器人视觉传感器数据&lt;/td&gt;
          &lt;td&gt;详尽的自然语言指令、机器人视觉传感器数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;技术侧重点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;行人轨迹预测、社交力模型、强化学习策略、舒适度与安全性评估&lt;/td&gt;
          &lt;td&gt;开放词汇目标检测、大语言模型任务分解、腿部机器人运动控制、抗运动模糊&lt;/td&gt;
          &lt;td&gt;视觉-语言对齐、指令理解、跨模态推理、路径规划&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;典型输出/动作&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;避让、保持社交距离、绕行、调整速度、非语言沟通&lt;/td&gt;
          &lt;td&gt;朝向目标物体的运动控制命令 (如速度、方向)，处理复杂地形&lt;/td&gt;
          &lt;td&gt;导航动作 (如&amp;quot;左转&amp;quot;、&amp;ldquo;前进1米&amp;rdquo;、&amp;ldquo;停止&amp;rdquo;)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心挑战&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;对人类意图的预测、复杂社会规则的建模与量化、安全性、舒适感&lt;/td&gt;
          &lt;td&gt;长时序任务规划、动态模糊下的稳定感知、复杂地形下的稳定移动、开放词汇识别泛化能力&lt;/td&gt;
          &lt;td&gt;指令与环境的关联、未知环境泛化、长指令理解、跨模态表示学习&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;学术社区与行业洞察&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;学术社区与行业洞察&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%a6%e6%9c%af%e7%a4%be%e5%8c%ba%e4%b8%8e%e8%a1%8c%e4%b8%9a%e6%b4%9e%e5%af%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;然后去&lt;a href=&#34;http://xhslink.com/o/6M94ZS8vHHm&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;学术社区（迫真）&lt;/a&gt;上搜索了一下，这里 &lt;strong&gt;seven17&lt;/strong&gt; 这位大佬也在2025年11月16-17给出了自己作为人形公司 &lt;strong&gt;SLAM 面试官&lt;/strong&gt;对业界人形机器人在研究的算法的一些经验，非常有参考意义。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;有一说一小红书真的比很多像是CSDN之类的更好的学术交流平台&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;我就很赞同这里在小红书的某个 Ask Me Anything 上看到的&lt;strong&gt;港科广的梁老师&lt;/strong&gt;的话：&lt;/p&gt;
&lt;div style={{display: &#39;flex&#39;, justifyContent: &#39;space-between&#39;, gap: &#39;10px&#39;}}&gt;
  &lt;div style={{flex: 1}}&gt;
    &lt;img src=&#34;http://localhost:1313/blog/2025/gkg-liang1.jpg&#34; alt=&#34;港科广梁老师观点1&#34; style={{width: &#39;100%&#39;}} /&gt;
  &lt;/div&gt;
  &lt;div style={{flex: 1}}&gt;
    &lt;img src=&#34;http://localhost:1313/blog/2025/gkg-liang2.jpg&#34; alt=&#34;港科广梁老师观点2&#34; style={{width: &#39;100%&#39;}} /&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;相关竞赛与研讨会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关竞赛与研讨会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e7%ab%9e%e8%b5%9b%e4%b8%8e%e7%a0%94%e8%ae%a8%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;活动名称&lt;/th&gt;
          &lt;th&gt;主要关联会议&lt;/th&gt;
          &lt;th&gt;活动形式&lt;/th&gt;
          &lt;th&gt;核心侧重点&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RoboSense机器感知挑战赛&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;IROS 2025 (官方认证竞赛)&lt;/td&gt;
          &lt;td&gt;竞赛&lt;/td&gt;
          &lt;td&gt;在动态人群环境中，使机器人的导航行为符合人类的社会规范。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Advances in Social Robot Navigation研讨会&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;ICRA 2025&lt;/td&gt;
          &lt;td&gt;研讨会&lt;/td&gt;
          &lt;td&gt;探讨社交机器人导航在规划、人机交互等领域的最新进展，并包含基准测试挑战。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;RoboSense 挑战赛&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;robosense-挑战赛&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#robosense-%e6%8c%91%e6%88%98%e8%b5%9b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;RoboSense挑战赛&lt;/strong&gt; 是 &lt;strong&gt;IROS 2025&lt;/strong&gt; 的官方认证竞赛，它设置了专门的&lt;strong&gt;社交导航赛道&lt;/strong&gt;，旨在解决机器人在真实动态环境中的导航问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;任务目标：&lt;/strong&gt; 参赛者需要开发一个基于 &lt;strong&gt;RGB-D 输入&lt;/strong&gt;的移动机器人导航模型。该模型的核心任务是让机器人在&lt;strong&gt;不影响周围人类行为&lt;/strong&gt;的前提下完成导航，并使其行为符合人类的社会规范，例如主动避让、保持合适的社交距离等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战与评测：&lt;/strong&gt; 除了衡量导航成功率和路径效率，比赛还特别引入了&lt;strong&gt;个人空间合规性（PSC）&lt;strong&gt;和&lt;/strong&gt;人机碰撞次数（H-Coll）&lt;strong&gt;等指标，专门用于量化机器人行为的&lt;/strong&gt;&amp;ldquo;社交友好度&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;前沿技术：&lt;/strong&gt; 该赛道推荐的基线模型（Baseline）是 &lt;strong&gt;Falcon&lt;/strong&gt;，这是一个由&lt;strong&gt;港科广和港科大联合提出&lt;/strong&gt;的新算法，它通过将&lt;strong&gt;轨迹预测算法融入强化学习框架&lt;/strong&gt;，让机器人能够预测行人未来的移动路径，从而实现&lt;strong&gt;更超前、更安全的规划&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ICRA 2025 研讨会&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;icra-2025-研讨会&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#icra-2025-%e7%a0%94%e8%ae%a8%e4%bc%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;除了竞争激烈的比赛，&lt;strong&gt;ICRA 的&amp;quot;Advances in Social Robot Navigation&amp;quot;研讨会&lt;/strong&gt;则是深入了解该领域学术研究和前沿发展的绝佳平台。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;活动形式：&lt;/strong&gt; 这是一个学术研讨会，会邀请领域内的专家进行讲座和专题讨论。同时，它也主办 &lt;strong&gt;Arena 4.0 挑战赛&lt;/strong&gt;，旨在为不同的社交导航策略建立基准和评测体系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心议题：&lt;/strong&gt; 研讨会关注如何使机器人的导航行为&lt;strong&gt;更易于理解、更符合社交场景&lt;/strong&gt;。探讨的技术方向包括运动任务规划、&lt;strong&gt;基础模型的应用&lt;/strong&gt;、人机交互策略等。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;我的研究计划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;我的研究计划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%88%91%e7%9a%84%e7%a0%94%e7%a9%b6%e8%ae%a1%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我打算接下来的核心往 &lt;strong&gt;Social Navigation&lt;/strong&gt; 上面靠，这里很符合&lt;strong&gt;以人为本的设计特点&lt;/strong&gt;，而 &lt;strong&gt;LOVON&lt;/strong&gt; 也确实面临这一困境。也如梁老师所言，这是个&lt;strong&gt;容易入门具身的领域&lt;/strong&gt;。可惜这个比赛在这个时候已经结束了，下面计划的第一步是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;研读 Falcon 这个 baseline&lt;/strong&gt;（也就是上面提到的 ICRA 2025 中稿文章）&lt;/li&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://robosense2025.github.io/track2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robosense&lt;/a&gt; 提供的 GitHub 代码和数据集去&lt;strong&gt;复现基线&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;参考排行榜的改进去思考参赛者解决的问题集中在哪里，又是如何进行的&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;相关资源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关资源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%b5%84%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Resource&lt;/th&gt;
          &lt;th&gt;Link&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;GitHub Repository&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/robosense2025/track2&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/robosense2025/track2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Baseline code and setup instructions&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Dataset&lt;/td&gt;
          &lt;td&gt;HuggingFace Dataset&lt;/td&gt;
          &lt;td&gt;Dataset with training and test splits&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Baseline Model&lt;/td&gt;
          &lt;td&gt;Pre-Trained Model&lt;/td&gt;
          &lt;td&gt;Weights of the baseline model&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Registration&lt;/td&gt;
          &lt;td&gt;Google Form (Closed on August 15th)&lt;/td&gt;
          &lt;td&gt;Team registration for the challenge&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Evaluation Server&lt;/td&gt;
          &lt;td&gt;EvalAI Platform&lt;/td&gt;
          &lt;td&gt;Online evaluation platform&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;HuggingFace 上的热门研究&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;huggingface-上的热门研究&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#huggingface-%e4%b8%8a%e7%9a%84%e7%83%ad%e9%97%a8%e7%a0%94%e7%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在 huggingface 上按 trending 搜索 social navigation 的&lt;a href=&#34;https://huggingface.co/papers/trending?q=social&amp;#43;navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;结果&lt;/a&gt;如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;论文标题&lt;/th&gt;
          &lt;th&gt;核心工作摘要&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SACSoN&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过最小化机器人对行人行为的**&amp;ldquo;反事实扰动&amp;rdquo;&lt;strong&gt;，学习一种&lt;/strong&gt;不打扰人类的导航策略**。其关键在于使用大量真实人机交互数据进行训练。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Exploiting Proximity-Aware Tasks&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出&lt;strong&gt;邻近感知任务&lt;/strong&gt;，通过让策略理解即时和未来的碰撞危险，为强化学习导航策略注入&lt;strong&gt;常识性社交行为&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SELFI&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出一种&lt;strong&gt;在线自学习方法&lt;/strong&gt;，在预训练策略的基础上，利用在线模型无关的强化学习进行快速微调，使机器人能根据实际经验持续改进社交导航行为。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SocialNav-SUB&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;引入了首个用于评估&lt;strong&gt;视觉语言模型（VLM）&lt;strong&gt;在社交导航场景中理解能力的基准，发现当前 VLM 在&lt;/strong&gt;空间、时空和社交推理&lt;/strong&gt;方面仍有明显不足。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;OLiVia-Nav&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;将&lt;strong&gt;视觉语言模型与在线终身学习框架&lt;/strong&gt;结合，通过独特的蒸馏方法让轻量级 VLM 直接理解社交和环境上下文，并规划符合社交规范的轨迹。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Habitat 3.0&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;推出了一个支持&lt;strong&gt;人、虚拟化身和机器人协同&lt;/strong&gt;的模拟平台，用于研究社交导航等协作任务，并提供了&lt;strong&gt;人类在环的基础设施&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;GOAT&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;通用导航系统&lt;/strong&gt;，能够处理多模态目标，并通过持续构建实例感知的语义记忆，实现&lt;strong&gt;终身学习和跨平台部署&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;GRUtopia&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;构建了一个&lt;strong&gt;大规模的模拟交互式3D社会&lt;/strong&gt;，包含多样化的场景和由 &lt;strong&gt;LLM 驱动的虚拟角色&lt;/strong&gt;，用于支持社交移动导航等具身AI任务的训练与评估。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RoboSense&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;大规模的以自我为中心的多模态数据集&lt;/strong&gt;，专注于拥挤和非结构化环境中的感知与导航，为近场场景理解提供丰富标注。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Social NCE&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过&lt;strong&gt;对比学习&lt;/strong&gt;来提升运动表示的社交感知能力，显式地建模危险负样本，以此降低轨迹预测和行为克隆中的碰撞率。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;DriVLMe&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;探索了基于&lt;strong&gt;视频语言模型的自动驾驶智能体&lt;/strong&gt;，通过模拟环境和真实人类对话进行训练，旨在实现与人类的自然有效沟通。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;EPO&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出&lt;strong&gt;显式策略优化方法&lt;/strong&gt;，利用多轮强化学习和自我博弈来提升大语言模型在社交对话等任务中的战略推理能力。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;EmbodiedEval&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个&lt;strong&gt;统一的、交互式的基准&lt;/strong&gt;，用于全面评估多模态大模型在具身任务（如导航、社交交互）中的能力。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SocialEval&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提出了一个评估&lt;strong&gt;大语言模型社交智能的双语基准&lt;/strong&gt;，通过叙事脚本从结果和过程两个维度评估模型的人际交往能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;研究趋势分析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究趋势分析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e8%b6%8b%e5%8a%bf%e5%88%86%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;从这些论文可以看出，社交导航领域的研究呈现出一些明显的趋势和重点方向：&lt;/p&gt;
&lt;h3&gt;1. 从&amp;quot;避障&amp;quot;到&amp;quot;避人&amp;quot;，再到&amp;quot;不扰人&amp;quot;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-从避障到避人再到不扰人&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%8e%e9%81%bf%e9%9a%9c%e5%88%b0%e9%81%bf%e4%ba%ba%e5%86%8d%e5%88%b0%e4%b8%8d%e6%89%b0%e4%ba%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;像 &lt;strong&gt;SACSoN&lt;/strong&gt; 这样的工作，其目标已经超越了基础的安全避障，而是追求&lt;strong&gt;更高级的社交合规性&lt;/strong&gt;，希望机器人的存在和行为&lt;strong&gt;尽可能不改变人类的自然行为&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;2. 学习与规划的关键：预测与上下文理解&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-学习与规划的关键预测与上下文理解&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%ad%a6%e4%b9%a0%e4%b8%8e%e8%a7%84%e5%88%92%e7%9a%84%e5%85%b3%e9%94%ae%e9%a2%84%e6%b5%8b%e4%b8%8e%e4%b8%8a%e4%b8%8b%e6%96%87%e7%90%86%e8%a7%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;许多研究致力于让机器人更好地&lt;strong&gt;预测未来&lt;/strong&gt;（如行人轨迹）和&lt;strong&gt;理解环境上下文&lt;/strong&gt;（如社交规则）。&lt;strong&gt;Exploiting Proximity-Aware Tasks&lt;/strong&gt; 和 &lt;strong&gt;Social NCE&lt;/strong&gt; 都是通过不同的方式让模型内化对潜在危险和社交规范的理解。&lt;/p&gt;
&lt;h3&gt;3. 基础模型与终身学习成为新风向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-基础模型与终身学习成为新风向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e7%bb%88%e8%ba%ab%e5%ad%a6%e4%b9%a0%e6%88%90%e4%b8%ba%e6%96%b0%e9%a3%8e%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;OLiVia-Nav&lt;/strong&gt; 和 &lt;strong&gt;GOAT&lt;/strong&gt; 等论文清晰地展示了如何利用&lt;strong&gt;视觉语言模型（VLM）的先验知识&lt;/strong&gt;进行社交推理，并强调通过&lt;strong&gt;终身学习&lt;/strong&gt;使机器人能够适应不断变化的环境和新遇到的社交场景。&lt;/p&gt;
&lt;h3&gt;4. 对仿真、数据与评估的持续投入&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-对仿真数据与评估的持续投入&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e5%af%b9%e4%bb%bf%e7%9c%9f%e6%95%b0%e6%8d%ae%e4%b8%8e%e8%af%84%e4%bc%b0%e7%9a%84%e6%8c%81%e7%bb%ad%e6%8a%95%e5%85%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;高质量的仿真平台（&lt;strong&gt;Habitat 3.0&lt;/strong&gt;, &lt;strong&gt;GRUtopia&lt;/strong&gt;）、大规模数据集（&lt;strong&gt;RoboSense&lt;/strong&gt;）和专门的评估基准（&lt;strong&gt;SocialNav-SUB&lt;/strong&gt;, &lt;strong&gt;EmbodiedEval&lt;/strong&gt;, &lt;strong&gt;SocialEval&lt;/strong&gt;）是推动领域发展的&lt;strong&gt;关键基础设施&lt;/strong&gt;，这些工作为训练、测试和公平比较不同算法提供了坚实基础。&lt;/p&gt;
&lt;h2&gt;核心挑战与思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;核心挑战与思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%a0%b8%e5%bf%83%e6%8c%91%e6%88%98%e4%b8%8e%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;社会导航的终极目标&lt;/strong&gt;是实现&lt;strong&gt;安全、舒适、符合社会规范的人机共存与协作&lt;/strong&gt;。它关注的是导航行为的**&amp;ldquo;社交智能&amp;quot;和&amp;quot;礼仪&amp;rdquo;&lt;strong&gt;。相比之下，许多&lt;/strong&gt;视觉语言导航（VLN）&lt;strong&gt;或其变体（如 &lt;strong&gt;LOVON&lt;/strong&gt;）更侧重于理解指令、识别物体或地点，并完成具身的导航任务，其核心是&lt;/strong&gt;&amp;ldquo;完成任务&amp;quot;的准确性**。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大的难点在于&lt;/strong&gt;，它需要让机器人理解并量化人类社会中那些&lt;strong&gt;不言自明、动态变化的社交潜规则&lt;/strong&gt;。例如，如何定义并计算**&amp;ldquo;个人空间&amp;rdquo;&lt;strong&gt;？如何判断什么样的路径是&lt;/strong&gt;&amp;ldquo;优雅&amp;quot;而非&amp;quot;冒犯&amp;quot;的**？这与开放词汇任务中要求模型识别未曾见过的物体类别（如 &lt;strong&gt;LOVON&lt;/strong&gt;）相比，是不同类型和层次的挑战。&lt;strong&gt;开放词汇扩展了机器人的&amp;quot;知识面&amp;rdquo;，而社会导航则是在塑造机器人的&amp;quot;情商&amp;quot;和行为方式&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如说 &lt;strong&gt;Track2&lt;/strong&gt; 的工作，核心任务是让机器人学会在充满动态行人的室内环境中（如办公楼、商场），实现&lt;strong&gt;安全、高效且符合社会规范的导航&lt;/strong&gt;。不仅要求机器人成功到达目的地（成功率 &lt;strong&gt;SR&lt;/strong&gt;），还要求其行为**&amp;ldquo;像个有礼貌的人&amp;rdquo;&lt;strong&gt;，比如主动保持舒适的社交距离（个人空间合规性 &lt;strong&gt;PSC&lt;/strong&gt;）、避免碰撞（人类碰撞率 &lt;strong&gt;H-Coll&lt;/strong&gt;），并规划出高效的路径（路径长度加权成功率 &lt;strong&gt;SPL&lt;/strong&gt;）。赛事提供的基线模型是基于 &lt;strong&gt;Falcon 框架&lt;/strong&gt;，它通过融入对&lt;/strong&gt;行人未来轨迹的预测**，来让机器人实现更具前瞻性的导航决策。&lt;/p&gt;
&lt;h2&gt;未来方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;未来方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;而在 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;awesome系列&lt;/a&gt; 里，我们可以看到以下几个重要方向：&lt;/p&gt;
&lt;h3&gt;1. 融合基础模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-融合基础模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e8%9e%8d%e5%90%88%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;这是一个明显的趋势。探索如何利用&lt;strong&gt;大型语言模型（LLM）&lt;strong&gt;和&lt;/strong&gt;视觉语言模型（VLM）&lt;/strong&gt;，让机器人能够理解和遵从复杂、抽象的社会规则（例如，**&amp;ldquo;在拥挤处耐心跟随&amp;rdquo;**而不仅仅是&amp;quot;避开人群&amp;rdquo;），或者更好地解读人类的行为意图。&lt;/p&gt;
&lt;h3&gt;2. 提升仿真环境的真实性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-提升仿真环境的真实性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e6%8f%90%e5%8d%87%e4%bb%bf%e7%9c%9f%e7%8e%af%e5%a2%83%e7%9a%84%e7%9c%9f%e5%ae%9e%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;开发更先进的仿真平台（如持续更新的 &lt;strong&gt;Arena 系列&lt;/strong&gt;），模拟更复杂的人类行为（如&lt;strong&gt;突然驻足、群体交谈、协作避让&lt;/strong&gt;），这对于在低成本前提下验证算法的鲁棒性至关重要。&lt;/p&gt;
&lt;h3&gt;3. 增强算法的可解释性与信任度&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-增强算法的可解释性与信任度&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%a2%9e%e5%bc%ba%e7%ae%97%e6%b3%95%e7%9a%84%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e4%b8%8e%e4%bf%a1%e4%bb%bb%e5%ba%a6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;研究如何让机器人的导航决策过程对人类而言&lt;strong&gt;更透明、更容易理解&lt;/strong&gt;。例如，生成机器人为何选择某条路径的**&amp;ldquo;因果解释&amp;rdquo;**，这能极大地增强人类对机器人的信任，促进人机共处。&lt;/p&gt;
&lt;h3&gt;4. 深化人机交互研究&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-深化人机交互研究&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e6%b7%b1%e5%8c%96%e4%ba%ba%e6%9c%ba%e4%ba%a4%e4%ba%92%e7%a0%94%e7%a9%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;关注机器人的导航行为如何影响人类的感受和效率。通过用户研究，量化什么是让人感到**&amp;ldquo;舒适&amp;rdquo;、&amp;ldquo;自然&amp;rdquo;**的机器人行为，并将这些发现转化为算法设计的指导原则。&lt;/p&gt;
&lt;h3&gt;5. 应对极端与复杂场景&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-应对极端与复杂场景&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e5%ba%94%e5%af%b9%e6%9e%81%e7%ab%af%e4%b8%8e%e5%a4%8d%e6%9d%82%e5%9c%ba%e6%99%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;专注于解决更具挑战性的场景，例如&lt;strong&gt;重度遮挡&lt;/strong&gt;（在人群中&amp;quot;看不见&amp;quot;部分行人）、对**&amp;ldquo;不可预测&amp;quot;行人的识别与避让**，以及在密集人群中如何寻找安全路径。&lt;/p&gt;
&lt;h3&gt;研究方向与未来工作规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究方向与未来工作规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91%e4%b8%8e%e6%9c%aa%e6%9d%a5%e5%b7%a5%e4%bd%9c%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于 &lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Robot Social Navigation&lt;/a&gt; 的梳理，当前研究主要集中在以下几个方向：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;研究方向&lt;/th&gt;
          &lt;th&gt;具体未来工作规划&lt;/th&gt;
          &lt;th&gt;来源论文&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;模型泛化与适应性&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt;
          &lt;td&gt;开发轻量化VLM便于机器人部署；探索&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;多模态融合（视觉、语言、传感器）&lt;/strong&gt;&lt;/span&gt;；研究&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;在线/终身学习框架以适应新场景&lt;/strong&gt;&lt;/span&gt;；提升对&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;动态场景和长时序任务的理解与规划能力&lt;/strong&gt;&lt;/span&gt;。&lt;/td&gt;
          &lt;td&gt;VLM-Social-Nav, OLiVia-Nav, Following the Human Thread&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;场景理解与交互&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt;
          &lt;td&gt;研究人类轨迹预测与社交动态的&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;实时、精准推断&lt;/strong&gt;&lt;/span&gt;；探索&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;多智能体协同与群体行为建模&lt;/strong&gt;&lt;/span&gt;；开发更强大的&amp;lt;span style={{color: &amp;lsquo;red&amp;rsquo;}}&amp;gt;&lt;strong&gt;场景表征与上下文理解能力&lt;/strong&gt;&lt;/span&gt;，以处理复杂的社会规则。&lt;/td&gt;
          &lt;td&gt;Following the Human Thread, DiPCAN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;评估体系与伦理&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;建立更全面的评估指标（如引入&amp;quot;人类赋权&amp;quot;概念）；设计标准化基准测试与仿真环境；关注算法的公平性、透明度、隐私保护及人类舒适度等社会伦理影响。&lt;/td&gt;
          &lt;td&gt;In Search of a Lost Metric, Frontiers Research Topic&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; benchmark一般貌似都要自己提出一个，这样能增大工作量说是，像TrackVLA就是这样提出了一个EVTbench开源使用&lt;/p&gt;

&lt;/blockquote&gt;
&lt;h3&gt;基于Awesome系列的具体研究方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;基于awesome系列的具体研究方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9f%ba%e4%ba%8eawesome%e7%b3%bb%e5%88%97%e7%9a%84%e5%85%b7%e4%bd%93%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;根据&lt;a href=&#34;https://github.com/Shuijing725/awesome-robot-social-navigation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome系列&lt;/a&gt;的详细梳理，以下是从&lt;strong&gt;方法、数据集、评估&lt;/strong&gt;等多个维度总结的具体研究方向：&lt;/p&gt;
&lt;h4&gt;1. 基础模型在社交导航中的应用&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-基础模型在社交导航中的应用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%9c%a8%e7%a4%be%e4%ba%a4%e5%af%bc%e8%88%aa%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; VLM-Social-Nav, OLiVia-Nav, Social-LLaVA, CoNVOI, BehAV&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;轻量化部署：&lt;/strong&gt; 研究如何将大型**视觉语言模型（VLM）&lt;strong&gt;和&lt;/strong&gt;大语言模型（LLM）**蒸馏或微调到适合机器人实时部署的规模&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态融合：&lt;/strong&gt; 探索视觉、语言、传感器数据的深度融合，提升对复杂社交场景的理解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在线终身学习：&lt;/strong&gt; 开发能够持续适应新场景和人类行为的在线学习框架&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. 轨迹预测与场景理解&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-轨迹预测与场景理解&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e8%bd%a8%e8%bf%b9%e9%a2%84%e6%b5%8b%e4%b8%8e%e5%9c%ba%e6%99%af%e7%90%86%e8%a7%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Social LSTM, STGAT, From Cognition to Precognition, Following the Human Thread&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;实时轨迹预测：&lt;/strong&gt; 提升对人类未来移动路径的预测精度和实时性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;群体行为建模：&lt;/strong&gt; 研究多智能体协同、群体动态（如群体分裂与合并）的建模方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上下文理解：&lt;/strong&gt; 开发更强大的场景表征能力，理解复杂的社会规则和社交动态&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. 强化学习与混合方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-强化学习与混合方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%b7%b7%e5%90%88%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; SACSoN, SELFI, DR-MPC, Hybrid Approaches&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;奖励函数设计：&lt;/strong&gt; 探索如何将社交规范、舒适度等抽象概念量化为强化学习的奖励信号&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;混合方法：&lt;/strong&gt; 结合模型预测控制（MPC）、采样规划等传统方法与深度强化学习&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;样本效率：&lt;/strong&gt; 提升强化学习在社交导航任务中的样本效率，减少真实世界训练成本&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4. 可解释性与信任&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-可解释性与信任&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e4%b8%8e%e4%bf%a1%e4%bb%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Generating Causal Explanations, Explainability and Trust&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;因果解释：&lt;/strong&gt; 生成机器人导航决策的因果解释，增强人类对机器人的信任&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;透明度：&lt;/strong&gt; 研究如何让机器人的决策过程对人类更透明、更容易理解&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;5. 数据集与评估基准&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-数据集与评估基准&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e6%95%b0%e6%8d%ae%e9%9b%86%e4%b8%8e%e8%af%84%e4%bc%b0%e5%9f%ba%e5%87%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; SCAND, MuSoHu, SocNavBench, Arena系列, SocialNav-SUB&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;真实世界数据集：&lt;/strong&gt; 构建大规模、多模态的真实人机交互数据集&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真平台：&lt;/strong&gt; 开发更真实的仿真环境（如Arena 4.0, Habitat 3.0），支持复杂人类行为模拟&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标：&lt;/strong&gt; 设计更全面的评估体系，包括&lt;strong&gt;人类赋权&lt;/strong&gt;、个人空间合规性（PSC）、碰撞率（H-Coll）等&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;6. 用户研究与伦理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;6-用户研究与伦理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#6-%e7%94%a8%e6%88%b7%e7%a0%94%e7%a9%b6%e4%b8%8e%e4%bc%a6%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;相关论文：&lt;/strong&gt; Social Momentum, How Do Robot Experts Measure Success, Overlapping Social Navigation Principles&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;用户研究：&lt;/strong&gt; 通过用户研究量化什么是&amp;quot;舒适&amp;rdquo;、&amp;ldquo;自然&amp;quot;的机器人行为&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;伦理考量：&lt;/strong&gt; 关注算法的公平性、透明度、隐私保护及对人类舒适度的影响&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;研究计划建议&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究计划建议&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e8%ae%a1%e5%88%92%e5%bb%ba%e8%ae%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于以上分析，可以从以下几个方面思考研究计划：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;关注新兴的评估范式：&lt;/strong&gt; 像&lt;a href=&#34;http://export.arxiv.org/abs/2501.01539&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;人类赋权&amp;rdquo;&lt;/a&gt;这类新指标方兴未艾，如何量化、验证并将其有效融入强化学习奖励函数或模型预测控制的代价函数中，是一个很有潜力的方向。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;探索基础模型的高效应用：&lt;/strong&gt; 研究如何蒸馏或微调大型VLM/LMM，在保持其社交推理能力的同时，满足机器人平台对低延迟和低功耗的严苛要求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;致力于弥合仿真与现实差距：&lt;/strong&gt; 开发更好的**领域自适应（Domain Adaptation）&lt;strong&gt;技术或&lt;/strong&gt;元学习（Meta-Learning）**策略，让模型在离开仿真环境后能快速适应真实世界的复杂性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;挑战更复杂的社交场景：&lt;/strong&gt; 可以专注于研究机器人在密集人群、群组交互（如穿越一个正在交谈的群体）或长程、多目标导航任务中的表现，这些场景对现有技术提出了更高要求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;构建自己的评估基准：&lt;/strong&gt; 参考TrackVLA提出EVTbench的做法，开发针对特定场景或问题的标准化评估基准，这不仅能增加研究工作量，还能为领域提供有价值的工具。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>重新思考三维空间感知与具身导航决策在毕设中的研究点</title>
      <link>http://localhost:1313/blog/2025/2025-11-12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-12/</guid>
      <description>
        
        
        &lt;p&gt;首先，我们的Baseline —— LOVON (Legged Open-Vocabulary Object Navigator, 2025) 是一个在 Gym-Unreal（即 Gym-UnrealCV 风格的仿真 benchmark）上做了大规模仿真实验来验证其开阔词表目标搜索与导航能力；文中强调用虚幻环境来做长航时、动态目标搜索的系统验证（包括视觉抖动、目标短暂消失等问题）并在仿真里验证 Laplacian Variance Filtering、语言→运动模型等模块。也进行了真实腿式机器人（Unitree 系列）上的跨域验证以检验 sim→real。&lt;/p&gt;
&lt;!-- truncate --&gt;
&lt;h2&gt;思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们第一步要做的就是 Define Problem。若能有清晰的问题定位 + 合理指标 +实证结果 +对比分析，就有很大机会产出成果。一个很好的方法就是自问自答：&lt;/p&gt;
&lt;h3&gt;一、现状定位：用 LOVON 的方法在真机上效果很差──最关键的失败点是什么？“机器狗撞门框”？那是什么原因？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一现状定位用-lovon-的方法在真机上效果很差最关键的失败点是什么机器狗撞门框那是什么原因&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e7%8e%b0%e7%8a%b6%e5%ae%9a%e4%bd%8d%e7%94%a8-lovon-%e7%9a%84%e6%96%b9%e6%b3%95%e5%9c%a8%e7%9c%9f%e6%9c%ba%e4%b8%8a%e6%95%88%e6%9e%9c%e5%be%88%e5%b7%ae%e6%9c%80%e5%85%b3%e9%94%ae%e7%9a%84%e5%a4%b1%e8%b4%a5%e7%82%b9%e6%98%af%e4%bb%80%e4%b9%88%e6%9c%ba%e5%99%a8%e7%8b%97%e6%92%9e%e9%97%a8%e6%a1%86%e9%82%a3%e6%98%af%e4%bb%80%e4%b9%88%e5%8e%9f%e5%9b%a0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;是因为纯2D检测＋动作映射，缺乏深度／3D理解？&lt;/li&gt;
&lt;li&gt;是因为没有障碍物避障规划？&lt;/li&gt;
&lt;li&gt;是因为导航规划缺失，仅“向目标走”而不考虑路径？&lt;/li&gt;
&lt;li&gt;还是别的问题（如机器狗控制延迟、检测误差大、目标消失后无追踪策略）？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LOVON的原理，也就是视觉追踪的原理在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;目标提取与筛选（&lt;code&gt;_yolo_image_post_process&lt;/code&gt; 方法）&lt;/p&gt;
&lt;p&gt;先通过 &lt;code&gt;object_extractor&lt;/code&gt; 从任务指令（默认任务是 run to the person at speed of 0.36 m/s，提取目标为 “person”）中提取目标类别。YOLO 模型输出所有检测框后，只保留类别与提取目标一致的框，过滤无关目标。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;滑动窗口历史管理&lt;/p&gt;
&lt;p&gt;初始化 5 个历史缓存列表，分别存储目标类别、置信度、归一化坐标（xyn）、归一化宽高（whn）、像素坐标（xyxy）。每帧仅保留置信度最高的检测框，加入缓存列表；当列表长度超过 &lt;code&gt;lengthen_filter&lt;/code&gt; 时，删除最早的帧，维持窗口大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;追踪结果计算&lt;/p&gt;
&lt;p&gt;对缓存列表中的数据取平均值，得到平滑后的置信度、坐标和宽高。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;object_xyn[0]&lt;/code&gt; 是目标中心的水平归一化坐标（0~1，0 为左边界、0.5 为图像中心、1 为右边界）。&lt;/li&gt;
&lt;li&gt;若目标在图像中心（&lt;code&gt;xyn[0] ≈ 0.5&lt;/code&gt;）：机器狗沿前后方向运动（&lt;code&gt;v_x&lt;/code&gt; 按任务指令速度，如 0.36m/s，&lt;code&gt;v_y = 0&lt;/code&gt;，&lt;code&gt;w_z = 0&lt;/code&gt;），即 “往前走”。&lt;/li&gt;
&lt;li&gt;若目标偏左（&lt;code&gt;xyn[0] &amp;lt; 0.5&lt;/code&gt;）：&lt;code&gt;w_z&lt;/code&gt; 为正（顺时针旋转），同时 &lt;code&gt;v_x&lt;/code&gt; 降低，直到目标回到中心；偏右则相反。&lt;/li&gt;
&lt;li&gt;任务指令中的 “speed” 仅限制 &lt;code&gt;v_x&lt;/code&gt; 的最大值，而非强制固定 &lt;code&gt;v_x&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;统计列表中出现次数最多的目标类别，作为当前追踪目标（避免单帧误检影响）。若目标类别为 “NULL”（无有效检测），则重置追踪结果为默认值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;跟丢的判定标准&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单帧无检测：YOLO 未检测到与 &lt;code&gt;extracted_object&lt;/code&gt; 匹配的框 → 往历史缓存中添加 “NULL” 和 0 置信度。&lt;/li&gt;
&lt;li&gt;连续跟丢：当历史缓存（长度由 &lt;code&gt;lengthen_filter&lt;/code&gt; 控制）中 “NULL” 出现次数最多 → &lt;code&gt;most_common_object&lt;/code&gt; 变为 “NULL”，&lt;code&gt;avg_confidence&lt;/code&gt; 设为 0 → 判定为 “跟丢”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;motion_predictor&lt;/code&gt; 接收 “跟丢状态” 后，生成搜索型 &lt;code&gt;motion_vector&lt;/code&gt;：&lt;/p&gt;
&lt;p&gt;通常是「旋转搜索」：&lt;code&gt;v_x = 0&lt;/code&gt;（不前后动）、&lt;code&gt;v_y = 0&lt;/code&gt;（不左右动）、&lt;code&gt;w_z ≠ 0&lt;/code&gt;（缓慢旋转，扫描周围环境）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;机器狗撞门框的原因在于，这里现实环境的部署代码通过 YOLO 只识别到了目标但是没有理解环境与障碍物，而当人消失在门后时，最后一帧这个目标是在画面中心的，因此机器狗会往前走直到撞到门框，又或者笨笨的在门框那个位置旋转搜索。因为没有开源其仿真智能体的代码所以不知道模拟环境是怎么规避这个问题的&lt;/p&gt;
&lt;h3&gt;二、Gap 与定位：基于你上面的回答，问题在哪儿？用一句话描述这里的 gap（研究空白）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二gap-与定位基于你上面的回答问题在哪儿用一句话描述这里的-gap研究空白&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cgap-%e4%b8%8e%e5%ae%9a%e4%bd%8d%e5%9f%ba%e4%ba%8e%e4%bd%a0%e4%b8%8a%e9%9d%a2%e7%9a%84%e5%9b%9e%e7%ad%94%e9%97%ae%e9%a2%98%e5%9c%a8%e5%93%aa%e5%84%bf%e7%94%a8%e4%b8%80%e5%8f%a5%e8%af%9d%e6%8f%8f%e8%bf%b0%e8%bf%99%e9%87%8c%e7%9a%84-gap%e7%a0%94%e7%a9%b6%e7%a9%ba%e7%99%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;比如：“在足式机器人真实场景下，当前Open-vocab检测＋简单动作生成不能有效处理目标暂时丢失和复杂障碍物，导致跟踪／导航失败”。还是要聚焦 “障碍物避障” 或 “三维深度理解”？&lt;/p&gt;
&lt;p&gt;在足式机器人开放世界目标追踪任务中，现有基于纯 2D 视觉目标检测的追踪 - 运动映射方案，因缺乏环境障碍物感知与三维空间理解，且目标暂时丢失后仅采用无环境适配的旋转搜索策略，导致无法应对 “目标被遮挡 / 消失后因路径误判碰撞障碍物” 等真实场景挑战，难以实现稳健的长时追踪与运动控制（具体有没有3D视觉目标检测的论文工作，现在还没有做过调研）&lt;/p&gt;
&lt;h3&gt;三、重要性在哪里？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三重要性在哪里&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e9%87%8d%e8%a6%81%e6%80%a7%e5%9c%a8%e5%93%aa%e9%87%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对学术来说：为什么“足式机器人 + open-vocab目标导航/跟踪”值得研究？是否当前工作少？&lt;/li&gt;
&lt;li&gt;对应用来说：在真实环境（室内／复杂家具／光照变化）中，解决这个问题会带来什么改进？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于我来说，我还只是一个入门新手，打算通过本科毕设的机会，从3D世界理解和具身导航决策这个小角度切入来入门具身领域，所以我也说不清楚学术和应用上的重要性，只求发ccfb以上的paper证明自己&lt;/p&gt;
&lt;h3&gt;四、创新点初步想法？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四创新点初步想法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9b%e5%88%9b%e6%96%b0%e7%82%b9%e5%88%9d%e6%ad%a5%e6%83%b3%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过和导师学长们讨论列出了很多可能的优化方向（环境理解增强、分层决策升级），从这些中最可能做出论文中&lt;strong&gt;可量化贡献&lt;/strong&gt;的一个或两个是什么？
比如：“用 BEV 俯视地图 +轨迹预测 来增强 open-vocab 目标导航”；或者：“在足式机器人上验证视觉+深度融合检测在目标丢失场景下的跟踪稳定性提升”。哪一个更倾向？为什么？&lt;/p&gt;
&lt;p&gt;我不知道量化贡献的指标可以在哪里进一步优化啊，原因也可能在于我读的文献太少了，LOVON在仿真里所使用的指标为衡量 100 次实验中完成任务的平均步数、衡量 100 次实验中成功完成任务的比例两个，而如何去量化现实任务的指标与sim2real的优化，因为文献读的不多所以暂时我还不能回答这个问题&lt;/p&gt;
&lt;h3&gt;五、可量化指标与对比：要发论文，必须有可测量的结果，可以测量哪些指标？例如：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五可量化指标与对比要发论文必须有可测量的结果可以测量哪些指标例如&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94%e5%8f%af%e9%87%8f%e5%8c%96%e6%8c%87%e6%a0%87%e4%b8%8e%e5%af%b9%e6%af%94%e8%a6%81%e5%8f%91%e8%ae%ba%e6%96%87%e5%bf%85%e9%a1%bb%e6%9c%89%e5%8f%af%e6%b5%8b%e9%87%8f%e7%9a%84%e7%bb%93%e6%9e%9c%e5%8f%af%e4%bb%a5%e6%b5%8b%e9%87%8f%e5%93%aa%e4%ba%9b%e6%8c%87%e6%a0%87%e4%be%8b%e5%a6%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;目标被丢失的次数／恢复次数&lt;/li&gt;
&lt;li&gt;障碍物碰撞次数&lt;/li&gt;
&lt;li&gt;成功到达目标的比例&lt;/li&gt;
&lt;li&gt;路径长度／时间／效率&lt;/li&gt;
&lt;li&gt;跟踪保持时间／跟丢时间
− 真机 vs 仿真的差距（sim2real gap）
能够在实机上测这些指标吗？哪些可能无法测？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;六、实验平台／可行性：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六实验平台可行性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e5%ae%9e%e9%aa%8c%e5%b9%b3%e5%8f%b0%e5%8f%af%e8%a1%8c%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;已有的硬件是 Unitree Go2 足式机器人，这很好。你能控制机器人做什么动作（向前、转、停止、避障）？你能获取哪些传感器数据（RGB、深度、IMU、里程计）？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否有仿真实验环境（如 Gym-UnrealCV 场景）可以先做仿真再到实机？仿真与实机之间能记录相同指标吗？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;时间上本科毕设资源有限，预计能做多少场景／多少实验次数？这个对决定指标和可行性很重要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于硬件设备，要关注[官方SDK文档](&lt;a href=&#34;https://support.unitree.com/home/en/developer&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://support.unitree.com/home/en/developer&lt;/a&gt;）：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;一、动作控制能力&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基础运动控制
&lt;ul&gt;
&lt;li&gt;前进 / 后退 / 转向：通过Move(vx, vy, vyaw)函数直接设置线速度（vx/vy）和角速度（vyaw），支持相对于世界坐标系的运动控制。例如，Move(0.5, 0, 0)使机器人以 0.5m/s 速度向前移动。&lt;/li&gt;
&lt;li&gt;停止：调用StopMove()立即终止所有运动，进入静止状态。&lt;/li&gt;
&lt;li&gt;步态切换：通过SwitchGait(int d)选择不同步态（如小跑、踱步），或使用ContinuousGait(bool flag)启用连续步态模式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;高级动作与姿态调整
&lt;ul&gt;
&lt;li&gt;站立 / 坐下：StandUp()和Sit()实现起立和坐下动作，RecoveryStand()用于从侧翻状态恢复。&lt;/li&gt;
&lt;li&gt;身体姿态控制：Euler(roll, pitch, yaw)可调整机身倾角，BodyHeight(float height)动态改变离地高度。&lt;/li&gt;
&lt;li&gt;特技动作：支持FrontFlip()前空翻、FrontJump()跳跃等复杂动作（需硬件支持）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;避障功能
&lt;ul&gt;
&lt;li&gt;自主避障：通过ObstacleAvoidClient类启用避障模块，机器人可实时检测障碍物并调整路径。需调用EnableObstacleAvoidance()激活，并在移动时保持避障服务运行。&lt;/li&gt;
&lt;li&gt;传感器融合：避障依赖激光雷达（PRO/EDU 版）或深度相机（AIR 版）与 IMU 数据融合，实现动态环境下的安全导航。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;二、传感器数据获取&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉传感器
&lt;ul&gt;
&lt;li&gt;RGB 图像：通过 ROS2 话题/camera/image_raw获取 720P/1080P 实时视频流，支持 WebRTC 低延迟传输。&lt;/li&gt;
&lt;li&gt;深度数据：PRO/EDU 版搭载 4D 激光雷达（L1），可输出 360°×90° 点云数据（/go2/camera/depth）；AIR 版通过 Intel RealSense D435i 深度相机提供毫米级深度信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;惯性测量单元（IMU）
&lt;ul&gt;
&lt;li&gt;原始数据：通过 ROS2 话题/imu/data获取加速度（a_x, a_y, a_z）、角速度（ω_x, ω_y, ω_z）和四元数姿态（q_w, q_x, q_y, q_z）。&lt;/li&gt;
&lt;li&gt;坐标系转换：SDK 提供工具函数处理不同框架下的四元数顺序（如 Isaac Gym 与 Isaac Sim 的差异）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;里程计与定位
&lt;ul&gt;
&lt;li&gt;状态估计：通过激光雷达 + IMU 融合（如 LIO-SAM 算法）或腿部运动学模型（关节编码器数据）实现里程计输出。ROS2 话题/odom提供机器人位姿（x, y, θ）和速度信息。&lt;/li&gt;
&lt;li&gt;精度优化：紧耦合 LiDAR-IMU - 腿部里程计系统可在无特征环境下实现亚米级定位精度，在线学习机制适应负载和地形变化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他传感器
&lt;ul&gt;
&lt;li&gt;关节状态：实时获取 12 个关节的角度、角速度和扭矩（/joint_states），支持电机健康监测。&lt;/li&gt;
&lt;li&gt;足端力反馈：PRO/EDU 版配备足端力传感器（F_z），用于复杂地形下的步态调整。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于仿真环境，我有本地的Gym-Unrealcv仿真场景，但是苦恼于LOVON没有开源其仿真代码所以搁置着，不清楚下一步是根据部署代码反推仿真代码还是换一个仿真环境如MatterPort3D
时间本身还是比较充裕的，到开题答辩之前至少有1个月时间&lt;/p&gt;
&lt;h2&gt;研究现状&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究现状&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e7%8e%b0%e7%8a%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;维度&lt;/th&gt;
          &lt;th&gt;内容总结&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;基线模型&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON（LoVi: Open-vocabulary Visual Navigation and Tracking）在仿真中近乎完美（≈100% success rate），但在真实环境严重失效。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心问题&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON 只用 YOLO 的 2D 框坐标来做“视觉 → 动作”映射，没有任何 3D 环境建模或避障机制。目标消失（如进门）时，机器人仍执行“往前走”动作 → 撞门框。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;可用硬件&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Unitree Go2（有RGB、深度、IMU、里程计、足端力传感器）。具备基本避障API、Move(vx,vy,vyaw)控制接口。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;仿真环境&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;有Gym-UnrealCV，但缺少LOVON仿真智能体代码。可能考虑复刻或转向MatterPort3D。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;研究目标雏形&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;希望提升LOVON从2D视觉到更稳健3D环境理解（environment understanding + navigation fusion）的能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;问题定义&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题定义&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e5%ae%9a%e4%b9%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;当前的 open-vocabulary 视觉追踪方法（如 LOVON）在仿真中表现优异，但在真实足式机器人环境中严重退化，其原因在于缺乏对三维环境几何与障碍物的建模能力。
其技术设计恰好规避了仿真环境的局限性，同时最大化了自身优势，具体体现在 3 个 “无冲突”：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “真实场景的 3D 感知需求”，纯 2D 视觉足够&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真环境中，目标的 “2D 图像坐标” 与 “实际空间位置” 完全对齐（如虚拟场景中 xyn=0.5 即代表物理上的正前方，无门框等 3D 遮挡物），无需深度信息即可判断路径是否可行。而 LOVON 的核心是 “2D 视觉 + 运动向量映射”，恰好适配这种需求，无需额外的 3D 深度理解模块。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “不可控干扰”，搜索策略高效&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真中的 “目标丢失” 仅为 “目标移出 90 度扇形视野”（可通过旋转搜索快速重新捕获），无真实场景的 “目标被门框完全遮挡”“机器人被碰撞” 等不可控干扰。LOVON 的旋转搜索策略（vx=0、w_z≠0）在仿真中能高效覆盖视野，而不会像真实场景那样因 “旋转时忽略障碍物” 导致碰撞。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真数据与模型训练 “高度同源”&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真使用的目标类别（背包、椅子、行人）、运动速度（0.3~0.7m/s）、场景光照均与 LOVON 的训练数据集（100 万样本，摘要 1）高度匹配：IOE 对 “椅子”“行人” 的类别映射无误差，L2MM 的运动预测参数（如 β=10）也针对仿真场景校准（摘要 3），避免了真实场景中 “未见过的目标形态”“突发速度变化” 导致的误差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当目标被暂时遮挡（如进入门后）或在复杂结构环境中移动时，机器人仅凭2D像素坐标进行动作决策，无法有效区分“自由空间”与“障碍区域”，导致运动策略失效（如撞门、原地旋转）。
因此，本研究旨在探索一种融合3D环境理解的目标跟踪与导航方法，在保持LOVON开放词汇指令能力的前提下，提高其在真实环境中的鲁棒性与安全性。&lt;/p&gt;
&lt;h2&gt;研究方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;方向&lt;/th&gt;
          &lt;th&gt;名称&lt;/th&gt;
          &lt;th&gt;思路简述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;A. 环境理解增强（BEV / Depth / 3D Occupancy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;给LOVON加“视觉深度感知”，即在YOLO检测的基础上，通过深度图重投影到3D坐标系或BEV平面，建立占用图。再利用该图进行避障或规划。&lt;/td&gt;
          &lt;td&gt;你能做仿真+实机对比，提出一种“轻量级3D-aware追踪方法”。 → 投稿到 &lt;strong&gt;IROS/ICRA workshop 或 CCF-C AI Robotics会议&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;B. 跟踪 + 导航分层融合（Hierarchical Policy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;把“跟踪”和“导航”分成两个层次：高层目标预测、低层路径规划。你可以用简单预测（如卡尔曼滤波预测目标短期轨迹）+ BEV局部避障（A*或DWA）。&lt;/td&gt;
          &lt;td&gt;可以与LOVON对比“复杂场景成功率”→ 写出完整paper。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;选择 A + B 结合的小主题：“基于3D视觉感知与分层导航策略的开放词汇足式机器人目标追踪”(但这个一听就感觉不少人做过类似的课题非常卷)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;含义&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;任务层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Success Rate (SR)&lt;/td&gt;
          &lt;td&gt;机器人在有限步数内到达目标的比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Average Steps (AS)&lt;/td&gt;
          &lt;td&gt;成功任务平均步数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Collision Rate (CR)&lt;/td&gt;
          &lt;td&gt;发生障碍碰撞的任务比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;视觉层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Target Loss Time (TLT)&lt;/td&gt;
          &lt;td&gt;目标丢失后重新识别的平均时间&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Tracking Stability (TS)&lt;/td&gt;
          &lt;td&gt;目标检测框抖动方差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Sim2Real 层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;ΔSR (Sim→Real)&lt;/td&gt;
          &lt;td&gt;仿真与实机成功率差距&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;效率指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;FPS / Latency&lt;/td&gt;
          &lt;td&gt;模型推理帧率与系统延迟&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;安全指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Distance Margin&lt;/td&gt;
          &lt;td&gt;与障碍最近距离的平均值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;在仿真中先实现自动收集 SR、AS、CR。&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;实机可手动统计 SR 和 CR，或用里程计测轨迹。&lt;/p&gt;
&lt;p&gt;可定义 3 个场景（开阔场 / 门框 / 桌椅环境）各跑10次。&lt;/p&gt;
&lt;h2&gt;创新点（暂定）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;创新点暂定&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%88%9b%e6%96%b0%e7%82%b9%e6%9a%82%e5%ae%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们提出一种融合深度视觉感知与分层控制的开放词汇目标追踪框架。
相较于LOVON仅依赖2D目标检测进行运动控制，我们的方法通过深度投影构建局部BEV占用图，并引入预测-驱动的路径规划层，从而显著减少在真实环境中因遮挡或障碍导致的失败&lt;/p&gt;
&lt;h2&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;时间&lt;/th&gt;
          &lt;th&gt;任务&lt;/th&gt;
          &lt;th&gt;目标&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;第1阶段&lt;/td&gt;
          &lt;td&gt;阅读文献：LOVON、LOVi、BEVFusion、LIO-SAM、SceneGPT&lt;/td&gt;
          &lt;td&gt;明确3D环境理解技术路线&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第2阶段&lt;/td&gt;
          &lt;td&gt;在Gym-UnrealCV中复现或简化LOVON策略（YOLO+Motion mapping）&lt;/td&gt;
          &lt;td&gt;建立baseline可控环境&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第3阶段&lt;/td&gt;
          &lt;td&gt;集成深度图或BEV投影模块，实现障碍建模与避障决策&lt;/td&gt;
          &lt;td&gt;形成改进方法&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第4阶段&lt;/td&gt;
          &lt;td&gt;实机测试 + 指标对比 + 论文撰写&lt;/td&gt;
          &lt;td&gt;形成可投稿版本&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;第一阶段的文献调研一方面要包括LOVON引用的和引用LOVON的文献（但是因为VPN节点问题我的Scholar Google给我挂掉了，说我是机器人不让我访问），另一方面是尽可能的调研3D-aware Tracking/Navigation&lt;/p&gt;
&lt;h2&gt;医疗交叉（答辩）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;医疗交叉答辩&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8c%bb%e7%96%97%e4%ba%a4%e5%8f%89%e7%ad%94%e8%be%a9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里值得注意的是，论文里面要写的内容是一个宏大的改进，但是本院答辩时要突出和BME相关、医疗交叉的内容，HexGuide可以作为一个很大的参考&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;《HexGuide: A Hexapod Robot for Autonomous Blind Guidance in Challenging Environments》，一篇期刊论文&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;层级&lt;/th&gt;
          &lt;th&gt;内容&lt;/th&gt;
          &lt;th&gt;对应写作作用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;① 背景&lt;/td&gt;
          &lt;td&gt;世界上有数亿视障人群，对自主出行有刚性需求&lt;/td&gt;
          &lt;td&gt;让读者意识到社会价值和痛点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;② 矛盾&lt;/td&gt;
          &lt;td&gt;现有导盲设备（如导盲犬或轮式机器人）有明显局限，不能稳定地应对复杂地形&lt;/td&gt;
          &lt;td&gt;设置“冲突”——为什么我们必须做新系统&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;③ 概念&lt;/td&gt;
          &lt;td&gt;上交高峰团队设计了一个六足机器人 &lt;strong&gt;HexGuide&lt;/strong&gt;，模仿昆虫式稳定步态，在复杂环境中实现安全引导&lt;/td&gt;
          &lt;td&gt;提出核心创新点和愿景&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;④ 方法&lt;/td&gt;
          &lt;td&gt;通过算法与机械协同，实现&lt;strong&gt;路径规划 + 稳定行走 + 动态避障 + 交通识别 + 人机交互&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;展示技术路线是如何支撑“稳定、安全”这两个关键词的&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;⑤ 验证&lt;/td&gt;
          &lt;td&gt;在机场、十字路口等复杂场景下实测验证，引导成功率高，路径平滑且避障成功&lt;/td&gt;
          &lt;td&gt;用结果“闭环”故事——愿景得以实现&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;论文的立意不是“做一个六足机器人”，而是要证明“六足+智能控制” = 可靠的盲人引导方式。这篇论文的核心任务不是“跟踪一个已知目标”或“视觉跟随”，而是“带领盲人从一个地点到另一个地点”，比如：“from the arrival gate to the baggage claim area in Shanghai Hongqiao Airport.”&lt;/p&gt;
&lt;p&gt;核心流程是盲人用户通过语音指令（如“去出口”）输入目标；在地图上自动规划从当前位置到目标的安全路径；机器人沿着规划路径行走；实时感知环境并修正轨迹。&lt;/p&gt;
&lt;p&gt;目标不是视觉追踪的对象，而是一个空间位置目标，因此这种导航是Goal-based而非Object-based tracking，而且泛化性有限：“The system can autonomously navigate in challenging environments once a map is available.”&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模块&lt;/th&gt;
          &lt;th&gt;故事逻辑&lt;/th&gt;
          &lt;th&gt;手法&lt;/th&gt;
          &lt;th&gt;指标体现&lt;/th&gt;
          &lt;th&gt;补充说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;机械稳定性：六足结构的天然稳态&lt;/td&gt;
          &lt;td&gt;盲人行走必须安全 → 足式比轮式更抗地形 → 六足比四足更稳&lt;/td&gt;
          &lt;td&gt;· “三足支撑步态（Tripod gait）”确保任意时刻三条腿接地&lt;br /&gt;· 单腿轨迹采用三次样条插值，区分支撑相与摆动相以减冲击&lt;br /&gt;· 控制顶点高度以跨越障碍、维持步态连续&lt;/td&gt;
          &lt;td&gt;· 平均支撑腿数 ≥ 3&lt;br /&gt;· 步态周期内质心（CoM）位移波动 &amp;lt; 5 mm&lt;br /&gt;· 10° 坡面及不平地面仍能维持姿态&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;规划稳定性：安全路径生成&lt;/td&gt;
          &lt;td&gt;“安全通行”要求路径不过度摆动、不贴近障碍&lt;/td&gt;
          &lt;td&gt;· 基于 A* 进行全局规划&lt;br /&gt;· 融合人工势场（APF）调整代价，使路径自动远离障碍&lt;br /&gt;· Bézier 曲线平滑路径&lt;br /&gt;· 拐点以贪心方式优化，减少急转角&lt;/td&gt;
          &lt;td&gt;· 路径平滑度提升（转向角波动减少约 40%）&lt;br /&gt;· 路径与障碍最小距离 ≥ 0.3 m&lt;br /&gt;· 平均路径长度仅比最短路径长 ≤ 5%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;运动控制稳定性：MPC 路径跟踪控制&lt;/td&gt;
          &lt;td&gt;六足控制复杂，需让行走对路径偏差“有反馈、能预测”&lt;/td&gt;
          &lt;td&gt;· 使用模型预测控制（MPC）&lt;br /&gt;· 目标函数最小化未来时域的位姿偏差&lt;br /&gt;· 实时约束关节速度与姿态角&lt;br /&gt;· 借助力矩传感器反馈修正步态&lt;/td&gt;
          &lt;td&gt;· 路径跟踪误差 &amp;lt; 3 cm&lt;br /&gt;· 姿态偏角误差 &amp;lt; 2°&lt;br /&gt;· 延迟控制补偿 ≤ 100 ms&lt;/td&gt;
          &lt;td&gt;核心体现“动态预测 + 约束最优控制”，区别于传统 PID&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;环境与交互稳定性：避免危险与错误指令&lt;/td&gt;
          &lt;td&gt;盲人处于动态环境，需识别行人、车辆、信号灯并安全互动&lt;/td&gt;
          &lt;td&gt;· LiDAR + IMU + RGB 摄像头多传感融合&lt;br /&gt;· 基于 LiDAR 点云的区域划分与加权速度修正，实现动态避障&lt;br /&gt;· YOLOv5 交通灯识别结合模板匹配&lt;br /&gt;· 语音识别与反馈交互（“请跟我走”“前方有障碍”）&lt;/td&gt;
          &lt;td&gt;· 动态障碍避让成功率 95%&lt;br /&gt;· 信号灯识别准确率 97.8%&lt;br /&gt;· 平均避障响应时间 &amp;lt; 0.3 s&lt;br /&gt;· 机场/路口场景连续引导成功率 100%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所以这里的Navigation就比较难讲故事了。我们的亮点在于LLM对于自然语言指令能够分解成子任务，但足式机器人比较尴尬的一点是没有手，导致在医疗领域能实现的指令就局限了，比如说有一个RoboNurse-VLA enables the robot to recognize, grasp, and handover surgical instrument.是灵巧手的，但是足式机器人就只是狗了&lt;/p&gt;
&lt;p&gt;我也调研了其他的可能可以相关的领域，秉承**“助残/助盲/助老”**的理念：
第一个是家庭服务，这个还挺好说的，比如越疆 Rover X1/Unitree GO2可在光滑地板、草地、小坡坎等多场景行走，负载能力达日常物品级别，但最大的问题就是没有手，导致比如“帮老人取床头老花镜”“客厅物品递送”这种实现不了 —— 没有机械臂的足式机器人，到底“服务”什么？如果不能取物、开门，它的价值在哪里？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以胡诌做成可语音召唤的移动置物台 ，为上肢失能者提供 室内 5 m 范围内的即时物品可达性 ，用 3D 感知+分层导航解决 家用杂乱环境 下的 安全-连续 难题，从而 以移动代偿 而非 抓取代偿 的方式，提升上肢失能人群的 居家独立指数？
第二个是康复检测，问题是回答不了为什么需要一个狗跟着，而不是穿戴传感器设备/用固定的摄像头进行openpose骨骼分析，就算用了狗也不过是一个移动摄像头，那为什么不让残疾人动或者医生手动挪动摄像头？
第三个就是继续去纯助盲，我想通了，它不是Tracking，而是Object-based Tracking，只是默认命令是跟着person而已，没有说一定要跟在人后面，给他下一个其他目标的指令不就行了？但问题就出在了这里，传统SLAM的方法比结合AI的方法又快又好，你在AI基础上绑一个什么激光雷达/SLAM的话就有点尾大不掉很难绷。&lt;/li&gt;
&lt;li&gt;也有&lt;a href=&#34;https://github.com/Li-Ruiqi777/BlindGuideDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;导盲盲道的参考&lt;/a&gt;，不过是基于A1的本科毕设&lt;/li&gt;
&lt;li&gt;往&lt;a href=&#34;https://zhuanlan.zhihu.com/p/684356655&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RoboGuide这个方向&lt;/a&gt;去做的话也可以，只不过更多是放在VLM而非Tracking/Navigation本身了&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/BestAnHongjun/InternDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InternDog&lt;/a&gt;这篇西工大的工作不知道是怎么做的，看起来很牛，还上了&lt;a href=&#34;https://www.bilibili.com/video/BV1kK421a7sP/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;央视&lt;/a&gt;，据说是我国首个应用在导盲任务/场景下的四足机器人？&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bestxiangest/Intelligent-Guide-Cane&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intelligent-Guide-Cane&lt;/a&gt;或者回归ESP32的导盲？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感觉越调研越有信心了，那就继续往导盲这个领域讲故事应该没有问题！&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
