<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Open-Review</title>
    <link>http://localhost:1313/tags/open-review/</link>
    <description>Recent content in Open-Review on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/open-review/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>全文献检索工具方案设计</title>
      <link>http://localhost:1313/blog/2025/2025-12-12-paper-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-12-paper-agent/</guid>
      <description>
        
        
        &lt;p&gt;科研paper读着实在是不得劲。独立研究者有着自己的一套&lt;strong&gt;taste&lt;/strong&gt;，正如杨振宁先生所说，这种taste是可以通过&lt;strong&gt;大量的喂养培养出来的&lt;/strong&gt;——但是这有一点&lt;strong&gt;太厚积薄发了&lt;/strong&gt;，看着zotero里面贫瘠的二十多篇论文，我打算做一个全文献检索的工具。&lt;/p&gt;
&lt;p&gt;首先想到的当然是&lt;strong&gt;ArXiv提供的官方库&lt;/strong&gt;，但是养taste的话，&lt;strong&gt;顶会中稿+开源的才有用&lt;/strong&gt;，普通的preprint属于垃圾食品，然而&lt;strong&gt;arXiv API 本身并没有一个专门的 strict 字段来筛选&amp;quot;是否被会议录用&amp;quot;&lt;/strong&gt;，完全依赖于作者手动更新 metadata 中的 comment。&lt;/p&gt;
&lt;h2&gt;方案一：DBLP + Semantic Scholar + ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;方案一dblp--semantic-scholar--arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%96%b9%e6%a1%88%e4%b8%80dblp--semantic-scholar--arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;所以我想到了下面这样的产品流：&lt;/p&gt;
&lt;h3&gt;DBLP（作为&amp;quot;权威判官&amp;quot;）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;dblp作为权威判官&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dblp%e4%bd%9c%e4%b8%ba%e6%9d%83%e5%a8%81%e5%88%a4%e5%ae%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 只负责提供**&amp;ldquo;真·录用名单&amp;rdquo;**&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; DBLP 是计算机领域的**&amp;ldquo;户籍科&amp;rdquo;&lt;strong&gt;，它的数据是&lt;/strong&gt;人工校对的**，只有真正被 CVPR 录用的论文才会出现在 &lt;code&gt;conf/cvpr/2025&lt;/code&gt; 列表里&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决痛点：&lt;/strong&gt; 解决了 arXiv 上作者**&amp;ldquo;自吹自擂&amp;quot;或&amp;quot;撒谎&amp;rdquo;**的问题（比如有的作者被拒稿了也敢写 Accepted）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Semantic Scholar（作为&amp;quot;连接器&amp;quot;）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;semantic-scholar作为连接器&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#semantic-scholar%e4%bd%9c%e4%b8%ba%e8%bf%9e%e6%8e%a5%e5%99%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 负责把 DBLP 的会议论文标题，映射到 &lt;strong&gt;arXiv ID&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; DBLP 的会议条目和 arXiv 条目通常是独立的（两条记录）。Semantic Scholar 构建了&lt;strong&gt;巨大的图谱&lt;/strong&gt;，它能识别出&amp;quot;这篇 CVPR 论文其实就是那篇 arXiv 预印本&amp;quot;，并提供 &lt;code&gt;externalIds&lt;/code&gt; 字段&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决痛点：&lt;/strong&gt; 解决了 &lt;strong&gt;DBLP 不直接提供 PDF 下载链接&lt;/strong&gt;，以及 DBLP 和 arXiv 割裂的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ArXiv（作为&amp;quot;内容仓库&amp;quot;）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;arxiv作为内容仓库&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#arxiv%e4%bd%9c%e4%b8%ba%e5%86%85%e5%ae%b9%e4%bb%93%e5%ba%93&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 负责下载&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; 一旦知道了 arXiv ID，用 Python 的 &lt;strong&gt;arxiv 库下载 PDF 或源码是最稳定、最合规的&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决痛点：&lt;/strong&gt; Semantic Scholar 的 API 有时会&lt;strong&gt;限流&lt;/strong&gt;或不直接提供 PDF 文件流&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;存在的问题&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;存在的问题&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%98%e5%9c%a8%e7%9a%84%e9%97%ae%e9%a2%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;DBLP 的标题有时候会把句号放在最后，或者包含特殊字符。Semantic Scholar 的搜索能力很强，通常能模糊匹配，但偶尔也会因为特殊符号对不上。代码里做一点简单的 &lt;code&gt;strip()&lt;/code&gt; 清洗很有必要，但这不是最大的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Semantic Scholar 和 DBLP 都有频率限制&lt;/strong&gt;。如果名单有几千篇，一定要加 &lt;code&gt;time.sleep(1)&lt;/code&gt; 或者使用 &lt;strong&gt;API Key&lt;/strong&gt;（Semantic Scholar 申请 Key 后并发度更高），但是这导致了一个非常大的问题：&lt;strong&gt;调用Semantic Scholar太久了&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;方案二：直接爬取 CVF&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;方案二直接爬取-cvf&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%96%b9%e6%a1%88%e4%ba%8c%e7%9b%b4%e6%8e%a5%e7%88%ac%e5%8f%96-cvf&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;所以接下来又想了第二条路：直接爬取 &lt;strong&gt;CVF (Computer Vision Foundation)&lt;/strong&gt; 的官方 &lt;strong&gt;Open Access 仓库&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CVPR、ICCV、WACV&lt;/strong&gt; 的所有论文（包括 Supplemental Material）都全量、免费托管在这里，它就是&lt;strong&gt;简单的静态 HTML 页面&lt;/strong&gt;，没有复杂的反爬机制，&lt;strong&gt;没有 API 限流&lt;/strong&gt;，直接 &lt;code&gt;requests&lt;/code&gt; 请求一次就能拿到几千篇论文的列表，通过暴力爬虫直接访问 CVPR 2022 的&amp;quot;所有论文&amp;quot;页面，瞬间解析出所有论文的标题和 PDF 下载链接，并保存为 CSV 文件。&lt;/p&gt;
&lt;p&gt;然而没有投过paper的本科生被这个数量吓晕了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/cvpr.png&#34; alt=&#34;&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CVPR 2020: ~&lt;strong&gt;1,467&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2021: ~&lt;strong&gt;1,660&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2022: &lt;strong&gt;2,074&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2023: ~&lt;strong&gt;2,359&lt;/strong&gt; 篇&lt;/li&gt;
&lt;li&gt;CVPR 2024: ~&lt;strong&gt;2,719&lt;/strong&gt; 篇&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实在是太疯狂了。如果你一天读 5 篇，读完这一年的会，明年的会都开完了。&lt;/p&gt;
&lt;h2&gt;各顶会的处理方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;各顶会的处理方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%90%84%e9%a1%b6%e4%bc%9a%e7%9a%84%e5%a4%84%e7%90%86%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里还是继续完善离线方法，前面&lt;strong&gt;CVF的方法只适用于3个&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ECCV：&lt;/strong&gt; 虽然不属于 CVF，但 &lt;strong&gt;ECVA 的网站结构和 CVF 惊人的相似&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ICLR 和 CoRL：&lt;/strong&gt; 最优雅的方法不是爬网页，而是使用 &lt;strong&gt;OpenReview 的官方 Python 库&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ICRA, IROS（机器人双雄）：&lt;/strong&gt; 是最麻烦的两个。因为它们版权属于 &lt;strong&gt;IEEE&lt;/strong&gt;；&lt;strong&gt;IEEE Xplore 有很强的反爬&lt;/strong&gt;，且下载 PDF 需要&lt;strong&gt;学校 IP 验证&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NeurIPS &amp;amp; RSS：&lt;/strong&gt; 非常良心，&lt;strong&gt;历史归档都在静态网页上，HTML 结构十年不变&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以对于双雄，只能用&lt;strong&gt;DBLP的方法&lt;/strong&gt;，然后用Arxiv库交叉验证。&lt;/p&gt;
&lt;p&gt;根据经验，机器人领域（Robotics）的 &lt;strong&gt;ArXiv 覆盖率大约在 60% - 80%&lt;/strong&gt;。剩下 &lt;strong&gt;20%&lt;/strong&gt; 的文章，作者可能根本没传 ArXiv，或者传了但是标题改得面目全非（比如从 &amp;ldquo;A Fast Method&amp;hellip;&amp;rdquo; 改成了 &amp;ldquo;FastMethod:&amp;hellip;&amp;quot;）。&lt;/p&gt;
&lt;p&gt;计算机视觉顶会有严格的**“奇偶年份”**规律？！这里修正了我的爬取ICCV和ECCV的方法&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
