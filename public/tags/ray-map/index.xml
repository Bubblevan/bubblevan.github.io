<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Ray-Map</title>
    <link>http://localhost:1313/tags/ray-map/</link>
    <description>Recent content in Ray-Map on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/ray-map/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>3D 场景理解工作</title>
      <link>http://localhost:1313/blog/2025/2025-11-19/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-19/</guid>
      <description>
        
        
        &lt;h1&gt;3D 场景理解工作&lt;/h1&gt;&lt;h2&gt;Depth Anything 3&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;depth-anything-3&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#depth-anything-3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ByteDance-Seed/depth-anything-3&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Depth Anything 3 (DA3)&lt;/strong&gt;&lt;/a&gt; 是字节跳动提出的 3D 视觉模型，核心目标是从&lt;strong&gt;任意数量&lt;/strong&gt;（单张 / 多张 / 视频流）、&lt;strong&gt;已知或未知相机姿态&lt;/strong&gt;的视觉输入中恢复空间一致的几何结构；其采用&lt;strong&gt;单 Transformer 骨干网络&lt;/strong&gt;（如 vanilla DINOv2 编码器）和&lt;strong&gt;深度-射线（depth-ray）表示&lt;/strong&gt;作为最小预测目标，避免复杂多任务学习，通过&lt;strong&gt;师生训练范式&lt;/strong&gt;（教师模型用合成数据生成高质量伪标签指导学生模型）实现与 DA2 相当的细节度和泛化性；同时构建了涵盖相机姿态估计、任意视图几何重建、视觉渲染的 &lt;strong&gt;Visual Geometry Benchmark&lt;/strong&gt;，在该基准上 DA3 刷新所有任务 SOTA，平均超越此前 SOTA 模型 VGGT &lt;strong&gt;35.7%&lt;/strong&gt; 的相机姿态精度和 &lt;strong&gt;23.6%&lt;/strong&gt; 的几何精度，且在单目深度估计任务上优于 DA2；此外，DA3 可通过微调扩展至前馈 &lt;strong&gt;3D 高斯 splatting（3DGS）&lt;/strong&gt; 等下游任务，为通用 3D 感知提供基础模型支持。&lt;/p&gt;
&lt;!-- truncate --&gt;
&lt;h3&gt;架构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;架构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%9e%b6%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/da3-pipeline.png&#34; alt=&#34;DA3 Pipeline&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模块&lt;/th&gt;
          &lt;th&gt;核心设计&lt;/th&gt;
          &lt;th&gt;作用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;单 Transformer 骨干&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;基于预训练 ViT（如 DINOv2），分 L_s（视图内注意力）和 L_g（交替跨视图 / 视图内）层，L_s:L_g=2:1&lt;/td&gt;
          &lt;td&gt;继承预训练特征，支持任意视图数（单图自动降级为单目）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;相机条件注入&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;每个视图前缀相机 token：已知姿态→MLP 编码（E_c (f,q,t)）；未知→共享可学习 token&lt;/td&gt;
          &lt;td&gt;无缝处理有 / 无姿态输入，提供几何上下文&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;双 DPT 头（Dual-DPT）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;共享重组模块（Reassemble），独立融合层 + 输出层，分别预测深度和射线&lt;/td&gt;
          &lt;td&gt;保证两任务特征交互，避免中间表示冗余，提升预测一致性&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;深度图（Depth Map）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;深度图depth-map&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%b7%b1%e5%ba%a6%e5%9b%bedepth-map&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在 &lt;strong&gt;Depth Anything 3（DA3）&lt;/strong&gt; 的 3D 几何重建框架中，&lt;strong&gt;深度图&lt;/strong&gt;是用于描述 &amp;ldquo;图像像素与相机之间物理距离&amp;rdquo; 的稠密矩阵，是构建 3D 结构的核心基础之一。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义与数学形式：&lt;/strong&gt; 深度图用符号 $D \in \mathbb{R}^{H \times W}$ 表示，其中 $H$ 和 $W$ 分别对应输入图像的高度和宽度，每个元素 $D(u,v)$ 代表图像中坐标 $(u,v)$ 的像素到相机光心的真实距离（单位通常为米），且与输入图像严格像素对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt; 深度图直接提供 &amp;ldquo;像素位置的远近信息&amp;rdquo;，是将 2D 图像映射到 3D 空间的关键桥梁 —— 结合相机姿态或射线信息，可通过 $P = t + D(u,v) \cdot d$（$t$ 为射线原点，$d$ 为射线方向）计算出该像素在世界坐标系中的 3D 坐标 $P$，进而生成完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DA3 中的特性：&lt;/strong&gt; DA3 通过 &lt;strong&gt;&amp;ldquo;师生训练范式&amp;rdquo;&lt;/strong&gt; 优化深度图质量 —— 教师模型（仅用合成数据训练）生成高质量伪深度标签，对齐真实场景的稀疏 / 噪声深度，确保深度图的细节完整性与几何一致性，避免近距区域（如物体表面）的距离预测偏差。&lt;/p&gt;
&lt;h3&gt;射线图（Ray Map）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;射线图ray-map&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%b0%84%e7%ba%bf%e5%9b%beray-map&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;射线图&lt;/strong&gt;是 DA3 提出的创新表示方式，用于隐式编码 &amp;ldquo;相机观测方向与位置&amp;rdquo;，避免直接预测相机姿态（如旋转矩阵、平移向量）的复杂约束，是实现 &lt;strong&gt;&amp;ldquo;最小建模&amp;rdquo;&lt;/strong&gt; 的关键设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义与数学形式：&lt;/strong&gt; 射线图用符号 $M \in \mathbb{R}^{H \times W \times 6}$ 表示，同样与输入图像像素对齐，6 个通道分为两组：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;前 3 通道&lt;/strong&gt;（$M(:,:,:3)$）：&lt;strong&gt;射线原点&lt;/strong&gt; $t \in \mathbb{R}^3$，代表该像素对应的相机光心在世界坐标系中的位置&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后 3 通道&lt;/strong&gt;（$M(:,:,3:)$）：&lt;strong&gt;射线方向&lt;/strong&gt; $d \in \mathbb{R}^3$，代表该像素从相机光心出发，指向世界空间目标点的单位向量（未归一化，保留投影尺度）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;规避姿态预测约束：&lt;/strong&gt; 传统方法需直接预测相机外参（旋转矩阵 $R$ 需满足正交性），而射线图通过 &amp;ldquo;像素级射线&amp;rdquo; 隐式包含姿态信息，简化建模&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反解相机参数：&lt;/strong&gt; 若需显式获取相机姿态，可从射线图推导 —— 相机中心 $t_c$ 为射线原点的平均值（$t_c = \frac{1}{H \times W} \sum M(h,w,:3)$），通过单应性矩阵 $H=KR$（DLT 算法求解）和 RQ 分解，进一步得到内参 $K$（上三角矩阵）和外参 $R$（正交矩阵）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成 3D 点云：&lt;/strong&gt; 与深度图协同，通过 $P = t + D(u, v) \cdot d$ 直接计算 3D 点，无需额外任务目标（如点云、相机姿态），实现 &amp;ldquo;最小预测目标&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;DA3 中的特性：&lt;/strong&gt; 射线图与深度图通过 &lt;strong&gt;&amp;ldquo;双 DPT 头&amp;rdquo;&lt;/strong&gt; 联合预测 —— 共享特征重组模块，独立融合层分别输出两者，确保射线方向与深度的空间一致性，避免多任务目标的纠缠（实验验证 &amp;ldquo;深度 + 射线&amp;rdquo; 组合的性能优于 &amp;ldquo;深度 + 点云 + 相机姿态&amp;rdquo; 等冗余方案）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;详细解释：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第一部分：射线原点（前 3 个通道）&lt;/strong&gt; 记录 &amp;ldquo;这条光线的起点&amp;rdquo;—— 也就是相机光心在 3D 空间中的位置（比如拍桌子时，手机所在的坐标：x=2 米、y=1 米、z=0.5 米）。所有像素的射线原点通常很接近（因为都来自同一台相机），所以射线图前 3 通道的数值差异很小，本质是 &amp;ldquo;相机位置的像素级体现&amp;rdquo;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第二部分：射线方向（后 3 个通道）&lt;/strong&gt; 记录 &amp;ldquo;这条光线的指向&amp;rdquo;—— 比如 &amp;ldquo;桌子角像素&amp;rdquo; 的光线指向是 &amp;ldquo;向右下方 15°&amp;quot;，&amp;ldquo;桌子边缘像素&amp;rdquo; 的光线指向是 &amp;ldquo;向左下方 10°&amp;quot;。用 3 个数值（x/y/z 方向分量）描述这个指向，比如（0.2, -0.3, 0.1）就代表 &amp;ldquo;在 x 轴正方向、y 轴负方向、z 轴正方向有一定延伸&amp;rdquo;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;为什么使用射线图？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;传统 3D 模型需要直接预测 &amp;ldquo;相机姿态&amp;rdquo;（比如相机朝哪个方向转、在哪个位置），但姿态计算有严格约束（比如旋转矩阵必须满足 &amp;ldquo;正交性&amp;rdquo;，算错一点就会严重偏差）。而射线图绕开了这个复杂问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它不直接算 &amp;ldquo;相机整体姿态&amp;rdquo;，而是给每个像素算 &amp;ldquo;专属光线&amp;rdquo;—— 这些光线天然包含了姿态信息（比如所有光线的原点平均下来就是相机中心，光线方向的规律就是相机朝向）&lt;/li&gt;
&lt;li&gt;后续要显式获取相机姿态时，只需从射线图反推（比如用文档 1-44-46 节的方法：原点求平均得相机中心，方向算单应性矩阵得内参 / 外参），无需在训练时额外优化姿态目标&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;点云&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;点云&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%82%b9%e4%ba%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;点云生成公式：&lt;/strong&gt; 3D 点坐标 = 射线原点 + 深度值 × 射线方向&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt; 比如某像素的射线原点是（2,1,0.5）、方向是（0.2,-0.3,0.1）、深度是 2 米，那么它的 3D 坐标就是（2+2×0.2, 1+2×(-0.3), 0.5+2×0.1）=（2.4, 0.4, 0.7）。所有像素的 3D 坐标拼起来，就是一张完整的点云（比如桌子的点云就是无数个 &amp;ldquo;桌子表面 3D 点&amp;rdquo; 组成的）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;点云评估指标：&lt;/strong&gt; 把生成的点云和 &amp;ldquo;真实点云&amp;rdquo;（比如用 LiDAR 扫描的精准点云）对比：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Chamfer 距离&lt;/strong&gt;：点云之间的平均距离，越小越准&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F1 分数&lt;/strong&gt;：点云的精度和召回率，越高越完整&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;实验&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;实验&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ae%9e%e9%aa%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;DA3 的实验核心围绕 &lt;strong&gt;&amp;ldquo;统一视觉几何基准&amp;rdquo;&lt;/strong&gt; 展开，覆盖 &lt;strong&gt;&amp;ldquo;相机姿态估计、几何重建、视觉渲染&amp;rdquo;&lt;/strong&gt; 三大任务，实验流程严格区分 &lt;strong&gt;&amp;ldquo;数据准备、任务执行、指标评估&amp;rdquo;&lt;/strong&gt; 三阶段。&lt;/p&gt;
&lt;h4&gt;（1）实验基础：构建 Visual Geometry Benchmark&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1实验基础构建-visual-geometry-benchmark&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1%e5%ae%9e%e9%aa%8c%e5%9f%ba%e7%a1%80%e6%9e%84%e5%bb%ba-visual-geometry-benchmark&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;为避免不同任务评估标准不统一的问题，DA3 首先建立了标准化基准，明确实验的 &lt;strong&gt;&amp;ldquo;输入 - 输出 - 评估&amp;rdquo;&lt;/strong&gt; 链路：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基准覆盖范围：&lt;/strong&gt; 包含 5 个场景多样化的数据集（共 89+ 场景），涵盖合成数据、真实 LiDAR 数据、低清噪声数据，确保实验泛化性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;合成数据：&lt;/strong&gt; HiRoom（29 个室内场景，Blender 渲染，用于验证几何细节）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真实 LiDAR 数据：&lt;/strong&gt; ETH3D（11 个室内外场景，激光雷达采集，高分辨率）、DTU（22 个物体级场景，控制条件下的真实点云）、ScanNet++（20 个室内场景，iPhone LiDAR + 激光重建深度）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;低清噪声数据：&lt;/strong&gt; 7Scenes（7 个室内场景，低分辨率 + 运动模糊，模拟真实复杂场景）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;基准流程：&lt;/strong&gt; 所有任务遵循 &lt;strong&gt;&amp;ldquo;输入→模型预测→结果对齐→指标计算&amp;rdquo;&lt;/strong&gt; 的统一逻辑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; 随机采样图像（若图像数超 100 张，固定随机种子采样 100 张，保证实验可复现）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测：&lt;/strong&gt; DA3 输出深度图、射线图、相机姿态（可选）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对齐：&lt;/strong&gt; 几何重建时用 RANSAC+evo 工具将预测姿态与真实姿态对齐，确保点云在同一坐标系&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估：&lt;/strong&gt; 按任务计算对应指标（姿态用 AUC3/AUC30，几何用 F1/Chamfer 距离，渲染用 PSNR/SSIM）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;（2）核心任务实验流程（以 &amp;ldquo;几何重建&amp;rdquo; 为例）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2核心任务实验流程以-几何重建-为例&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2%e6%a0%b8%e5%bf%83%e4%bb%bb%e5%8a%a1%e5%ae%9e%e9%aa%8c%e6%b5%81%e7%a8%8b%e4%bb%a5-%e5%87%a0%e4%bd%95%e9%87%8d%e5%bb%ba-%e4%b8%ba%e4%be%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;用户关心的 &lt;strong&gt;&amp;ldquo;图片流生成连续点云建模&amp;rdquo;&lt;/strong&gt;，正是 DA3 &lt;strong&gt;&amp;ldquo;多视图 / 视频几何重建&amp;rdquo;&lt;/strong&gt; 任务的核心，实验流程在论文 7.1 节和 6.1 节明确：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt; 支持 &lt;strong&gt;&amp;ldquo;图片流（视频帧）&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;多视图图像集&amp;rdquo;&lt;/strong&gt;（无数量限制，单帧即单目，多帧即多视图）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;点云生成逻辑：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型先为每张图像预测 &lt;strong&gt;&amp;ldquo;深度图 + 射线图&amp;rdquo;&lt;/strong&gt;（通过双 DPT 头联合输出，像素级对齐）&lt;/li&gt;
&lt;li&gt;用公式 $P = t + D(u,v) \cdot d$（$t$ 为射线原点，$d$ 为射线方向，$D$ 为深度）计算每个像素的 3D 坐标，生成单帧稀疏点云&lt;/li&gt;
&lt;li&gt;对图片流的连续帧，用 &lt;strong&gt;TSDF 融合&lt;/strong&gt;（Truncated Signed Distance Function）将多帧稀疏点云合并为稠密、连续的场景点云（不同数据集的 TSDF 参数不同，如 HiRoom 体素大小 0.007m，ETH3D 为 0.039m）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实验验证：&lt;/strong&gt; 论文 7.1 节用 ScanNet++、ETH3D 的图片流测试，&lt;strong&gt;DA3-Giant&lt;/strong&gt; 生成的连续点云在 F1 分数上超 VGGT &lt;strong&gt;23.6%&lt;/strong&gt;，且能保留桌角、墙面边缘等细粒度细节。&lt;/p&gt;
&lt;h4&gt;（1）评估阶段：用 &amp;ldquo;真实场景的标注数据&amp;rdquo; 作为 Ground Truth&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1评估阶段用-真实场景的标注数据-作为-ground-truth&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1%e8%af%84%e4%bc%b0%e9%98%b6%e6%ae%b5%e7%94%a8-%e7%9c%9f%e5%ae%9e%e5%9c%ba%e6%99%af%e7%9a%84%e6%a0%87%e6%b3%a8%e6%95%b0%e6%8d%ae-%e4%bd%9c%e4%b8%ba-ground-truth&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;实验中模型性能对比的是真实世界的物理测量结果，而非教师模型输出，具体来源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;姿态真值：&lt;/strong&gt; ETH3D、DTU 等数据集用 &lt;strong&gt;COLMAP&lt;/strong&gt;（基于特征匹配的 SfM 方法）** 或激光跟踪仪获取相机姿态&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;几何真值：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;ETH3D/ScanNet++：用 TSDF 融合 LiDAR 采集的稠密深度图，生成真实场景点云&lt;/li&gt;
&lt;li&gt;DTU：在实验室控制条件下，用高精度 3D 扫描仪获取物体的真实点云&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;渲染真值：&lt;/strong&gt; NVS 任务（视觉渲染）用 &lt;strong&gt;&amp;ldquo;真实场景的多视图图像&amp;rdquo;&lt;/strong&gt; 作为渲染目标真值（如 DL3DV 的测试帧）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;（2）训练阶段：用 &amp;ldquo;教师模型伪标签&amp;rdquo; 作为监督（非 Ground Truth）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2训练阶段用-教师模型伪标签-作为监督非-ground-truth&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5%e7%94%a8-%e6%95%99%e5%b8%88%e6%a8%a1%e5%9e%8b%e4%bc%aa%e6%a0%87%e7%ad%be-%e4%bd%9c%e4%b8%ba%e7%9b%91%e7%9d%a3%e9%9d%9e-ground-truth&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;教师模型的作用是 &lt;strong&gt;&amp;ldquo;为真实噪声数据生成高质量监督信号&amp;rdquo;&lt;/strong&gt;，而非作为评估的真值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实数据（如 ARKitScenes、WildRGBD）的深度标注常稀疏 / 噪声大，无法直接用&lt;/li&gt;
&lt;li&gt;教师模型（仅用合成数据训练）为这些真实图像生成 &lt;strong&gt;&amp;ldquo;伪深度图&amp;rdquo;&lt;/strong&gt;，再通过 RANSAC 对齐真实稀疏深度，得到 &lt;strong&gt;&amp;ldquo;干净的监督信号&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;论文 7.2.3 节验证：用教师伪标签训练的 DA3，深度图细节比无教师监督的版本丰富 &lt;strong&gt;30%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Navigation/Tracking 适配&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;navigationtracking-适配&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#navigationtracking-%e9%80%82%e9%85%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;DA3 适配机器人 SLAM 避障需解决 &lt;strong&gt;动态物体过滤、回环检测、语义融合&lt;/strong&gt; 三大核心问题。这些问题的本质是 &lt;strong&gt;&amp;ldquo;DA3 作为&amp;rsquo;几何重建模型&amp;rsquo;的功能边界，与机器人&amp;rsquo;自主环境交互&amp;rsquo;的实际需求之间的差距&amp;rdquo;&lt;/strong&gt;——DA3 仅负责输出高精度几何信息（深度、点云），但机器人避障需要 &lt;strong&gt;&amp;ldquo;干净的静态环境地图&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;长期建图的一致性&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;可解释的障碍决策依据&amp;rdquo;&lt;/strong&gt;，这三个适配问题正是填补这一差距的关键。以下结合你提供的新搜索资源（尤其是动态物体滤除、SLAM 实践相关内容），从 &lt;strong&gt;&amp;ldquo;问题本质、DA3 局限、解决方案、落地细节&amp;rdquo;&lt;/strong&gt; 四个维度详细拆解：&lt;/p&gt;
&lt;h4&gt;一、动态物体过滤：从 &amp;ldquo;无差别几何重建&amp;rdquo; 到 &amp;ldquo;精准静态环境提取&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一动态物体过滤从-无差别几何重建-到-精准静态环境提取&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%8a%a8%e6%80%81%e7%89%a9%e4%bd%93%e8%bf%87%e6%bb%a4%e4%bb%8e-%e6%97%a0%e5%b7%ae%e5%88%ab%e5%87%a0%e4%bd%95%e9%87%8d%e5%bb%ba-%e5%88%b0-%e7%b2%be%e5%87%86%e9%9d%99%e6%80%81%e7%8e%af%e5%a2%83%e6%8f%90%e5%8f%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 问题本质与影响&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人在真实场景（如商场、街道）中，环境充满动态物体（行人、移动车辆、开合的门）。DA3 会将这些动态物体与静态场景（地面、墙壁）一同重建为点云，导致两个严重问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;建图污染：&lt;/strong&gt; 动态物体的点云会被误判为静态障碍（如行人走过的区域，地图会残留 &amp;ldquo;人形障碍&amp;rdquo;），后续机器人再次经过时会误触发避障，无法通行&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;避障误判：&lt;/strong&gt; 若动态物体突然出现在路径上（如行人横穿），DA3 生成的点云包含该物体，但机器人无法区分 &lt;strong&gt;&amp;ldquo;静态障碍&amp;rdquo;&lt;/strong&gt; 和 &lt;strong&gt;&amp;ldquo;动态障碍&amp;rdquo;&lt;/strong&gt;，可能出现 &lt;strong&gt;&amp;ldquo;过度避障&amp;rdquo;&lt;/strong&gt;（避开缓慢移动的行人导致绕远）或 &lt;strong&gt;&amp;ldquo;避障不及时&amp;rdquo;&lt;/strong&gt;（未识别快速移动物体）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. DA3 当前的核心局限&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 的设计目标是 &lt;strong&gt;&amp;ldquo;恢复空间一致的几何结构&amp;rdquo;&lt;/strong&gt;，完全不区分 &lt;strong&gt;&amp;ldquo;静态&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;动态&amp;rdquo;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入图像中的动态物体，会被 DA3 当作 &lt;strong&gt;&amp;ldquo;场景几何的一部分&amp;rdquo;&lt;/strong&gt;，输出对应的深度图和射线图，进而生成动态物体的点云&lt;/li&gt;
&lt;li&gt;无任何时序动态检测模块（如光流跟踪、多帧运动一致性判断），无法从连续帧中识别 &lt;strong&gt;&amp;ldquo;运动的物体&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. 解决方案：结合 &amp;ldquo;几何 + 语义 + 时序&amp;rdquo; 多维度滤除，适配 DA3 输出&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据搜索资源（摘要 3：动态物体滤除算法、摘要 6：Dynablox 几何方法），可将 DA3 的输出与以下技术结合，实现动态物体精准滤除：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;技术路径&lt;/th&gt;
          &lt;th&gt;核心原理（结合 DA3）&lt;/th&gt;
          &lt;th&gt;优势与优化方向（来自搜索资源）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;2D 语义分割反投影&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. DA3 输出 &amp;ldquo;RGB 图像 + 深度图&amp;rdquo;；&lt;br&gt;&lt;/br&gt;2. 用语义分割模型（如 Mask R-CNN、DeepLab，摘要 3）分割 RGB 图像中的动态物体（如 &amp;ldquo;行人&amp;quot;&amp;ldquo;车辆&amp;rdquo;），得到动态区域掩码；&lt;br&gt;&lt;/br&gt;3. 将掩码反投影到 DA3 生成的 3D 点云中，滤除掩码对应的点（即动态物体点）。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 直接利用 DA3 的 RGB + 深度输出，无需额外传感器；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 结合时序一致性融合（摘要 3），对连续 3-5 帧的分割结果取交集，减少单帧误分割（如将静态停靠的车辆误判为动态）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;光流跟踪 + 运动一致性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. 对 DA3 处理的连续帧 RGB 图像，用光流模型（如 RAFT、FlowNet，摘要 3）计算像素运动向量；&lt;br&gt;&lt;/br&gt;2. 筛选运动向量大于阈值的像素（动态物体区域）；&lt;br&gt;&lt;/br&gt;3. 结合 DA3 的深度图，将动态像素反投影到点云滤除。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 无需训练语义模型，适用于 &amp;ldquo;未知动态物体&amp;rdquo;（如未见过的包裹）；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 补偿相机运动（Ego-motion）（摘要 3），用 DA3 估计的相机姿态（extrinsics）修正光流，避免将 &amp;ldquo;相机移动导致的静态物体运动&amp;rdquo; 误判为动态。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;纯几何时序检测（Dynablox）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. 用 DA3 生成的连续帧点云构建 TSDF 体素地图（摘要 6）；&lt;br&gt;&lt;/br&gt;2. 跟踪每个体素的 &amp;ldquo;占用状态时序&amp;rdquo;（如 &amp;ldquo;最近 10 帧是否持续被占用&amp;rdquo;）；&lt;br&gt;&lt;/br&gt;3. 若体素从 &amp;ldquo;空闲&amp;rdquo; 突然变为 &amp;ldquo;占用&amp;rdquo;（如行人闯入），标记为动态体素，滤除对应点云。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 无依赖外观，适用于黑暗、低纹理场景（DA3 深度图仍有效）；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 引入高置信度自由空间（摘要 6），通过激光射线穿透检测，排除 &amp;ldquo;静态物体遮挡导致的误判&amp;rdquo;（如行人挡住墙壁，墙壁仍为静态）。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;4. 落地示例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人在商场走廊移动时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 实时输出 640×480 分辨率的深度图 + RGB 图像（摘要 1、4 中 Jetson 平台配置）&lt;/li&gt;
&lt;li&gt;同步运行 Mask R-CNN 分割 RGB 图像，得到 &lt;strong&gt;&amp;ldquo;行人&amp;rdquo;&lt;/strong&gt; 掩码（动态区域）&lt;/li&gt;
&lt;li&gt;将掩码与 DA3 的深度图结合，计算动态区域的 3D 坐标范围，从点云中删除这些坐标的点&lt;/li&gt;
&lt;li&gt;同时用连续 5 帧的分割结果取交集，避免将 &lt;strong&gt;&amp;ldquo;短暂停留的行人&amp;rdquo;&lt;/strong&gt; 误判为静态障碍，最终得到 &lt;strong&gt;&amp;ldquo;仅含墙壁、地面的静态点云&amp;rdquo;&lt;/strong&gt;，供避障决策使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;二、回环检测：从 &amp;ldquo;短期几何一致&amp;rdquo; 到 &amp;ldquo;长期地图全局一致&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二回环检测从-短期几何一致-到-长期地图全局一致&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8c%e5%9b%9e%e7%8e%af%e6%a3%80%e6%b5%8b%e4%bb%8e-%e7%9f%ad%e6%9c%9f%e5%87%a0%e4%bd%95%e4%b8%80%e8%87%b4-%e5%88%b0-%e9%95%bf%e6%9c%9f%e5%9c%b0%e5%9b%be%e5%85%a8%e5%b1%80%e4%b8%80%e8%87%b4&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 问题本质与影响&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人在 &lt;strong&gt;&amp;ldquo;长期建图&amp;rdquo;&lt;/strong&gt;（如探索整个办公楼）时，会因&lt;strong&gt;位姿漂移&lt;/strong&gt;（相机姿态估计误差累积）导致地图扭曲：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如机器人从 &lt;strong&gt;&amp;ldquo;1 楼走廊&amp;rdquo;&lt;/strong&gt; 出发，绕一圈回到起点，若没有回环检测，DA3 生成的地图会出现 &lt;strong&gt;&amp;ldquo;起点与终点不重合&amp;rdquo;&lt;/strong&gt;（如走廊两端错开 1 米），严重时会导致机器人 &lt;strong&gt;&amp;ldquo;迷路&amp;rdquo;&lt;/strong&gt;（不知道自己的真实位置）&lt;/li&gt;
&lt;li&gt;DA3 仅能保证 &lt;strong&gt;&amp;ldquo;单帧 / 短序列帧&amp;rdquo;&lt;/strong&gt; 的几何一致性（如连续 10 帧的点云对齐），但无法判断 &lt;strong&gt;&amp;ldquo;当前场景是否与 10 分钟前访问过的场景相同&amp;rdquo;&lt;/strong&gt;—— 这一判断能力正是回环检测的核心&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. DA3 当前的核心局限&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 的定位是 &lt;strong&gt;&amp;ldquo;几何重建模型&amp;rdquo;&lt;/strong&gt;，无任何回环检测与位姿修正模块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;仅能根据输入图像估计 &lt;strong&gt;&amp;ldquo;相对姿态&amp;rdquo;&lt;/strong&gt;（相邻帧之间的姿态变化），无法估计 &lt;strong&gt;&amp;ldquo;绝对姿态&amp;rdquo;&lt;/strong&gt;（相对于全局地图的位置）&lt;/li&gt;
&lt;li&gt;长期运行后，相对姿态误差会累积，导致点云地图出现 &lt;strong&gt;&amp;ldquo;漂移&amp;rdquo;&lt;/strong&gt;（如直线走廊建图后变成曲线），进而影响避障精度（如机器人认为前方有障碍，实际是地图漂移导致的虚警）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. 解决方案：DA3 作为 &amp;ldquo;几何 backbone&amp;rdquo;，集成到成熟 SLAM 框架&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据搜索资源（摘要 5：DA3-Long 提升 SLAM 性能、摘要 3：SLAM 位姿优化），需将 DA3 的输出接入具备回环检测的 SLAM 框架，利用 SLAM 的回环模块修正漂移，具体流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;选择 SLAM 框架：&lt;/strong&gt; 优先选择支持 &lt;strong&gt;&amp;ldquo;深度图输入&amp;rdquo;&lt;/strong&gt; 的开源框架，如 &lt;strong&gt;ORB-SLAM3&lt;/strong&gt;（适用于单目 / RGB-D）、&lt;strong&gt;LDSO&lt;/strong&gt;（直接法 SLAM，高精度）、&lt;strong&gt;VINS-Mono&lt;/strong&gt;（视觉惯性融合，抗抖动）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DA3 与 SLAM 的数据交互：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 将 &lt;strong&gt;&amp;ldquo;深度图（prediction.depth）+ 相机内参（prediction.intrinsics）+ 相对姿态（prediction.extrinsics）&amp;rdquo;&lt;/strong&gt; 输出给 SLAM 框架&lt;/li&gt;
&lt;li&gt;SLAM 框架将 DA3 的深度图作为 &lt;strong&gt;&amp;ldquo;视觉观测数据&amp;rdquo;&lt;/strong&gt;，替代传统的 &lt;strong&gt;&amp;ldquo;单目相机的三角化深度&amp;rdquo;&lt;/strong&gt; 或 &lt;strong&gt;&amp;ldquo;LiDAR 点云&amp;rdquo;&lt;/strong&gt;，提升姿态估计精度（摘要 5 提到 DA3-Long 替换 VGGT 后，SLAM 漂移显著降低）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SLAM 回环检测模块修正漂移：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 SLAM 框架检测到 &lt;strong&gt;&amp;ldquo;当前场景与历史场景相似&amp;rdquo;&lt;/strong&gt;（如 ORB-SLAM3 通过 ORB 特征匹配判断回环），会触发 &lt;strong&gt;&amp;ldquo;回环优化&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;优化过程中，SLAM 会根据 DA3 生成的深度图重新计算 &lt;strong&gt;&amp;ldquo;回环帧&amp;rdquo;&lt;/strong&gt; 的姿态，修正之前累积的漂移，并更新全局地图，确保长期建图的一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4. 落地示例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器人探索办公楼时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 在 Jetson AGX Orin 上实时输出深度图（6.35 FPS，摘要 1、4），并将数据传入 ORB-SLAM3&lt;/li&gt;
&lt;li&gt;ORB-SLAM3 用 DA3 的深度图优化相邻帧的姿态（减少初始漂移），同时提取图像的 ORB 特征，与历史帧特征库比对&lt;/li&gt;
&lt;li&gt;当机器人回到 &lt;strong&gt;&amp;ldquo;1 楼大厅&amp;rdquo;&lt;/strong&gt;（之前访问过的场景），ORB-SLAM3 检测到 ORB 特征匹配度超过阈值（回环触发），用 DA3 的深度图验证 &lt;strong&gt;&amp;ldquo;当前深度与历史深度是否一致&amp;rdquo;&lt;/strong&gt;，若一致则优化全局位姿，修正之前的漂移&lt;/li&gt;
&lt;li&gt;最终生成的地图无明显扭曲，机器人能准确判断自己的位置，避障决策更可靠&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;三、语义融合：从 &amp;ldquo;纯几何点云&amp;rdquo; 到 &amp;ldquo;可解释的避障决策&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三语义融合从-纯几何点云-到-可解释的避障决策&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e8%af%ad%e4%b9%89%e8%9e%8d%e5%90%88%e4%bb%8e-%e7%ba%af%e5%87%a0%e4%bd%95%e7%82%b9%e4%ba%91-%e5%88%b0-%e5%8f%af%e8%a7%a3%e9%87%8a%e7%9a%84%e9%81%bf%e9%9a%9c%e5%86%b3%e7%ad%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1. 问题本质与影响&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 生成的点云仅包含 &lt;strong&gt;&amp;ldquo;3D 坐标 + 深度&amp;rdquo;&lt;/strong&gt; 等几何信息，但机器人避障需要语义理解—— 即 &lt;strong&gt;&amp;ldquo;知道障碍物是什么，以及是否需要避开&amp;rdquo;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如点云中的 &lt;strong&gt;&amp;ldquo;地面点&amp;rdquo;&lt;/strong&gt;（可通行）和 &lt;strong&gt;&amp;ldquo;桌子腿点&amp;rdquo;&lt;/strong&gt;（需避开），几何上都是 &lt;strong&gt;&amp;ldquo;空间中的点&amp;rdquo;&lt;/strong&gt;，但对机器人的意义完全不同&lt;/li&gt;
&lt;li&gt;若仅依赖几何信息，机器人可能出现 &lt;strong&gt;&amp;ldquo;荒谬避障&amp;rdquo;&lt;/strong&gt;（如避开地面上的阴影点云）或 &lt;strong&gt;&amp;ldquo;漏避障&amp;rdquo;&lt;/strong&gt;（如未识别细长的栏杆，几何上点云稀疏易被忽略）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. DA3 当前的核心局限&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DA3 是 &lt;strong&gt;&amp;ldquo;纯几何模型&amp;rdquo;&lt;/strong&gt;，无任何语义输出能力：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无法给点云添加 &lt;strong&gt;&amp;ldquo;类别标签&amp;rdquo;&lt;/strong&gt;（如 &amp;ldquo;地面&amp;quot;&amp;ldquo;行人&amp;quot;&amp;ldquo;栏杆&amp;rdquo;）&lt;/li&gt;
&lt;li&gt;无法区分 &lt;strong&gt;&amp;ldquo;可通行区域&amp;rdquo;&lt;/strong&gt;（如平坦地面）和 &lt;strong&gt;&amp;ldquo;不可通行障碍&amp;rdquo;&lt;/strong&gt;（如台阶、桌椅），仅能通过 &lt;strong&gt;&amp;ldquo;点云是否在路径上&amp;rdquo;&lt;/strong&gt; 判断是否避障，精度极低&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. 解决方案：DA3 深度图 + 多模态语义分割，给点云 &amp;ldquo;贴标签&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据搜索资源（摘要 3：RGB-D 语义分割、多模态融合），可利用 DA3 的深度图生成 &lt;strong&gt;&amp;ldquo;RGB-D 数据&amp;rdquo;&lt;/strong&gt;，输入语义分割模型，实现 &lt;strong&gt;&amp;ldquo;几何 + 语义&amp;rdquo;&lt;/strong&gt; 融合：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;技术路径&lt;/th&gt;
          &lt;th&gt;核心原理（结合 DA3）&lt;/th&gt;
          &lt;th&gt;优势与优化方向（来自搜索资源）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RGB-D 语义分割&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. DA3 输出 &amp;ldquo;RGB 图像 + 深度图&amp;rdquo;，组合为 RGB-D 数据（每个像素含 RGB 颜色 + 深度值）；&lt;br&gt;&lt;/br&gt;2. 将 RGB-D 数据输入专门的语义分割网络（如 Cylinder3D、RangeNet++，摘要 3）；&lt;br&gt;&lt;/br&gt;3. 分割网络输出 &amp;ldquo;语义标签图&amp;rdquo;（每个像素对应类别，如 &amp;ldquo;0 = 地面，1 = 行人，2 = 栏杆&amp;rdquo;）；&lt;br&gt;&lt;/br&gt;4. 将语义标签反投影到 DA3 生成的 3D 点云中，每个点云添加 &amp;ldquo;语义属性&amp;rdquo;。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; DA3 的深度图能弥补 RGB 图像的遮挡 / 光照问题（如阴影区域，深度图仍能区分地面与障碍）；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 多模态特征融合（摘要 3），将 DA3 的深度特征与 RGB 特征拼接输入分割网络，提升 &amp;ldquo;细长物体（栏杆）&amp;ldquo;&amp;ldquo;低纹理物体（白色墙壁）&amp;rdquo; 的分割精度。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;几何辅助语义修正&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1. 用 DA3 的深度图计算 &amp;ldquo;点云曲率&amp;rdquo;（高曲率区域多为物体边缘，如桌子角）；&lt;br&gt;&lt;/br&gt;2. 结合语义分割结果，若 &amp;ldquo;语义标签为地面，但曲率过高&amp;rdquo;（如地面上的石头），修正标签为 &amp;ldquo;障碍&amp;rdquo;；&lt;br&gt;&lt;/br&gt;3. 若 &amp;ldquo;语义标签为障碍，但深度值过大&amp;rdquo;（如远处的树木，超出避障范围），标记为 &amp;ldquo;无需避障&amp;rdquo;。&lt;/td&gt;
          &lt;td&gt;- &lt;strong&gt;优势：&lt;/strong&gt; 利用 DA3 的几何精度修正语义分割的误判；&lt;br&gt;&lt;/br&gt;- &lt;strong&gt;优化：&lt;/strong&gt; 高度阈值约束（摘要 3 提到的 ERASORS 算法），根据机器人高度（如 0.1-1.5 米为障碍范围），过滤掉 &amp;ldquo;过高（天花板）&amp;rdquo; 或 &amp;ldquo;过低（小石子）&amp;rdquo; 的点，减少无效避障。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;4. 落地示例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;服务机器人在办公室避障时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DA3 生成 RGB-D 数据（RGB 图像 + 深度图），输入 &lt;strong&gt;Cylinder3D&lt;/strong&gt; 语义分割网络&lt;/li&gt;
&lt;li&gt;分割网络输出语义标签图：&lt;strong&gt;&amp;ldquo;地面（绿色）&amp;ldquo;&amp;ldquo;办公桌（红色）&amp;ldquo;&amp;ldquo;行人（蓝色）&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;点云被标记为三类：&lt;strong&gt;绿色点&lt;/strong&gt;（可通行）、&lt;strong&gt;红色点&lt;/strong&gt;（静态障碍，需绕开）、&lt;strong&gt;蓝色点&lt;/strong&gt;（动态障碍，需实时跟踪）&lt;/li&gt;
&lt;li&gt;机器人的避障算法优先避开红色 / 蓝色点，且仅在 &lt;strong&gt;&amp;ldquo;障碍点位于机器人运动范围内（0.1-1.5 米高）&amp;rdquo;&lt;/strong&gt; 时触发避障，忽略地面小石子（过低）和天花板管道（过高），避免无效决策&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;GPT4Scene&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;gpt4scene&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gpt4scene&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;3D 视觉-语言任务&lt;/strong&gt;旨在将 3D 场景理解与自然语言处理相融合。然而，我们希望更进一步：将 3D 内容融入大型语言模型（LLMs），以实现更自然的人机交互。这一方向的研究最初始于 &lt;strong&gt;3D 点云大型语言模型（3D Point Cloud LLMs）&lt;/strong&gt;。此类模型以点云为输入，能够在 3D 场景中实现自然语言生成与交互。&lt;/p&gt;
&lt;p&gt;早期的 3D 大型语言模型主要关注物体级别的几何结构与外观；后续研究扩展至室内场景，开始侧重物体间的空间关系及场景整体特征，通常会利用场景点云，并结合辅助性 2D 多视图图像。为了更精准地捕捉物体间关系，近期的 3D 大型语言模型会先对场景中的物体进行解耦，再将其输入至 LLM。此外，部分方法会更依赖视觉输入来判断场景上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://gpt4scene.github.io/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT4Scene&lt;/a&gt;&lt;/strong&gt; 是旨在探索纯视觉输入的 VLM 是否能更有效地处理室内 3D 场景理解任务的框架，仅依赖视频输入（无需 3D 点云），核心通过 &lt;strong&gt;3D 重建生成鸟瞰图（BEV）&lt;/strong&gt; 提供全局场景布局，以及 &lt;strong&gt;时空对象标记（STO-markers）&lt;/strong&gt; 建立 BEV 与视频帧的全局-局部关联，解决 VLMs 缺全局表示、帧与时空上下文错位的问题；同时构建含 &lt;strong&gt;165K 文本标注&lt;/strong&gt;的 &lt;strong&gt;ScanAlign 数据集&lt;/strong&gt;，用于微调开源 VLMs。在零样本设置下，该框架显著提升 GPT-4o、Gemini-1.5-Pro 等大模型性能（如 GPT-4o 在 ScanQA 的 ROUGE 指标从 32.6 提升至 &lt;strong&gt;37.7&lt;/strong&gt;）；微调后 &lt;strong&gt;Qwen2-VL-7B&lt;/strong&gt; 在多任务达 SOTA，如 3D 问答（SQA3D）EM1 指标从 40.7 提升至 &lt;strong&gt;60.7&lt;/strong&gt;（相对 &lt;strong&gt;+48%&lt;/strong&gt;），超过此前 SOTA 模型 Chat-Scene &lt;strong&gt;11.0%&lt;/strong&gt;，且能让 VLMs 形成内在 3D 理解能力，为 VLMs 扩展 3D 场景理解提供无缝方案。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/gpt4scene-pipeline.png&#34; alt=&#34;GPT4Scene Pipeline&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;整体工作流程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;整体工作流程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%95%b4%e4%bd%93%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;VLMs 直接处理视频时存在两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;缺全局场景表示&lt;/strong&gt;（第一视角视频看不到房间整体布局）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;帧与时空上下文错位&lt;/strong&gt;（不知道不同帧的同一物体是同一个）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此，GPT4Scene 的框架设计围绕 &lt;strong&gt;&amp;ldquo;补全局&amp;rdquo;&lt;/strong&gt;、&lt;strong&gt;&amp;ldquo;建关联&amp;rdquo;&lt;/strong&gt; 展开：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Reconstruct 路径&amp;rdquo;&lt;/strong&gt; 负责 &lt;strong&gt;&amp;ldquo;补全局&amp;rdquo;&lt;/strong&gt;（生成 BEV）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Partition 路径&amp;rdquo;&lt;/strong&gt; 负责 &lt;strong&gt;&amp;ldquo;提细节&amp;rdquo;&lt;/strong&gt;（采样帧）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;STO-markers&amp;rdquo;&lt;/strong&gt; 负责 &lt;strong&gt;&amp;ldquo;建关联&amp;rdquo;&lt;/strong&gt;（让 BEV 和帧的物体对应）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;起点：&lt;/strong&gt; 一段围绕室内场景拍摄的第一视角视频 $V = {I_1, I_2, \ldots, I_N}$（比如 1000 帧的房间漫游视频）。这段视频会被 &lt;strong&gt;&amp;ldquo;复用&amp;rdquo;&lt;/strong&gt; 到两个预处理环节，而非拆分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;给 &amp;ldquo;Partition（采样）路径&amp;rdquo; 用：&lt;/strong&gt; 取 &lt;strong&gt;&amp;ldquo;部分帧&amp;rdquo;&lt;/strong&gt;（采样），目的是减少 VLM 的输入 token 数量（避免 1000 帧计算量过大），同时保留场景局部细节（如物体外观、局部视角）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;给 &amp;ldquo;Reconstruct（重建）路径&amp;rdquo; 用：&lt;/strong&gt; 用 &lt;strong&gt;&amp;ldquo;完整帧&amp;rdquo;&lt;/strong&gt;，目的是通过连续时序的图像 + 相机外参，还原场景的 3D 结构（点云），进而生成全局布局（BEV）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/2025/gpt4scene-framework.png&#34; alt=&#34;GPT4Scene Framework&#34;  loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如上图，给定一段围绕室内场景移动拍摄的视频 $V = {I_1, \dots, I_N}$，我们首先通过索引近似均匀地采样 $n$ 帧，采样公式为：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;⌊&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo stretchy=&#34;false&#34;&gt;⌋&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;s_i = \lfloor (i-1) \frac{N}{n} \rfloor + 1&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.5806em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3117em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;⌊(&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;−&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:2.0463em;vertical-align:-0.686em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mopen nulldelimiter&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mfrac&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:1.3603em;&#34;&gt;&lt;span style=&#34;top:-2.314em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.23em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;frac-line&#34; style=&#34;border-bottom-width:0.04em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-3.677em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.686em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose nulldelimiter&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;⌋&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mbin&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2222em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6444em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;其中 $\forall i \in {1, \dots, n}$，由此形成采样后的视频帧集合 $V^* = {I_{s_1}, \dots, I_{s_n}}$。该策略在保留场景上下文且无显著信息损失的前提下，减少了视觉语言模型（VLMs）的令牌（token）数量和计算开销。随后，我们利用完整的时间序列进行 3D 场景重建，生成全局鸟瞰图（BEV）。通过后续的 3D 实例分割，可实现物体的精准定位；将该定位结果投影到 BEV 地图和 2D 视频帧上，即可建立时空对象标记（STO-markers）。&lt;/p&gt;
&lt;h4&gt;Reconstruct → 生成 &amp;ldquo;带 STO-markers 的 BEV 图像 $\mathcal{I}_b&amp;rsquo;$&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;reconstruct--生成-带-sto-markers-的-bev-图像-mathcali_b&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#reconstruct--%e7%94%9f%e6%88%90-%e5%b8%a6-sto-markers-%e7%9a%84-bev-%e5%9b%be%e5%83%8f-mathcali_b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt; 给 VLM 提供 &lt;strong&gt;&amp;ldquo;全局场景布局&amp;rdquo;&lt;/strong&gt;（如房间里桌子、椅子的相对位置，墙壁的形状），解决 &lt;strong&gt;&amp;ldquo;缺全局表示&amp;rdquo;&lt;/strong&gt; 问题，同时通过 STO-markers 与采样帧关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 1：3D 重建生成点云&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;用 &lt;strong&gt;&amp;ldquo;完整原始视频&amp;rdquo;&lt;/strong&gt;（而非采样帧）+ 每帧的 &lt;strong&gt;&amp;ldquo;相机外参 $E = {E_1, \ldots, E_N}$&amp;rdquo;&lt;/strong&gt;（描述相机在现实中的位置和姿态），通过 3D 重建技术（如 BundleFusion）生成 3D 点云 $\mathcal{P}$：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&#34;script&#34;&gt;P&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;script&#34;&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mo fence=&#34;true&#34;&gt;(&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;{&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo stretchy=&#34;false&#34;&gt;}&lt;/mo&gt;&lt;mo fence=&#34;true&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{P} = \mathcal{R}\left(\{(I_t, E_t)\}\right)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.6833em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.08222em;&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1em;vertical-align:-0.25em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;minner&#34;&gt;&lt;span class=&#34;mopen delimcenter&#34; style=&#34;top:0em;&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;{(&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.07847em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)}&lt;/span&gt;&lt;span class=&#34;mclose delimcenter&#34; style=&#34;top:0em;&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;步骤 2：点云生成 BEV 图像&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设定 &lt;strong&gt;&amp;ldquo;俯视图相机外参 $E_{top}$&amp;rdquo;&lt;/strong&gt;（模拟从房间正上方往下拍的相机姿态，$E_{top} \in SE(3)$），通过渲染函数 $\mathcal{T}(\cdot)$ 将 3D 点云投影为 2D 的 BEV 图像 $\mathcal{I}_b$：&lt;/p&gt;
&lt;span class=&#34;katex-display&#34;&gt;&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&#34;script&#34;&gt;I&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&#34;script&#34;&gt;T&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi mathvariant=&#34;script&#34;&gt;P&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mathcal{I}_b = \mathcal{T}(\mathcal{P}, E_{top})&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8333em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.07382em;&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.3361em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0738em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mrel&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.2778em;&#34;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.0361em;vertical-align:-0.2861em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.25417em;&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;mopen&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mord mathcal&#34; style=&#34;margin-right:0.08222em;&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;mpunct&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mspace&#34; style=&#34;margin-right:0.1667em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.05764em;&#34;&gt;E&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2806em;&#34;&gt;&lt;span style=&#34;top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;mord mathnormal mtight&#34;&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.2861em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;mclose&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;strong&gt;步骤 3：叠加 STO-markers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;与 &lt;strong&gt;&amp;ldquo;路径 1&amp;rdquo;&lt;/strong&gt; 共享 3D 实例掩码 $M$，保证标记一致性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 3D 掩码 $M$ 投影到 BEV 的 xy 平面（忽略 z 轴高度，只保留平面位置），提取每个物体投影后的 &lt;strong&gt;&amp;ldquo;边界框中心坐标&amp;rdquo;&lt;/strong&gt; $C^{xy} = {C_1^{xy}, \ldots, C_K^{xy}}$&lt;/li&gt;
&lt;li&gt;用函数 $\mathcal{F}(\cdot)$ 将这些坐标作为标记叠加到 BEV 图像上，生成带 STO-markers 的 BEV $\mathcal{I}_b&amp;rsquo; = \mathcal{F}(\mathcal{I}_b, C^{xy})$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键：&lt;/strong&gt; BEV 上的标记 ID 与 &lt;strong&gt;&amp;ldquo;路径 1&amp;rdquo;&lt;/strong&gt; 采样帧的标记 ID 完全一致（比如 BEV 上的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 和采样帧上的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 是同一个桌子），建立 &lt;strong&gt;&amp;ldquo;全局布局&amp;rdquo;&lt;/strong&gt; 与 &lt;strong&gt;&amp;ldquo;局部细节&amp;rdquo;&lt;/strong&gt; 的关联。&lt;/p&gt;
&lt;h4&gt;Partition（视频帧采样）→ 生成 &amp;ldquo;带 STO-markers 的采样帧 $\mathcal{V}^{* \prime}$&amp;rdquo;&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;partition视频帧采样-生成-带-sto-markers-的采样帧-mathcalv-prime&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#partition%e8%a7%86%e9%a2%91%e5%b8%a7%e9%87%87%e6%a0%b7-%e7%94%9f%e6%88%90-%e5%b8%a6-sto-markers-%e7%9a%84%e9%87%87%e6%a0%b7%e5%b8%a7-mathcalv-prime&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;核心作用：&lt;/strong&gt; 给 VLM 提供 &lt;strong&gt;&amp;ldquo;局部物体细节&amp;rdquo;&lt;/strong&gt;（如椅子的颜色、杯子的形状），并通过 STO-markers 标记物体 ID，解决 &lt;strong&gt;&amp;ldquo;帧间物体错位&amp;rdquo;&lt;/strong&gt; 问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 1：帧采样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按公式 $s_i = \lfloor (i-1)\frac{N}{n} \rfloor + 1$（$n$ 默认 8 帧）从原始视频中 &lt;strong&gt;&amp;ldquo;均匀采样&amp;rdquo;&lt;/strong&gt; $n$ 帧，得到采样帧集合 $V^* = {I_{s_1}, \ldots, I_{s_n}}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤 2：叠加 STO-markers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这一步需要 &lt;strong&gt;&amp;ldquo;Reconstruct 路径&amp;rdquo;&lt;/strong&gt; 的输出（3D 实例掩码）作为前提，并非独立完成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从 &lt;strong&gt;&amp;ldquo;Reconstruct 路径&amp;rdquo;&lt;/strong&gt; 获取 3D 实例掩码 $M = {M_1, &amp;hellip;, M_K}$（每个 $M_k$ 对应 1 个物体的 3D 轮廓）&lt;/li&gt;
&lt;li&gt;根据每帧采样帧的 &lt;strong&gt;&amp;ldquo;相机外参&amp;rdquo;&lt;/strong&gt;，将 3D 掩码 $M$ 投影到该 2D 帧上，提取每个物体的 &lt;strong&gt;&amp;ldquo;2D 质心坐标&amp;rdquo;&lt;/strong&gt; $C_i^{uv}$（第 $i$ 帧中第 $k$ 个物体的标记是 $C_{i,k}^{uv}$）&lt;/li&gt;
&lt;li&gt;用函数 $\mathcal{F}(\cdot)$ 将这些坐标作为 &lt;strong&gt;&amp;ldquo;标记&amp;rdquo;&lt;/strong&gt;（如数字 ID &amp;ldquo;物体 1&amp;quot;&amp;ldquo;物体 2&amp;rdquo;）叠加到采样帧上，生成带 STO-markers 的采样帧 $\mathcal{V}^{* \prime} = {\mathcal{F}(I_i, C_i^{uv})}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键：&lt;/strong&gt; 不同采样帧中，同一物体的标记 ID（如 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt;）保持一致，解决 &lt;strong&gt;&amp;ldquo;帧间物体错位&amp;rdquo;&lt;/strong&gt;（比如第 1 帧的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 和第 5 帧的 &lt;strong&gt;&amp;ldquo;物体 3&amp;rdquo;&lt;/strong&gt; 都是同一个桌子）。&lt;/p&gt;
&lt;h4&gt;VLM 输入&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vlm-输入&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vlm-%e8%be%93%e5%85%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;The resulting frames and BEV image, enhanced with STO-markers, are inputs for VLM training and inference&amp;rdquo;&lt;/strong&gt;—— 即最终输入 VLM 的是两个核心组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;em&gt;$\mathcal{V}^{&lt;/em&gt; \prime}$：&lt;/em&gt;* 带 STO-markers 的采样帧（提供 &lt;strong&gt;&amp;ldquo;局部物体外观 + 帧间物体一致性&amp;rdquo;&lt;/strong&gt;）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$\mathcal{I}_b&amp;rsquo;$：&lt;/strong&gt; 带 STO-markers 的 BEV 图像（提供 &lt;strong&gt;&amp;ldquo;全局场景布局 + 物体空间位置&amp;rdquo;&lt;/strong&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两个组件结合，恰好解决了 VLMs 的 3D 理解缺陷：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缺全局？&lt;/strong&gt; BEV 提供房间整体布局&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;帧错位？&lt;/strong&gt; STO-markers 保证同一物体在 BEV 和所有采样帧中 ID 一致，让 VLM 能关联 &lt;strong&gt;&amp;ldquo;局部看到的物体&amp;rdquo;&lt;/strong&gt; 和 &lt;strong&gt;&amp;ldquo;全局中的位置&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
