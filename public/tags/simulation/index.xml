<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Simulation</title>
    <link>http://localhost:1313/tags/simulation/</link>
    <description>Recent content in Simulation on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/simulation/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Habitat 相关项目常识</title>
      <link>http://localhost:1313/blog/2025/2025-12-14-habitat-common-sense/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-14-habitat-common-sense/</guid>
      <description>
        
        
        &lt;p&gt;今天做的另一件事情是去autodl里复现falcon，具体可见blog\2025-11-29-falcon.md下半部分，图形学驱动这些真的很难绷的住。再就是今天我疑似罹患流感，晚上昏昏沉沉的，效率不高，RT-1没有看。&lt;/p&gt;
&lt;h2&gt;复盘：3D 视觉仿真与图形渲染基础科普&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;复盘3d-视觉仿真与图形渲染基础科普&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%a4%8d%e7%9b%983d-%e8%a7%86%e8%a7%89%e4%bb%bf%e7%9c%9f%e4%b8%8e%e5%9b%be%e5%bd%a2%e6%b8%b2%e6%9f%93%e5%9f%ba%e7%a1%80%e7%a7%91%e6%99%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;一、Habitat-Sim 是什么？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一habitat-sim-是什么&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80habitat-sim-%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Habitat-Sim&lt;/strong&gt; 是 Meta（Facebook）开发的 3D 场景仿真器，主要用于训练和评估机器人导航算法。它的核心作用是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;3D 场景渲染&lt;/strong&gt;：加载真实的 3D 场景（如 HM3D 数据集中的室内场景），生成机器人&amp;quot;看到&amp;quot;的图像&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;物理仿真&lt;/strong&gt;：模拟机器人的移动、碰撞检测等物理行为&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;传感器仿真&lt;/strong&gt;：模拟 RGB 相机、深度相机、激光雷达等传感器数据&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多智能体仿真&lt;/strong&gt;：可以同时模拟多个机器人或 NPC（如行人）的行为&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;为什么需要图形渲染？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机器人需要通过&amp;quot;视觉&amp;quot;感知环境，这需要从 3D 场景生成 2D 图像&lt;/li&gt;
&lt;li&gt;即使不显示图像，也需要在 GPU 上渲染以获取传感器数据（RGB、深度图等）&lt;/li&gt;
&lt;li&gt;这就是为什么即使是无头（headless）模式，也需要 GPU 和图形驱动&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;二、OpenGL、EGL、GLX 是什么？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二opengleglglx-是什么&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8copengleglglx-%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;OpenGL（Open Graphics Library）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;openglopen-graphics-library&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#openglopen-graphics-library&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：跨平台的图形渲染 API，用于在 GPU 上绘制 3D 图形&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：告诉 GPU 如何渲染三角形、纹理、光照等&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类比&lt;/strong&gt;：就像告诉画家&amp;quot;用红色画笔在画布上画一个圆&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;EGL（Embedded-System Graphics Library）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;eglembedded-system-graphics-library&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#eglembedded-system-graphics-library&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：OpenGL 和底层显示系统之间的接口层&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：创建 OpenGL 上下文（context），连接 OpenGL 和 GPU 驱动&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么重要&lt;/strong&gt;：在服务器上（没有显示器），EGL 是创建 OpenGL 上下文的唯一方式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无头模式&lt;/strong&gt;：&lt;code&gt;EGL_PLATFORM=surfaceless&lt;/code&gt; 告诉 EGL &amp;ldquo;我不需要显示窗口，只需要在内存中渲染&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;GLX（OpenGL Extension to the X Window System）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;glxopengl-extension-to-the-x-window-system&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#glxopengl-extension-to-the-x-window-system&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：Linux 上连接 OpenGL 和 X11 窗口系统的接口&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：在有显示器的 Linux 系统上创建 OpenGL 上下文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;服务器场景&lt;/strong&gt;：在无头服务器上通常不需要，但 NVIDIA 驱动可能需要它来初始化&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;X11（X Window System）是什么？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;x11x-window-system是什么&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#x11x-window-system%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：Linux/Unix 系统上的图形窗口系统，用于管理窗口、鼠标、键盘等&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;提供图形界面基础：窗口管理、事件处理、输入输出&lt;/li&gt;
&lt;li&gt;客户端-服务器架构：X Server（显示服务器）和 X Client（应用程序）分离&lt;/li&gt;
&lt;li&gt;支持远程显示：可以通过网络在远程机器上显示图形界面&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么服务器上没有 X11？&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;服务器通常没有显示器，不需要窗口系统&lt;/li&gt;
&lt;li&gt;X11 服务器需要显示器硬件支持&lt;/li&gt;
&lt;li&gt;无头服务器不需要图形界面，节省资源&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与 OpenGL 的关系&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;GLX 是 X11 的扩展，用于在 X11 窗口系统中创建 OpenGL 上下文&lt;/li&gt;
&lt;li&gt;有显示器时：应用程序 → GLX → X11 → 显示器&lt;/li&gt;
&lt;li&gt;无头服务器：无法使用 GLX，必须使用 EGL 的无头模式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;三、为什么服务器上需要特殊配置？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三为什么服务器上需要特殊配置&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a%e9%9c%80%e8%a6%81%e7%89%b9%e6%ae%8a%e9%85%8d%e7%bd%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;问题本质&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题本质&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e6%9c%ac%e8%b4%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;服务器（如 AutoDL）通常：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;没有显示器&lt;/strong&gt;：无法创建传统的窗口来显示图形&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;没有 X11 服务器&lt;/strong&gt;：无法使用 GLX 创建 OpenGL 上下文（GLX 依赖 X11）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要 GPU 加速&lt;/strong&gt;：仍然需要 GPU 来加速渲染计算&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;X11 vs EGL 的区别&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;X11 + GLX&lt;/strong&gt;：需要显示器，创建可见窗口，适合桌面环境
&lt;ul&gt;
&lt;li&gt;流程：应用程序 → GLX → X11 Server → 显示器&lt;/li&gt;
&lt;li&gt;示例：在本地 Linux 桌面运行图形程序&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EGL（无头模式）&lt;/strong&gt;：不需要显示器，直接在 GPU 内存渲染，适合服务器
&lt;ul&gt;
&lt;li&gt;流程：应用程序 → EGL → GPU（无窗口）&lt;/li&gt;
&lt;li&gt;示例：在服务器上运行 3D 仿真，渲染结果保存为图像或用于计算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;解决方案：无头渲染（Headless Rendering）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;解决方案无头渲染headless-rendering&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e6%97%a0%e5%a4%b4%e6%b8%b2%e6%9f%93headless-rendering&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;使用 EGL 的 &lt;code&gt;surfaceless&lt;/code&gt; 模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不创建可见窗口&lt;/li&gt;
&lt;li&gt;直接在 GPU 内存中渲染&lt;/li&gt;
&lt;li&gt;渲染结果可以保存为图像或用于计算&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;关键环境变量解释&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;关键环境变量解释&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%b3%e9%94%ae%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e8%a7%a3%e9%87%8a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1. 指定 EGL 平台为无头模式（不需要窗口）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_PLATFORM&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;surfaceless
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2. 强制使用 NVIDIA 的 EGL 实现（而不是 Mesa 软件渲染）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;__EGL_VENDOR_LIBRARY_FILENAMES&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/usr/share/glvnd/egl_vendor.d/10_nvidia.json
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 3. 指定使用哪个 GPU（多 GPU 系统）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EGL_DEVICE_ID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 4. 强制加载 NVIDIA 的图形库（确保子进程也使用正确的库）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;LD_PRELOAD&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/lib/x86_64-linux-gnu/libEGL_nvidia.so.0:/usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0&amp;#34;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;复制代码&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;为什么需要 &lt;code&gt;LD_PRELOAD&lt;/code&gt;？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux 的动态链接器默认会按顺序查找库文件&lt;/li&gt;
&lt;li&gt;系统可能有多个 EGL 实现（NVIDIA、Mesa 等）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LD_PRELOAD&lt;/code&gt; 强制优先加载指定的库，确保使用 NVIDIA 的硬件加速版本&lt;/li&gt;
&lt;li&gt;在多进程环境下，子进程也需要继承这个设置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;关于 Mesa vs NVIDIA 冲突的说明&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mesa&lt;/strong&gt; 是开源的 OpenGL/EGL 实现，提供软件渲染（CPU）或通过 DRI（Direct Rendering Infrastructure）使用集成显卡&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NVIDIA 驱动&lt;/strong&gt; 提供专有的硬件加速 OpenGL/EGL 实现，直接使用 NVIDIA GPU&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;冲突原因&lt;/strong&gt;：Linux 系统默认会优先加载 Mesa 库（&lt;code&gt;libEGL.so.1&lt;/code&gt;），但 Mesa 无法访问 NVIDIA GPU，导致无法创建硬件加速的 OpenGL 上下文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：虽然安装了 Mesa 库解决了 &lt;code&gt;libEGL.so.1&lt;/code&gt; 缺失的问题，但后续需要通过 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 和环境变量强制使用 NVIDIA 的 EGL 库（&lt;code&gt;libEGL_nvidia.so.0&lt;/code&gt;），这样才能利用 GPU 硬件加速&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;EGL/OpenGL 配置（核心难点）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;eglopengl-配置核心难点&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#eglopengl-%e9%85%8d%e7%bd%ae%e6%a0%b8%e5%bf%83%e9%9a%be%e7%82%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;问题诊断流程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;检查错误信息&lt;/strong&gt;：&lt;code&gt;GL::Context: cannot retrieve OpenGL version&lt;/code&gt; → EGL 初始化失败&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检查 GPU&lt;/strong&gt;：&lt;code&gt;nvidia-smi&lt;/code&gt; 确认 GPU 可用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检查库文件&lt;/strong&gt;：确认 NVIDIA EGL 库存在&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;设置环境变量&lt;/strong&gt;：配置 EGL 平台和库路径&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多进程问题&lt;/strong&gt;：使用 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 确保子进程继承设置&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;调试技巧&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设置 &lt;code&gt;MAGNUM_LOG=verbose&lt;/code&gt; 和 &lt;code&gt;HABITAT_SIM_LOG=verbose&lt;/code&gt; 查看详细日志&lt;/li&gt;
&lt;li&gt;使用最小测试脚本逐步验证每个环节&lt;/li&gt;
&lt;li&gt;检查日志中的 &lt;code&gt;found X EGL devices&lt;/code&gt; 确认 GPU 被检测到&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;配置文件路径问题&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;配置文件路径问题&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%b7%af%e5%be%84%e9%97%ae%e9%a2%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;常见问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置文件引用的数据集名称与实际目录不匹配&lt;/li&gt;
&lt;li&gt;场景数据集的实际目录结构与配置期望不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;排查方法&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;检查配置文件中的 &lt;code&gt;data_path&lt;/code&gt; 和 &lt;code&gt;scene_id&lt;/code&gt; 路径&lt;/li&gt;
&lt;li&gt;使用符号链接统一路径结构&lt;/li&gt;
&lt;li&gt;创建最小测试脚本验证路径是否正确&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;五、复现其他 3D 视觉工作的通用检查清单&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五复现其他-3d-视觉工作的通用检查清单&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94%e5%a4%8d%e7%8e%b0%e5%85%b6%e4%bb%96-3d-%e8%a7%86%e8%a7%89%e5%b7%a5%e4%bd%9c%e7%9a%84%e9%80%9a%e7%94%a8%e6%a3%80%e6%9f%a5%e6%b8%85%e5%8d%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;环境检查&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;环境检查&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%8e%af%e5%a2%83%e6%a3%80%e6%9f%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; GPU 驱动已安装：&lt;code&gt;nvidia-smi&lt;/code&gt; 能正常显示&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; CUDA 版本匹配：检查项目要求的 CUDA 版本&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; EGL 库已安装：&lt;code&gt;apt-get install libegl1-mesa&lt;/code&gt;（或使用 NVIDIA 版本）&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Python 环境正确：conda/venv 环境已激活&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;图形渲染配置&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;图形渲染配置&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%be%e5%bd%a2%e6%b8%b2%e6%9f%93%e9%85%8d%e7%bd%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 设置无头模式：&lt;code&gt;export EGL_PLATFORM=surfaceless&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 指定 GPU 设备：&lt;code&gt;export EGL_DEVICE_ID=0&lt;/code&gt;（多 GPU 时）&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 强制使用硬件加速：设置 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 指向 NVIDIA 库&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 多进程环境：确保子进程继承环境变量&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;数据集路径&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;数据集路径&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86%e8%b7%af%e5%be%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 场景数据集路径正确：检查 &lt;code&gt;scene_datasets/&lt;/code&gt; 目录&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Episode 数据集路径正确：检查 &lt;code&gt;datasets/&lt;/code&gt; 目录&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 配置文件中的路径与实际目录匹配&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 符号链接正确：使用 &lt;code&gt;ls -la&lt;/code&gt; 检查符号链接&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;调试技巧&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;调试技巧&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%b0%83%e8%af%95%e6%8a%80%e5%b7%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 启用详细日志：设置 &lt;code&gt;*_LOG=verbose&lt;/code&gt; 环境变量&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 最小测试：创建简单的测试脚本验证每个组件&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 逐步排查：先测试 EGL 初始化，再测试场景加载，最后测试完整流程&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 日志重定向：&lt;code&gt;&amp;gt; log.txt 2&amp;gt;&amp;amp;1&lt;/code&gt; 保存日志避免终端溢出&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;六、常见错误与解决方案速查&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六常见错误与解决方案速查&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e5%b8%b8%e8%a7%81%e9%94%99%e8%af%af%e4%b8%8e%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e9%80%9f%e6%9f%a5&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;错误信息&lt;/th&gt;
          &lt;th&gt;原因&lt;/th&gt;
          &lt;th&gt;解决方案&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;GL::Context: cannot retrieve OpenGL version&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;EGL 初始化失败&lt;/td&gt;
          &lt;td&gt;设置 &lt;code&gt;EGL_PLATFORM=surfaceless&lt;/code&gt; 和 &lt;code&gt;LD_PRELOAD&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;malloc_consolidate(): unaligned fastbin chunk detected&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;多进程环境下库加载冲突&lt;/td&gt;
          &lt;td&gt;使用 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 强制加载 NVIDIA 库&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;ImportError: libEGL.so.1&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;EGL 库未安装&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;apt-get install libegl1-mesa&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;FileNotFoundError: Could not find dataset file&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;数据集路径不匹配&lt;/td&gt;
          &lt;td&gt;检查配置文件中的路径，创建符号链接&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;No Stage Attributes exists for requested scene&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;场景文件路径错误&lt;/td&gt;
          &lt;td&gt;检查场景数据集目录结构，创建缺失的符号链接&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>世界模型的三种路线</title>
      <link>http://localhost:1313/blog/2025/2025-11-18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-18/</guid>
      <description>
        
        
        &lt;h1&gt;世界模型的三种路线&lt;/h1&gt;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzYzNDE2OTYxMw%3D%3D&amp;amp;mid=2247483737&amp;amp;idx=1&amp;amp;sn=085711a5575f31c0e7ebf14e06a49759&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这则推文&lt;/a&gt;给了我很大的启发，事实上，选择这条道路的一个原因就是 Embodied AI 这条道路并未收敛，而真正落地的成果，以 AI 领域的卷度读个博的4、5年时间以内应该就能吃上红利。而且这个时候恰如上个世纪的物理学界，哪怕一个三流的物理学家在那个年代也能做出一流的发现，我也打算依靠这股浪潮。&lt;/p&gt;
&lt;p&gt;好了，接下来进入正文，引用这篇推文的三个问题：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;什么是世界模型：
是一个供人类观看的东西，一个供智能体训练的场所，还是一个图标内部的黑箱——是系统其他部分需要咨询的实际内部模型？
它的输出是静态资产、实时帧、还是主要驱动和预测和控制的潜在状态？
如果撞倒一个虚拟花瓶，系统中的任何部分是否会记住——并利用该记忆来更新其未来的预期——持续超过一帧？&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;所以得从实际应用出发，从具体的原始论文来熟悉这3类工作。&lt;/p&gt;
&lt;h2&gt;World Model as Interface: Marble&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-interface-marble&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-interface-marble&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;World Model as Simulator: Genie/SIMA 2&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-simulator-geniesima-2&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-simulator-geniesima-2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h2&gt;World Model as Cognition: Prof. Lecun&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;world-model-as-cognition-prof-lecun&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#world-model-as-cognition-prof-lecun&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;等待更新中&amp;hellip;&amp;hellip;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>具身调研</title>
      <link>http://localhost:1313/blog/2025/2025-11-14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-14/</guid>
      <description>
        
        
        &lt;h1&gt;具身调研&lt;/h1&gt;&lt;p&gt;对于整个行业得有一个基础的宏观视野，这样一来才能更好地去规划学业与产业。同样的，在本升研的Giant Leap阶段，向老师解释自己的认知与观点并实现共鸣与双向选择是很重要且很有必要的。&lt;/p&gt;
&lt;p&gt;本调研主要基于**&lt;a href=&#34;https://github.com/jiangranlv/embodied-ai-start&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PKU EPIC Lab&lt;/a&gt;&lt;strong&gt;、&lt;/strong&gt;&lt;a href=&#34;https://github.com/TianxingChen/Embodied-AI-Guide&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lumina具身智能社区&lt;/a&gt;**&lt;/p&gt;
&lt;h2&gt;一、基础概念 (Basic Concepts)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一基础概念-basic-concepts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1、 什么是具身智能&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-什么是具身智能&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能（Embodied AI）是指能够在物理或虚拟环境中通过感知、行动和交互来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。&lt;/p&gt;
&lt;p&gt;典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心特征：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有多模态感知能力（视觉、触觉、语音等）&lt;/li&gt;
&lt;li&gt;能够执行动作并影响环境&lt;/li&gt;
&lt;li&gt;学习可以通过与环境交互而不仅仅是被动监督完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 具身智能与其他AI的区别&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-具身智能与其他ai的区别&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96ai%e7%9a%84%e5%8c%ba%e5%88%ab&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;具身智能与传统 AI 的主要区别在于它的主动性、交互性，以及对动作数据的依赖。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考LLM的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的bottleneck。那么很自然的两个关键问题是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;如何scale up机器人数据？&lt;/strong&gt; 例如：GraspVLA（在仿真中以合成的方式猛猛造）, pi0和AgiBot-World（在真实世界猛猛遥操采）, UMI和AirExo（可穿戴设备，如外骨骼的高效数据采集装置）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在不能scale up机器人数据的情况下，如何利用好已有的数据实现你的目的？&lt;/strong&gt; 例如：Diffusion Policy (100条机器人数据训一个特定任务的policy）, Being-H0（利用human video参与policy训练），MimicGen、DemoGen、Robosplat（从一条机器人轨迹中augment得到更多数据）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 研究具身智能的核心原则 (Core Principles)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-研究具身智能的核心原则-core-principles&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型。在CV领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的bounding box；&lt;/p&gt;
&lt;p&gt;但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的setting下？有没有有可能通过更好的setting来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）&lt;/p&gt;
&lt;p&gt;必须认识到用学习（learning）来解决机器人问题并不是理所当然的选择。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统robotics解决不了？为什么非得用learning？&lt;/p&gt;
&lt;h2&gt;二、AI and Robotics Basis&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二ai-and-robotics-basis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cai-and-robotics-basis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程Lab的project最好做到完整实现，而不仅局限于做“代码填空”。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-Embodied-AI&lt;/strong&gt;
王鹤老师《具身智能导论》，找找类似课程替代&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro-to-CV&lt;/strong&gt;
Stanford CS231N&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning (CS285)&lt;/strong&gt;
Berkeley的RL课程，涵盖了Imitation Learning，Online RL, Offline RL等Policy Learning范式，这里用西湖大学老师的代替&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;三、研究平台与工具&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三研究平台与工具&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Simulation Environments&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-simulation-environments&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-simulation-environments&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;2. Robot Platform&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-platform&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-platform&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3&gt;3. Daily ArXiv&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-daily-arxiv&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-daily-arxiv&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;原来只知道Github的awesome系列，想着要daily论文还得去CSDN、知乎、微信公众号和小红书上找，没想到arxiv直接就有了：
具身智能每日最新的论文，按manipulation，VLA， dexterous，humanoid等关键词进行划分：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jiangranlv/robotics_arXiv_daily&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jiangranlv/robotics_arXiv_daily&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;四、Research Field on Robots&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四research-field-on-robots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9bresearch-field-on-robots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Grasping&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-grasping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-grasping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;抓取（Grasping）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（force closure），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。&lt;/p&gt;
&lt;p&gt;常见任务有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single object grasping（单物体抓取）&lt;/strong&gt;：抓取一个物体，物体通常放在桌子上。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clutter scene grasping（堆叠场景抓取）&lt;/strong&gt;：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional grasping（带语义抓取）&lt;/strong&gt;：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的part和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用机械手末端有（难度依次递增）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suction cup（吸盘）&lt;/strong&gt;：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的0/1控制信号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel gripper（平行夹抓）&lt;/strong&gt;：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被DexNet和GraspNet两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-fingered hand（多指手）&lt;/strong&gt;，又称Dexterous hand（灵巧手）：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：DexGraspNet、Dexonomy（覆盖多样化手型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-loop methods（开环执行）&lt;/strong&gt;：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 grasping pose estimation。Data Source：Grasp Synthesis，如 DexNet、GraspNet-1B. Learning Approaches：GSNet。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closed-loop methods（闭环执行）&lt;/strong&gt;：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 policy，持续输入视觉信息并输出机械臂动作。代表工作：GraspVLA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Manipulation&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-manipulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-manipulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;操作（Manipulation）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。&lt;/p&gt;
&lt;p&gt;**Articulated Object Manipulation：**铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part理解（GAPartNet）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）
**Deformable Object Manipulation：**柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。
**Non-prehensile Manipulation：**非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 contact-rich 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。
**Dexterous Manipulation：**灵巧操作，与non-prehensile类似，但通常有更多的contact和更高的控制维度。一个经典的任务是in-hand reorientation，虽然它已经几乎被RL解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。
**Bimanual Manipulation：**双臂操作，重点在于如何实现双臂的协调与配合。
**Mobile Manipulation：**移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同&lt;/p&gt;
&lt;h3&gt;3. Navigation(NOW)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-navigationnow&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-navigationnow&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Navigation 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。&lt;/p&gt;
&lt;p&gt;常见任务包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Point Goal Navigation (PointNav)&lt;/strong&gt;：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Goal Navigation (ObjectNav)&lt;/strong&gt;：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Navigation (VLN)&lt;/strong&gt;：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embodied Question Answering (EQA)&lt;/strong&gt;：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：机器人持续感知并跟随动态目标（如人或移动物体）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map-based Navigation&lt;/strong&gt;：基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: Object Goal Navigation using Goal-Oriented Semantic Exploration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompting-Large-Model Navigation&lt;/strong&gt;：通过对物理世界进行解释得到prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: NavGPT, CogNav&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video-based VLM Navigation&lt;/strong&gt;：通过端到端训练基于视频输入的视觉语言大模型，通过tokens来建模导航历史，和用VLM直接输出未来导航动作。相关工作NaVid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unified Embodied Navigation&lt;/strong&gt;：最新研究趋势是将多种导航任务统一建模，常使用纯RGB输入，并将目标描述转换为语言指令。代表性工作：Uni-Navid，统一多种导航任务。NavFoM,统一导航任务和embodiment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Locomotion&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-locomotion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-locomotion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Locomotion 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 Whole-body Control (WBC) 的控制方法，用于实现 四足（Quadrupedal） 与 双足（Bipedal / Humanoid） 运动。&lt;/p&gt;
&lt;p&gt;技术路线上，2019年以前主要靠传统的MPC控制实现（例如波士顿动力），目前主流的方法是Sim2Real RL, 以下主要讨论这类主流范式。 既然谈及RL，又分为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning from manually designed reward&lt;/strong&gt; (自己写reward提供desired behavior) (WoCoCo【任务目的：通过reward设计让机器人完成某些特定任务】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from human data&lt;/strong&gt; (data提供desired behavior，也叫做tracking)【主流】 (ASAP)【任务目的：模仿某一段人类数据中的动作（输入：现在的state和目标的state；输出这一步的action）】
如果人形机器人能完成对特定人类动作的tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -&amp;gt; whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度） 这一系列的工作是H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack等等，至此Control最基本的问题应该well-defined了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下一个阶段会涉及到一点除了control之外的东西，就是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;引入【视觉】实现户外自主化（perceptive locomotion）&lt;/strong&gt;；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real 【visualmimic】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入【物体】实现loco-manipulation&lt;/strong&gt;；例如人型机器人搬箱子，难点：物体的dynamics【HDMI】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对上述两种task的组合&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调【语义的泛化性】&lt;/strong&gt;，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）【leverb】&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强调一些特殊的capability&lt;/strong&gt;（比如HuB做极端平衡，Any2Track受很大的力干扰摔不倒, Hitter做一个特殊的乒乓球task，spi-active做sim2real对齐让机器人能走直线）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;五、Learning based Research Field&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五learning-based-research-field&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94learning-based-research-field&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Few-shot Imitation Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-few-shot-imitation-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-few-shot-imitation-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向主要聚焦于 小模型 (small-model) 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如50条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：Behavior Cloning、DAgger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当前主流方法&lt;/strong&gt;：ACT、Diffusion Policy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。&lt;/p&gt;
&lt;h3&gt;2. Robot Foundation Model&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-robot-foundation-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-robot-foundation-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;该方向属于 大模型 (foundation model) 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。
目前主流的做法是Vision-Language-Action Models (VLA), 借助VLM的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenVLA&lt;/strong&gt;：第一个开源且易于follow的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi0 / Pi0.5&lt;/strong&gt;：目前公认最work的VLA，10K+ hours teleop data训练的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraspVLA&lt;/strong&gt;：基于纯仿真数据的抓取任务的VLA。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有少量工作没有借助VLM&lt;/strong&gt;，单纯靠机器人数据做scaling，代表有RDT-1B和Large Behavior Model (LBM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Sim-to-Real Reinforcement Learning (Distillation)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-sim-to-real-reinforcement-learning-distillation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-sim-to-real-reinforcement-learning-distillation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;从仿真到真实 (Sim-to-Real) 是强化学习在具身智能中的关键挑战之一。&lt;/p&gt;
&lt;p&gt;目前最成功的落地应用集中在 Locomotion（运动控制），而在 Manipulation（操作任务） 上仍面临sim2real Gap过大的问题。&lt;/p&gt;
&lt;p&gt;核心思路通常包括 策略蒸馏 (policy distillation)、域随机化 (domain randomization) 与 现实校准 (real calibration) 等技术。&lt;/p&gt;
&lt;h3&gt;4. Real-World Reinforcement Learning&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-real-world-reinforcement-learning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-real-world-reinforcement-learning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-world RL 指直接在现实环境中进行探索式学习。&lt;/p&gt;
&lt;p&gt;这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**从零开始的真实世界强化学习：**Hil-Serl&lt;/li&gt;
&lt;li&gt;**基于VLA的真实世界微调 (Fine-tuning)：**部分近期工作尝试利用预训练VLA进行现实强化学习微调，但仍处于早期探索阶段。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. World Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-world-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-world-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;World Model 最早起源于 基于模型的强化学习 (Model-based RL)，旨在通过内部世界建模来提升采样效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代表性工作包括 Dreamer 系列&lt;/strong&gt;（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在具身智能的最新语境中，World Model 的概念被拓展为 条件视频生成模型 (conditioned video generation model)，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。&lt;/p&gt;
&lt;h2&gt;六、相关领域&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六相关领域&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1. Graphics&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-graphics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-graphics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;图形学在机器人与具身智能中的两大重要应用是 simulation（仿真） 与 rendering（渲染）。&lt;/p&gt;
&lt;p&gt;**Simulation：**用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述IsaacLab等
**Rendering：**用于生成高质量的图像或视频，支撑感知模型（如视觉Transformer）的训练与评估。例如：Blender：开源的三维建模与渲染软件。
**系统性学习图形学推荐课程：**Games 101, 103&lt;/p&gt;
&lt;h3&gt;2. Hardware&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tele-operation（遥操作）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**末端操作设备：**如 Space Mouse，用于控制机械臂的末端姿态。
**主从臂系统：**如 Gello，实现高精度的力控遥操作。
**可穿戴设备：**如 AirExo 或 UMI，通过外骨骼或手部设备实现自然交互与示教。
&lt;strong&gt;Sensors（传感器）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**Camera（视觉）：**RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。&lt;/li&gt;
&lt;li&gt;**Force Sensor（力传感器）：**用于检测接触力矩，常安装于末端。&lt;/li&gt;
&lt;li&gt;**Tactile Sensor（触觉传感器）：**如 GelSight、DIGIT，用于捕捉表面接触信息。&lt;/li&gt;
&lt;li&gt;**Mocap System（动作捕捉系统）：**用于精确追踪人体或机器人位姿，常用于收集示教数据或标定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Mainstream Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-mainstream-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-mainstream-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Transformer&lt;/li&gt;
&lt;li&gt;Diffusion、Flow Matching 由于能够有效建模多峰分布的生成模型sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Foundation Models&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-foundation-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-foundation-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LLM（Large Language Model） 通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：GPT / Claude / Gemini：通用语言推理模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DINO系列：通过大规模的自监督学习 (self-supervised learning) 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。&lt;/li&gt;
&lt;li&gt;CLIP：通过大规模的图文匹配对上的 对比学习 (contrastive learning) ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。&lt;/li&gt;
&lt;li&gt;VLM（Vision-Language Model） 通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于VLA模型的初始化，或用于场景理解与任务规划。代表模型包括：Qwen-VL系列、GPT4-o、Gemini。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5. 3D Vision&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-3d-vision&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-3d-vision&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;详见Intro-to-CV课程，此处仅给出一些具身任务中常用的三维视觉技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三维生成与重建&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**相机标定：**利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。&lt;/li&gt;
&lt;li&gt;**单目三维生成：**根据单张RGB图片生成对应物体的三维几何，在real-to-sim中是一种常用的获得物体几何的方法。&lt;/li&gt;
&lt;li&gt;**单目深度估计：**通过单张RGB图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。&lt;/li&gt;
&lt;li&gt;**位姿估计与追踪：**通过单张或多张RGB图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为action的一种表征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维表示&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**网格（Mesh）：**通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。&lt;/li&gt;
&lt;li&gt;**点云（Point Cloud）：**通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此GraspNet使用点云作为输入，实现了非常鲁棒的抓取位姿预测。&lt;/li&gt;
&lt;li&gt;**Gaussian Splatting：**通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在real-to-sim中是一种常用的重建场景几何的表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;三维理解&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>重新思考三维空间感知与具身导航决策在毕设中的研究点</title>
      <link>http://localhost:1313/blog/2025/2025-11-12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-11-12/</guid>
      <description>
        
        
        &lt;p&gt;首先，我们的Baseline —— LOVON (Legged Open-Vocabulary Object Navigator, 2025) 是一个在 Gym-Unreal（即 Gym-UnrealCV 风格的仿真 benchmark）上做了大规模仿真实验来验证其开阔词表目标搜索与导航能力；文中强调用虚幻环境来做长航时、动态目标搜索的系统验证（包括视觉抖动、目标短暂消失等问题）并在仿真里验证 Laplacian Variance Filtering、语言→运动模型等模块。也进行了真实腿式机器人（Unitree 系列）上的跨域验证以检验 sim→real。&lt;/p&gt;
&lt;!-- truncate --&gt;
&lt;h2&gt;思考&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;思考&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%9d%e8%80%83&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们第一步要做的就是 Define Problem。若能有清晰的问题定位 + 合理指标 +实证结果 +对比分析，就有很大机会产出成果。一个很好的方法就是自问自答：&lt;/p&gt;
&lt;h3&gt;一、现状定位：用 LOVON 的方法在真机上效果很差──最关键的失败点是什么？“机器狗撞门框”？那是什么原因？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;一现状定位用-lovon-的方法在真机上效果很差最关键的失败点是什么机器狗撞门框那是什么原因&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%80%e7%8e%b0%e7%8a%b6%e5%ae%9a%e4%bd%8d%e7%94%a8-lovon-%e7%9a%84%e6%96%b9%e6%b3%95%e5%9c%a8%e7%9c%9f%e6%9c%ba%e4%b8%8a%e6%95%88%e6%9e%9c%e5%be%88%e5%b7%ae%e6%9c%80%e5%85%b3%e9%94%ae%e7%9a%84%e5%a4%b1%e8%b4%a5%e7%82%b9%e6%98%af%e4%bb%80%e4%b9%88%e6%9c%ba%e5%99%a8%e7%8b%97%e6%92%9e%e9%97%a8%e6%a1%86%e9%82%a3%e6%98%af%e4%bb%80%e4%b9%88%e5%8e%9f%e5%9b%a0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;是因为纯2D检测＋动作映射，缺乏深度／3D理解？&lt;/li&gt;
&lt;li&gt;是因为没有障碍物避障规划？&lt;/li&gt;
&lt;li&gt;是因为导航规划缺失，仅“向目标走”而不考虑路径？&lt;/li&gt;
&lt;li&gt;还是别的问题（如机器狗控制延迟、检测误差大、目标消失后无追踪策略）？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LOVON的原理，也就是视觉追踪的原理在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;目标提取与筛选（&lt;code&gt;_yolo_image_post_process&lt;/code&gt; 方法）&lt;/p&gt;
&lt;p&gt;先通过 &lt;code&gt;object_extractor&lt;/code&gt; 从任务指令（默认任务是 run to the person at speed of 0.36 m/s，提取目标为 “person”）中提取目标类别。YOLO 模型输出所有检测框后，只保留类别与提取目标一致的框，过滤无关目标。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;滑动窗口历史管理&lt;/p&gt;
&lt;p&gt;初始化 5 个历史缓存列表，分别存储目标类别、置信度、归一化坐标（xyn）、归一化宽高（whn）、像素坐标（xyxy）。每帧仅保留置信度最高的检测框，加入缓存列表；当列表长度超过 &lt;code&gt;lengthen_filter&lt;/code&gt; 时，删除最早的帧，维持窗口大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;追踪结果计算&lt;/p&gt;
&lt;p&gt;对缓存列表中的数据取平均值，得到平滑后的置信度、坐标和宽高。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;object_xyn[0]&lt;/code&gt; 是目标中心的水平归一化坐标（0~1，0 为左边界、0.5 为图像中心、1 为右边界）。&lt;/li&gt;
&lt;li&gt;若目标在图像中心（&lt;code&gt;xyn[0] ≈ 0.5&lt;/code&gt;）：机器狗沿前后方向运动（&lt;code&gt;v_x&lt;/code&gt; 按任务指令速度，如 0.36m/s，&lt;code&gt;v_y = 0&lt;/code&gt;，&lt;code&gt;w_z = 0&lt;/code&gt;），即 “往前走”。&lt;/li&gt;
&lt;li&gt;若目标偏左（&lt;code&gt;xyn[0] &amp;lt; 0.5&lt;/code&gt;）：&lt;code&gt;w_z&lt;/code&gt; 为正（顺时针旋转），同时 &lt;code&gt;v_x&lt;/code&gt; 降低，直到目标回到中心；偏右则相反。&lt;/li&gt;
&lt;li&gt;任务指令中的 “speed” 仅限制 &lt;code&gt;v_x&lt;/code&gt; 的最大值，而非强制固定 &lt;code&gt;v_x&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;统计列表中出现次数最多的目标类别，作为当前追踪目标（避免单帧误检影响）。若目标类别为 “NULL”（无有效检测），则重置追踪结果为默认值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;跟丢的判定标准&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单帧无检测：YOLO 未检测到与 &lt;code&gt;extracted_object&lt;/code&gt; 匹配的框 → 往历史缓存中添加 “NULL” 和 0 置信度。&lt;/li&gt;
&lt;li&gt;连续跟丢：当历史缓存（长度由 &lt;code&gt;lengthen_filter&lt;/code&gt; 控制）中 “NULL” 出现次数最多 → &lt;code&gt;most_common_object&lt;/code&gt; 变为 “NULL”，&lt;code&gt;avg_confidence&lt;/code&gt; 设为 0 → 判定为 “跟丢”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;motion_predictor&lt;/code&gt; 接收 “跟丢状态” 后，生成搜索型 &lt;code&gt;motion_vector&lt;/code&gt;：&lt;/p&gt;
&lt;p&gt;通常是「旋转搜索」：&lt;code&gt;v_x = 0&lt;/code&gt;（不前后动）、&lt;code&gt;v_y = 0&lt;/code&gt;（不左右动）、&lt;code&gt;w_z ≠ 0&lt;/code&gt;（缓慢旋转，扫描周围环境）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;机器狗撞门框的原因在于，这里现实环境的部署代码通过 YOLO 只识别到了目标但是没有理解环境与障碍物，而当人消失在门后时，最后一帧这个目标是在画面中心的，因此机器狗会往前走直到撞到门框，又或者笨笨的在门框那个位置旋转搜索。因为没有开源其仿真智能体的代码所以不知道模拟环境是怎么规避这个问题的&lt;/p&gt;
&lt;h3&gt;二、Gap 与定位：基于你上面的回答，问题在哪儿？用一句话描述这里的 gap（研究空白）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;二gap-与定位基于你上面的回答问题在哪儿用一句话描述这里的-gap研究空白&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%8cgap-%e4%b8%8e%e5%ae%9a%e4%bd%8d%e5%9f%ba%e4%ba%8e%e4%bd%a0%e4%b8%8a%e9%9d%a2%e7%9a%84%e5%9b%9e%e7%ad%94%e9%97%ae%e9%a2%98%e5%9c%a8%e5%93%aa%e5%84%bf%e7%94%a8%e4%b8%80%e5%8f%a5%e8%af%9d%e6%8f%8f%e8%bf%b0%e8%bf%99%e9%87%8c%e7%9a%84-gap%e7%a0%94%e7%a9%b6%e7%a9%ba%e7%99%bd&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;比如：“在足式机器人真实场景下，当前Open-vocab检测＋简单动作生成不能有效处理目标暂时丢失和复杂障碍物，导致跟踪／导航失败”。还是要聚焦 “障碍物避障” 或 “三维深度理解”？&lt;/p&gt;
&lt;p&gt;在足式机器人开放世界目标追踪任务中，现有基于纯 2D 视觉目标检测的追踪 - 运动映射方案，因缺乏环境障碍物感知与三维空间理解，且目标暂时丢失后仅采用无环境适配的旋转搜索策略，导致无法应对 “目标被遮挡 / 消失后因路径误判碰撞障碍物” 等真实场景挑战，难以实现稳健的长时追踪与运动控制（具体有没有3D视觉目标检测的论文工作，现在还没有做过调研）&lt;/p&gt;
&lt;h3&gt;三、重要性在哪里？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;三重要性在哪里&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%b8%89%e9%87%8d%e8%a6%81%e6%80%a7%e5%9c%a8%e5%93%aa%e9%87%8c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对学术来说：为什么“足式机器人 + open-vocab目标导航/跟踪”值得研究？是否当前工作少？&lt;/li&gt;
&lt;li&gt;对应用来说：在真实环境（室内／复杂家具／光照变化）中，解决这个问题会带来什么改进？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于我来说，我还只是一个入门新手，打算通过本科毕设的机会，从3D世界理解和具身导航决策这个小角度切入来入门具身领域，所以我也说不清楚学术和应用上的重要性，只求发ccfb以上的paper证明自己&lt;/p&gt;
&lt;h3&gt;四、创新点初步想法？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;四创新点初步想法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%9b%9b%e5%88%9b%e6%96%b0%e7%82%b9%e5%88%9d%e6%ad%a5%e6%83%b3%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过和导师学长们讨论列出了很多可能的优化方向（环境理解增强、分层决策升级），从这些中最可能做出论文中&lt;strong&gt;可量化贡献&lt;/strong&gt;的一个或两个是什么？
比如：“用 BEV 俯视地图 +轨迹预测 来增强 open-vocab 目标导航”；或者：“在足式机器人上验证视觉+深度融合检测在目标丢失场景下的跟踪稳定性提升”。哪一个更倾向？为什么？&lt;/p&gt;
&lt;p&gt;我不知道量化贡献的指标可以在哪里进一步优化啊，原因也可能在于我读的文献太少了，LOVON在仿真里所使用的指标为衡量 100 次实验中完成任务的平均步数、衡量 100 次实验中成功完成任务的比例两个，而如何去量化现实任务的指标与sim2real的优化，因为文献读的不多所以暂时我还不能回答这个问题&lt;/p&gt;
&lt;h3&gt;五、可量化指标与对比：要发论文，必须有可测量的结果，可以测量哪些指标？例如：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;五可量化指标与对比要发论文必须有可测量的结果可以测量哪些指标例如&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e4%ba%94%e5%8f%af%e9%87%8f%e5%8c%96%e6%8c%87%e6%a0%87%e4%b8%8e%e5%af%b9%e6%af%94%e8%a6%81%e5%8f%91%e8%ae%ba%e6%96%87%e5%bf%85%e9%a1%bb%e6%9c%89%e5%8f%af%e6%b5%8b%e9%87%8f%e7%9a%84%e7%bb%93%e6%9e%9c%e5%8f%af%e4%bb%a5%e6%b5%8b%e9%87%8f%e5%93%aa%e4%ba%9b%e6%8c%87%e6%a0%87%e4%be%8b%e5%a6%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;目标被丢失的次数／恢复次数&lt;/li&gt;
&lt;li&gt;障碍物碰撞次数&lt;/li&gt;
&lt;li&gt;成功到达目标的比例&lt;/li&gt;
&lt;li&gt;路径长度／时间／效率&lt;/li&gt;
&lt;li&gt;跟踪保持时间／跟丢时间
− 真机 vs 仿真的差距（sim2real gap）
能够在实机上测这些指标吗？哪些可能无法测？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;六、实验平台／可行性：&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;六实验平台可行性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%85%ad%e5%ae%9e%e9%aa%8c%e5%b9%b3%e5%8f%b0%e5%8f%af%e8%a1%8c%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;已有的硬件是 Unitree Go2 足式机器人，这很好。你能控制机器人做什么动作（向前、转、停止、避障）？你能获取哪些传感器数据（RGB、深度、IMU、里程计）？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否有仿真实验环境（如 Gym-UnrealCV 场景）可以先做仿真再到实机？仿真与实机之间能记录相同指标吗？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;时间上本科毕设资源有限，预计能做多少场景／多少实验次数？这个对决定指标和可行性很重要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于硬件设备，要关注[官方SDK文档](&lt;a href=&#34;https://support.unitree.com/home/en/developer&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://support.unitree.com/home/en/developer&lt;/a&gt;）：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;一、动作控制能力&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基础运动控制
&lt;ul&gt;
&lt;li&gt;前进 / 后退 / 转向：通过Move(vx, vy, vyaw)函数直接设置线速度（vx/vy）和角速度（vyaw），支持相对于世界坐标系的运动控制。例如，Move(0.5, 0, 0)使机器人以 0.5m/s 速度向前移动。&lt;/li&gt;
&lt;li&gt;停止：调用StopMove()立即终止所有运动，进入静止状态。&lt;/li&gt;
&lt;li&gt;步态切换：通过SwitchGait(int d)选择不同步态（如小跑、踱步），或使用ContinuousGait(bool flag)启用连续步态模式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;高级动作与姿态调整
&lt;ul&gt;
&lt;li&gt;站立 / 坐下：StandUp()和Sit()实现起立和坐下动作，RecoveryStand()用于从侧翻状态恢复。&lt;/li&gt;
&lt;li&gt;身体姿态控制：Euler(roll, pitch, yaw)可调整机身倾角，BodyHeight(float height)动态改变离地高度。&lt;/li&gt;
&lt;li&gt;特技动作：支持FrontFlip()前空翻、FrontJump()跳跃等复杂动作（需硬件支持）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;避障功能
&lt;ul&gt;
&lt;li&gt;自主避障：通过ObstacleAvoidClient类启用避障模块，机器人可实时检测障碍物并调整路径。需调用EnableObstacleAvoidance()激活，并在移动时保持避障服务运行。&lt;/li&gt;
&lt;li&gt;传感器融合：避障依赖激光雷达（PRO/EDU 版）或深度相机（AIR 版）与 IMU 数据融合，实现动态环境下的安全导航。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;二、传感器数据获取&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉传感器
&lt;ul&gt;
&lt;li&gt;RGB 图像：通过 ROS2 话题/camera/image_raw获取 720P/1080P 实时视频流，支持 WebRTC 低延迟传输。&lt;/li&gt;
&lt;li&gt;深度数据：PRO/EDU 版搭载 4D 激光雷达（L1），可输出 360°×90° 点云数据（/go2/camera/depth）；AIR 版通过 Intel RealSense D435i 深度相机提供毫米级深度信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;惯性测量单元（IMU）
&lt;ul&gt;
&lt;li&gt;原始数据：通过 ROS2 话题/imu/data获取加速度（a_x, a_y, a_z）、角速度（ω_x, ω_y, ω_z）和四元数姿态（q_w, q_x, q_y, q_z）。&lt;/li&gt;
&lt;li&gt;坐标系转换：SDK 提供工具函数处理不同框架下的四元数顺序（如 Isaac Gym 与 Isaac Sim 的差异）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;里程计与定位
&lt;ul&gt;
&lt;li&gt;状态估计：通过激光雷达 + IMU 融合（如 LIO-SAM 算法）或腿部运动学模型（关节编码器数据）实现里程计输出。ROS2 话题/odom提供机器人位姿（x, y, θ）和速度信息。&lt;/li&gt;
&lt;li&gt;精度优化：紧耦合 LiDAR-IMU - 腿部里程计系统可在无特征环境下实现亚米级定位精度，在线学习机制适应负载和地形变化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他传感器
&lt;ul&gt;
&lt;li&gt;关节状态：实时获取 12 个关节的角度、角速度和扭矩（/joint_states），支持电机健康监测。&lt;/li&gt;
&lt;li&gt;足端力反馈：PRO/EDU 版配备足端力传感器（F_z），用于复杂地形下的步态调整。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于仿真环境，我有本地的Gym-Unrealcv仿真场景，但是苦恼于LOVON没有开源其仿真代码所以搁置着，不清楚下一步是根据部署代码反推仿真代码还是换一个仿真环境如MatterPort3D
时间本身还是比较充裕的，到开题答辩之前至少有1个月时间&lt;/p&gt;
&lt;h2&gt;研究现状&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究现状&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e7%8e%b0%e7%8a%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;维度&lt;/th&gt;
          &lt;th&gt;内容总结&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;基线模型&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON（LoVi: Open-vocabulary Visual Navigation and Tracking）在仿真中近乎完美（≈100% success rate），但在真实环境严重失效。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心问题&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;LOVON 只用 YOLO 的 2D 框坐标来做“视觉 → 动作”映射，没有任何 3D 环境建模或避障机制。目标消失（如进门）时，机器人仍执行“往前走”动作 → 撞门框。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;可用硬件&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Unitree Go2（有RGB、深度、IMU、里程计、足端力传感器）。具备基本避障API、Move(vx,vy,vyaw)控制接口。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;仿真环境&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;有Gym-UnrealCV，但缺少LOVON仿真智能体代码。可能考虑复刻或转向MatterPort3D。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;研究目标雏形&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;希望提升LOVON从2D视觉到更稳健3D环境理解（environment understanding + navigation fusion）的能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;问题定义&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题定义&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e5%ae%9a%e4%b9%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;当前的 open-vocabulary 视觉追踪方法（如 LOVON）在仿真中表现优异，但在真实足式机器人环境中严重退化，其原因在于缺乏对三维环境几何与障碍物的建模能力。
其技术设计恰好规避了仿真环境的局限性，同时最大化了自身优势，具体体现在 3 个 “无冲突”：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “真实场景的 3D 感知需求”，纯 2D 视觉足够&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真环境中，目标的 “2D 图像坐标” 与 “实际空间位置” 完全对齐（如虚拟场景中 xyn=0.5 即代表物理上的正前方，无门框等 3D 遮挡物），无需深度信息即可判断路径是否可行。而 LOVON 的核心是 “2D 视觉 + 运动向量映射”，恰好适配这种需求，无需额外的 3D 深度理解模块。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真无 “不可控干扰”，搜索策略高效&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真中的 “目标丢失” 仅为 “目标移出 90 度扇形视野”（可通过旋转搜索快速重新捕获），无真实场景的 “目标被门框完全遮挡”“机器人被碰撞” 等不可控干扰。LOVON 的旋转搜索策略（vx=0、w_z≠0）在仿真中能高效覆盖视野，而不会像真实场景那样因 “旋转时忽略障碍物” 导致碰撞。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;仿真数据与模型训练 “高度同源”&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仿真使用的目标类别（背包、椅子、行人）、运动速度（0.3~0.7m/s）、场景光照均与 LOVON 的训练数据集（100 万样本，摘要 1）高度匹配：IOE 对 “椅子”“行人” 的类别映射无误差，L2MM 的运动预测参数（如 β=10）也针对仿真场景校准（摘要 3），避免了真实场景中 “未见过的目标形态”“突发速度变化” 导致的误差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当目标被暂时遮挡（如进入门后）或在复杂结构环境中移动时，机器人仅凭2D像素坐标进行动作决策，无法有效区分“自由空间”与“障碍区域”，导致运动策略失效（如撞门、原地旋转）。
因此，本研究旨在探索一种融合3D环境理解的目标跟踪与导航方法，在保持LOVON开放词汇指令能力的前提下，提高其在真实环境中的鲁棒性与安全性。&lt;/p&gt;
&lt;h2&gt;研究方向&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;研究方向&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;方向&lt;/th&gt;
          &lt;th&gt;名称&lt;/th&gt;
          &lt;th&gt;思路简述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;A. 环境理解增强（BEV / Depth / 3D Occupancy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;给LOVON加“视觉深度感知”，即在YOLO检测的基础上，通过深度图重投影到3D坐标系或BEV平面，建立占用图。再利用该图进行避障或规划。&lt;/td&gt;
          &lt;td&gt;你能做仿真+实机对比，提出一种“轻量级3D-aware追踪方法”。 → 投稿到 &lt;strong&gt;IROS/ICRA workshop 或 CCF-C AI Robotics会议&lt;/strong&gt;。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;B. 跟踪 + 导航分层融合（Hierarchical Policy）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;把“跟踪”和“导航”分成两个层次：高层目标预测、低层路径规划。你可以用简单预测（如卡尔曼滤波预测目标短期轨迹）+ BEV局部避障（A*或DWA）。&lt;/td&gt;
          &lt;td&gt;可以与LOVON对比“复杂场景成功率”→ 写出完整paper。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;选择 A + B 结合的小主题：“基于3D视觉感知与分层导航策略的开放词汇足式机器人目标追踪”(但这个一听就感觉不少人做过类似的课题非常卷)&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类别&lt;/th&gt;
          &lt;th&gt;指标&lt;/th&gt;
          &lt;th&gt;含义&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;任务层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Success Rate (SR)&lt;/td&gt;
          &lt;td&gt;机器人在有限步数内到达目标的比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Average Steps (AS)&lt;/td&gt;
          &lt;td&gt;成功任务平均步数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Collision Rate (CR)&lt;/td&gt;
          &lt;td&gt;发生障碍碰撞的任务比例&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;视觉层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Target Loss Time (TLT)&lt;/td&gt;
          &lt;td&gt;目标丢失后重新识别的平均时间&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Tracking Stability (TS)&lt;/td&gt;
          &lt;td&gt;目标检测框抖动方差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Sim2Real 层面&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;ΔSR (Sim→Real)&lt;/td&gt;
          &lt;td&gt;仿真与实机成功率差距&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;效率指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;FPS / Latency&lt;/td&gt;
          &lt;td&gt;模型推理帧率与系统延迟&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;安全指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Distance Margin&lt;/td&gt;
          &lt;td&gt;与障碍最近距离的平均值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;在仿真中先实现自动收集 SR、AS、CR。&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;实机可手动统计 SR 和 CR，或用里程计测轨迹。&lt;/p&gt;
&lt;p&gt;可定义 3 个场景（开阔场 / 门框 / 桌椅环境）各跑10次。&lt;/p&gt;
&lt;h2&gt;创新点（暂定）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;创新点暂定&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%88%9b%e6%96%b0%e7%82%b9%e6%9a%82%e5%ae%9a&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我们提出一种融合深度视觉感知与分层控制的开放词汇目标追踪框架。
相较于LOVON仅依赖2D目标检测进行运动控制，我们的方法通过深度投影构建局部BEV占用图，并引入预测-驱动的路径规划层，从而显著减少在真实环境中因遮挡或障碍导致的失败&lt;/p&gt;
&lt;h2&gt;规划&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;规划&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%84%e5%88%92&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;时间&lt;/th&gt;
          &lt;th&gt;任务&lt;/th&gt;
          &lt;th&gt;目标&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;第1阶段&lt;/td&gt;
          &lt;td&gt;阅读文献：LOVON、LOVi、BEVFusion、LIO-SAM、SceneGPT&lt;/td&gt;
          &lt;td&gt;明确3D环境理解技术路线&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第2阶段&lt;/td&gt;
          &lt;td&gt;在Gym-UnrealCV中复现或简化LOVON策略（YOLO+Motion mapping）&lt;/td&gt;
          &lt;td&gt;建立baseline可控环境&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第3阶段&lt;/td&gt;
          &lt;td&gt;集成深度图或BEV投影模块，实现障碍建模与避障决策&lt;/td&gt;
          &lt;td&gt;形成改进方法&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;第4阶段&lt;/td&gt;
          &lt;td&gt;实机测试 + 指标对比 + 论文撰写&lt;/td&gt;
          &lt;td&gt;形成可投稿版本&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;第一阶段的文献调研一方面要包括LOVON引用的和引用LOVON的文献（但是因为VPN节点问题我的Scholar Google给我挂掉了，说我是机器人不让我访问），另一方面是尽可能的调研3D-aware Tracking/Navigation&lt;/p&gt;
&lt;h2&gt;医疗交叉（答辩）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;医疗交叉答辩&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%8c%bb%e7%96%97%e4%ba%a4%e5%8f%89%e7%ad%94%e8%be%a9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这里值得注意的是，论文里面要写的内容是一个宏大的改进，但是本院答辩时要突出和BME相关、医疗交叉的内容，HexGuide可以作为一个很大的参考&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;《HexGuide: A Hexapod Robot for Autonomous Blind Guidance in Challenging Environments》，一篇期刊论文&lt;/p&gt;

&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;层级&lt;/th&gt;
          &lt;th&gt;内容&lt;/th&gt;
          &lt;th&gt;对应写作作用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;① 背景&lt;/td&gt;
          &lt;td&gt;世界上有数亿视障人群，对自主出行有刚性需求&lt;/td&gt;
          &lt;td&gt;让读者意识到社会价值和痛点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;② 矛盾&lt;/td&gt;
          &lt;td&gt;现有导盲设备（如导盲犬或轮式机器人）有明显局限，不能稳定地应对复杂地形&lt;/td&gt;
          &lt;td&gt;设置“冲突”——为什么我们必须做新系统&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;③ 概念&lt;/td&gt;
          &lt;td&gt;上交高峰团队设计了一个六足机器人 &lt;strong&gt;HexGuide&lt;/strong&gt;，模仿昆虫式稳定步态，在复杂环境中实现安全引导&lt;/td&gt;
          &lt;td&gt;提出核心创新点和愿景&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;④ 方法&lt;/td&gt;
          &lt;td&gt;通过算法与机械协同，实现&lt;strong&gt;路径规划 + 稳定行走 + 动态避障 + 交通识别 + 人机交互&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;展示技术路线是如何支撑“稳定、安全”这两个关键词的&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;⑤ 验证&lt;/td&gt;
          &lt;td&gt;在机场、十字路口等复杂场景下实测验证，引导成功率高，路径平滑且避障成功&lt;/td&gt;
          &lt;td&gt;用结果“闭环”故事——愿景得以实现&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;论文的立意不是“做一个六足机器人”，而是要证明“六足+智能控制” = 可靠的盲人引导方式。这篇论文的核心任务不是“跟踪一个已知目标”或“视觉跟随”，而是“带领盲人从一个地点到另一个地点”，比如：“from the arrival gate to the baggage claim area in Shanghai Hongqiao Airport.”&lt;/p&gt;
&lt;p&gt;核心流程是盲人用户通过语音指令（如“去出口”）输入目标；在地图上自动规划从当前位置到目标的安全路径；机器人沿着规划路径行走；实时感知环境并修正轨迹。&lt;/p&gt;
&lt;p&gt;目标不是视觉追踪的对象，而是一个空间位置目标，因此这种导航是Goal-based而非Object-based tracking，而且泛化性有限：“The system can autonomously navigate in challenging environments once a map is available.”&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模块&lt;/th&gt;
          &lt;th&gt;故事逻辑&lt;/th&gt;
          &lt;th&gt;手法&lt;/th&gt;
          &lt;th&gt;指标体现&lt;/th&gt;
          &lt;th&gt;补充说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;机械稳定性：六足结构的天然稳态&lt;/td&gt;
          &lt;td&gt;盲人行走必须安全 → 足式比轮式更抗地形 → 六足比四足更稳&lt;/td&gt;
          &lt;td&gt;· “三足支撑步态（Tripod gait）”确保任意时刻三条腿接地&lt;br /&gt;· 单腿轨迹采用三次样条插值，区分支撑相与摆动相以减冲击&lt;br /&gt;· 控制顶点高度以跨越障碍、维持步态连续&lt;/td&gt;
          &lt;td&gt;· 平均支撑腿数 ≥ 3&lt;br /&gt;· 步态周期内质心（CoM）位移波动 &amp;lt; 5 mm&lt;br /&gt;· 10° 坡面及不平地面仍能维持姿态&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;规划稳定性：安全路径生成&lt;/td&gt;
          &lt;td&gt;“安全通行”要求路径不过度摆动、不贴近障碍&lt;/td&gt;
          &lt;td&gt;· 基于 A* 进行全局规划&lt;br /&gt;· 融合人工势场（APF）调整代价，使路径自动远离障碍&lt;br /&gt;· Bézier 曲线平滑路径&lt;br /&gt;· 拐点以贪心方式优化，减少急转角&lt;/td&gt;
          &lt;td&gt;· 路径平滑度提升（转向角波动减少约 40%）&lt;br /&gt;· 路径与障碍最小距离 ≥ 0.3 m&lt;br /&gt;· 平均路径长度仅比最短路径长 ≤ 5%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;运动控制稳定性：MPC 路径跟踪控制&lt;/td&gt;
          &lt;td&gt;六足控制复杂，需让行走对路径偏差“有反馈、能预测”&lt;/td&gt;
          &lt;td&gt;· 使用模型预测控制（MPC）&lt;br /&gt;· 目标函数最小化未来时域的位姿偏差&lt;br /&gt;· 实时约束关节速度与姿态角&lt;br /&gt;· 借助力矩传感器反馈修正步态&lt;/td&gt;
          &lt;td&gt;· 路径跟踪误差 &amp;lt; 3 cm&lt;br /&gt;· 姿态偏角误差 &amp;lt; 2°&lt;br /&gt;· 延迟控制补偿 ≤ 100 ms&lt;/td&gt;
          &lt;td&gt;核心体现“动态预测 + 约束最优控制”，区别于传统 PID&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;环境与交互稳定性：避免危险与错误指令&lt;/td&gt;
          &lt;td&gt;盲人处于动态环境，需识别行人、车辆、信号灯并安全互动&lt;/td&gt;
          &lt;td&gt;· LiDAR + IMU + RGB 摄像头多传感融合&lt;br /&gt;· 基于 LiDAR 点云的区域划分与加权速度修正，实现动态避障&lt;br /&gt;· YOLOv5 交通灯识别结合模板匹配&lt;br /&gt;· 语音识别与反馈交互（“请跟我走”“前方有障碍”）&lt;/td&gt;
          &lt;td&gt;· 动态障碍避让成功率 95%&lt;br /&gt;· 信号灯识别准确率 97.8%&lt;br /&gt;· 平均避障响应时间 &amp;lt; 0.3 s&lt;br /&gt;· 机场/路口场景连续引导成功率 100%&lt;/td&gt;
          &lt;td&gt;——&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所以这里的Navigation就比较难讲故事了。我们的亮点在于LLM对于自然语言指令能够分解成子任务，但足式机器人比较尴尬的一点是没有手，导致在医疗领域能实现的指令就局限了，比如说有一个RoboNurse-VLA enables the robot to recognize, grasp, and handover surgical instrument.是灵巧手的，但是足式机器人就只是狗了&lt;/p&gt;
&lt;p&gt;我也调研了其他的可能可以相关的领域，秉承**“助残/助盲/助老”**的理念：
第一个是家庭服务，这个还挺好说的，比如越疆 Rover X1/Unitree GO2可在光滑地板、草地、小坡坎等多场景行走，负载能力达日常物品级别，但最大的问题就是没有手，导致比如“帮老人取床头老花镜”“客厅物品递送”这种实现不了 —— 没有机械臂的足式机器人，到底“服务”什么？如果不能取物、开门，它的价值在哪里？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以胡诌做成可语音召唤的移动置物台 ，为上肢失能者提供 室内 5 m 范围内的即时物品可达性 ，用 3D 感知+分层导航解决 家用杂乱环境 下的 安全-连续 难题，从而 以移动代偿 而非 抓取代偿 的方式，提升上肢失能人群的 居家独立指数？
第二个是康复检测，问题是回答不了为什么需要一个狗跟着，而不是穿戴传感器设备/用固定的摄像头进行openpose骨骼分析，就算用了狗也不过是一个移动摄像头，那为什么不让残疾人动或者医生手动挪动摄像头？
第三个就是继续去纯助盲，我想通了，它不是Tracking，而是Object-based Tracking，只是默认命令是跟着person而已，没有说一定要跟在人后面，给他下一个其他目标的指令不就行了？但问题就出在了这里，传统SLAM的方法比结合AI的方法又快又好，你在AI基础上绑一个什么激光雷达/SLAM的话就有点尾大不掉很难绷。&lt;/li&gt;
&lt;li&gt;也有&lt;a href=&#34;https://github.com/Li-Ruiqi777/BlindGuideDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;导盲盲道的参考&lt;/a&gt;，不过是基于A1的本科毕设&lt;/li&gt;
&lt;li&gt;往&lt;a href=&#34;https://zhuanlan.zhihu.com/p/684356655&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RoboGuide这个方向&lt;/a&gt;去做的话也可以，只不过更多是放在VLM而非Tracking/Navigation本身了&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/BestAnHongjun/InternDog&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InternDog&lt;/a&gt;这篇西工大的工作不知道是怎么做的，看起来很牛，还上了&lt;a href=&#34;https://www.bilibili.com/video/BV1kK421a7sP/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;央视&lt;/a&gt;，据说是我国首个应用在导盲任务/场景下的四足机器人？&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bestxiangest/Intelligent-Guide-Cane&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intelligent-Guide-Cane&lt;/a&gt;或者回归ESP32的导盲？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感觉越调研越有信心了，那就继续往导盲这个领域讲故事应该没有问题！&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
