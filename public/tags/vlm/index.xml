<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bubblevan – Vlm</title>
    <link>http://localhost:1313/tags/vlm/</link>
    <description>Recent content in Vlm on Bubblevan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="http://localhost:1313/tags/vlm/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Social Navigation Idea</title>
      <link>http://localhost:1313/blog/2025/2025-12-15-social-navigation-idea/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-15-social-navigation-idea/</guid>
      <description>
        
        
        &lt;h2&gt;自问自答&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;自问自答&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%87%aa%e9%97%ae%e8%87%aa%e7%ad%94&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在接触 LOVON 这类开放词汇物体导航工作时，发现它们对语义的理解停留在 YOLO 层面，追目标时避障效果很差。当时考虑过用边走边建图的 3D 重建方式，但看到 social navigation 后就搁置了。这里有两个问题值得深入思考。&lt;/p&gt;
&lt;h3&gt;问题一：Social Navigation 需要 3D 重建吗？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题一social-navigation-需要-3d-重建吗&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e4%b8%80social-navigation-%e9%9c%80%e8%a6%81-3d-%e9%87%8d%e5%bb%ba%e5%90%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在 Social Navigation 领域，做显式 3D 重建是一条歧路。这个结论需要从 ObjectNav 和 SocialNav 的本质区别说起。&lt;/p&gt;
&lt;p&gt;ObjectNav 的核心难点是记忆。机器人需要记住&amp;quot;我去过厨房没？那个杯子在哪？&amp;ldquo;这类问题。这里用 3D 建图（如语义地图）是有用的，因为它解决的是静态环境的探索与回溯。LOVON 避障差，通常是因为它是模块化的（YOLO 指方向，然后 Planner 走），尽管利用了 LLM 进行 NER 分解，这也算是一种传统 Planner，对近距离动态避障很弱。&lt;/p&gt;
&lt;p&gt;但 SocialNav 的核心难点是动态性。人是会动的。如果做 3D 重建（TSDF Fusion、NeRF-SLAM 等），会面临严重的鬼影问题。一个人从左走到右，3D 地图上会留下一串残影，不仅不能辅助导航，反而会变成一堆不存在的障碍物墙，把路堵死。&lt;/p&gt;
&lt;p&gt;Falcon、Rank 1、Rank 2 的成功证明了：处理动态环境，需要的是第一视角感知（Egocentric Perception）加上时序记忆（RNN/Transformer），而不是全局静态地图。&lt;/p&gt;
&lt;p&gt;关于&amp;quot;避障不行&amp;quot;的问题，之前遇到的 YOLO 检测到了但还是撞上去的情况，正是端到端强化学习（SocialNav）试图解决的。关键是把&amp;quot;识别&amp;quot;和&amp;quot;运动控制&amp;quot;分开。SocialNav 的方法（如 Falcon）是把深度/RGB 直接映射到动作。如果智能体撞了，它会直接受到惩罚。这比&amp;quot;YOLO 告诉 Planner 有人，Planner 计算路径&amp;quot;的链路反应更快，且更能处理复杂交互。&lt;/p&gt;
&lt;h3&gt;问题二：Depth Anything V3 是合适的&amp;quot;新锤子&amp;quot;吗？&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;问题二depth-anything-v3-是合适的新锤子吗&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e9%97%ae%e9%a2%98%e4%ba%8cdepth-anything-v3-%e6%98%af%e5%90%88%e9%80%82%e7%9a%84%e6%96%b0%e9%94%a4%e5%ad%90%e5%90%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;用目标驱动研究的标准来评估这个&amp;quot;锤子&amp;rdquo;，会发现两种截然不同的使用方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;工程思维（不推荐）&lt;/strong&gt;：把 Falcon 输入端的深度传感器换成 Depth Anything V3 生成的深度。问题在于，在仿真器（如 Habitat）里，已经有了完美深度（Ground Truth Depth）。Depth Anything V3 生成的深度再好，也不可能比仿真器自带的完美深度更好。结果可能是性能下降（因为引入了推理延迟和误差），且没有任何创新性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;科研思维（推荐）&lt;/strong&gt;：Depth Anything V3 的真正价值在于&amp;quot;仿真到现实的鸿沟&amp;quot;或&amp;quot;语义感知&amp;quot;。&lt;/p&gt;
&lt;p&gt;第一个方向是鲁棒性/仿真到现实。SocialNav 在仿真里跑得好，是因为仿真里的深度是完美的。真实世界的深度传感器（Realsense/LiDAR）有噪声、有盲区（玻璃、强光）。Depth Anything V3 是一个鲁棒感知探针，它是从大量真实图片训练出来的，对真实世界的噪声有极强的鲁棒性。可以提出一个框架，证明在仿真里使用 Depth Anything V3 提取的特征（而非原始深度），能够让智能体在零样本迁移到真实世界时表现得更好，因为它学到的是通用的深度特征，而不是仿真特定的几何特征。&lt;/p&gt;
&lt;p&gt;第二个方向是隐式语义引导（更高级的）。Depth Anything 不仅仅是深度，它其实是基础模型。为了估计深度，它必须&amp;quot;理解&amp;quot;物体是什么（比如理解这块平滑的像素是墙而不是空洞）。利用 Depth Anything V3 的编码器特征作为 SocialNav 的额外输入，它的特征里不仅包含距离，还隐式包含了物体语义。这可能解决 Falcon&amp;quot;把人当圆柱体&amp;quot;的问题，让智能体能区分&amp;quot;人&amp;quot;和&amp;quot;人形雕塑&amp;quot;，或者区分&amp;quot;柔软的窗帘&amp;quot;和&amp;quot;坚硬的墙&amp;quot;。&lt;/p&gt;
&lt;p&gt;Falcon（基线）和 Rank 2 都依赖几何信息（位置、速度、距离）来判断风险。但有些情况仅靠深度和坐标无法区分：一个人站着不动是在玩手机（不会突然动），还是在等人（可能会突然拥抱）？一个人跑过来是冲着我来的（攻击性），还是只是路过（中性）？&lt;/p&gt;
&lt;p&gt;科学的做法是：不要用 Depth Anything V3 做深度，用它（或者 VLM 如 CLIP/SigLIP）做视觉语义特征提取。参考 Rank 2 的架构，加一个辅助模块，但不是预测&amp;quot;风险分数&amp;quot;，而是预测&amp;quot;社会意图&amp;quot;或&amp;quot;语义状态&amp;quot;。&lt;/p&gt;
&lt;p&gt;为什么这样做有效？Rank 1 证明了数据重要，可以利用基础模型里的海量数据。Rank 2 证明了显式预测隐变量重要，可以预测比风险更高级的意图。&lt;/p&gt;
&lt;h2&gt;Literature Review&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;literature-review&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#literature-review&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在 Google Scholar 里检索引用了 Falcon 的工作，整理如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;序号&lt;/th&gt;
          &lt;th&gt;论文标题&lt;/th&gt;
          &lt;th&gt;中稿状态&lt;/th&gt;
          &lt;th&gt;主要功能&lt;/th&gt;
          &lt;th&gt;核心内容&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding&lt;/td&gt;
          &lt;td&gt;已中稿 CVPR 2025&lt;/td&gt;
          &lt;td&gt;解决零样本开放词汇的 3D 视觉定位问题，即根据自然语言描述在 3D 场景中找到特定物体&lt;/td&gt;
          &lt;td&gt;提出 SeeGround 方法，无需针对 3D 数据进行专门训练。核心思想是将 3D 场景转化为 2D VLM 可理解的格式，通过渲染 3D 场景图像并使用视觉提示技术建立 2D 图像与 3D 空间信息的对应关系，利用预训练的 2D VLM 理解场景并定位物体&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 技术报告，Top-2&lt;/td&gt;
          &lt;td&gt;解决跨模态无人机导航中的图像检索问题，即根据自然语言描述从大规模数据库中检索对应的无人机视角图像&lt;/td&gt;
          &lt;td&gt;提出 CGRS 两阶段检索增强框架：第一阶段使用基线模型进行粗略排序；第二阶段利用 VLM 为候选图像生成详细描述，计算查询文本与生成描述的相似度进行精细重排序，显著提高检索精度&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;Zero-Shot 3D Visual Grounding from Vision-Language Models&lt;/td&gt;
          &lt;td&gt;arXiv Preprint (arXiv:2505.22429)&lt;/td&gt;
          &lt;td&gt;零样本 3D 视觉定位&lt;/td&gt;
          &lt;td&gt;从作者和题目看，极大概率是 Seeground (CVPR 2025) 的预印本或其前身，内容与 Seeground 一致，探讨如何利用 VLM 实现零样本 3D 视觉定位&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;Learning to Navigate Socially Through Proactive Risk Perception&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告，第 2 名&lt;/td&gt;
          &lt;td&gt;解决社会导航问题，让机器人在人群密集的动态环境中安全、合乎社会规范地导航&lt;/td&gt;
          &lt;td&gt;基于 Falcon 模型改进，增加主动风险感知模块，能够预测周围行人的基于距离的碰撞风险分数，让机器人具备更强的空间感知能力，主动采取避障行为并保持合适的社交距离&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;解决多楼层环境下的零样本物体目标导航&lt;/td&gt;
          &lt;td&gt;提出 ASCENT 框架，结合多楼层空间抽象和基于 LLM 的由粗到细的边界探索，利用 LLM 的常识推理能力（如&amp;quot;瑜伽垫可能在健身房，而健身房在楼下&amp;quot;）指导机器人跨楼层搜索&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;零样本社会导航&lt;/td&gt;
          &lt;td&gt;提出 SocialNav-Map 框架，结合动态人类轨迹预测和占据栅格地图。不需要针对特定环境训练，使用两种互补方法预测人类轨迹（基于历史路径和基于朝向），将预测结果作为动态障碍物整合进地图，使机器人能预见人的移动并提前规划路径&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;7&lt;/td&gt;
          &lt;td&gt;View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;零样本 3D 视觉定位&lt;/td&gt;
          &lt;td&gt;提出 VoG 方法，不同于直接把图像喂给模型，该方法将 3D 场景构建为多模态、多层级的场景图。VLM 被设计为主动代理，在图上进行遍历和推理，逐步搜索并定位目标物体，这种结构化方式降低了推理难度，提高了可解释性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms&lt;/td&gt;
          &lt;td&gt;arXiv Preprint&lt;/td&gt;
          &lt;td&gt;社会导航，强调符合人类社会规范的舒适度&lt;/td&gt;
          &lt;td&gt;提出 RLSLM 混合强化学习框架，将心理学实验推导出的规则基社会运动模型整合到 RL 的奖励函数中，让 RL 智能体在学习导航策略时天生倾向于遵守人类的社交舒适区，实现规则可解释性与 RL 适应性的结合&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;Comfort-Aware Trajectory Optimization for Immersive Human-Robot Interaction&lt;/td&gt;
          &lt;td&gt;已发表于 IEEE Open Journal on Immersive Displays (2025)&lt;/td&gt;
          &lt;td&gt;针对沉浸式环境（如 VR）中的人机交互，优化机器人运动轨迹&lt;/td&gt;
          &lt;td&gt;提出轨迹预测与优化框架，专门针对舒适度和路径合理性进行优化。在 VR 环境中进行用户研究，证明该方法生成的轨迹比传统方法更自然、更让用户感到舒适&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;Where to Fuse in the VLM Era: A Survey on Integrating Knowledge into Object Goal Navigation&lt;/td&gt;
          &lt;td&gt;Workshop Paper，发表于 HEAI Workshop&lt;/td&gt;
          &lt;td&gt;综述&lt;/td&gt;
          &lt;td&gt;探讨在物体目标导航任务中应该在&amp;quot;哪里&amp;quot;融合 VLM/LLM 的知识。借鉴自动驾驶的感知-预测-规划范式，将现有工作分类为在感知层融合、在预测层融合或在规划层融合，并分析各类优缺点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;Layout-Robust LiDAR 3D Object Detection via Multi-Representation Fusion&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;解决跨不同车辆平台的 LiDAR 3D 目标检测问题&lt;/td&gt;
          &lt;td&gt;针对不同车辆上 LiDAR 传感器布局（位置、数量、角度）不同导致模型泛化能力差的问题，提出统一表示框架，包含多视图融合模块（通过点-体素注意力机制学习统一视图不变表示）和运动引导的时空融合模块&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;Enhancing Multi-View Driving VLMs via Pseudo-Label Pretraining and Long-Tail Balancing&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;提升视觉语言模型在自动驾驶场景中的理解能力（感知、预测、规划）&lt;/td&gt;
          &lt;td&gt;基于 InternVL3-8B 模型提出两阶段优化框架：第一阶段利用伪标签预训练，结合思维链推理，将多视角图像按固定序列拼接；第二阶段针对长尾数据进行平衡处理，结合官方数据与合成数据进行混合微调，通过模型集成提升鲁棒性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;13&lt;/td&gt;
          &lt;td&gt;Robust 3D Object Detection under Sensor Placement Variability&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;增强 3D 目标检测模型对传感器安装位置变化的鲁棒性&lt;/td&gt;
          &lt;td&gt;针对不同车型 LiDAR 安装位置差异大导致模型失效的问题，提出三种策略集成：时序增强（聚合连续 LiDAR 扫描帧以丰富几何信息）、混合位置训练（在训练中模拟多种传感器配置）、推理时增强。在 Track 5 基准测试中表现出色&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;14&lt;/td&gt;
          &lt;td&gt;Enhancing VLMs for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;解决 VLM 在自动驾驶中空间推理弱和多任务干扰的问题&lt;/td&gt;
          &lt;td&gt;提出系统解决方案，核心是任务特定的提示。Prompt Routing：根据问题类型（感知、预测、规划等）将问题路由到专门的 expert prompt。空间推理增强：显式定义多视图坐标系和领域约束（如&amp;quot;后视摄像头的物体一定在车后&amp;quot;），帮助 VLM 理解空间关系。在 Track 1 的 Phase-1 和 Phase-2 中均取得很高准确率（70%+）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;15&lt;/td&gt;
          &lt;td&gt;Towards Socially Compliant Navigation: Hybrid Parameter Optimization for Falcon in Dynamic Environments&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Social Navigation Track) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;优化社会导航模型 Falcon 的参数，使其更符合社会规范&lt;/td&gt;
          &lt;td&gt;针对 Falcon 模型在平衡&amp;quot;任务效率&amp;quot;和&amp;quot;遵守社会规范&amp;quot;之间的矛盾，提出混合参数优化策略，结合比例约束的参数耦合和网格搜索，解决奖励函数参数过多导致的维度爆炸问题，更有效地找到让机器人既跑得快又懂礼貌的参数组合&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;16&lt;/td&gt;
          &lt;td&gt;HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 技术报告/预印本&lt;/td&gt;
          &lt;td&gt;解决无人机跨模态导航中的定位匹配问题&lt;/td&gt;
          &lt;td&gt;针对无人机视角（俯视、广角）与文本描述之间的巨大差异，提出 HCCM 框架，核心在于分层和跨粒度。不仅做整体图像匹配，还将图像和文本分解为不同层级（如全局场景 vs. 局部地标），在不同粒度上进行对比学习和匹配，提高在大范围航拍图像中定位具体目标的准确性&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;17&lt;/td&gt;
          &lt;td&gt;Unsupervised Domain Adaptation for 3D Object Detection via Adversarial Learning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;解决跨平台 3D 目标检测的域适应问题&lt;/td&gt;
          &lt;td&gt;针对源域和目标域 LiDAR 配置不同导致的数据分布差异，采用无监督域适应方法，核心引入对抗学习。通过训练域判别器区分特征来自哪个平台，同时强制特征提取器欺骗判别器，提取平台无关特征，使模型能泛化到新车型上&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;18&lt;/td&gt;
          &lt;td&gt;Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 获奖方案&lt;/td&gt;
          &lt;td&gt;高效的跨平台 3D 检测&lt;/td&gt;
          &lt;td&gt;基于强力的 PVRCNN++ 基线模型，使用两项关键技术弥补域差异：强数据增强（在数据层面模拟不同传感器噪声和几何变换）和伪标签（使用模型在未标注目标域数据上生成的置信度高的预测结果作为伪标签进行自我训练，逐步适应新环境）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;19&lt;/td&gt;
          &lt;td&gt;Task Aware Prompt Routing and CoT Augmented Fine Tuning for Driving VQA&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;提升自动驾驶 VLM 处理复杂问答（感知、预测、规划）的能力&lt;/td&gt;
          &lt;td&gt;提出任务感知提示路由：不使用通用提示，先判断问题属于哪类任务（如&amp;quot;前方有车吗？&amp;ldquo;属感知，&amp;ldquo;它会左转吗？&amp;ldquo;属预测），然后路由到专门优化的 Prompt 模板。思维链增强微调：在微调过程中加入推理步骤，强迫模型在给出结论前先生成推理过程，显著提升复杂逻辑问题的回答准确率&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;Driving Robustly through Corruptions: Multi-Source LoRA Fine-Tuning of Driving VLMs for Multi-View Reasoning&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 1) 技术报告&lt;/td&gt;
          &lt;td&gt;增强 VLM 对图像腐蚀/干扰的鲁棒性&lt;/td&gt;
          &lt;td&gt;针对雨雪雾、传感器噪声等恶劣视觉条件，采用 Multi-Source LoRA 微调策略。在训练时故意引入多种类型的图像腐蚀数据作为多源输入，通过轻量级 LoRA 模块让大模型快速适应这些低质量输入，保证在视觉条件退化时仍能安全推理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;21&lt;/td&gt;
          &lt;td&gt;SegSy3D: Segmentation-Guided Self-Training and Model Synergy for Cross-Platform 3D Detection&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;利用语义信息辅助跨平台 3D 检测&lt;/td&gt;
          &lt;td&gt;分割引导：认为仅仅做检测不够，利用点云的语义分割任务作为辅助，帮助模型更好地理解物体形状和背景，从而提升检测器的特征质量。模型协同：涉及多个模型（如分割模型和检测模型）之间的互助学习或集成，以克服单一模型的偏差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;22&lt;/td&gt;
          &lt;td&gt;Towards Generalizable 3D Object Detection Across Sensor Placements&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;解决 LiDAR 安装位置变化带来的检测失效问题&lt;/td&gt;
          &lt;td&gt;重点研究当 LiDAR 安装高度、俯仰角发生变化时点云分布的改变，提出通用检测框架，可能包含几何校正模块，或在特征空间进行视角对齐，确保无论雷达装在车顶还是车头，提取出的车辆特征是一致的&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;23&lt;/td&gt;
          &lt;td&gt;PlaceRecover: A Transformer-based Point Cloud Recovery Network with Implicit Neural Representations for Robust LiDAR Placement Adaptation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;通过重建/恢复点云来解决传感器位置差异问题&lt;/td&gt;
          &lt;td&gt;独特思路：不同于调整检测器，试图直接调整数据。PlaceRecover 是基于 Transformer 的网络，结合隐式神经表示，目标是将不同位置采集的畸变点云恢复/重构为标准视角下的点云，这样后续检测模型不需要修改，直接在恢复后的标准点云上运行即可&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;24&lt;/td&gt;
          &lt;td&gt;A Parameter-Efficient MoE Framework for Cross-Modal Drone Navigation&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 4) 冠军方案&lt;/td&gt;
          &lt;td&gt;高效、高精度的无人机跨模态检索与导航&lt;/td&gt;
          &lt;td&gt;引入混合专家模型架构，MoE 允许模型拥有巨大参数量但推理计算量很小（每次只激活部分专家）。在无人机导航任务中，不同专家可能分别负责处理文本理解、视觉特征提取或地理空间推理。参数高效通常意味着使用 Adapter 或 LoRA 等技术，使模型在有限算力下快速适应新任务&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;25&lt;/td&gt;
          &lt;td&gt;Robust 3D Object Detection via Physical-Aware Augmentation and Class-Specific Model Ensembling&lt;/td&gt;
          &lt;td&gt;IROS 2025 RoboSense Challenge (Track 5) 技术报告&lt;/td&gt;
          &lt;td&gt;通过物理感知增强和集成学习提升检测鲁棒性&lt;/td&gt;
          &lt;td&gt;物理感知增强：传统复制粘贴增强可能把车放在天上或穿墙，该方法设计符合物理规律的增强策略（如贴地、防碰撞），生成更逼真的训练样本。类别特定模型集成：针对不同类别（如车、人、骑行者）训练专门检测器，最后进行集成，利用不同模型在不同类别上的优势，最大化整体分数&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;除了上述文献，RoboSense 2025 Track 2 的两篇冠亚军方案也值得关注。它们的改进思路截然不同：一个改了模型架构，另一个改了训练策略。这种对比很有意思，展现了同一个问题可以从不同角度切入。&lt;/p&gt;
&lt;p&gt;亚军方案来自小米的工作，核心思路是**&amp;ldquo;预知不够，还需要风险评估&amp;rdquo;**。这个洞察很有意思。&lt;/p&gt;
&lt;p&gt;Falcon 虽然能预测人类未来的轨迹，但它对危险的感知是滞后的。Falcon 主要靠碰撞后的惩罚来学习，这导致智能体知道人要去哪，但不知道**&amp;ldquo;离得近有多危险&amp;rdquo;**。就像一个人能预测另一个人会走到哪里，但不知道保持多远的距离才安全。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，研究者在 Falcon 的架构上增加了一个&lt;strong&gt;主动风险感知模块&lt;/strong&gt;。这是一个轻量级的神经网络，利用共享的隐层状态，显式地预测周围每个人类的碰撞风险分数。它把风险分成了三个等级：&lt;strong&gt;Safe、Warning、Danger&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;这个模块的贡献在于将&lt;strong&gt;隐式的避障逻辑变成了显式的风险监督信号&lt;/strong&gt;。智能体在还没撞上之前，就学会了&amp;quot;这种距离是不舒服的&amp;rdquo;，从而更早地进行微调。这种从隐式到显式的转变，让模型的行为更加可解释，也更容易调试。&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;这个模块的贡献在于将隐式的避障逻辑变成了显式的风险监督信号智能体在还没撞上之前就学会了这种距离是不舒服的从而更早地进行微调这种从隐式到显式的转变让模型的行为更加可解释也更容易调试&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%bf%99%e4%b8%aa%e6%a8%a1%e5%9d%97%e7%9a%84%e8%b4%a1%e7%8c%ae%e5%9c%a8%e4%ba%8e%e5%b0%86%e9%9a%90%e5%bc%8f%e7%9a%84%e9%81%bf%e9%9a%9c%e9%80%bb%e8%be%91%e5%8f%98%e6%88%90%e4%ba%86%e6%98%be%e5%bc%8f%e7%9a%84%e9%a3%8e%e9%99%a9%e7%9b%91%e7%9d%a3%e4%bf%a1%e5%8f%b7%e6%99%ba%e8%83%bd%e4%bd%93%e5%9c%a8%e8%bf%98%e6%b2%a1%e6%92%9e%e4%b8%8a%e4%b9%8b%e5%89%8d%e5%b0%b1%e5%ad%a6%e4%bc%9a%e4%ba%86%e8%bf%99%e7%a7%8d%e8%b7%9d%e7%a6%bb%e6%98%af%e4%b8%8d%e8%88%92%e6%9c%8d%e7%9a%84%e4%bb%8e%e8%80%8c%e6%9b%b4%e6%97%a9%e5%9c%b0%e8%bf%9b%e8%a1%8c%e5%be%ae%e8%b0%83%e8%bf%99%e7%a7%8d%e4%bb%8e%e9%9a%90%e5%bc%8f%e5%88%b0%e6%98%be%e5%bc%8f%e7%9a%84%e8%bd%ac%e5%8f%98%e8%ae%a9%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%a1%8c%e4%b8%ba%e6%9b%b4%e5%8a%a0%e5%8f%af%e8%a7%a3%e9%87%8a%e4%b9%9f%e6%9b%b4%e5%ae%b9%e6%98%93%e8%b0%83%e8%af%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;冠军方案来自 Zhang 等人的 PER-Falcon，核心思路是**&amp;ldquo;模型不需要动，是数据利用效率太低&amp;rdquo;**。这个角度很独特，大多数工作都在改模型，但这个方法选择改训练策略。&lt;/p&gt;
&lt;p&gt;在强化学习训练社会导航时，大部分回合都是失败的，要么撞人，要么超时。成功的回合非常稀缺且珍贵，包含了完美的绕行和避让操作。Falcon 在训练时对所有数据一视同仁，导致智能体学了一堆&amp;quot;怎么死&amp;rdquo;，却没学够&amp;quot;怎么活&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;PER-Falcon 引入了&lt;strong&gt;正样本回放机制&lt;/strong&gt;。这是一种数据为中心的方法：把那些回报大于 10 的回合（即成功到达且避障良好）存到一个专门的缓冲区里。每隔一段时间，把这些&amp;quot;满分作业&amp;quot;拿出来让智能体再复习一遍，通过辅助的 PPO 更新来强化这些好的行为。&lt;/p&gt;
&lt;p&gt;这个方法的贡献在于证明了在社会导航中，&lt;strong&gt;强化好的行为比单纯修补坏的行为更有效&lt;/strong&gt;。这其实是一个训练技巧，但效果极好，提升了 7 个百分点。有时候，简单的方法反而最有效，关键是要找到问题的本质。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;特征&lt;/th&gt;
          &lt;th&gt;Rank 2 (Risk Perception)&lt;/th&gt;
          &lt;th&gt;Rank 1 (PER-Falcon)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;改进维度&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Perception / Loss Design (感知/损失函数)&lt;/td&gt;
          &lt;td&gt;Data Efficiency / Optimization (数据效率/优化)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;把&amp;quot;危险&amp;quot;显式量化，作为辅助监督信号&lt;/td&gt;
          &lt;td&gt;把&amp;quot;成功经验&amp;quot;加权，避免被噪音数据淹没&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;新增参数量&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;极小 (两层 MLP)&lt;/td&gt;
          &lt;td&gt;0 (仅改变训练流程)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;性能&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;SR 0.656, H-Coll 0.33&lt;/td&gt;
          &lt;td&gt;SR 0.660, H-Coll 0.32&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;借鉴点&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;引入 Dense Signal 辅助 RL 训练&lt;/td&gt;
          &lt;td&gt;在 World Model 训练中进行数据筛选 (Data Curation)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Paper List&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;paper-list&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#paper-list&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;基于上述分析，可以梳理出两类值得深入阅读的论文。第一类是直接处理社会导航核心问题的论文，第二类是可以作为方法论迁移的&amp;quot;新锤子&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;直接相关的必读论文&lt;/strong&gt;主要解决如何在人群中导航的核心问题。&lt;strong&gt;SocialNav-Map&lt;/strong&gt; 是一个很好的基线或对比对象，它尝试把&amp;quot;预测&amp;quot;显式地画在地图上（Occupancy Map），而 Falcon 是隐式编码在特征里。对比这两者的优劣是很好的讨论点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RLSLM&lt;/strong&gt; 试图将基于规则的社会力模型的可解释性融合进强化学习。这直接关联到 Falcon 缺乏显式社会规范的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Towards Socially Compliant Navigation&lt;/strong&gt; 是针对 Falcon 的超参数调优。虽然技术含量可能不高，但它揭示了奖励函数设计的敏感性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comfort-Aware Trajectory Optimization&lt;/strong&gt; 关注&amp;quot;舒适度&amp;quot;指标。如果目标是 Level 5（社会智能导航），这篇论文定义的指标可能比单纯的成功率更有用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论迁移的选读论文&lt;/strong&gt;虽然不在 SocialNav 领域，但其中的技术（VLM、Visual Grounding）正是需要的&amp;quot;新锤子&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Seeground / Zero-Shot 3D Visual Grounding&lt;/strong&gt; 是极其重要的&amp;quot;新锤子&amp;quot;来源。如果想做语义社会导航，需要这篇论文的方法：如何把 3D 场景转化为 2D VLM 能理解的 Prompt。可以把它的&amp;quot;物体定位&amp;quot;任务替换为&amp;quot;社会规范定位&amp;quot;任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where to Fuse in the VLM Era&lt;/strong&gt; 是一本&amp;quot;操作手册&amp;quot;。当决定引入 VLM 时，这篇综述告诉应该把它放在感知层（用来理解人）、预测层（用来预测意图）还是规划层（用来写代码）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enhancing VLMs for Autonomous Driving&lt;/strong&gt; 虽然是自动驾驶（室外），但它处理思维链（Chain of Thought）和空间推理的 Prompt Engineering 技巧，完全可以迁移到室内 SocialNav。例如：&amp;ldquo;那个人在看手机 -&amp;gt; 所以他不会让路 -&amp;gt; 我应该从左边绕&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;Top 2 的方法依然在&lt;strong&gt;几何空间（距离、坐标）&lt;strong&gt;和&lt;/strong&gt;RL 优化（Loss、Replay）&lt;strong&gt;里打转。它们都没有解决&lt;/strong&gt;语义理解（Semantic Understanding）&lt;strong&gt;和&lt;/strong&gt;显式博弈（Explicit Negotiation）&lt;/strong&gt;。这正是未来研究的机会所在。&lt;/p&gt;
&lt;h3&gt;After&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;after&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#after&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;读完这 7 篇论文后，可以清晰地看到当前社会导航领域的图景。目前正在见证一场**&amp;ldquo;分裂&amp;rdquo;**：一边在优化几何强化学习范式（从深度传感器和 PPO 中榨取性能），另一边则倡导结构化/混合方法（地图、规则和 VLM）来绕过纯强化学习的低效。&lt;/p&gt;
&lt;p&gt;这个文献图谱的中心节点是 &lt;strong&gt;Falcon&lt;/strong&gt;，它通过将范式从反应式避障转向预测式轨迹规划，建立了一个强基线。&lt;/p&gt;
&lt;p&gt;**Falcon（基础）**提出了&amp;quot;未来感知&amp;quot;框架。它不再仅仅对当前人类位置做出反应，而是使用辅助任务显式预测人类轨迹（未来 $H$ 步），并惩罚机器人阻塞这些未来路径。关键指标是在 Social-HM3D 上达到了 55% 的成功率。但它的弱点是依赖&amp;quot;盲目&amp;quot;的端到端强化学习，需要大量训练（约 2400 GPU 小时），并且将人类视为简单的移动障碍物，缺乏语义上下文。&lt;/p&gt;
&lt;p&gt;有三篇论文接受 Falcon 的架构，但认为其训练方法有缺陷。它们旨在解决样本效率和奖励稀疏性问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据效率（&amp;ldquo;好学生&amp;quot;方法）&lt;/strong&gt;：PER-Falcon（Rank 1）发现 Falcon 通过平等对待所有训练回合而浪费数据。它引入了正样本回放（PER）机制，缓存&amp;quot;高价值&amp;quot;回合（成功导航）并定期回放给策略网络。结果是在成功率上比基线提升了约 12%，证明了课程/数据质量比模型大小更重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;信号密度（&amp;ldquo;直觉&amp;quot;方法）&lt;/strong&gt;：主动风险感知（Rank 2）认为 Falcon 的碰撞惩罚太&amp;quot;稀疏&amp;rdquo;（只有在撞到人时才受到惩罚）。它添加了一个模块来基于距离预测连续的风险分数，让机器人在碰撞发生前很久就能&amp;quot;感知危险&amp;rdquo;。结果获得了第 2 名，证明了密集监督有助于强化学习收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;超参数调优（&amp;ldquo;暴力&amp;quot;方法）&lt;/strong&gt;：混合参数优化认为 Falcon 的奖励权重（平衡效率与社会合规性）不是最优的。它使用网格搜索和耦合调优来找到参数的&amp;quot;帕累托前沿&amp;rdquo;。结果仅通过调参就实现了 15% 的成功率提升，凸显了强化学习基线的脆弱性。&lt;/p&gt;
&lt;p&gt;有两篇论文挑战端到端强化学习的主导地位，认为结构和心理学是比试错更好的老师。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;零样本&amp;quot;反叛&lt;/strong&gt;：SocialNav-Map 是一个关键的挑战者。它认为强化学习&amp;quot;不透明&amp;quot;且&amp;quot;难以泛化&amp;rdquo;。它实时构建动态占据地图，预测人类轨迹（使用历史+朝向）并将其&amp;quot;绘制&amp;quot;到地图上作为临时障碍物，然后使用经典路径规划器（快速行进方法）。洞察是它在没有训练的情况下击败了基于强化学习的 Falcon（后者需要 2396 GPU 小时训练），这表明显式世界建模可能优于隐式强化学习记忆。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;以人为中心&amp;quot;的混合&lt;/strong&gt;：RLSLM 认为机器人不应该只是&amp;quot;猜测&amp;quot;社会规范。它将从心理学实验推导出的社会运动模型（SLM）直接整合到奖励函数中，创建了一个&amp;quot;非对称舒适场&amp;rdquo;（人类讨厌从前方接近，而不是从后方）。验证是通过 VR 让人类评价机器人，证明它比标准基于规则的方法&amp;quot;更有礼貌&amp;quot;。&lt;/p&gt;
&lt;p&gt;最后两篇论文虽然不严格属于&amp;quot;社会导航&amp;quot;论文，但它们提供了解决社会导航中&amp;quot;语义鸿沟&amp;quot;的&amp;quot;新锤子&amp;quot;（技术）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SeeGround&lt;/strong&gt; 解决了 3D 视觉定位（通过文本找到物体）而无需 3D 训练数据。技术是使用&amp;quot;视角适应模块&amp;quot;（模拟相机看向物体）从 3D 场景渲染 2D 图像，并将其输入到 2D VLM。相关性在于它证明了如果正确格式化数据（渲染图像+空间提示），可以使用冻结的 2D VLM 来理解 3D 空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where to Fuse&lt;/strong&gt; 是一篇综述，分类了如何将 VLM/LLM 知识注入导航。框架将融合分为感知（识别物体）、预测（猜测关系）和规划（边界选择）。相关性在于它明确将&amp;quot;社会交互导航&amp;quot;称为未来，指出当前方法&amp;quot;将人类简化为移动障碍物&amp;quot;。&lt;/p&gt;
&lt;p&gt;基于这些论文，可以绘制出研究路线图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Novelty Tree（技术演进）&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Level 1：几何反应（过去）&lt;/strong&gt; → ORCA、Social Force&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 2：几何预测（Falcon 时代）&lt;/strong&gt; → Falcon、PER-Falcon（预测轨迹）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 3：结构化混合（当前 SOTA）&lt;/strong&gt; → SocialNav-Map（显式映射动态风险）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 4：语义社会智能（空白）&lt;/strong&gt; → 理解上下文（例如：&amp;ldquo;那两个人正在交谈，不要从中间走过&amp;rdquo;，或&amp;quot;那个人在赶路，让路&amp;quot;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge-Insight Tree（工具箱）&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;社会导航中的挑战&lt;/th&gt;
          &lt;th&gt;当前解决方案（&amp;ldquo;旧&amp;quot;方法）&lt;/th&gt;
          &lt;th&gt;提出的洞察（使用文献）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;数据效率&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;训练 1000 万步（Falcon）&lt;/td&gt;
          &lt;td&gt;回放缓冲区：优先&amp;quot;正样本回合&amp;rdquo;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;奖励稀疏性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;稀疏碰撞惩罚&lt;/td&gt;
          &lt;td&gt;密集风险：预测连续&amp;quot;风险分数&amp;quot;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;泛化性&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;在新地图上微调&lt;/td&gt;
          &lt;td&gt;零样本映射：使用动态占据地图&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;社会规范&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;希望强化学习&amp;quot;学会&amp;quot;它们&lt;/td&gt;
          &lt;td&gt;显式建模：注入心理学规则（SLM）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;语义盲区&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;仅深度输入（看不到&amp;quot;活动&amp;quot;）&lt;/td&gt;
          &lt;td&gt;VLM 注入：使用 SeeGround 的&amp;quot;视角渲染&amp;quot;来分类社会情境&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;文献清楚地表明，&lt;strong&gt;几何问题已经解决&lt;/strong&gt;（SocialNav-Map 证明了可以零样本完成）。下一个前沿是&lt;strong&gt;语义&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;机会：&amp;ldquo;语义社会地图&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;空白：Falcon 和 SocialNav-Map 将人类视为动态圆柱体。RLSLM 将它们视为磁场。它们都不知道人类在做什么。&lt;/p&gt;
&lt;p&gt;新锤子：SeeGround 证明了可以使用 VLM 来&amp;quot;看&amp;quot;特定的 3D 坐标并理解它。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;提出的想法：&amp;ldquo;VLM 驱动的社会可供性地图&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;感知&lt;/strong&gt;：使用 VLM（如 SeeGround）分析来自 RGB 相机的人类裁剪图像。分类它们的状态：&amp;ldquo;交互中&amp;rdquo;、&amp;ldquo;等待中&amp;rdquo;、&amp;ldquo;看手机&amp;rdquo;、&amp;ldquo;匆忙&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;映射&lt;/strong&gt;：不仅仅是&amp;quot;占据地图&amp;quot;（如 SocialNav-Map），而是构建&amp;quot;社会规范地图&amp;quot;。
&lt;ul&gt;
&lt;li&gt;示例：如果两个人正在&amp;quot;交互&amp;quot;，在它们之间创建&amp;quot;禁止通行区&amp;quot;。&lt;/li&gt;
&lt;li&gt;示例：如果一个人正在&amp;quot;看手机&amp;quot;，扩大其风险半径（他们分心了）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规划&lt;/strong&gt;：在这个新的语义地图上使用 SocialNav-Map 规划器（FMM）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这结合了 SocialNav-Map 的结构和 SeeGround 的语义能力，解决了 Where to Fuse 中识别的局限性。&lt;/p&gt;
&lt;p&gt;Google Search 验证建议：你可以搜一下 &amp;ldquo;Foundation model for social navigation&amp;rdquo; 或 &amp;ldquo;Language-guided social navigation&amp;rdquo;，看看Level 4/5目前是否已经有人在用VLM/DepthAnything的Feature做SocialNav了。如果没有，这就是Blue Ocean。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>通过图像推理 VLM</title>
      <link>http://localhost:1313/blog/2025/2025-12-18-vlm-think-with-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/2025/2025-12-18-vlm-think-with-images/</guid>
      <description>
        
        
        &lt;h2&gt;VLM-Think with images 必读论文&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;vlm-think-with-images-必读论文&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vlm-think-with-images-%e5%bf%85%e8%af%bb%e8%ae%ba%e6%96%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&#34;https://www.xiaohongshu.com/explore/693f9ea3000000001e032e66?note_flow_source=wechat&amp;amp;xsec_token=CBE05j1SOO06inQfnWwTQXQDMo4ZSS_m0ussgoq0HO25M=&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VLM-Think with images必读论文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VLM&lt;/strong&gt; 尽管具备多模态输入的能力，但在推理过程中完全依赖&lt;strong&gt;纯文本的形式&lt;/strong&gt;进行思考，无论是对视觉内容进行描述，还是输出语言化的推理依据，其内部推理路径始终局限于文本上。然而仅通过文本进行多模态推理，并不总是最有效的策略，尤其对于那些&lt;strong&gt;高度依赖视觉信息的任务&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;O3 范式 Think with images&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;o3-范式-think-with-images&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#o3-%e8%8c%83%e5%bc%8f-think-with-images&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;一种方法是 &lt;strong&gt;OpenAI-O3 范式的 Think with images&lt;/strong&gt;，是指在推理过程中通过&lt;strong&gt;视觉工具&lt;/strong&gt;（如放大、裁剪、旋转、绘制辅助线、草图）来进行辅助思考，从而将视觉操作和操作后图像融入思维链，目的是为了让模型可以更深入地理解图像内容。&lt;/p&gt;
&lt;p&gt;但从这样的角度，&lt;strong&gt;本质并没有变&lt;/strong&gt;，思维链还是&lt;strong&gt;纯文本驱动&lt;/strong&gt;，也就是得到视觉信息不是模型生成的，只是借助工具得到的。&lt;strong&gt;模态鸿沟仍然存在&lt;/strong&gt;，即将视觉信息落地为文本后再进行推理，阻碍了模型对视觉特征的精准捕捉。&lt;/p&gt;
&lt;h4&gt;总结使用工具的方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;总结使用工具的方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%80%bb%e7%bb%93%e4%bd%bf%e7%94%a8%e5%b7%a5%e5%85%b7%e7%9a%84%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;设置工作流固定使用工具&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;固定的一步，调用固定工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型自主决定使用什么工具&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在哪一步，是直接输出答案，还是调用工具。如果使用工具，自主决定使用什么工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;固定的工具代码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码是设置好的，模型只需要预测输入参数。所支持的操作空间相对受限，也依赖精确的参数输入。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型生成工具代码&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码由模型生成，支持广泛的视觉操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;相关论文&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关论文&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%ae%ba%e6%96%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Visual SKETCHPAD: Sketching as a Visual Chain of Thought for Multimodal Language Models&lt;/li&gt;
&lt;li&gt;V-Thinker: Interactive Thinking with Images&lt;/li&gt;
&lt;li&gt;DeepEyes: Incentivizing &amp;ldquo;Thinking with Images&amp;rdquo; via Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Thyme: Think Beyond Images&lt;/li&gt;
&lt;li&gt;Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization&lt;/li&gt;
&lt;li&gt;Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning&lt;/li&gt;
&lt;li&gt;From Illusion to Intention- Visual Rationale Learning for Vision-Language Reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;潜在视觉推理（Latent Visual CoT）&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;潜在视觉推理latent-visual-cot&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e6%bd%9c%e5%9c%a8%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86latent-visual-cot&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;所以另一种方法是&lt;strong&gt;潜在视觉推理&lt;/strong&gt;，模型不再调用外部工具得到视觉信息，而是将视觉信息&lt;strong&gt;内化、表征化&lt;/strong&gt;，也就是模型直接生成&lt;strong&gt;视觉 token&lt;/strong&gt;。模型需要学会运用视觉 token 进行推理，而这些视觉 token 通常包括与&lt;strong&gt;分割、深度、边缘、特征&lt;/strong&gt;等视觉线索。&lt;/p&gt;
&lt;p&gt;训练模型生成视觉 token，就需要加入&lt;strong&gt;视觉重建任务&lt;/strong&gt;。&lt;/p&gt;
&lt;h4&gt;视觉 token 重建的 label 有以下几种&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;视觉-token-重建的-label-有以下几种&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e8%a7%86%e8%a7%89-token-%e9%87%8d%e5%bb%ba%e7%9a%84-label-%e6%9c%89%e4%bb%a5%e4%b8%8b%e5%87%a0%e7%a7%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;引入辅助模型&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入辅助图像&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入原图 ROI 边界框&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;学习的视觉 token 表示有以下几种&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;学习的视觉-token-表示有以下几种&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e5%ad%a6%e4%b9%a0%e7%9a%84%e8%a7%86%e8%a7%89-token-%e8%a1%a8%e7%a4%ba%e6%9c%89%e4%bb%a5%e4%b8%8b%e5%87%a0%e7%a7%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;VIT 特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VIT 投影特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型中间特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VQVAE 中间离散 token&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;相关论文&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;相关论文-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#%e7%9b%b8%e5%85%b3%e8%ae%ba%e6%96%87-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens&lt;/li&gt;
&lt;li&gt;Perception tokens enhance visual reasoning in multimodal language models&lt;/li&gt;
&lt;li&gt;DeepSketcher- Internalizing Visual Manipulation for Multimodal Reasoning&lt;/li&gt;
&lt;li&gt;Machine mental imagery: Empower multimodal reasoning with latent visual tokens&lt;/li&gt;
&lt;li&gt;Latent Visual Reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Thinking with Images&lt;/h1&gt;&lt;p&gt;**多模态大语言模型（Multimodal Large Language Models, MLLMs）&lt;strong&gt;正处于从&lt;/strong&gt;&amp;ldquo;感知智能&amp;quot;向&amp;quot;推理智能&amp;rdquo;&lt;strong&gt;跃迁的关键转折点。尽管早期的视觉-语言模型（VLMs）如 &lt;strong&gt;CLIP&lt;/strong&gt; 或 &lt;strong&gt;LLaVA&lt;/strong&gt; 成功实现了图像与文本的语义对齐，但它们在本质上仍遵循&lt;/strong&gt;&amp;ldquo;Thinking about Images&amp;rdquo;&lt;strong&gt;的范式——即迅速将视觉信号转化为文本特征，随后完全依赖语言模型的&lt;/strong&gt;文本思维链（Text-based Chain-of-Thought, CoT）**进行推理。&lt;/p&gt;
&lt;p&gt;这种**&amp;ldquo;模态早融合&amp;rdquo;&lt;strong&gt;与&lt;/strong&gt;&amp;ldquo;推理纯文本化&amp;rdquo;&lt;strong&gt;的架构，导致了严重的&lt;/strong&gt;模态鸿沟（Modality Gap）**：在处理需要空间几何感知、细粒度视觉验证或多步视觉逻辑推演的任务时，模型往往因丢失视觉细节而产生幻觉。&lt;/p&gt;
&lt;p&gt;为了克服这一局限，学术界与工业界正在积极探索**&amp;ldquo;Thinking with Images&amp;rdquo;**的新范式，即让视觉模态深度参与推理的中间过程，甚至主导推理链条。本报告旨在打破现有的二元分类局限（即简单的&amp;quot;工具调用&amp;quot;与&amp;quot;潜在推理&amp;quot;之分），基于对 &lt;strong&gt;2023 年至 2025 年间&lt;/strong&gt;发表于 &lt;strong&gt;CVPR、ICCV、NeurIPS、ICLR、ICML&lt;/strong&gt; 等顶级会议的 &lt;strong&gt;50 余篇里程碑文献&lt;/strong&gt;的详尽调研，构建了一个包含五大范式的全新分类体系：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;工具中介与程序化视觉推理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;显式生成意象与心智模拟&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;潜在空间视觉推理与连续思维&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主动感知与强化视觉搜索&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结构化与组合式视觉推理&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本报告将深入剖析每一范式的核心机制、技术演进路径及代表性工作，揭示 MLLMs 如何通过引入&lt;strong&gt;视觉中间态（Visual Intermediates）&lt;/strong&gt;——无论是代码、像素、潜在向量、动作序列还是结构化图谱——来模拟人类的**&amp;ldquo;系统 2&amp;quot;慢思考能力**，从而实现真正的视觉通用智能。&lt;/p&gt;
&lt;h2&gt;1. 绪论：模态鸿沟与&amp;quot;系统 2&amp;quot;视觉推理的崛起&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-绪论模态鸿沟与系统-2视觉推理的崛起&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e7%bb%aa%e8%ae%ba%e6%a8%a1%e6%80%81%e9%b8%bf%e6%b2%9f%e4%b8%8e%e7%b3%bb%e7%bb%9f-2%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86%e7%9a%84%e5%b4%9b%e8%b5%b7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1.1 纯文本思维链在多模态语境下的局限性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;11-纯文本思维链在多模态语境下的局限性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#11-%e7%ba%af%e6%96%87%e6%9c%ac%e6%80%9d%e7%bb%b4%e9%93%be%e5%9c%a8%e5%a4%9a%e6%a8%a1%e6%80%81%e8%af%ad%e5%a2%83%e4%b8%8b%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;传统的视觉语言模型（如 &lt;strong&gt;LLaVA 系列&lt;/strong&gt;、&lt;strong&gt;GPT-4V&lt;/strong&gt;）主要采用**&amp;ldquo;编码器-解码器&amp;quot;架构**：视觉编码器（如 &lt;strong&gt;ViT&lt;/strong&gt;）将图像压缩为特征向量，随后投影到大语言模型（&lt;strong&gt;LLM&lt;/strong&gt;）的词嵌入空间。一旦进入 LLM，视觉信息便被视为某种&amp;quot;外语&amp;rdquo;，推理过程完全由预训练的语言概率分布主导。&lt;/p&gt;
&lt;p&gt;这种架构在图像描述（&lt;strong&gt;Captioning&lt;/strong&gt;）等**&amp;ldquo;系统 1&amp;quot;直觉任务**上表现出色，但在需要多步逻辑的复杂任务中面临显著瓶颈：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;信息有损压缩（Information Bottleneck）&lt;/strong&gt;：视觉编码器通常将高分辨率图像压缩为有限数量的 token（例如 &lt;strong&gt;256 或 576 个&lt;/strong&gt;），导致高频细节（如微小文字、物体精确坐标）在推理开始前即丢失。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;视觉-语言非同构性（Isomorphism Deficit）&lt;/strong&gt;：语言是离散、符号化且高度抽象的，而视觉是连续、密集且具象的。强行用文本 CoT 描述复杂的空间拓扑（如&amp;quot;左边第三个红球稍微偏上一点&amp;rdquo;）会导致语义精度的急剧下降，模型往往因此退化为依赖语言先验而非视觉事实进行猜想，即产生**&amp;ldquo;幻觉&amp;rdquo;**。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;缺乏回溯机制（Lack of Retracing）&lt;/strong&gt;：人类在解决视觉难题时会反复观察图像（&lt;strong&gt;Visual Re-scanning&lt;/strong&gt;），而标准 VLM 往往是**&amp;ldquo;看一眼，然后闭眼推理&amp;rdquo;**，缺乏在推理中途重新审视视觉输入的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;1.2 &amp;ldquo;Thinking with Images&amp;rdquo; 的定义与新分类体系&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;12-thinking-with-images-的定义与新分类体系&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#12-thinking-with-images-%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%8e%e6%96%b0%e5%88%86%e7%b1%bb%e4%bd%93%e7%b3%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;Thinking with Images&amp;rdquo;&lt;/strong&gt; 指的是模型在推理过程中，不仅生成文本，还显式或隐式地操作视觉信息，将其作为推理链条中不可或缺的一环。这对应于认知科学中的**&amp;ldquo;心智意象&amp;rdquo;（Mental Imagery）**理论，即人类在思考空间问题时，会在大脑中构建视觉模拟。&lt;/p&gt;
&lt;p&gt;基于对现有文献的系统梳理，我们将这一领域的解决方案扩展为以下五大范式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;范式 I：工具中介与程序化视觉推理（Tool-Mediated &amp;amp; Programmatic Reasoning）&lt;/strong&gt; —— 借用外部引擎的&amp;quot;手&amp;quot;来操作视觉。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 II：显式生成意象与心智模拟（Explicit Generative Imagery &amp;amp; Mental Simulation）&lt;/strong&gt; —— 利用生成模型的&amp;quot;想象力&amp;quot;进行预演。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 III：潜在空间视觉推理（Latent Visual Reasoning）&lt;/strong&gt; —— 在高维特征空间进行高效的&amp;quot;内隐视觉思考&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 IV：主动感知与强化视觉搜索（Active Perception &amp;amp; Agentic Grounding）&lt;/strong&gt; —— 像人类眼动一样主动&amp;quot;寻找&amp;quot;视觉证据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;范式 V：结构化与组合式视觉推理（Compositional &amp;amp; Structured Grounding）&lt;/strong&gt; —— 将图像解构为图谱或掩码进行逻辑运算。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2. 范式 I：工具中介与程序化视觉推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-范式-i工具中介与程序化视觉推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e8%8c%83%e5%bc%8f-i%e5%b7%a5%e5%85%b7%e4%b8%ad%e4%bb%8b%e4%b8%8e%e7%a8%8b%e5%ba%8f%e5%8c%96%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式代表了**&amp;ldquo;符号主义&amp;quot;与&amp;quot;联结主义&amp;rdquo;&lt;strong&gt;的结合。其核心假设是：神经网络在精确计算（如计数、几何测量）和逻辑执行上存在先天不足，应当将这些任务&lt;/strong&gt;&amp;ldquo;外包&amp;rdquo;&lt;strong&gt;给擅长此道的外部工具（如 &lt;strong&gt;Python 解释器&lt;/strong&gt;、&lt;strong&gt;OpenCV 库&lt;/strong&gt;或&lt;/strong&gt;绘图 API**）。VLM 在此扮演**&amp;ldquo;控制器&amp;rdquo;&lt;strong&gt;的角色，负责理解意图、编写程序并解析执行结果。这也就是用户查询中提到的&lt;/strong&gt;&amp;ldquo;OpenAI-O3 范式&amp;rdquo;**的典型体现。&lt;/p&gt;
&lt;h3&gt;2.1 核心机制：思维的外化&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;21-核心机制思维的外化&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#21-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e6%80%9d%e7%bb%b4%e7%9a%84%e5%a4%96%e5%8c%96&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在该范式中，中间推理步骤被显式地转化为可执行的代码或可视化操作。这种**&amp;ldquo;外化&amp;rdquo;**不仅提高了推理的准确性，还赋予了模型极强的可解释性。&lt;/p&gt;
&lt;h3&gt;2.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;22-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#22-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.2.1 Visual Sketchpad (NeurIPS 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;221-visual-sketchpad-neurips-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#221-visual-sketchpad-neurips-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：传统的 CoT 仅在文本层面分解问题，但对于几何题或地图导航，纯文本描述极其低效且易错。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：作者提出了一种**&amp;ldquo;视觉草稿本&amp;quot;机制**。模型在推理过程中，可以生成代码来调用绘图 API（如 Matplotlib），在原图上绘制辅助线、标记框或圈出关键区域。这些绘制了标记的新图像被重新输入模型，作为下一步推理的视觉上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：首次将**&amp;ldquo;草图绘制&amp;rdquo;（Sketching）**引入 VLM 推理链。这模拟了人类做几何题时画辅助线的行为。实验表明，这种视觉辅助能显著提升数学几何和视觉逻辑任务的准确率，证明了视觉符号不仅是输出，更是推理的支架。&lt;/p&gt;
&lt;h4&gt;2.2.2 ViperGPT (ICCV 2023)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;222-vipergpt-iccv-2023&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#222-vipergpt-iccv-2023&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：ViperGPT: Visual Inference via Python Execution for Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：**端到端（End-to-End）**模型不仅是黑盒，而且经常在简单的逻辑组合上失败（例如&amp;quot;红帽子的人左边是不是有一辆车&amp;rdquo;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：ViperGPT 彻底摒弃了端到端的视觉问答模式。它利用专门针对代码微调的 LLM（如 &lt;strong&gt;Codex&lt;/strong&gt;）将自然语言问题转化为 &lt;strong&gt;Python 程序&lt;/strong&gt;。该程序调用一系列视觉 API（如对象检测、深度估计模型）。程序执行的结果即为最终答案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：提出了**&amp;ldquo;以代码为策略&amp;rdquo;（Code as Policy）&lt;strong&gt;的视觉推理极致形态。它不需要训练多模态模型，而是通过组合现有的&lt;/strong&gt;视觉专家模型（Vision Experts）**来解决问题，实现了极高的可解释性和组合泛化能力。&lt;/p&gt;
&lt;h4&gt;2.2.3 Visual Program Distillation (VPD) (CVPR 2024 Oral)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;223-visual-program-distillation-vpd-cvpr-2024-oral&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#223-visual-program-distillation-vpd-cvpr-2024-oral&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：像 ViperGPT 这样的工具调用方法虽然准确，但推理延迟高、计算开销大，且依赖外部 API 的稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：VPD 提出了一种**&amp;ldquo;蒸馏&amp;quot;策略**。它首先利用大型模型生成成千上万条高质量的&amp;quot;视觉程序&amp;quot;轨迹（即问题-程序-答案三元组），然后用这些数据微调一个较小的端到端 VLM（如 &lt;strong&gt;PaLM-E&lt;/strong&gt; 或 &lt;strong&gt;LLaVA&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：成功将工具调用的逻辑推理能力**&amp;ldquo;内化&amp;rdquo;**到模型权重中。微调后的模型不再需要外部 API，而是直接预测出类似于程序执行步骤的推理结果，兼顾了工具方法的逻辑严密性和端到端模型的高效性。&lt;/p&gt;
&lt;h4&gt;2.2.4 Task Navigator (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;224-task-navigator-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#224-task-navigator-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Task Navigator: Decomposing Complex Tasks for Multimodal Large Language Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：面对极其复杂的视觉任务（如&amp;quot;分析这张监控截图中所有异常行为&amp;rdquo;），模型往往不知从何下手，因为它的注意力机制无法一次性处理所有信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：引入了一个**导航器（Navigator）**模块，负责将宏观任务分解为一系列子查询（&lt;strong&gt;Sub-queries&lt;/strong&gt;）。系统根据子查询的需要，动态选择调用特定的视觉工具（如 &lt;strong&gt;OCR&lt;/strong&gt;、检测器、知识库检索），并根据工具返回的结果规划下一步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：强调了**&amp;ldquo;规划&amp;rdquo;（Planning）&lt;strong&gt;在视觉推理中的核心地位。证明了&lt;/strong&gt;系统 2 推理**的关键在于将复杂问题降维，并通过工具迭代式地获取信息。&lt;/p&gt;
&lt;h4&gt;2.2.5 DeepSketcher (ArXiv 2025 / ICLR Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;225-deepsketcher-arxiv-2025--iclr-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#225-deepsketcher-arxiv-2025--iclr-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：外部绘图工具（如 Visual Sketchpad）的操作是离散且不可微的，阻断了梯度回传，限制了模型的学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：DeepSketcher 设计了一个**&amp;ldquo;图像嵌入编辑模块&amp;rdquo;（Image Embedding Editing Module）**。它虽然受到代码渲染图像的监督，但在推理时，模型是直接在图像的特征空间（&lt;strong&gt;Embedding Space&lt;/strong&gt;）中进行&amp;quot;涂抹&amp;quot;和&amp;quot;高亮&amp;quot;，模拟绘图操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了工具操作的**&amp;ldquo;软化&amp;quot;和&amp;quot;可微化&amp;rdquo;**。它不仅保留了绘图辅助推理的直观优势，还允许模型通过梯度下降端到端地学习&amp;quot;该在哪里画线&amp;quot;，是范式 I 向范式 III（潜在推理）演进的过渡形态。&lt;/p&gt;
&lt;h2&gt;3. 范式 II：显式生成意象与心智模拟&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-范式-ii显式生成意象与心智模拟&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e8%8c%83%e5%bc%8f-ii%e6%98%be%e5%bc%8f%e7%94%9f%e6%88%90%e6%84%8f%e8%b1%a1%e4%b8%8e%e5%bf%83%e6%99%ba%e6%a8%a1%e6%8b%9f&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式深受&lt;strong&gt;认知心理学&lt;/strong&gt;启发。人类在回答&amp;quot;大象能不能装进冰箱&amp;quot;这个问题时，会在脑海中生成大象和冰箱的视觉意象并进行比对。同样，该范式赋予 VLM 调用生成模型（如 &lt;strong&gt;Stable Diffusion&lt;/strong&gt;）的能力，通过生成&lt;strong&gt;像素级的图像&lt;/strong&gt;来辅助推理，特别是针对空间预测、反事实推理和未来预测任务。&lt;/p&gt;
&lt;h3&gt;3.1 核心机制：视觉想象循环&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;31-核心机制视觉想象循环&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#31-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e8%a7%86%e8%a7%89%e6%83%b3%e8%b1%a1%e5%be%aa%e7%8e%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;推理过程不再是单向的（图像 -&amp;gt; 文本），而是&lt;strong&gt;闭环的&lt;/strong&gt;：文本/图像 -&amp;gt; 生成新图像 -&amp;gt; 视觉感知 -&amp;gt; 文本结论。生成的图像充当了推理的**&amp;ldquo;视觉草稿&amp;rdquo;**。&lt;/p&gt;
&lt;h3&gt;3.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;32-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#32-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;3.2.1 Multimodal Visualization-of-Thought (MVoT) (ICML 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;321-multimodal-visualization-of-thought-mvot-icml-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#321-multimodal-visualization-of-thought-mvot-icml-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Imagine While Reasoning in Space: Multimodal Visualization-of-Thought&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：现有的多模态思维链（&lt;strong&gt;Multimodal CoT&lt;/strong&gt;）大多只是文本 CoT 加上静态图像输入，缺乏真正的&amp;quot;视觉思考&amp;quot;过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：MVoT 提出了一种&lt;strong&gt;交错式的推理模式&lt;/strong&gt;，模型可以像输出词语一样输出图像。为了实现这一点，作者设计了**&amp;ldquo;Token Discrepancy Loss&amp;rdquo;**，解决了 LLM 文本 Token 与图像生成器（如 &lt;strong&gt;VQ-VAE&lt;/strong&gt;）离散 Codebook 之间的分布差异问题。模型在推理过程中会生成一系列中间图像（&lt;strong&gt;Visual Thoughts&lt;/strong&gt;），展示其空间变换的构思过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将图像生成内化为 LLM 的原生能力，实现了真正的**&amp;ldquo;图文交错思维流&amp;rdquo;**。实验证明，这种显式的视觉化过程显著提升了空间旋转、物体拼接等任务的性能。&lt;/p&gt;
&lt;h4&gt;3.2.2 ImagineNav (ICLR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;322-imaginenav-iclr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#322-imaginenav-iclr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：在&lt;strong&gt;具身智能导航&lt;/strong&gt;中，代理（Agent）受限于视野，无法看到墙后或拐角处的物体，导致规划短视。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：ImagineNav 利用 VLM 结合&lt;strong&gt;新视角合成技术（Novel View Synthesis）&lt;/strong&gt;，根据当前观测&amp;quot;想象&amp;quot;出未知区域的景象。模型基于这些生成的幻觉图像来评估路径的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将生成式视觉 CoT 应用于决策规划。证明了**&amp;ldquo;合理的幻觉&amp;rdquo;**（基于先验的预测）是智能体在非结构化环境中生存的关键能力。&lt;/p&gt;
&lt;h4&gt;3.2.3 Perspective-Aware Reasoning (APC) (ICCV 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;323-perspective-aware-reasoning-apc-iccv-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#323-perspective-aware-reasoning-apc-iccv-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：VLM 存在严重的**&amp;ldquo;自我中心偏差&amp;rdquo;（Egocentric Bias）**，难以理解&amp;quot;如果我们换个角度看这个物体会怎样&amp;quot;这类问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：APC 框架模拟了人类的**心智旋转（Mental Rotation）**能力。它首先利用视觉基础模型从输入图像中重建一个粗糙的 &lt;strong&gt;3D 抽象场景&lt;/strong&gt;，然后将该 3D 场景旋转到目标视角，并重新投影为 2D 图像输入 VLM 进行回答。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：引入了 &lt;strong&gt;3D 抽象&lt;/strong&gt;作为中间推理模态。它表明，解决复杂的视觉关系问题需要超越 2D 像素，进入 &lt;strong&gt;3D 语义空间&lt;/strong&gt;的模拟。&lt;/p&gt;
&lt;h4&gt;3.2.4 SpatialDreamer (ArXiv 2025 / Top Venue Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;324-spatialdreamer-arxiv-2025--top-venue-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#324-spatialdreamer-arxiv-2025--top-venue-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Incentivizing Spatial Reasoning via Active Mental Imagery&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：仅仅生成图像是不够的，模型需要知道&amp;quot;生成什么图像&amp;quot;对解题最有帮助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：SpatialDreamer 引入了&lt;strong&gt;强化学习（RL）&lt;strong&gt;来优化生成策略。它训练模型主动进行&lt;/strong&gt;&amp;ldquo;视觉做梦&amp;rdquo;（Dreaming）&lt;/strong&gt;，并通过**世界模型（World Model）**验证这些梦境的物理一致性。奖励机制鼓励模型生成那些能最大程度减少不确定性的视角或状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将生成式 CoT 与强化学习结合，使视觉想象具有了&lt;strong&gt;目的性（Goal-oriented Imagination）&lt;/strong&gt;，这是向自主智能迈进的重要一步。&lt;/p&gt;
&lt;h4&gt;3.2.5 Self-Imagine (ArXiv 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;325-self-imagine-arxiv-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#325-self-imagine-arxiv-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：即使是纯文本的逻辑题，人类也往往需要画图辅助理解，而 LLM 缺乏这种能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：该方法无需训练，通过**提示工程（Prompting）**让 VLM 将抽象的文本问题转化为结构化的 &lt;strong&gt;HTML 或 SVG 代码&lt;/strong&gt;，然后渲染成图像。模型再次读取这张自己生成的图表来回答问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：揭示了**&amp;ldquo;跨模态转换&amp;rdquo;**本身就是一种强大的推理增强手段。将文本转化为视觉结构，能够利用 VLM 强大的视觉模式识别能力来破解复杂的逻辑谜题。&lt;/p&gt;
&lt;h2&gt;4. 范式 III：潜在空间视觉推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-范式-iii潜在空间视觉推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e8%8c%83%e5%bc%8f-iii%e6%bd%9c%e5%9c%a8%e7%a9%ba%e9%97%b4%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式是目前&lt;strong&gt;效率最高、最具理论深度&lt;/strong&gt;的方向。它认为，显式地生成像素图像（如范式 II）虽然直观但计算极其昂贵，且容易受到生成伪影的干扰。真正的&amp;quot;视觉思维&amp;quot;应当发生在紧凑、高维的**潜在空间（Latent Space）**中，类似于人类大脑处理视觉信号时并不需要在视网膜上重新成像。&lt;/p&gt;
&lt;h3&gt;4.1 核心机制：连续视觉思维链&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;41-核心机制连续视觉思维链&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#41-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e8%bf%9e%e7%bb%ad%e8%a7%86%e8%a7%89%e6%80%9d%e7%bb%b4%e9%93%be&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;模型在推理过程中生成特殊的**&amp;ldquo;视觉 Token&amp;rdquo;&lt;strong&gt;或&lt;/strong&gt;&amp;ldquo;思维向量&amp;rdquo;&lt;strong&gt;。这些向量不对应任何具体的单词，也不必解码为像素，而是保留了&lt;/strong&gt;连续的梯度信息**，能够承载比离散文本丰富得多的感知细节（如纹理、深度、精确坐标）。&lt;/p&gt;
&lt;h3&gt;4.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;42-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#42-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;4.2.1 Chain-of-Visual-Thought (CoVT / VChain) (ICCV 2025 / ArXiv 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;421-chain-of-visual-thought-covt--vchain-iccv-2025--arxiv-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#421-chain-of-visual-thought-covt--vchain-iccv-2025--arxiv-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：语言是高度压缩的符号系统，用语言描述视觉细节（如&amp;quot;这个不规则物体的边缘&amp;quot;）会造成巨大的信息丢失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：作者提出了一组**&amp;ldquo;连续视觉 Token&amp;rdquo;（Continuous Visual Tokens）**。模型经过训练，可以在推理步骤中输出这些连续向量。这些向量在训练时通过重构任务（如重构深度图、分割掩码）进行监督，确保其包含物理意义，但在推理时直接作为后续层的输入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：重新定义了 CoT 的载体。思维链不再局限于离散的文本，而是变成了**&amp;ldquo;文本-视觉向量&amp;quot;混合流**。实验表明，这种方法在细粒度感知任务上大幅超越了纯文本 CoT。&lt;/p&gt;
&lt;h4&gt;4.2.2 Mirage (ArXiv 2025 / CVPR Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;422-mirage-arxiv-2025--cvpr-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#422-mirage-arxiv-2025--cvpr-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：如何在不引入沉重的图像生成解码器的情况下，赋予模型**&amp;ldquo;心智意象&amp;rdquo;**能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Mirage 提出了**&amp;ldquo;潜在意象&amp;rdquo;（Latent Imagery）**。它利用轻量级的投影层将视觉编码器的特征映射到 LLM 的嵌入空间。通过两阶段训练（先对齐感知，再强化推理），模型学会了在遇到视觉难题时&amp;quot;调用&amp;quot;潜在视觉记忆，并在多轮对话中保持这些视觉状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了高效的**&amp;ldquo;长上下文视觉推理&amp;rdquo;**。由于潜在 Token 占用的显存远小于生成图像，Mirage 能够支持更长的推理步骤和更复杂的视觉逻辑操作。&lt;/p&gt;
&lt;h4&gt;4.2.3 Latent Visual Reasoning (LVR) (CVPR/ECCV 2025 Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;423-latent-visual-reasoning-lvr-cvpreccv-2025-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#423-latent-visual-reasoning-lvr-cvpreccv-2025-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Latent Visual Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：现有的 VLM 往往在特征提取阶段就丢失了与问题相关的细微特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：LVR 引入了**&amp;ldquo;潜在重放&amp;rdquo;（Latent Replay）&lt;strong&gt;机制。利用强化学习（&lt;strong&gt;GRPO&lt;/strong&gt;），模型在推理过程中学会&amp;quot;回溯&amp;quot;并重新激活与当前推理步骤最相关的视觉潜在特征。它实际上是在潜在空间中进行了一种&lt;/strong&gt;&amp;ldquo;注意力重分配&amp;rdquo;**。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：提出了 **VLPO（Visual-Latent Policy Optimization）**算法，证明了可以通过强化学习直接优化潜在空间的推理路径，而无需显式的监督信号。&lt;/p&gt;
&lt;h4&gt;4.2.4 Slot-VLM (NeurIPS 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;424-slot-vlm-neurips-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#424-slot-vlm-neurips-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Slot-VLM: Object-Centric Learning with Slot Attention&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：标准 Transformer 的注意力机制是全局的、纠缠的，难以分离独立的物体概念，导致数数或关系判断出错。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：引入了**&amp;ldquo;Slot Attention&amp;quot;机制**。图像特征被强制分解为一组独立的**&amp;ldquo;Slot&amp;quot;向量**，每个 Slot 代表一个物体或实体。推理过程变成了对这些 Slot 的操作（如比较 Slot A 和 Slot B 的属性）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将**&amp;ldquo;以物体为中心&amp;rdquo;（Object-Centric）**的归纳偏置引入 VLM。这使得模型在物理推理、物体计数等任务上具有了类似于符号系统的鲁棒性，同时保留了神经网络的灵活性。&lt;/p&gt;
&lt;h4&gt;4.2.5 Coconut (COLM 2025 / ArXiv 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;425-coconut-colm-2025--arxiv-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#425-coconut-colm-2025--arxiv-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Training Large Language Models to Reason in a Continuous Latent Space&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：语言空间的推理往往会过早塌缩（&lt;strong&gt;Collapse&lt;/strong&gt;）到一个确定的路径，限制了**广度优先搜索（BFS）**的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Coconut 提出将 LLM 的最后一个**隐藏状态（Hidden State）&lt;strong&gt;直接作为下一个时间步的输入，而不是解码为离散的词。这种&lt;/strong&gt;&amp;ldquo;连续思维&amp;rdquo;**允许模型在潜在空间中同时探索多个推理分支（&lt;strong&gt;Superposition of thoughts&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：虽然最初针对 LLM 提出，但其理论框架是 2025 年多个视觉潜在推理工作（如 VChain）的基石。它从理论上证明了&lt;strong&gt;连续空间推理比离散空间推理具有更高的表达能力上限&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;5. 范式 IV：主动感知与强化视觉搜索&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;5-范式-iv主动感知与强化视觉搜索&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#5-%e8%8c%83%e5%bc%8f-iv%e4%b8%bb%e5%8a%a8%e6%84%9f%e7%9f%a5%e4%b8%8e%e5%bc%ba%e5%8c%96%e8%a7%86%e8%a7%89%e6%90%9c%e7%b4%a2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式将 VLM 从被动的观察者转变为主动的&lt;strong&gt;视觉智能体（Visual Agent）&lt;/strong&gt;。人类在观察复杂场景时，眼球会不断进行&lt;strong&gt;扫视（Saccade）&lt;strong&gt;和&lt;/strong&gt;注视（Fixation）&lt;/strong&gt;，通过主动改变关注点来获取信息。该范式试图在 VLM 中复现这一机制，通过&amp;quot;动作&amp;quot;来弥补&amp;quot;分辨率&amp;quot;和&amp;quot;注意力&amp;quot;的不足。&lt;/p&gt;
&lt;h3&gt;5.1 核心机制：感知即行动&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;51-核心机制感知即行动&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#51-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e6%84%9f%e7%9f%a5%e5%8d%b3%e8%a1%8c%e5%8a%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;推理过程被建模为一个&lt;strong&gt;马尔可夫决策过程（MDP）&lt;/strong&gt;。模型输出的不仅仅是答案，还有一系列感知动作指令：&lt;code&gt;&amp;lt;Zoom [x,y,w,h]&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;Crop&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;Look_Back&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;Scroll&amp;gt;&lt;/code&gt;。这些动作改变了模型的输入，从而形成了&lt;strong&gt;动态的推理链&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;5.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;52-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#52-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;5.2.1 DeepEyes (ArXiv 2025 / Likely CVPR/NeurIPS 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;521-deepeyes-arxiv-2025--likely-cvprneurips-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#521-deepeyes-arxiv-2025--likely-cvprneurips-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：DeepEyes: Incentivizing &amp;ldquo;Thinking with Images&amp;rdquo; via Reinforcement Learning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：现有的主动感知模型往往需要大量标注数据（如&amp;quot;先看这里，再看那里&amp;rdquo;），不仅昂贵且难以覆盖所有场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：DeepEyes 采用了端到端的&lt;strong&gt;强化学习（RL）&lt;/strong&gt;，具体使用了 &lt;strong&gt;GRPO 算法&lt;/strong&gt;。模型没有被教导如何看，而是仅在最终答案正确时获得奖励。在数万次训练迭代中，模型**自发涌现（Emerged）**出了类似人类的视觉策略：面对小物体会主动&amp;quot;放大&amp;rdquo;，面对大场景会&amp;quot;扫视&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：它是视觉领域的**&amp;ldquo;DeepSeek-R1 时刻&amp;rdquo;**。证明了复杂的视觉推理策略（&lt;strong&gt;System 2&lt;/strong&gt;）可以通过简单的结果奖励从零训练出来，而不需要模仿人类的中间步骤。&lt;/p&gt;
&lt;h4&gt;5.2.2 Visual CoT (NeurIPS 2024 Spotlight)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;522-visual-cot-neurips-2024-spotlight&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#522-visual-cot-neurips-2024-spotlight&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：缺乏一个标准化的基准来衡量模型&amp;quot;看图说话&amp;quot;过程中的中间对齐能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：作者构建了一个大规模数据集，其中的推理链不仅包含文本，还包含&lt;strong&gt;边界框（Bounding Boxes）&lt;/strong&gt;。模型被训练执行**&amp;ldquo;多轮聚焦&amp;quot;策略**：Step 1 预测感兴趣区域（&lt;strong&gt;RoI&lt;/strong&gt;）-&amp;gt; Step 2 裁剪该区域 -&amp;gt; Step 3 基于裁剪图回答。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：提出了**&amp;ldquo;可追踪的视觉推理&amp;rdquo;（Traceable Visual Reasoning）**。通过强制模型每一步都输出坐标，极大地减少了幻觉，并为错误分析提供了精确的依据。&lt;/p&gt;
&lt;h4&gt;5.2.3 Ferret (ICLR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;523-ferret-iclr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#523-ferret-iclr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Ferret: Refer and Ground Anything Anywhere at Any Granularity&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：传统的 &lt;strong&gt;Bounding Box&lt;/strong&gt; 过于粗糙，无法处理不规则形状（如蛇、线缆）或极细小的物体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Ferret 引入了**&amp;ldquo;混合区域编码器&amp;rdquo;（Hybrid Region Encoder）&lt;strong&gt;。它允许模型接受点、框、不规则形状（&lt;strong&gt;Sketch&lt;/strong&gt;）作为输入，并输出精细的区域掩码。这赋予了模型&lt;/strong&gt;&amp;ldquo;指哪打哪&amp;rdquo;**的精细感知能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将**&amp;ldquo;视觉定位&amp;rdquo;（Grounding）**的分辨率提升到了一个新的层级，是主动感知范式中处理细节信息的基石技术。&lt;/p&gt;
&lt;h4&gt;5.2.4 Shikra (ICCV 2023 / ArXiv 2023)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;524-shikra-iccv-2023--arxiv-2023&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#524-shikra-iccv-2023--arxiv-2023&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Shikra: Unleashing Multimodal LLM&amp;rsquo;s Referential Dialogue Magic&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：在 2023 年之前，VLM 的定位能力和对话能力往往是分离的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Shikra 是最早将空间坐标 &lt;strong&gt;[x, y]&lt;/strong&gt; 离散化为自然语言 Token 并参与自回归生成的模型之一。它证明了模型可以像学习外语一样学习**&amp;ldquo;位置语言&amp;rdquo;**，从而在对话中流畅地进行指代（&lt;strong&gt;Referential Dialogue&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：奠定了**&amp;ldquo;统一建模&amp;rdquo;**的基础。后来的 Visual CoT 和 DeepEyes 的坐标输出机制很大程度上继承了 Shikra 的设计哲学。&lt;/p&gt;
&lt;h4&gt;5.2.5 Look Twice Before You Answer (CVPR 2024 Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;525-look-twice-before-you-answer-cvpr-2024-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#525-look-twice-before-you-answer-cvpr-2024-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：VLM 的**&amp;ldquo;遗忘&amp;quot;现象**——随着文本生成越来越长，模型逐渐忘记了最初看到的图像，开始胡编乱造。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：提出了一种**&amp;ldquo;视觉回溯&amp;rdquo;（Visual Retracing）&lt;strong&gt;机制。在生成文本的过程中，模型会动态计算当前文本 Token 与原始图像特征的注意力权重。如果发现注意力发散，模型会强制&lt;/strong&gt;&amp;ldquo;回头看&amp;rdquo;**，重新加权图像特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将主动感知应用到了**&amp;ldquo;时间/记忆&amp;quot;维度**。它不是空间上的移动，而是注意力在时间轴上的回溯，是解决长文本幻觉的关键技术。&lt;/p&gt;
&lt;h2&gt;6. 范式 V：结构化与组合式视觉推理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;6-范式-v结构化与组合式视觉推理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#6-%e8%8c%83%e5%bc%8f-v%e7%bb%93%e6%9e%84%e5%8c%96%e4%b8%8e%e7%bb%84%e5%90%88%e5%bc%8f%e8%a7%86%e8%a7%89%e6%8e%a8%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这一范式认为，视觉推理的本质是&lt;strong&gt;结构化（Structure）&lt;strong&gt;和&lt;/strong&gt;组合性（Compositionality）&lt;/strong&gt;。图像不应被视为一堆像素或 Token，而应被解析为对象、属性和关系的集合。该范式致力于在推理过程中显式地构建或利用这种结构化表征（如&lt;strong&gt;场景图&lt;/strong&gt;、&lt;strong&gt;布局树&lt;/strong&gt;）。&lt;/p&gt;
&lt;h3&gt;6.1 核心机制：从像素到图谱&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;61-核心机制从像素到图谱&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#61-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e4%bb%8e%e5%83%8f%e7%b4%a0%e5%88%b0%e5%9b%be%e8%b0%b1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;推理过程包含显式的结构化步骤：&lt;code&gt;Image -&amp;gt; Scene Graph Generation -&amp;gt; Symbolic Reasoning -&amp;gt; Answer&lt;/code&gt;。这种方法将**感知（Perception）&lt;strong&gt;与&lt;/strong&gt;推理（Reasoning）**解耦，使得推理过程更加鲁棒和可控。&lt;/p&gt;
&lt;h3&gt;6.2 里程碑文献深度解析&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;62-里程碑文献深度解析&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#62-%e9%87%8c%e7%a8%8b%e7%a2%91%e6%96%87%e7%8c%ae%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;6.2.1 Compositional Chain-of-Thought (CCoT) (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;621-compositional-chain-of-thought-ccot-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#621-compositional-chain-of-thought-ccot-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Compositional Chain-of-Thought Prompting for Large Multimodal Models&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：VLM 经常犯**&amp;ldquo;属性绑定错误&amp;rdquo;（Attribute Binding Error）**，例如把穿红衣服的人看成穿蓝衣服，或者混淆两个物体的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：CCoT 强制模型分两步走：首先生成一个&lt;strong&gt;场景图（Scene Graph）&lt;/strong&gt;，明确列出所有对象节点（&lt;strong&gt;Nodes&lt;/strong&gt;）及其属性和关系边（&lt;strong&gt;Edges&lt;/strong&gt;）；然后基于这个场景图生成答案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：将结构化数据作为 CoT 的中间模态。实验表明，显式的结构化描述迫使模型理清对象关系，大幅减少了组合性错误。&lt;/p&gt;
&lt;h4&gt;6.2.2 PixelLM (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;622-pixellm-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#622-pixellm-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：PixelLM: Pixel Reasoning with Large Multimodal Model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：许多推理任务需要&lt;strong&gt;像素级的理解&lt;/strong&gt;（例如&amp;quot;这两个重叠的物体谁在上面？&amp;quot;），传统的 Box 无法表达这种遮挡关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：PixelLM 在 LLM 输出端挂载了一个轻量级的&lt;strong&gt;像素解码器（Pixel Decoder）&lt;/strong&gt;。LLM 能够输出特定的**&amp;ldquo;分割 Token&amp;rdquo;**，这些 Token 解码后形成精细的物体掩码（&lt;strong&gt;Masks&lt;/strong&gt;）。推理过程基于这些掩码的拓扑关系进行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了**&amp;ldquo;像素级思维&amp;rdquo;**。它证明了 VLM 的推理粒度可以下沉到像素级别，为处理复杂的物理接触和遮挡关系提供了可能。&lt;/p&gt;
&lt;h4&gt;6.2.3 Osprey (CVPR 2024)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;623-osprey-cvpr-2024&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#623-osprey-cvpr-2024&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Osprey: Pixel Understanding with Visual Instruction Tuning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：如何让用户对图像中任意不规则区域进行提问？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Osprey 提出了一种**&amp;ldquo;掩码感知视觉提取器&amp;rdquo;（Mask-Aware Visual Extractor）**。它不仅接受图像，还接受一个掩码作为输入，能够提取该掩码覆盖区域的精细视觉特征。这使得推理可以针对图像的任何局部细节进行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：实现了细粒度的**&amp;ldquo;交互式推理&amp;rdquo;**。它不仅是模型看图，更是用户通过 &lt;strong&gt;Point/Mask&lt;/strong&gt; 与模型进行精准的视觉对话。&lt;/p&gt;
&lt;h4&gt;6.2.4 Sphinx (NeurIPS 2024 Context)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;624-sphinx-neurips-2024-context&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#624-sphinx-neurips-2024-context&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Sphinx / ReasonBench Context&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：&lt;strong&gt;抽象视觉推理&lt;/strong&gt;（如瑞文智商测试、图表逻辑）是 VLM 的短板。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Sphinx 通过混合**高分辨率主动缩放（Active Scaling）**和多样化的视觉任务训练，增强了模型对抽象几何结构的感知能力。配合 &lt;strong&gt;ReasonBench 基准&lt;/strong&gt;，它展示了结构化数据训练对提升逻辑推理的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：探索了 VLM 在纯抽象视觉逻辑上的边界，证明了通过丰富的数据结构（如合成图表、几何题）可以提升模型的&lt;strong&gt;通用推理智商&lt;/strong&gt;。&lt;/p&gt;
&lt;h4&gt;6.2.5 Argus (CVPR 2025)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;625-argus-cvpr-2025&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#625-argus-cvpr-2025&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;论文标题&lt;/strong&gt;：Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：**视觉定位（Grounding）&lt;strong&gt;与&lt;/strong&gt;文本推理（Reasoning）**往往是割裂的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：Argus 提出了一种&lt;strong&gt;以视觉为中心的推理框架&lt;/strong&gt;，强制模型在生成每一个推理步骤时，都要同步输出对应的视觉证据区域。这不仅仅是 Visual CoT 的应用，更是一种由于架构设计带来的&lt;strong&gt;强对齐约束&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;创新点&lt;/strong&gt;：强调了**&amp;ldquo;对齐即推理&amp;rdquo;**。只有当模型能够准确指出&amp;quot;我为什么这么说&amp;quot;的视觉依据时，我们才能认为它真正具备了视觉推理能力。&lt;/p&gt;
&lt;h2&gt;7. 综合对比与未来展望&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;7-综合对比与未来展望&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#7-%e7%bb%bc%e5%90%88%e5%af%b9%e6%af%94%e4%b8%8e%e6%9c%aa%e6%9d%a5%e5%b1%95%e6%9c%9b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;7.1 五大范式横向对比表&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;71-五大范式横向对比表&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#71-%e4%ba%94%e5%a4%a7%e8%8c%83%e5%bc%8f%e6%a8%aa%e5%90%91%e5%af%b9%e6%af%94%e8%a1%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;范式维度&lt;/th&gt;
          &lt;th&gt;范式 I：工具中介&lt;/th&gt;
          &lt;th&gt;范式 II：显式生成&lt;/th&gt;
          &lt;th&gt;范式 III：潜在推理&lt;/th&gt;
          &lt;th&gt;范式 IV：主动感知&lt;/th&gt;
          &lt;th&gt;范式 V：结构化组合&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心隐喻&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;外包给计算器&lt;/td&gt;
          &lt;td&gt;脑海中的草稿纸&lt;/td&gt;
          &lt;td&gt;直觉与内隐思考&lt;/td&gt;
          &lt;td&gt;眼动与探索&lt;/td&gt;
          &lt;td&gt;逻辑大纲与图谱&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;中间模态&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;代码 (Python)、API&lt;/td&gt;
          &lt;td&gt;像素图像 (Images)&lt;/td&gt;
          &lt;td&gt;连续向量 (Vectors)&lt;/td&gt;
          &lt;td&gt;动作指令 (Actions)&lt;/td&gt;
          &lt;td&gt;场景图、掩码 (Graphs)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;优势&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;逻辑严密，可验证，计算精准&lt;/td&gt;
          &lt;td&gt;处理空间变换、反事实推理极强&lt;/td&gt;
          &lt;td&gt;信息密度最高，推理效率高&lt;/td&gt;
          &lt;td&gt;模拟人类行为，适应大图/视频&lt;/td&gt;
          &lt;td&gt;解决复杂关系，鲁棒性高&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;劣势&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;依赖工具库，非端到端，慢&lt;/td&gt;
          &lt;td&gt;计算开销极大，生成误差累积&lt;/td&gt;
          &lt;td&gt;黑盒不可解释，调试困难&lt;/td&gt;
          &lt;td&gt;训练难度大 (RL)，收敛慢&lt;/td&gt;
          &lt;td&gt;依赖解析器，灵活性受限&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;代表作&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Visual Sketchpad, ViperGPT&lt;/td&gt;
          &lt;td&gt;MVoT, ImagineNav&lt;/td&gt;
          &lt;td&gt;CoVT, DeepSketcher&lt;/td&gt;
          &lt;td&gt;DeepEyes, Ferret&lt;/td&gt;
          &lt;td&gt;CCoT, PixelLM&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;数学几何、精确计数、测量&lt;/td&gt;
          &lt;td&gt;导航规划、物理预测、拼图&lt;/td&gt;
          &lt;td&gt;通用视觉问答、细粒度感知&lt;/td&gt;
          &lt;td&gt;极高分辨率图像、监控视频&lt;/td&gt;
          &lt;td&gt;复杂场景理解、关系推理&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;7.2 技术融合的趋势：迈向多模态 AGI&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;72-技术融合的趋势迈向多模态-agi&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#72-%e6%8a%80%e6%9c%af%e8%9e%8d%e5%90%88%e7%9a%84%e8%b6%8b%e5%8a%bf%e8%bf%88%e5%90%91%e5%a4%9a%e6%a8%a1%e6%80%81-agi&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过对 &lt;strong&gt;2025 年最新文献&lt;/strong&gt;的分析，我们可以清晰地看到不同范式正在发生融合：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RL 的全面渗透&lt;/strong&gt;：&lt;strong&gt;DeepEyes&lt;/strong&gt; 和 &lt;strong&gt;SpatialDreamer&lt;/strong&gt; 的成功表明，&lt;strong&gt;强化学习&lt;/strong&gt;正在成为训练 &lt;strong&gt;System 2 视觉推理&lt;/strong&gt;的标准范式。未来的 VLM 将不再仅仅依靠监督微调（&lt;strong&gt;SFT&lt;/strong&gt;），而是通过 RL 自我探索出最优的&amp;quot;看图策略&amp;rdquo;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;潜在与显式的结合&lt;/strong&gt;：&lt;strong&gt;DeepSketcher&lt;/strong&gt; 展示了可以在潜在空间中进行&amp;quot;显式&amp;quot;的操作（如编辑 Embedding）。未来的模型可能会在潜在空间中进行高效推理，仅在需要验证时才&amp;quot;解码&amp;quot;为像素图像。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;从感知到行动&lt;/strong&gt;：随着**具身智能（Embodied AI）**的兴起，Paradigm IV（主动感知）将变得越来越重要。视觉推理将不再局限于静态图像，而是延伸到对环境的主动探索和交互中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;8. 结语&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;8-结语&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#8-%e7%bb%93%e8%af%ad&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;从**&amp;ldquo;Thinking about Images&amp;rdquo;&lt;strong&gt;到&lt;/strong&gt;&amp;ldquo;Thinking with Images&amp;rdquo;**的转变，标志着多模态大模型正在跨越感知的门槛，迈向认知的殿堂。无论是通过代码外化思维、通过生成模拟未来、通过向量内隐推演，还是通过动作主动探索，这些新兴范式都在试图弥合语言与视觉之间的鸿沟。&lt;/p&gt;
&lt;p&gt;本次调研表明，&lt;strong&gt;单纯的文本 CoT 已不足以支撑下一代 VLM 的发展&lt;/strong&gt;。未来的多模态模型必将是&lt;strong&gt;混合架构&lt;/strong&gt;的——它拥有类似 &lt;strong&gt;System 1&lt;/strong&gt; 的快速感知编码器，同时也拥有由 &lt;strong&gt;RL 训练而成的 System 2 推理引擎&lt;/strong&gt;，能够灵活地调用工具、生成意象、在潜在空间深思熟虑，并像人类一样主动去&amp;quot;看&amp;quot;清这个世界。&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
